A Maximum-Entropy Patch Sampler for Few-Shot Image Classiï¬cation

Spot and Learn:

Wen-Hsuan Chu1âˆ—

, Yu-Jhe Li2,3, Jing-Cheng Chang2,3, Yu-Chiang Frank Wang2,3

1 Carnegie Mellon University, Pittsburgh, PA, USA

2 National Taiwan University, Taipei, Taiwan

3 MOST Joint Research Center for AI Technology and All Vista Healthcare

1chuwenhsuan@cmu.edu, 2{yujheli, b04901138, ycwang}@ntu.edu.tw

Abstract

Few-shot learning (FSL) requires one to learn from ob-
ject categories with a small amount of training data (as
novel classes), while the remaining categories (as base
classes) contain a sufï¬cient amount of data for training. It is
often desirable to transfer knowledge from the base classes
and derive dominant features efï¬ciently for the novel sam-
ples. In this work, we propose a sampling method that de-
correlates an image based on maximum entropy reinforce-
ment learning, and extracts varying sequences of patches
on every forward-pass with discriminative information ob-
served. This can be viewed as a form of â€œlearnedâ€ data
augmentation in the sense that we search for different se-
quences of patches within an image and performs classiï¬-
cation with aggregation of the extracted features, resulting
in improved FSL performances. In addition, our positive
and negative sampling policies along with a newly deï¬ned
reward function would favorably improve the effectiveness
of our model. Our experiments on two benchmark datasets
conï¬rm the effectiveness of our framework and its superior-
ity over recent FSL approaches.

1. Introduction

Deep neural networks have achieved extraordinary per-
formance in supervised visual learning tasks [13, 23, 10]
in recent years. However, these supervised learning meth-
ods often require a large amount of training data and an-
notations to achieve such performance. This signiï¬cantly
limits the problems they can be applied to, as it may be
expensive to acquire enough annotations for the training
data, or worse, the training data may be difï¬cult to ac-
quire themselves. In comparison, humans are surprisingly
good at learning new concepts using very little supervised

âˆ—Work done while at National Taiwan University.

Figure 1: Illustration of our proposed patch sampling strat-
egy for FSL. If varying glimpse trajectories can be obtained
on each forward-pass, one can create a variety of input patch
sequences for training from the same input image.

information. For example, humans (both adult and chil-
dren) can learn to recognize a new animal just from a
couple of pictures in books or from online sources. On
the other hand, neural networks would suffer from the is-
sue of severe data over-ï¬tting, resulting in poor general-
ization results during inference. This has motivated re-
searchers to propose different methods for few-shot learn-
ing [12, 27, 20, 1, 21, 24, 5, 9, 25, 6, 29, 7, 19, 28]. Few-shot
learning aims to classify novel visual classes from very few
labeled samples. Contemporary methods usually tackle this
challenge using meta-learning approaches [5, 20] or metric-
learning approaches [27, 24]. Another branch of algorithms
focus on data hallucination to generate more training sam-
ples [9, 29, 32]. There has also been some works on using
soft attention for Few-Shot Learning using attention gener-
ated from semantic information [28, 3]. Current â€œattentionâ€
mechanisms are largely inspired by the human visual sys-
tems, where we only focus on small regions located at the
center of our view or gaze, while the areas further away
from the center are in fact very â€œblurryâ€. As a result, we
build our understanding of a scene by aggregating infor-

6251

mation from different regions of the scene as needed over
time. It is believed that this behaviour allows us to ignore
the â€œclutterâ€ that is outside of the salient regions of inter-
est [17], hence the name of â€œattentionâ€.

Inspired by the aforementioned concept, we present an
alternative view to why humans exhibit this behaviour: in
addition to being able to ignore â€œclutterâ€ during inference,
decomposing or de-correlating an image or a scene into se-
quences of patches can allow us to increase the input variety
for any given image during training. For example, when we
stare at the same image or scene twice, our gazes would
likely follow any possible trajectory that allows us to un-
derstand the image or the scene. This motivates us to make
one crucial observation: if we use the whole image or scene
as the input, we only get one possible input variety for train-
ing; on the contrary, if we could model a more human-like
behavior and sample randomly from any possible regions
of interest for training, we would increase the input variety,
which may lead to better generalization results (see Fig. 1).
In this paper, we propose an end-to-end trainable frame-
work to model this novel interpretation of the human vi-
sual system. Our model aims to produce possible patch
sequences from an input that would lead to correct classi-
ï¬cation by applying the maximum entropy reinforcement
learning objective [26]. Moreover, we utilize a negative
trajectory sampler for non-interesting regions. Combined
with the proposed positive trajectory sampler enforcing cor-
rect classiï¬cations, this positive/negative sampling strategy
would further help regularize the network. Finally, the ex-
periments on two open datasets demonstrate the effective-
ness of our proposed model. We note that, to the best of
our knowledge, we are among the ï¬rst to advance reinforce-
ment learning for few-shot learning as a form of â€œlearnedâ€
data augmentation, which is orthogonal to many of the other
contemporary few-shot learning methods.

The contributions of this paper are highlighted below:

â€¢ We propose a novel deep reinforcement learning based

approach for few-shot learning.

â€¢ During training, our model samples varying candidate
patch sequences from an input image, which would
satisfy FSL and result in improved performances.

â€¢ Our proposed sampling mechanism jointly utilizes
both positive and negative sampling policies, which
are able to determine â€œpatches of interestsâ€ and â€œback-
groundâ€.

â€¢ Experimental results on two open datasets conï¬rm that
our method performs favorably against other existing
few-shot learning approaches.

to learn to classify samples from â€œnovelâ€ classes using only
a small amount of labeled samples from the novel classes.
One category of algorithms tackle this using meta-learning
approaches by learning to learn, such as learning to initial-
ize [5] or optimize [20] for few-shot learning settings. An-
other category of algorithms explore metric-learning based
approaches, which can be viewed as learning to compare.
For examples, siamese networks [12], cosine similiarity
[27], Euclidean distance to the mean [24], CNN-based rela-
tion modules [25], or Graph Neural Networks [6] have all
been explored in literature. Approaches that learn to â€œhal-
lucinateâ€ or generate new data samples for novel classes
[9, 29, 32] have also been explored recently. We note that
this is different from our work as weâ€™re not explicitly gener-
ating any new data. Finally, there has also been works that
predict weights [1] or the novel class classiï¬er [7] directly,
or uses novel class features as weights [19].

Attention Models Visual attention has been studied ex-
tensively and can be broadly categorized into two cate-
gories. The ï¬rst category is referred to as hard attention,
where cropped patches of the original image is returned
[17, 30]. The second category is referred to as soft atten-
tion, where an â€œattention mapâ€ corresponding to the entire
image is returned [30]. Soft attention models have the ad-
vantage of being fully differentiable, which makes it eas-
ier to train, while hard attention models have some form of
stochasticity in them, and has to be trained using reinforce-
ment learning methods (like policy gradients) due to the
non-differentiability in cropping. Spatial Transformer Net-
works [11], while designed for general image transforma-
tions, can also be used for â€œcroppingâ€ images and therefore,
a form of attention. The work with the closest motivation
to ours is [31], in which they employ a soft attention model
that aims to extract all the important regions of an image by
minimizing the correlation between multiple attention maps
and restricts the overlap across attention maps to be lower
than some threshold. While their method also wishes to
ï¬nd all areas of interest in an image, they imposed strong
constraints and there is no stochasticity between different
forward passes. In contrast, we explicitly maximize the va-
riety of patches extracted from a given input image in dif-
ferent forward passes, allowing us to deal with the scarcity
of data in FSL settings. While soft attention schemes have
also been applied in few-shot learning very recently [28, 3],
they typically require semantic information and the lack of
stochatiscity would be a concern.

3. Preliminaries

2. Related Works

Few-Shot Learning Given abundant
labeled training
samples from some â€œbaseâ€ classes, few-shot learning aims

To make our paper more self-contained, we brieï¬‚y re-
view reinforcement learning (RL) algorithms related to our
work. This section will serve as a theoretical basis to our
framework in the following section.

6252

counted rewards PN

Reinforcement Learning. To favorably sample a se-
quence of patches for an image through RL, we would like
to ï¬nd a sequence of actions, (a1, a2, ..., aN ) given an im-
age x. These actions indicate the location of the patch that
will be extracted from the image, speciï¬cally, they corre-
spond to a normalized 2D coordinate in the image x. These
taken actions aim to maximize the total amount of RL dis-
t=1 Î³tâˆ’1rt, where the discount factor
Î³ â‰¤ 1 is a constant. In the classiï¬cation tasks, a common
choice for the reward function is often set as rN = 1 if the
classiï¬cation is correct after the Nth time-step and 0 oth-
erwise [17].
In addition, these actions are often sampled
from a learned policy Ï€, i.e. ai âˆ¼ Ï€Î¸(a|s), where the policy
is parameterized by Î¸, which could be modeled as neural
networks. Our objective in reinforcement learning can be
written as:

arg max

Ï€

N

Xt=1

E[Î³tâˆ’1rt].

(1)

Standard ways to solve for the policy Ï€ include Policy Gra-
dient [22] and Q-Learning [18] methods. Policy Gradient
based methods aim at learning desirable policies directly,
and have been explored in attention models [17], where a
family of distributions is considered for the policy Ï€ (e.g. a
Gaussian policy). On the other hand, Q-Learning methods
learn a Q-function corresponding to the â€œvalueâ€ of an action
(Q-value) given some state s, and the best scoring action
is then chosen at every time step. However, Policy Gradi-
ent methods require one to pre-deï¬ne a form for the policy
(e.g., Gaussian) which implies that the optimal behavior is
unimodal. As for Q-Learning, one only takes the action
with the maximum Q-value, resulting a single â€œgoodâ€ be-
havior [9] without any stochasticity. This would cause the
sampling policy to collapse into a single mode, i.e., loca-
tions near the peak of the Gaussian or the highest Q-value,
neither of which would be satisfactory in FSL settings.

Maximum Entropy Reinforcement Learning. To ad-
dress the aforementioned issue, a maximum entropy rein-
forcement learning objective can be employed, which ad-
ditionally maximizes the entropy of the action distribution
H(Ï€(Â·|s)) given the state s we are in. Intuitively speaking,
we would like to maximize the variety of our actions while
also obtaining a high reward (i.e. making the correct classi-
ï¬cation). The objective of maximum entropy reinforcement
learning can be written as:

arg max
Ï€M axEnt

N

Xt=1

E[Î³tâˆ’1rt + Î±H(Ï€(Â·|s))],

(2)

where Î± is a constant that balances the importance of the
entropy term relative to the rewards. Soft Q-Learning [8]
has recently been proposed to solve this objective function

by using an ï¬xed-point iteration method:

Qsof t(st, at) â† rt + E[Vsof t(st+1)]

Vsof t(st) â† Î± logZA

exp(

1
Î±

Qsof t(st, a

â€²

))da

â€²

,

(3)

(4)

where Qsof t(s, a) is the Q-function and Vsof t(s) represents
the value function, a measure of how high a stateâ€™s value is.
The maximum entropy policy then can be calculated as a
softmax over the advantage function, which is a measure of
how good an action is relative to the other actions:

Ï€M axEnt(at|st) âˆ exp(

1
Î±

Qsof t(st, at) âˆ’ Vsof t(st)). (5)

Thus, we are able to derive a multimodal policy without
pre-deï¬ning its form from particular distributions that maxi-
mize the total reward (i.e., correct classiï¬cations), while ex-
hibitng varying behaviours during sampling. Please see [8]
for thorough derivations of how the algorithm could be ap-
proximated using deep neural networks.

4. Proposed Framework

Given a set of K input images X = {xj}K

j=1 and its cor-
j=1, where xj âˆˆ RHÃ—W Ã—3
responding label set Y = {yj}K
and yj âˆˆ R are the jth image and its label respectively, our
goal is to correctly predict the labels given these input im-
ages, especially in few-shot scenarios. To achieve this, we
propose a Maximum Entropy RL-based framework which
learns to sample sequences of patches from the input image
xj , denoted as P j = {pj
i=1. In this section, we ï¬rst de-
scribe our model architecture and provide details for each
component. We then further explain the sampling mech-
anism governed by the designed reward function, which
helps enhance and regulate our model. Finally, we pro-
vide the details about the training objective and the infer-
ence process of our model.

i }N

4.1. Architecture Overview

As depicted in Fig. 2, our model consists of ï¬ve com-
ponents: feature extractor, action context encoder, state en-
coder, maximum entropy sampler, and a ï¬nal classiï¬er. We
now describe the details of each component.

1 of the
Feature Extractor fe. To encode the patches pi
input image x, we introduce the feature extractor fe to ex-
tract the feature embedding ei of the patch pi at every time
step using a CNN. The feature extractor only has access to
the local patches given by the maximum entropy sampler
(described in later subsections) in the form of cropped win-
dows of the original image x.

1For simplicity, we omit the subscript j, and represent their correspond-

ing input image as x and label as y

6253

	ğ‘$

ğ‘$%&

Feature
Extractor

ğ‘“)

ğ‘“)

State

Maximum Entropy

Encoder
ğ‘ $E&

ğ‘$E&

ğ‘’$

ğ‘“(

ğ‘ $

ğ‘$

ğ‘’$%&

ğ‘¥

ğ‘ $

Sampler

ğ‘“3

ğœ‹5

ğ‘”

ğ‘“:

â„’ğ‘¸

â„’ğ…

ğ‘“(

Action Context 

Encoder

ğ‘ 2

Classifier

ğ‘“7

Input Image ğ‘¥

ğ‘$%&

Q-value

ğ‘$

ğ‘™

ğ‘¦D

ğ‘¦

softmax
â„’ğ’„ğ’ğ’‚ğ’”ğ’”

Figure 2: Our framework mainly comprises of ï¬ve components: feature extractor, action context encoder, state encoder,
maximum entropy sampler, and a ï¬nal classiï¬er. The feature extractor fe takes an input patch pi to derive the feature ei,
which is used by the state encoder fs to produce current state si. Next, the maximum entropy sampler (with the Q-function
fQ and the actual policy Ï€Î¸) takes the input image x with si to sample an action ai, that produces the next patch pi+1. The
action context encoder fa then encodes the context ci of the current action using ai and the features of the image g (extracted
by Ï€Î¸). Finally, the state encoder fs takes the newly extracted feature ei+1 (extracted from pi+1 by fe) and action context
ci to produce the next state si+1 (not depicted in ï¬gure). The ï¬nal state sN is fed into the classiï¬er to determine its output
vector l and the predicted label Ë†y. We note that N is selected as a hyperparameter.

State Encoder fs. To aggregate the features of prior ex-
tracted patches for the input image x, a recurrent neural net-
work (RNN) is used as the state encoder fs to encode the
state si from the previous state siâˆ’1 and the sampled patch
pi. The state encoder fs also takes the current action context
ciâˆ’1 (produced by the module fa in the Fig. 2 and will be
detailed later when introducing fa) and derives the current
state si.

More speciï¬cally, we implement this RNN with a GRU
[2]. On each forward-pass, we initialize the GRU with the
features extracted from a random patch and the initial action
context c0 set to a zero vector.

Maximum Entropy Sampler fQ & Ï€Î¸. Since our goal
is to generate varying patch trajectories from the input im-
age x, we employ a maximum entropy sampler to sam-
ple the next candidate patch pi+1. The Maximum Entropy
Sampler is built based on the Soft Q-Learning algorithm in
[8]. Generally, the sampler takes the whole image x and
utilizes the current state si (aggregated information from
all prior patches) to produce a 2D action vector ai, which
corresponds to the coordinates of the center of the next
patch pi+1. More speciï¬cally, the sampler itself contains
the Q-function fQ and the actual sam-
two components:
pling policy Ï€Î¸, which are related by (5). The architectures
of both components are identical, containing a small CNN
with fully connected layers. The Q-function fQ evaluates
how â€œgoodâ€ actions are, while the sampling policy Ï€Î¸ out-

puts the actual 2D action ai and the features of the image
x, denoted by g. We note that only the sampling policy Ï€Î¸
is used during inference. More details about the sampling
mechanism can be found in Sect. 4.2.

Action Context Encoder fa. The action context encoder
fa in Fig. 2 takes the output features g of the image x from
the convolution layers in the aforementioned policy Ï€Î¸ and
the sampled 2D action ai to produce a context ci. Intuitively
speaking, the action context encoder aims to account for the
global information produced by Ï€Î¸, which is utilized by fs.

Classiï¬er fc. Since our goal is to correctly classify the
input image x as its corresponding label y, we introduce the
classiï¬er fc which takes the ï¬nal state sN and produces the
output vector l and the label prediction Ë†y.

4.2. Sampling Mechanism

In this section, we present the sampling mechanism for
our maximum entropy sampler (fQ and Ï€Î¸), which is based
on the Maximum Entropy RL algorithm in Sect. 3. While
the maximum entropy sampler alone would sample patches
that is beneï¬cial for classiï¬cation, we choose to further reg-
ulate both the maximum entropy sampler and the feature
extractor for performance guarantees. To achieve this, we
introduce the concept of negative sampling into our model.
For example, a negative sequence sample may be a se-
quence of patches that land outside of the object or regions

6254

Figure 3: Illustration of the relationship between the Q-
function Qsof t (blue), the positive policy Ï€+ (green) and
the negative policy Ï€âˆ’ (red) satisfying (5) and (6), respec-
tively. The Q-function is expected to output higher values
for patches within the regions of interest, and lower values
for irrelevant regions like the background. If the traditional
RL objective is applied, one would only sample from one of
the peaks. Note that the two overlapping policies are shown
in different scales.

of interest, like the background. To be more precise, we
deï¬ne the negative sampling policy Ï€âˆ’ as:

Ï€âˆ’(at|st) = exp(âˆ’

1
Î±

Qsof t(st, at) + Vsof t(st))

(6)

This allows us to derive a policy that samples from unde-
sirable points given a Q-function (see Fig. 3). Intuitively,
we train the positive policy Ï€+ to match the Q-function
Qsof t(st, at), which has a higher value in the regions of
interest, and train the negative policy to match the nega-
tive of the Q-function âˆ’Qsof t(st, at), which now outputs
a higher value in non-relevant regions. Note that the two
policies are both conditioned on the same Q-function, and
can be viewed as the â€œinverseâ€ of the other policy.

To jointly apply both policies, we create an artiï¬cial
â€œbackgroundâ€ class and randomly select one policy on each
forward-pass, i.e., the positive policy Ï€+ or the negative
policy Ï€âˆ’. We encourage the classiï¬er to regress to the
ground truth y if the positive policy is chosen and the back-
ground class if the negative policy is selected. This moti-
vates us to assign a reward value of âˆ’1 if the predicted la-
bel corresponds to the background class. Thus, the reward
function is deï¬ned as:

1,
âˆ’1,
0,

if i = N and Ë†y = y
if i = N and Ë†y = background

(7)

otherwise

Ri = ï£±ï£´ï£²
ï£´ï£³

The reward function Ri can be interpreted as a ranking
of the three possible outcomes: for the positive policy Ï€+
(corresponding to the normal Q-function fQ), an action se-
quence that results in the correct label will be preferred,

followed by an action sequence that results in a wrong la-
bel, and an action sequence that causes the predicted label
to be â€œbackgroundâ€ class would generally be avoided. For
the negative policy Ï€âˆ’ (corresponding to the inverted Q-
function âˆ’fQ), this ranking is inverted, and the best action
sequences for Ï€âˆ’ would result in a predicted label of the
â€œbackgroundâ€ class.

It is worth noting that, the beneï¬ts of this joint sampling
policy is two-fold. First, it allows the encoders to â€œseeâ€ and
learn to encode poorly chosen patches. Second, this helps
pull the values of different actions apart, which avoids ex-
treme scenarios like when all action scores are about the
same, which might occur when the model overï¬ts and clas-
siï¬es everything correctly regardless of the quality of the
sampled patches.

4.3. Training Objective and Evaluation

Training In the training stage, we ï¬rst randomly select
the policy â€œmodeâ€ of our network: the positive policy Ï€+,
regressing towards the ground truth label y, or the negative
policy Ï€âˆ’, regressing towards the artiï¬cial â€œbackgroundâ€
class yb. We then obtain the predicted label Ë†y and use the re-
ward function in (7) to update the Maximum Entropy Sam-
pler. This is done by updating the Q-function fQ based on
the equations (3) and (4), and the two policies (Ï€+ and Ï€âˆ’)
such that they follow the formulas in (5) and (6) by applying
Soft Q-Learning [8] with a Replay Buffer [15]. To better
optimize the framework, we incorporate the classiï¬cation
loss, i.e. negative log-likelihood, into our training objective
along with RL-based training following [17]. The classiï¬-
cation loss Lclass is calculated to jointly update feature ex-
tractor fe, the action context encoder fa, the state encoder
fs, and the classiï¬er fc:

Lclass =

1

|M1| XjâŠ‚M1

yT
j log Ë†yj + Î±

1

|M2| XjâŠ‚M2

yT
j log Ë†yj

(8)
where yT
j and Ë†yj denotes the (transposed) ground truth la-
bel (augmented with the â€œbackgroundâ€ class) and the pre-
dicted label at the ï¬nal time step N for the j-th image in the
sampled batch, respectively. M1 and M2 denotes the set of
indexes in the batch that were selected by the two â€œmodesâ€,
i.e. the positive policy Ï€+ or the negative policy Ï€âˆ’. In the
same equation, Î± is a scaling constant to balance the im-
portance of the two loss terms. We summarize our training
algorithm in the pseudo-code in Algorithm 1.

Inference During inference, we only select the positive
policy Ï€+ to extract patches for a given test image because
we aim to produce the correct label for the input image in-
stead of the background label.

6255

Algorithm 1 Training algorithm

Input: Data, label tuples {(xj, yj)}; Replay Buffer D; pa-

rameter Î²

Output: Network modules fe, fs, fa, fc, fQ, Ï€+, Ï€âˆ’

for number of training iterations do

Sample k from uniform distribution between [0, 1]
if k < Î² then

Sample action sequences (a1, a2, ...., aN ) us-
ing the positive policy Ï€+, extract the patches
(p1, p2, ...., pN ) and compute the predicted
label Ë†yj

else

Sample action sequences (a1, a2, ...., aN ) us-
ing the negative policy Ï€âˆ’, extract the patches
(p1, p2, ...., pN ) and compute the predicted
label Ë†yj

end if
Store all transitions (xj , si, si+1, ai, ri, i) in replay
buffer D
Calculate the classiï¬cation loss Lclass with yj and
Ë†yj according to (8)
Update fe, fs, fa, fc using Lclass
Sample transition batch (xj , si, si+1, ai, ri, i) from
replay buffer D
Update fs, fQ, Ï€+, Ï€âˆ’ using Soft Q-Learning [8]

end for

5. Experiments

We ï¬rst highlight the experimental datasets and provide
our experimental settings in the ï¬rst two subsections. Af-
terwards, the evaluation results are presented in the third
subsection followed by the ablation studies with respects to
different voting strategies in the fourth subsection. Finally,
we present the sampling trajectory analysis on our trained
sampling policy. More experiments are provided in the sup-
plementary materials.

5.1. Datasets

of 90, 180, and 270 degrees randomly, which results in a
total of 1200 + 3600 base classes and 423 + 1269 novel
classes.

miniImagenet miniImagenet was ï¬rst proposed by [27],
with 80 base classes and 20 novel classes sampled from the
original Imagenet dataset [4], but recent work uses the splits
proposed by [20], where there are 64 base classes, 16 vali-
dation classes, and 20 novel classes. We follow this split so
our results can be compared to other work. Each class con-
tains 600 images. The images are resized to 84 Ã— 84, and
we perform standard data augmentation techniques: color
jittering, random left-right ï¬‚ips, and random crops. Only
the 64 base classes were used for training and the 16 valida-
tion classes were used for modeling generalization perfor-
mance and for choosing hyperparameters (e.g. number of
ï¬netuning iterations).

5.2. Experimental Setting

Implementation for few-shot setting. To ensure a fair
comparison with other proposed methods, we employ a
Conv-4 backbone structure for our feature extractor fe,
which is identical to the one used by [24]. For baselines, we
experiment with two different kinds of classiï¬ers following
the same Conv-4 backbone. The ï¬rst one, which we de-
note as Baseline-FC, uses a standard fully connected layer
followed by a softmax activation to output the label predic-
tion. The second one, which we denote as Baseline-CS,
applies a cosine similarity measure instead of a dot product
in standard fully connected layers. We would like to clarify
that using cosine similarity as an alternative classiï¬er for
few-shot learning is not our contribution. Cosine similarity
layers has been recently explored in [16] and has been ap-
plied to few-shot learning in [7, 19]. We set the number of
extracted patches to be 4 (i.e. N = 4), with an additional
patch that is randomly sampled independent of the sampling
policies Ï€+ and Ï€âˆ’ to initialize the state encoder GRU (fs)
for all experiments.

We tested our model on two widely adopted datasets for
few-shot learning: the Omniglot [14] and the miniImagenet
[27]. We describe the two datasets below.

Omniglot Omniglot [14] contains 1623 different charac-
ters from 50 different alphabets. Each character contains
images of hand-drawn characters by 20 different people.
We follow the same evaluation strategy in [27, 24, 5, 21],
where 1200 random character classes are sampled (inde-
pendent of alphabet) as â€œbaseâ€ classes, and the remaining
423 character classes are considered the â€œnovelâ€ classes, i.e.
only a small amount of labeled samples per class is avail-
able. The images are all resized to 28 Ã— 28, and we perform
the common practice of class augmentation using rotations

Inference. During inference, we apply a Best-of-N Voting
method to obtain the ï¬nal prediction label Ë†ynway. We note
that due to the stochastic properties of the policy Ï€+, there
is a chance (albeit small) that it will select an â€œirrelevantâ€
region at any time-step. A simple workaround for this is to
repeat the classiï¬cation a number of times, i.e. N times, and
aggregate the prediction results before outputting the class
with the highest prediction probability/score. We ï¬rst ex-
plain two ways of performing this aggregation: Hard Vot-
ing and Soft Voting as follows, while the study of the voting
behavior can be found in our ablation studies in Section 5.4.
For hard voting, we take the argmax of the N predicted
labels before aggregating them and see which label has the
most votes. This can be seen as a form of â€œdiscreteâ€ vot-

6256

Table 1: Results on Omniglot. FC denotes a fully-connected
classiï¬er and CS denotes a cosine similiarity classiï¬er. The
number in bold indicates the best result.

5-Way

1-Shot

5-Shot

Baseline-FC 91.95Â±0.48
93.30Â±0.44
Baseline-CS
97.43Â±0.28
97.56Â±0.31

Ours-FC
Ours-CS

98.97Â±0.10
99.33Â±0.09
99.51Â±0.07
99.65Â±0.06

ing and it discards the â€œuncertaintyâ€ information in the pre-
dicted labels. For example, if we had a binary label, the
prediction (0.6, 0.4) and (0.99, 0.01) would both reduce to
(1, 0). For soft voting, we aggregate the N predicted labels
without performing the argmax and see which label has the
highest accumulated probability.

Evaluation protocol. We evaluate using a K-Shot 5-Way
evaluation protocol with Best-of-7 Soft Voting, where K is
the number of labeled samples we have per novel class. For
each testing episode, we randomly select 5 classes from all
the novel classes, and out of these 5 classes, we sample 5K
labeled examples and 15 testing samples. The 5K labeled
sample are ï¬rst used for ï¬netuning our model, then we
perform inference by calculating the predicted label Ë†ynway
based on the distance of the output vector l of the query im-
age to the output vectors of the 5K labeled samples ls. This
is akin to a nearest neighbor method.

Ë†ynway =

exp(âˆ’dist(l âˆ’ ls))

Psâˆˆ5K exp(âˆ’dist(l âˆ’ ls)))

(9)

We perform 600 test episodes and report the mean and 95%
conï¬dence interval for all evaluation settings.

5.3. Evaluation Results and Comparisons

Omniglot For Omniglot, we select the sampling patch
size to be 16 Ã— 16 during experiments. We perform 1-shot
and 5-shot 5-Way experiments on the novel classes, and
then compare with the two baseline methods: Baseline-FC
and Baseline-CS. The results on Omniglot are presented in
Table 1. For 1-shot, the performance gain over Baseline-FC
is 5.5% while the gain over Baseline-CS is 4.2%. For 5-
shot, the performance gain over Baseline-FC is 0.5%, while
the gain over Baseline-CS is 0.3%. Here we observed that
our model shows effectiveness in few-shot settings even if
itâ€™s presented one image per class (one-shot setting) com-
pared to the baseline methods. We also ï¬nd that simply
replacing the fully connected classiï¬er with a cosine simi-
larity based one yields a noticeable improvement.

We note that, the images in Omniglot consist of a sin-
gle character occupying a big portion of the image, and the

Table 2: Results on miniImagenet. FC denotes a fully-
connected classiï¬er and CS denotes a cosine similiarity
classiï¬er. ProtoNet# denotes the training method (30-Way
for 1-shot and 20-Way for 5-shot) in the original paper. Pro-
toNet denotes a 5-Way training strategy (as other methods
do). The results of Matching Network is cited from [24]
(denoted with a star). Bold and underlined numbers indi-
cate top two scores, respectively.

Baseline-FC
Baseline-CS

Matching Network* [27]

ProtoNet [24]
ProtoNet# [24]

MAML [5]

RelationNet [25]

Ours-FC
Ours-CS

5-Way

1-Shot

5-Shot

42.02Â±0.73
46.84Â±0.77
46.61Â±0.78
46.14Â±0.77
49.42Â±0.78
48.07Â±1.75
50.44Â±0.82
47.18Â±0.83
51.03Â±0.78

61.54Â±0.68
64.13Â±0.69
60.97Â±0.67
65.77Â±0.70
68.20Â±0.66
63.15Â±0.91
65.32Â±0.70
66.41Â±0.67
67.96Â±0.71

â€œlocalâ€ feature in patches simply reduces to oriented lines,
which is why we select a larger patch size (16 Ã— 16 to input
image size 28 Ã— 28), thereby allowing some global informa-
tion to be captured and encoded.

miniImagenet Compare with Omniglot, miniImagenet is
more realistic dataset. We select the patch size as 24 Ã— 24
where the input size is 64 Ã— 64 (after performing data aug-
mentation). We compare our proposed model with existing
methods [27, 24, 5, 25] which also fairly applies the sim-
ilar setup with the same Conv-4 backbone. The compari-
son results can be obtained in Table 2. Similar to the re-
sults reported in Omniglot, we see a cosine similarity clas-
siï¬er provides better results compared to a fully connected
layer. For 1-shot, the performance gain over Baseline-FC is
5.1% while the gain over Baseline-CS is 4.2%. For 5-shot,
the performance gain over Baseline-FC is 4.9%, while the
gain over Baseline-CS is 3.8%. Our proposed method per-
forms favorably against the state-of-the-art methods in 1-
shot learning. For 5-shot, our proposed model outperforms
the best competitor ProtoNet [24] under fair comparisons
(equal training and evaluation schemes).

We note that ProtoNet# [24] used a slightly more dif-
ferent training scheme, where they performed training us-
ing 30-way episodes (i.e. 30 training classes) for 1-shot
and 20-way episodes (i.e. 20 training classes) for 5-shot
during meta-training, which they reported that resulted in
better performance than just training from samples from 5
classes. Compared to this, we fall slightly behind by 0.28%,
but there is a high overlapping of the conï¬dence intervals.
We thereby include the performance of their model under
5-way meta-training episodes for completeness.

6257

Figure 4: Effects of hard and soft voting and the number of votes N on miniImagenet on (a) 1-shot and (b) 5-shot classiï¬ca-
tion. FC denotes a fully connected classiï¬er, and CS denotes a cosine similarity classiï¬er.

(a) One-shot

(b) Five-shot

Base

Novel

Figure 5: Sampling trajectories on miniImagenet. The top
two rows are from base classes and the bottom row is from
a novel class. The order of sampled patches is: blue, green,
red, and white. Note that the sampled patch trajectories dif-
fer in every feed-forward pass.

Our performance gain can be ascribed to incorporating
maximum entropy samplers and our unique design of model
for aggregating features from patches.

5.4. Ablation Studies

To explore the effects of voting strategies, we experiment
the hard and soft voting strategies during inference with N
as 1, 3, 5, and 7, using both the fully connected classiï¬er
and the cosine similarity classiï¬er. We evaluate on mini-
Imagenet and compare the results in Fig. 4. For soft vot-
ing, the increase in performance comes from the addition
of voting (N =1 to N =3), with an accuracy increase of 3%
to 4% for 5-shot settings and 2% to 3% for 1-shot settings.
We observe the same trend for hard voting with 1-shot set-
tings. However, for the 5-shot settings with hard voting,
the highest increase in accuracy is achieved at going from
3 to 5 votes, with an increase of 2%. The preserved uncer-
tainty information also seems preferable, as the soft voting
scheme outperforms to hard voting scheme (1% to 2% for
5-shot settings, around 1% for 1-shot settings).

5.5. Sampling Trajectory Analysis

Here we plot some of the sampling trajectories from
the base classes and novel classes from the miniImagenet

dataset in Fig. 5. Note that the sampling policies were not
ï¬netuned on the novel classes, thus the policies must be able
to generalize beyond the seen classes.

At ï¬rst glance, we can see that the sampling policy learns
to sample on the regions of interest and may also sometimes
choose to sample on background patches. We would like
to clarify that this is, in fact, the intended behavior, and is
the results of our main objective function in (2), where we
aim to maximize the action variety of the sampling policy.
Consider the scenario where, after seeing the ï¬rst couple of
patches, we are already certain of the object present in the
image. In this case, we can sample anywhere on the image
to maximize the entropy term in the objective function (2).

6. Conclusion

We presented a deep learning framework employing the
maximum entropy reinforcement learning objective. The
novelty of our model lies in the incorporation of maximum
entropy reinforcement learning and soft Q-Learning for the
sampling policies, with applications to few-shot learning.
We utilize both positive and negative sampling policies to
determine the favorable regions in an image and regular-
ize the learning process. Thus, our approach is able to in-
crease the input variety for the feature extractors (CNN)
during training, which can be seen as a form of â€œlearnedâ€
data augmentation. Moreover, during inference, the sam-
pling policies would be able to â€œattendâ€ to the relevant re-
gions of the test images, which allows us to elegantly deal
with any potential clutter in test images. Experiments on
two FSL datasets demonstrated that our model is able to
improve FSL performances and performs favorably against
the state-of-the-art methods.

Acknowledgements. This work is supported by the Min-
istry of Science and Technology of Taiwan under grant
MOST 108-2634-F-002-018.

6258

References

[1] Luca Bertinetto, JoËœao F. Henriques, Jack Valmadre, Philip
H. S. Torr, and Andrea Vedaldi. Learning feed-forward one-
shot learners. In Advances in Neural Information Processing
Systems (NIPS), 2016. 1, 2

[2] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau,
and Yoshua Bengio. On the properties of neural machine
translation: Encoder-decoder approaches. In Proceedings of
SSST@EMNLP 2014, Eighth Workshop on Syntax, Seman-
tics and Structure in Statistical Translation, 2014. 4

[3] Wen-Hsuan Chu and Yu-Chiang Frank Wang. Learning
semantics-guided visual attention for few-shot image clas-
siï¬cation. In Proceedings of the IEEE International Confer-
ence on Image Processing (ICIP), 2018. 1, 2

[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2009. 6

[5] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
In Proceedings of the International Conference on Machine
Learning (ICML), 2017. 1, 2, 6, 7

[6] Victor Garcia and Joan Bruna. Few-shot learning with graph
neural networks. Proceedings of the International Confer-
ence on Learning Representations (ICLR), 2018. 1, 2

[7] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot
visual learning without forgetting. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 1, 2, 6

[8] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey
Levine. Reinforcement learning with deep energy-based
policies. In Proceedings of the International Conference on
Machine Learning (ICML), 2017. 3, 4, 5, 6

[9] Bharath Hariharan and Ross B. Girshick. Low-shot visual
recognition by shrinking and hallucinating features. In Pro-
ceedings of the IEEE International Conference on Computer
Vision (ICCV), 2017. 1, 2, 3

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016. 1

[11] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and
Koray Kavukcuoglu. Spatial transformer networks. In Ad-
vances in Neural Information Processing Systems (NIPS),
2015. 2

[12] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov.
Siamese neural networks for one-shot image recognition. In
ICML Deep Learning Workshop, 2015. 1, 2

[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiï¬cation with deep convolutional neural net-
works. Advances in Neural Information Processing Systems
(NIPS), 2012. 1

[14] Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and
Joshua B. Tenenbaum. One shot learning of simple visual
concepts. In Proceedings of the 33th Annual Meeting of the
Cognitive Science Society, CogSci, 2011. 6

[15] Long-Ji Lin. Reinforcement learning for robots using neu-
ral networks. Technical report, Carnegie-Mellon Univ Pitts-
burgh PA School of Computer Science, 1993. 5

[16] Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui
Ren, and Qiang Yang. Cosine normalization: Using cosine
similarity instead of dot product in neural networks. In In-
ternational Conference on Artiï¬cial Neural Networks, 2018.
6

[17] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray
Kavukcuoglu. Recurrent models of visual attention. In Ad-
vances in Neural Information Processing Systems (NIPS),
2014. 2, 3, 5

[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, An-
drei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves,
Martin A. Riedmiller, Andreas Fidjeland, Georg Ostro-
vski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control
through deep reinforcement learning. Nature, 2015. 3

[19] Hang Qi, Matthew Brown, and David G Lowe. Low-shot
learning with imprinted weights. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 1, 2, 6

[20] Sachin Ravi and Hugo Larochelle. Optimization as a model
for few-shot learning.
In Proceedings of the International
Conference on Learning Representations (ICLR), 2017. 1, 2,
6

[21] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan
Wierstra, and Timothy P. Lillicrap. Meta-learning with
memory-augmented neural networks.
In Proceedings of
the International Conference on Machine Learning (ICML),
2016. 1, 6

[22] John Schulman, Sergey Levine, Pieter Abbeel, Michael I.
Jordan, and Philipp Moritz. Trust region policy optimization.
In Proceedings of the International Conference on Machine
Learning (ICML), 2015. 3

[23] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint, 2014. 1

[24] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-
ical networks for few-shot learning. In Advances in Neural
Information Processing Systems (NIPS), 2017. 1, 2, 6, 7

[25] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip
H. S. Torr, and Timothy M. Hospedales. Learning to com-
pare: Relation network for few-shot learning. Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 1, 2, 7

[26] Marc Toussaint. Robot trajectory optimization using approx-
imate inference. In Proceedings of the International Confer-
ence on Machine Learning (ICML), 2009. 2

[27] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray
Kavukcuoglu, and Daan Wierstra. Matching networks for
one shot learning. In Advances in Neural Information Pro-
cessing Systems (NIPS), 2016. 1, 2, 6, 7

[28] Peng Wang, Lingqiao Liu, Chunhua Shen, Zi Huang, An-
ton van den Hengel, and Heng Tao Shen. Multi-attention
network for one shot learning. In Proceedings of the IEEE

6259

Conference on Computer Vision and Pattern Recognition
(CVPR), 2017. 1, 2

[29] Yu-Xiong Wang, Ross B. Girshick, Martial Hebert, and
Bharath Hariharan. Low-shot learning from imaginary data.
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 1, 2

[30] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C. Courville, Ruslan Salakhutdinov, Richard S.
Zemel, and Yoshua Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In Proceed-
ings of the International Conference on Machine Learning
(ICML), 2015. 2

[31] Bo Zhao, Xiao Wu, Jiashi Feng, Qiang Peng, and Shuicheng
Yan. Diversiï¬ed visual attention networks for ï¬ne-grained
object classiï¬cation. IEEE Trans. Multimedia, 2017. 2

[32] Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, Xi Peng,
and Ahmed Elgammal. A generative adversarial approach
for zero-shot learning from noisy texts. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1, 2

6260

