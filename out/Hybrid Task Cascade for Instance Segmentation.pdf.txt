Hybrid Task Cascade for Instance Segmentation

Kai Chen1
Wansen Feng2

Jiangmiao Pang2
Ziwei Liu1

3

,

Jiaqi Wang1 Yu Xiong1 Xiaoxiao Li1

Shuyang Sun4

Jianping Shi2 Wanli Ouyang4 Chen Change Loy5 Dahua Lin1

1The Chinese University of Hong Kong

2SenseTime Research

3Zhejiang University

4The University of Sydney

5Nanyang Technological University

Abstract

Cascade is a classic yet powerful architecture that has
boosted performance on various tasks. However, how to in-
troduce cascade to instance segmentation remains an open
question. A simple combination of Cascade R-CNN and
Mask R-CNN only brings limited gain. In exploring a more
effective approach, we Ô¨Ånd that the key to a successful in-
stance segmentation cascade is to fully leverage the recip-
rocal relationship between detection and segmentation. In
this work, we propose a new framework, Hybrid Task Cas-
cade (HTC), which differs in two important aspects: (1) in-
stead of performing cascaded reÔ¨Ånement on these two tasks
separately, it interweaves them for a joint multi-stage pro-
cessing; (2) it adopts a fully convolutional branch to pro-
vide spatial context, which can help distinguishing hard
foreground from cluttered background. Overall, this frame-
work can learn more discriminative features progressively
while integrating complementary features together in each
stage. Without bells and whistles, a single HTC obtains
38.4% and 1.5% improvement over a strong Cascade Mask
R-CNN baseline on MSCOCO dataset. Moreover, our over-
all system achieves 48.6 mask AP on the test-challenge split,
ranking 1st in the COCO 2018 Challenge Object Detection
Task. Code is available at: https://github.com/
open-mmlab/mmdetection.

1. Introduction

Instance segmentation is a fundamental computer vision
task that performs per-pixel labeling of objects at instance
level. Achieving accurate and robust instance segmenta-
tion in real-world scenarios such as autonomous driving and
video surveillance is challenging. Firstly, visual objects are
often subject to deformation, occlusion and scale changes.
Secondly, background clutters make object instances hard
to be isolated. To tackle these issues, we need a robust rep-
resentation that is resilient to appearance variations. At the
same time, it needs to capture rich contextual information
for discriminating objects from cluttered background.

Cascade is a classic yet powerful architecture that has
boosted performance on various tasks by multi-stage reÔ¨Åne-
ment. Cascade R-CNN [5] presented a multi-stage archi-
tecture for object detection and achieved promising results.
The success of Cascade R-CNN can be ascribed to two key
aspects: (1) progressive reÔ¨Ånement of predictions and (2)
adaptive handling of training distributions.

Though being effective on detection tasks, integrating
the idea of cascade into instance segmentation is nontriv-
ial. A direct combination of Cascade R-CNN and Mask
R-CNN [18] only brings limited gain in terms of mask AP
compared to bbox AP. SpeciÔ¨Åcally, it improves bbox AP by
3.5% but mask AP by 1.2%, as shown in Table 1. An im-
portant reason for this large gap is the suboptimal informa-
tion Ô¨Çow among mask branches of different stages. Mask
branches in later stages only beneÔ¨Åt from better localized
bounding boxes, without direct connections.

To bridge this gap, we propose Hybrid Task Cascade
(HTC), a new cascade architecture for instance segmen-
tation. The key idea is to improve the information Ô¨Çow
by incorporating cascade and multi-tasking at each stage
and leverage spatial context to further boost the accuracy.
SpeciÔ¨Åcally, we design a cascaded pipeline for progressive
reÔ¨Ånement. At each stage, both bounding box regression
and mask prediction are combined in a multi-tasking man-
ner. Moreover, direct connections are introduced between
the mask branches at different stages ‚Äì the mask features
of each stage will be embedded and fed to the next one,
as demonstrated in Figure 2. The overall design strength-
ens the information Ô¨Çow between tasks and across stages,
leading to better reÔ¨Ånement at each stage and more accurate
predictions on all tasks.

For object detection, the scene context also provides use-
ful clues, e.g. for inferring the categories, scales, etc. To
leverage this context, we incorporate a fully convolutional
branch that performs pixel-level stuff segmentation. This
branch encodes contextual information, not only from fore-
ground instances but also from background regions, thus
complementing the bounding boxes and instance masks.
Our study shows that the use of the spatial contexts helps

4974

to learn more discriminative features.

HTC is easy to implement and can be trained end-to-
end. Without bells and whistles, it achieves 2.6% and 1.4%
higher mask AP than Mask R-CNN and Cascade Mask
R-CNN baselines respectively on the challenging COCO
dataset. Together with better backbones and other common
components, e.g. deformable convolution, multi-scale train-
ing and testing, model ensembling, we achieve 49.0 mask
AP on test-dev dataset, which is 2.3% higher than the win-
ning approach [28] of COCO Challenge 2017.

Our main contributions are summarized as follows: (1)
We propose Hybrid Task Cascade (HTC), which effectively
integrates cascade into instance segmentation by interweav-
ing detection and segmentation features together for a joint
multi-stage processing. It achieves the state-of-the-art per-
formance on COCO test-dev and test-challenge.
(2) We
demonstrate that spatial contexts beneÔ¨Åt instance segmenta-
tion by discriminating foreground objects from background
clutters. (3) We perform extensive study on various compo-
nents and designs, which provides a reference and is helpful
for futher research on object detection and instance segmen-
tation.

2. Related Work

Instance Segmentation.
Instance segmentation is a task
to localize objects of interest in an image at the pixel-
level, where segmented objects are generally represented by
masks. This task is closely related to both object detection
and semantic segmentation [30, 22]. Hence, existing meth-
ods for this task roughly fall into two categories, namely
detection-based and segmentation-based.

Detection-based methods resort to a conventional de-
tector to generate bounding boxes or region proposals,
and then predict the object masks within the bounding
boxes. Many of these methods are based on CNN, in-
cluding DeepMask [36], SharpMask [37], and Instance-
FCN [10]. MNC [11] formulates instance segmentation
as a pipeline that consists of three sub-tasks: instance lo-
calization, mask prediction and object categorization, and
trains the whole network end-to-end in a cascaded man-
ner. In a recent work, FCIS [23] extends InstanceFCN and
presents a fully convolutional approach for instance seg-
mentation. Mask-RCNN [18] adds an extra branch based
on Faster R-CNN [39] to obtain pixel-level mask predic-
tions, which shows that a simple pipeline can yield promis-
ing results. PANet [28] adds a bottom-up path besides the
top-down path in FPN [24] to facilitate the information Ô¨Çow.
MaskLab [7] produces instance-aware masks by combining
semantic and direction predictions.

Segmentation-based methods, on the contrary, Ô¨Årst ob-
tains a pixel-level segmentation map over the image, and
then identiÔ¨Åes object instances therefrom. Along this line,
Zhang et al. [46, 45] propose to predict instance labels

based on local patches and integrate the local results with
an MRF. Arnab and Torr [1] also use CRF to identify in-
stances. Bai and Urtasun [2] propose an alternative way,
which combines watershed transform and deep learning to
produce an energy map, and then derive the instances by
dividing the output of the watershed transform. Other ap-
proaches include bridging category-leval and instance-level
segmentation [42], learning a boundary-aware mask rep-
resentation [17], and employing a sequence of neural net-
works to deal with different sub-grouping problems [27].

Multi-stage Object Detection. The past several years
have seen remarkable progress in object detection. Main-
stream object detection frameworks are often categorized
into two types, single-stage detectors [29, 38, 25] and two-
stage detectors [39, 12, 18, 32]. Recently, detection frame-
works with multiple stages emerge as an increasingly pop-
ular paradigm for object detection. Multi-region CNN [14]
incorporates an iterative localization mechanism that alter-
nates between box scoring and location reÔ¨Ånement. Attrac-
tioNet [15] introduces an Attend & ReÔ¨Åne module to up-
date bounding box locations iteratively. CRAFT [44] in-
corporates a cascade structure into RPN [39] and Fast R-
CNN [16] to improve the quality of the proposal and detec-
tion results. IoU-Net [20] performs progressive bounding
box reÔ¨Ånement (even though not presenting a cascade struc-
ture explicitly). Cascade structures are also used to exclude
easy negative samples. For example, CC-Net [31] rejects
easy RoIs at shallow layers. Li et al. [21] propose to operate
at multiple resolutions to reject simple samples. Among all
the works that use cascade structures, Cascade R-CNN [5]
is perhaps the most relevant to ours. Cascade R-CNN com-
prises multiple stages, where the output of each stage is fed
into the next one for higher quality reÔ¨Ånement. Moreover,
the training data of each stage is sampled with increasing
IoU thresholds, which inherently handles different training
distributions.

While the proposed framework also adopts a cascade
structure, it differs in several important aspects. First, multi-
ple tasks, including detection, mask prediction, and seman-
tic segmentation, are combined at each stage, thus form-
ing a joint multi-stage processing pipeline. In this way, the
reÔ¨Ånement at each stage beneÔ¨Åts from the reciprocal rela-
tions among these tasks. Moreover, contextual information
is leveraged through an additional branch for stuff segmen-
tation and a direction path is added to allow direct informa-
tion Ô¨Çow across stages.

3. Hybrid Task Cascade

Cascade demonstrated its effectiveness on various tasks
such as object detection [5]. However, it is non-trivial to
design a successful architecture for instance segmentation.
In this work, we Ô¨Ånd that the key to a successful instance

4975

RPN

M1

B1

M2

B2

M3

B3

l

o
o
p

l

o
o
p

l

o
o
p

(a) Cascade Mask R-CNN

RPN

M1

B1

l

o
o
p

M2

B2

l

o
o
p

M3

B3

l

o
o
p

l

o
o
p

F

F

M1

B1

l

o
o
p

M2

B2

l

o
o
p

(b) Interleaved execution

M1

B1

l

o
o
p

M2

B2

l

o
o
p

M3

B3

l

o
o
p

M3

B3

l

o
o
p

l

o
o
p

l

o
o
p

RPN

RPN

F

S

F

(c) Mask information Ô¨Çow

(d) Hybrid Task Cascade (semantic feature fusion with box
branches is not shown on the Ô¨Ågure for neat presentation.)

Figure 1: The architecture evolution from Cascade Mask R-CNN to Hybrid Task Cascade.

ùëÄ"

ùëÄ"#$

v
n
o
c
 

1
x
1

v
n
o
c
 

3
x
3

v
n
o
c
 

3
x
3

v
n
o
c
 

3
x
3

v
n
o
c
 

3
x
3

v
n
o
c
e
d

v
n
o
c
 

3
x
3

v
n
o
c
 

3
x
3

v
n
o
c
 

3
x
3

v
n
o
c
 

3
x
3

v
n
o
c
e
d

v
n
o
c
 

1
x
1

ùë•

Figure 2: Architecture of multi-stage mask branches.

segmentation cascade is to fully leverage the reciprocal re-
lationship between detection and segmentation.

Overview. In this work, we propose Hybrid Task Cascade
(HTC), a new framework of instance segmentation. Com-
pared to existing frameworks, it is distinctive in several as-
pects: (1) It interleaves bounding box regression and mask
prediction instead of executing them in parallel. (2) It in-
corporates a direct path to reinforce the information Ô¨Çow
between mask branches by feeding the mask features of
the preceding stage to the current one. (3) It aims to ex-
plore more contextual information by adding an additional
semantic segmentation branch and fusing it with box and
mask branches. Overall, these changes to the framework
architecture effectively improve the information Ô¨Çow, not
only across stages but also between tasks.

3.1. Multi task Cascade

ing the architecture of Mask R-CNN is added to each stage
of Cascade R-CNN, as shown in Figure 1a. The pipeline is
formulated as:

x

box
t = P(x, rt‚àí1),

box
t
= P(x, rt‚àí1), mt = Mt(x

rt = Bt(x

),

mask
t

x

mask
t

(1)

).

t

t

and xmask

Here, x indicates the CNN features of backbone network,
xbox
indicates box and mask features derived
from x and the input RoIs. P(¬∑) is a pooling operator, e.g.,
RoI Align or ROI pooling, Bt and Mt denote the box and
mask head at the t-th stage, rt and mt represent the corre-
sponding box predictions and mask predictions. By com-
bining the advantages of cascaded reÔ¨Ånement and the mu-
tual beneÔ¨Åts between bounding box and mask predictions,
this design improves the box AP, compared to Mask R-CNN
and Cascade R-CNN alone. However, the mask prediction
performance remains unsatisfying.

Interleaved Execution. One drawback of the above de-
sign is that the two branches at each stage are executed in
parallel during training, both taking the bounding box pre-
dictions from the preceding stage as input. Consequently,
the two branches are not directly interacted within a stage.
In response to this issue, we explore an improved design,
which interleaves the box and mask branches, as illustrated
in Figure 1b. The interleaved execution is expressed as:

x

box
t = P(x, rt‚àí1),
mask
t

box
t
= P(x, rt), mt = Mt(x

rt = Bt(x

x

),

mask
t

(2)

).

Cascade Mask R-CNN. We begin with a direct combi-
nation of Mask R-CNN and Cascade R-CNN, denoted as
Cascade Mask R-CNN. SpeciÔ¨Åcally, a mask branch follow-

In this way, the mask branch can take advantage of the up-
dated bounding box predictions. We found that this yields
improved performance.

4976

Mask Information Flow.
In the design above, the mask
prediction at each stage is based purely on the ROI features
x and the box prediction rt. There is no direct information
Ô¨Çow between mask branches at different stages, which pre-
vents further improvements on mask prediction accuracy.
Towards a good design of mask information Ô¨Çow, we Ô¨Årst
recall the design of the cascaded box branches in Cascade
R-CNN [5]. An important point is the input feature of box
branch is jointly determined by the output of the preced-
ing stage and backbone. Following similar principles, we
introduce an information Ô¨Çow between mask branches by
feeding the mask features of the preceding stage to the cur-
rent stage, as illustrated in Figure 1c. With the direct path
between mask branches, the pipeline can be written as:

x

box
t = P(x, rt‚àí1),
mask
t

x

rt = Bt(x

box
t

),

= P(x, rt), mt = Mt(F(x

mask
t

, m

‚àí

t‚àí1)),

(3)

‚àí

t‚àí1 denotes the intermediate feature of Mt‚àí1 and
where m
we use it as the mask representation of stage t ‚àí 1. F is
a function to combine the features of the current stage and
the preceding one. This information Ô¨Çow makes it possible
for progressive reÔ¨Ånement of masks, instead of predicting
masks on progressively reÔ¨Åned bounding boxes.

8x up

4x up

1x1 

conv

1x1 

conv

2x up

1x1 

conv

1x1 

conv

1x1 

conv

3x3 

conv

‚Ä¶

3x3 

conv

4 conv

1x1 

conv

1x1 

conv

semantic feature

2x down

segmentation prediction

Figure 3: We introduce complementary contextual informa-
tion by adding semantic segmentation branch.

3.2. Spatial Contexts from Segmentation

To further help distinguishing the foreground from the
cluttered background, we use the spatial contexts as an ef-
fective cue. We add an additional branch to predict per-pixel
semantic segmentation for the whole image, which adopts
the fully convolutional architecture and is jointly trained
with other branches, as shown in Figure 1d. The seman-
tic segmentation feature is a strong complement to existing
box and mask features, thus we combine them together for
better predictions:

Implementation. Following the discussion above, we
propose a simple implementation as below.

x

box
t = P(x, rt‚àí1) + P(S(x), rt‚àí1),
rt = Bt(x

),

box
t

F(x

mask
t

, mt‚àí1) = x

mask
t

+ Gt(m

‚àí

t‚àí1)

(4)

In this implementation, we adopt the RoI feature before
the deconvolutional layer as the mask representation m
t‚àí1,
whose spatial size is 14 √ó14. At stage t, we need to forward
all preceding mask heads with RoIs of the current stage to
compute m

‚àí

‚àí

t‚àí1.

m

m

‚àí

1 = M ‚àí

1 (x

mask
t

),

2 (F(x

‚àí

2 = M ‚àí
...

m

‚àí

t‚àí1 = M ‚àí

t (F(x

mask
t

, m

‚àí

1 )),

mask
t

, m

‚àí

t‚àí2)).

(5)

‚àí

Here, M ‚àí
t denotes the feature transformation component
of the mask head Mt, which is comprised of 4 consecutive
3 √ó 3 convolutional layers, as shown in Figure 2. The trans-
t‚àí1 are then embedded with a 1 √ó 1 con-
formed features m
volutional layer Gt in order to be aligned with the pooled
backbone features xmask
t‚àí1) is added to
xmask
through element-wise sum. With this introduced
bridge, adjacent mask branches are brought into direct in-
teraction. Mask features in different stages are no longer
isolated and all get supervised through backpropagation.

. Finally, Gt(m

t

t

‚àí

x

mask
t

= P(x, rt) + P(S(x), rt),

mt = Mt(F(x

mask
t

, m

‚àí

t‚àí1)),

(6)

where S indicates the semantic segmentation head. In the
above formulation, the box and mask heads of each stage
take not only the RoI features extracted from the backbone
as input, but also exploit semantic features, which can be
more discriminative on cluttered background.

Semantic Segmentation Branch. SpeciÔ¨Åcally,
the se-
mantic segmentation branch S is constructed based on the
output of the Feature Pyramid [24]. Note that for semantic
segmentation, the features at a single level may not be able
to provide enough discriminative power. Hence, our design
incorporates the features at multiple levels. In addition to
the mid-level features, we also incorporate higher-level fea-
tures with global information and lower-level features with
local information for better feature representation.

Figure 3 shows the architecture of this branch. Each level
of the feature pyramid is Ô¨Årst aligned to a common represen-
tation space via a 1 √ó 1 convolutional layer. Then low level
feature maps are upsampled, and high level feature maps are
downsampled to the same spatial scale, where the stride is
set to 8. We found empirically that this setting is sufÔ¨Åcient
for Ô¨Åne pixel-level predictions on the whole image. These

4977

transformed feature maps from different levels are subse-
quently fused by element-wise sum. Moreover, we add four
convolutional layers thereon to further bridge the semantic
gap. At the end, we simply adopt a convolutional layer to
predict the pixel-wise segmentation map. Overall, we try
to keep the design of semantic segmentation branch simple
and straightforward. Though a more delicate structure can
further improve the performance, It goes beyond our scope
and we leave it for future work.

Fusing Contexts Feature into Main Framework.
It is
well known that joint training of closely related tasks can
improve feature representation and bring performance gains
to original tasks. Here, we propose to fuse the semantic fea-
tures with box/mask features to allow more interaction be-
tween different branches. In this way, the semantic branch
directly contributes to the prediction of bounding boxes and
masks with the encoded spatial contexts. Following the
standard practice, given a RoI, we use RoIAlign to extract a
small (e.g., 7 √ó 7 or 14 √ó 14) feature patch from the corre-
sponding level of feature pyramid outputs as the representa-
tion. At the same time, we also apply RoIAlign on the fea-
ture map of the semantic branch and obtain a feature patch
of the same shape, and then combine the features from both
branches by element-wise sum.

3.3. Learning

Since all the modules described above are differentiable,
Hybrid Task Cascade (HTC) can be trained in an end-to-end
manner. At each stage t, the box head predicts the classiÔ¨Åca-
tion score ct and regression offset rt for all sampled RoIs.
The mask head predicts pixel-wise masks mt for positive
RoIs. The semantic branch predicts a full image seman-
tic segmentation map s. The overall loss function takes the
form of a multi-task learning:

L =

T

X

t=1

Œ±t(Lt

bbox + Lt

mask) + Œ≤Lseg,

bbox(ci, rt, ÀÜct, ÀÜrt) = Lcls(ct, ÀÜct) + Lreg(rt, ÀÜrt),
mask(mt, ÀÜmt) = BCE(mt, ÀÜmt),

Lt
Lt
Lseg = CE(s, ÀÜs).

(7)

Here, Lt
bbox is the loss of the bounding box predictions at
stage t, which follows the same deÔ¨Ånition as in Cascade
R-CNN [5] and combines two terms Lcls and Lreg, re-
spectively for classiÔ¨Åcation and bounding box regression.
Lt
mask is the loss of mask prediction at stage t, which adopts
the binary cross entropy form as in Mask R-CNN [18]. Lseg
is the semantic segmentation loss in the form of cross en-
tropy. The coefÔ¨Åcients Œ±t and Œ≤ are used to balance the con-
tributions of different stages and tasks. We follow the hy-
perparameter settings in Cascade R-CNN [5]. Unless other-
wise noted, we set Œ± = [1, 0.5, 0.25], T = 3 and Œ≤ = 1 by
default.

4. Experiments

4.1. Datasets and Evaluation Metrics

Datasets. We perform experiments on the challenging
COCO dataset [26]. We train our models on the split of
2017train (115k images) and report results on 2017val and
2017test-dev. Typical instance annotations are used to su-
pervise box and mask branches, and the semantic branch is
supervised by COCO-stuff [4] annotations.
Evaluation Metrics. We report the standard COCO-style
Average Precision (AP) metric which averages APs across
IoU thresholds from 0.5 to 0.95 with an interval of 0.05.
Both box AP and mask AP are evaluated. For mask AP,
we also report AP50, AP75 (AP at different IoU thresholds)
and APS, APM , APL (AP at different scales). Runtime is
measured on a single TITAN Xp GPU.

4.2. Implementation Details

In all experiments, we adopt a 3-stage cascade. FPN is
used in all backbones. For fair comparison, Mask R-CNN
and Cascade R-CNN are reimplemented with PyTorch [33]
and mmdetection [6], which are slightly higher than the re-
ported performance in the original papers. We train detec-
tors with 16 GPUs (one image per GPU) for 20 epoches
with an initial learning rate of 0.02, and decrease it by 0.1
after 16 and 19 epoches, respectively. The long edge and
short edge of images are resized to 1333 and 800 respec-
tively without changing the aspect ratio.

During inference, object proposals are reÔ¨Åned progres-
sively by box heads of different stages. ClassiÔ¨Åcation scores
of multiple stages are ensembled as in Cascade R-CNN.
Mask branches are only applied to detection boxes with
higher scores than a threshold (0.001 by default).

4.3. Benchmarking Results

We compare HTC with the state-of-the-art instance seg-
mentation approaches on the COCO dataset in Table 1. We
also evaluate Cascade Mask R-CNN, which is described
in Section 1, as a strong baseline of our method. Com-
pared to Mask R-CNN, the naive cascaded baseline brings
3.5% and 1.2% gains in terms of box AP and mask AP re-
spectively.
It is noted that this baseline is already higher
than PANet [28], the state-of-the-art instance segmentation
method. Our HTC achieves consistent improvements on
different backbones, proving its effectiveness. It achieves
a gain of 1.5%, 1.3% and 1.1% for ResNet-50, ResNet-101
and ResNeXt-101, respectively.

4.4. Ablation Study

Component-wise Analysis. Firstly, we investigate the
effects of main components in our framework.
‚ÄúInter-
leaved‚Äù denotes the interleaved execution of bbox and mask

4978

Table 1: Comparison with state-of-the-art methods on COCO test-dev dataset.

Method

Backbone

box AP mask AP AP50 AP75 APS APM APL runtime (fps)

Mask R-CNN [18]

PANet[28]

ResNet-50-FPN
ResNet-50-FPN

Cascade Mask R-CNN
ResNet-50-FPN
Cascade Mask R-CNN ResNet-101-FPN
Cascade Mask R-CNN ResNeXt-101-FPN

HTC (ours)
HTC (ours)
HTC (ours)

ResNet-50-FPN
ResNet-101-FPN
ResNeXt-101-FPN

39.1
41.2

42.7
44.4
46.6

43.6
45.3
47.1

35.6
36.6

36.9
38.4
40.1

38.4
39.7
41.2

57.6
58.0

58.6
60.2
62.7

60.0
61.8
63.9

38.1
39.3

39.7
41.4
43.4

41.5
43.1
44.7

18.7
16.3

19.6
20.2
22.0

20.4
21.0
22.8

38.3
38.1

39.3
41.0
42.8

40.7
42.2
43.9

46.6
52.4

48.8
50.6
52.9

51.2
53.5
54.6

5.3

-

3.0
2.9
2.5

2.5
2.4
2.1

branches, ‚ÄúMask Info‚Äù indicates the mask branch informa-
tion Ô¨Çow and ‚ÄúSemantic‚Äù means introducing the seman-
tic segmentation branch. From Table 2, we can learn that
the interleaved execution slightly improves the mask AP by
0.2%. The mask information Ô¨Çow contributes to a further
0.6% improvement, and the semantic segmentation branch
leads to a gain of 0.6%.
Effectiveness of Interleaved Branch Execution. In Sec-
tion 3.1, we design the interleaved branch execution to ben-
eÔ¨Åt the mask branch from updated bounding boxes dur-
ing training. To investigate the effeciveness of this strat-
egy, we compare it with the conventional parallel execution
pipeline on both Mask R-CNN and Cascade Mask R-CNN.
As shown in Table 3, interleaved execution outperforms par-
allel execution on both methods, with an improvement of
0.5% and 0.2% respectively.
Effectiveness of Mask Information Flow. We study how
the introduced mask information Ô¨Çow helps mask prediction
by comparing stage-wise performance. Semantic segmen-
tation branch is not involved to exclude possible distraction.
From Table 4, we Ô¨Ånd that introducing the mask informa-
tion Ô¨Çow greatly improves the the mask AP in the second
stage. Without direct connections between mask branches,
the second stage only beneÔ¨Åts from better localized bound-
ing boxes, so the improvement is limited (0.8%). With the
mask information Ô¨Çow, the gain is more signiÔ¨Åcant (1.5%),
because it makes each stage aware of the preceding stage‚Äôs
features. Similar to Cascade R-CNN, stage 3 does not out-
perform stage 2, but it contributes to the ensembled results.
Effectiveness of Semantic Feature Fusion. We exploit
contextual features by introducing a semantic segmentation
branch and fuse the features of different branches. Multi-
task learning is known to be beneÔ¨Åcial, here we study the
necessity of semantic feature fusion. We train different
models that fuse semantic features with the box or mask
or both branches, and the results are shown in Table 5. Sim-
ply adding a full image segmentation task achieves 0.6%
improvement, mainly resulting from additional supervision.
Feature fusion also contributes to further gains,e.g., fusing

the semantic features with both the box and mask branches
brings an extra 0.4% gain, which indicates that complemen-
tary information increases feature discrimination for box
and mask branches.
InÔ¨Çuence of Loss Weight. The new hyper-parameter Œ≤ is
introduced, since we involve one more task for joint train-
ing. We tested different loss weight for the semantic branch,
as shown in Table 6. Results show that our method is insen-
sitive to the loss weight.

4.5. Extensions on HTC

With the proposed HTC, we achieve 49.0 mask AP and
2.3% absolute improvement compared to the winning entry
last year. Here we list all the steps and additional mod-
ules used to obtain the performance. The step-by-step gains
brought by each component are illustrated in Table 7.
HTC Baseline. The ResNet-50 baseline achieves 38.2
mask AP.
DCN. We adopt deformable convolution [13] in the last
stage (res5) of the backbone.
SyncBN. Synchronized Batch Normalization [34, 28] is
used in the backbone and heads.
Multi-scale Training. We adopt multi-scale training.
In
each iteration, the scale of short edge is randomly sampled
from [400, 1400], and the scale of long edge is Ô¨Åxed as 1600.
SENet-154. We tried different backbones besides ResNet-
50, and SENet-154 [19] achieves best single model perfor-
mance among them.
GA-RPN. We Ô¨Ånetune trained detectors with the propos-
als generated by GA-RPN [41], which achieves near 10%
higher recall than RPN.
Multi-scale Testing. We use 5 scales as well as horizontal
Ô¨Çip at test time and ensemble the results. Testing scales are
(600, 900), (800, 1200), (1000, 1500), (1200, 1800), (1400,
2100).
Ensemble. We utilize an emsemble of Ô¨Åve networks:
SENet-154 [19], ResNeXt-101 [43] 64x4d, ResNeXt-101
32x8d, DPN-107 [9], FishNet [40].

4979

Table 2: Effects of each component in our design. Results are reported on COCO 2017 val.

Cascade

Interleaved Mask Info

Semantic

X

X

X

X

X

X

X

X

X

X

box AP mask AP AP50 AP75 APS APM APL
50.8
50.8
51.5
52.3

57.9
57.7
58.1
59.4

39.4
39.4
40.3
40.7

18.9
18.9
19.6
20.3

39.5
39.7
40.3
40.9

42.5
42.5
42.5
43.2

36.5
36.7
37.4
38.0

Table 3: Results of parallel/interleaved branch execution on different methods.

Method

execution

box AP mask AP AP50 AP75 APS APM APL

Mask R-CNN

Cascade Mask R-CNN

parallel

interleaved

parallel

interleaved

38.4
38.7

42.5
42.5

35.1
35.6

36.5
36.7

56.6
57.2

57.9
57.7

37.4
37.9

39.4
39.4

18.7
19.0

18.9
18.9

38.4
39.0

39.5
39.7

47.7
48.3

50.8
50.8

Table 4: Effects of the mask information Ô¨Çow. We evaluate
the stage-wise and ensembled performance with or without
the information Ô¨Çow (denoted as I.F.).

Table 6: Ablation study of semantic branch loss weight Œ≤
on COCO 2017 val.

I.F.

test stage

AP AP50 AP75 APS APM APL

N

Y

stage 1
stage 2
stage 3

35.5 56.7
36.3 57.5
35.9 56.5
stage 1 ‚àº 3 36.7 57.7

stage 1
stage 2
stage 3

35.5 56.8
37.0 58.0
36.8 57.2
stage 1 ‚àº 3 37.4 58.1

37.8
39.0
38.7
39.4

37.8
39.8
39.9
40.3

18.7
18.8
18.2
18.9

19.0
19.4
18.7
19.6

38.8
39.4
39.1
39.7

38.8
39.8
39.8
40.3

48.6
50.6
49.9
50.8

49.0
51.3
51.1
51.5

Table 5: Ablation study of semantic feature fusion on
COCO 2017 val.

Fusion AP AP50 AP75 APS APM APL

-

none
bbox
mask
both

36.5 57.9
37.1 58.6
37.3 58.9
37.4 58.7
37.5 59.1

39.4
39.9
40.2
40.2
40.4

18.9
19.3
19.4
19.4
19.6

39.5
40.0
40.2
40.1
40.3

50.8
51.7
52.3
52.4
52.6

Œ≤ AP AP50 AP75 APS APM APL

0.5 37.9 59.3
38.0 59.4
1
37.9 59.3
2
3
37.8 59.0

40.7
40.7
40.6
40.5

19.7
20.3
19.6
19.9

41.0
40.9
40.8
40.5

52.5
52.3
52.8
53.2

Table 7: Results (mask AP) with better backbones and bells
and whistles on COCO test-dev dataset.

AP AP50 AP75 APS APM APL

2017 winner [28] 46.7 69.5
49.0 73.0

Ours

HTC baseline

+ DCN

+ SyncBN
+ ms train

+ SENet-154
+ GA-RPN

+ ms test

+ ensemble

38.4 60.0
39.5 61.3
40.7 62.8
42.5 64.8
44.3 67.5
45.3 68.9
47.4 70.6
49.0 73.0

51.3
53.9

41.5
42.8
44.2
46.4
48.3
49.4
52.1
53.9

26.0
33.9

20.4
20.9
22.2
23.7
25.0
27.0
30.2
33.9

49.1
52.3

40.7
41.8
43.1
45.3
47.5
48.3
50.1
52.3

64.0
61.2

51.2
52.7
54.4
56.7
58.9
59.6
61.8
61.2

4.6. Extensive Study on Common Modules

We also perform extensive study on some components
designed for detection and segmentation. Components are
often compared under different conditions such as back-
bones, codebase, etc. Here we provide a uniÔ¨Åed environ-
ment with state-of-the-art object detection and instance seg-
mentation framework to investigate the functionality of ex-
tensive components. We integrate several common modules

designed for detection and segmentation and evaluate them
under the same settings, and the results are shown in Ta-
ble 8. Limited by our experience and resources, some im-
plementations and the integration methods may not be op-
timal and worth further study. Code will be released as a
benchmark to test more components.
ASPP. We adopt
the Atrous Spatial Pyramid Pooling
(ASPP) [8] module from the semantic segmentation com-
munity to capture more image context at multiple scales.

4980

Figure 4: Examples of segmentation results on COCO dataset.

We append an ASPP module after FPN.

5. Conclusion

PAFPN. We test the PAFPN module from PANet [28]. The
difference from the original implementation is that we do
not use Synchronized BatchNorm.

GCN. We adopt Global Convolutional Network (GCN) [35]
in the semantic segmentation branch.

PreciseRoIPooling. We replace the RoI align layers in
HTC with Precise RoI Pooling [20].

SoftNMS. We apply SoftNMS [3] to box results.

Table 8: Extensive study on related modules on COCO
2017 val.

Method

AP AP50 AP75 APS APM APL

We propose Hybrid Task Cascade (HTC), a new cas-
cade architecture for instance segmentation. It interweaves
box and mask branches for a joint multi-stage processing,
and adopts a semantic segmentation branch to provide spa-
tial context. This framework progressively reÔ¨Ånes mask
predictions and integrates complementary features together
in each stage. Without bells and whistles, the proposed
method obtains 1.5% improvement over a strong Cascade
Mask R-CNN baseline on MSCOCO dataset. Notably, our
overall system achieves 48.6 mask AP on the test-challenge
dataset and 49.0 mask AP on test-dev.

HTC

38.0 59.4
HTC+ASPP
38.1 59.9
HTC+PAFPN 38.1 59.5
37.9 59.2
HTC+PrRoIPool 37.9 59.1
HTC+SoftNMS 38.3 59.6

HTC+GCN

40.7
41.0
41.0
40.7
40.9
41.2

20.3
20.0
20.0
20.0
19.7
20.4

40.9
41.2
41.2
40.6
40.9
41.2

52.3
52.8
53.0
52.3
52.7
52.7

Acknowledgments. This work is partially supported by
the Collaborative Research grant from SenseTime Group
(CUHK Agreement No. TS1610626 & No. TS1712093),
the General Research Fund (GRF) of Hong Kong (No.
14236516, No. 14203518 & No. 14224316), and Singa-
pore MOE AcRF Tier 1 (M4012082.020).

4981

References

[1] Anurag Arnab and Philip HS Torr. Bottom-up instance seg-
mentation using deep higher-order crfs. In British Machine
Vision Conference, 2016.

[2] Min Bai and Raquel Urtasun. Deep watershed transform for
In IEEE Conference on Computer

instance segmentation.
Vision and Pattern Recognition, 2017.

[3] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and
Larry S Davis. Soft-nms‚Äîimproving object detection with
one line of code. In IEEE International Conference on Com-
puter Vision. IEEE, 2017.

[4] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-
stuff: Thing and stuff classes in context. In IEEE Conference
on Computer Vision and Pattern Recognition, 2018.

[5] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving
into high quality object detection. In IEEE Conference on
Computer Vision and Pattern Recognition, 2018.

[6] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-
iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping
Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin.
mmdetection. https://github.com/open-mmlab/
mmdetection, 2018.

[7] Liang-Chieh Chen, Alexander Hermans, George Papan-
dreou, Florian Schroff, Peng Wang, and Hartwig Adam.
Masklab: Instance segmentation by reÔ¨Åning object detection
with semantic and direction features. In IEEE Conference on
Computer Vision and Pattern Recognition, 2018.

[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE Transactions on Pattern
Analysis and Machine Intelligence, (4):834‚Äì848, 2018.

[9] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin,
Shuicheng Yan, and Jiashi Feng. Dual path networks.
In
Advances in Neural Information Processing Systems, 2017.

[10] Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, and Jian Sun.
Instance-sensitive fully convolutional networks. In European
Conference on Computer Vision, 2016.

[11] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-
In
mantic segmentation via multi-task network cascades.
IEEE Conference on Computer Vision and Pattern Recog-
nition, 2016.

[12] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: Object
detection via region-based fully convolutional networks. In
Advances in Neural Information Processing Systems, 2016.

[13] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In IEEE International Conference on Computer
Vision, 2017.

[14] Spyros Gidaris and Nikos Komodakis. Object detection
via a multi-region and semantic segmentation-aware cnn
model. In IEEE International Conference on Computer Vi-
sion, 2015.

[16] Ross Girshick. Fast r-cnn. In IEEE International Conference

on Computer Vision, 2015.

[17] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.
Boundary-aware instance segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition, 2017.

[18] Kaiming He, Georgia Gkioxari, Piotr Doll¬¥ar, and Ross Gir-
In IEEE International Conference on

shick. Mask r-cnn.
Computer Vision, 2017.

[19] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In IEEE Conference on Computer Vision and Pattern
Recognition, 2018.

[20] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yun-
ing Jiang. Acquisition of localization conÔ¨Ådence for accurate
object detection. In European Conference on Computer Vi-
sion, 2018.

[21] Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and
Gang Hua. A convolutional neural network cascade for face
detection. In IEEE Conference on Computer Vision and Pat-
tern Recognition, 2015.

[22] Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, and
Xiaoou Tang. Not all pixels are equal: DifÔ¨Åculty-aware se-
mantic segmentation via deep layer cascade. In IEEE Con-
ference on Computer Vision and Pattern Recognition, 2017.

[23] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
Fully convolutional instance-aware semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, 2017.

[24] Tsung-Yi Lin, Piotr Doll¬¥ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In IEEE Conference on Com-
puter Vision and Pattern Recognition, 2017.

[25] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll¬¥ar. Focal loss for dense object detection. In IEEE
International Conference on Computer Vision, 2017.

[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European Conference on Computer Vision, 2014.

[27] Shu Liu, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. Sgn:
Sequential grouping networks for instance segmentation. In
IEEE International Conference on Computer Vision, 2017.

[28] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
Path aggregation network for instance segmentation. In IEEE
Conference on Computer Vision and Pattern Recognition,
2018.

[29] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In European Con-
ference on Computer Vision, 2016.

[30] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen-Change Loy, and
Xiaoou Tang. Semantic image segmentation via deep parsing
network.
In IEEE International Conference on Computer
Vision, 2015.

[15] Spyros Gidaris and Nikos Komodakis. Attend reÔ¨Åne repeat:
In

Active box proposal generation via in-out localization.
British Machine Vision Conference, 2016.

[31] Wanli Ouyang, Kun Wang, Xin Zhu, and Xiaogang Wang.
Chained cascade network for object detection. In IEEE In-
ternational Conference on Computer Vision, 2017.

4982

[32] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng,
Wanli Ouyang, and Dahua Lin. Libra r-cnn: Towards bal-
anced learning for object detection. In IEEE Conference on
Computer Vision and Pattern Recognition, 2019.

[33] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban
Desmaison, Luca Antiga, and Adam Lerer. Automatic dif-
ferentiation in pytorch. In Advances in Neural Information
Processing Systems Workshop, 2017.

[34] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu
Zhang, Kai Jia, Gang Yu, and Jian Sun. Megdet: A large
mini-batch object detector.
In IEEE Conference on Com-
puter Vision and Pattern Recognition, 2018.

[35] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and
Jian Sun. Large kernel matters‚Äîimprove semantic segmen-
tation by global convolutional network. In IEEE Conference
on Computer Vision and Pattern Recognition. IEEE, 2017.

[36] Pedro O Pinheiro, Ronan Collobert, and Piotr Doll¬¥ar. Learn-
In Advances in Neural

ing to segment object candidates.
Information Processing Systems, 2015.

[37] Pedro O Pinheiro, Tsung-Yi Lin, Ronan Collobert, and Piotr
In European

Doll¬¥ar. Learning to reÔ¨Åne object segments.
Conference on Computer Vision, 2016.

[38] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: UniÔ¨Åed, real-time object de-
tection. In IEEE Conference on Computer Vision and Pattern
Recognition, 2016.

[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in Neural Information Pro-
cessing Systems, 2015.

[40] Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, and
Wanli Ouyang. Fishnet: A versatile backbone for image,
region, and pixel level prediction.
In Advances in Neural
Information Processing Systems, 2018.

[41] Jiaqi Wang, Kai Chen, Shuo Yang, Chen Change Loy, and
Dahua Lin. Region proposal by guided anchoring. In IEEE
Conference on Computer Vision and Pattern Recognition,
2019.

[42] Zifeng Wu, Chunhua Shen, and Anton van den Hengel.
Bridging category-level and instance-level semantic image
segmentation. arXiv preprint arXiv:1605.06885, 2016.

[43] Saining Xie, Ross Girshick, Piotr Doll¬¥ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition, 2017.

[44] Bin Yang, Junjie Yan, Zhen Lei, and Stan Li. Craft objects
from images. In IEEE Conference on Computer Vision and
Pattern Recognition, 2016.

[45] Ziyu Zhang, Sanja Fidler, and Raquel Urtasun.

Instance-
level segmentation for autonomous driving with deep
densely connected mrfs. In IEEE Conference on Computer
Vision and Pattern Recognition, 2016.

[46] Ziyu Zhang, Alexander G Schwing, Sanja Fidler, and Raquel
Urtasun. Monocular object instance segmentation and depth
ordering with cnns.
In IEEE International Conference on
Computer Vision, 2015.

4983

