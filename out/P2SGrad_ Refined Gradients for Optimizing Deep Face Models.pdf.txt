P2SGrad: Reï¬ned Gradients for Optimizing Deep Face Models

Xiao Zhang1 Rui Zhao2

Junjie Yan2 Mengya Gao2 Yu Qiao3 Xiaogang Wang1 Hongsheng Li1

1CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong

2SenseTime Research

3SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences

zhangx9411@gmail.com hsli@ee.cuhk.edu.hk

Abstract

Cosine-based softmax losses [20, 29, 27, 3] signiï¬-
cantly improve the performance of deep face recognition
networks. However, these losses always include sensitive
hyper-parameters which can make training process unsta-
ble, and it is very tricky to set suitable hyper parameters
for a speciï¬c dataset. This paper addresses this challenge
by directly designing the gradients for training in an adap-
tive manner. We ï¬rst investigate and unify previous co-
sine softmax losses from the perspective of gradients. This
uniï¬ed view inspires us to propose a novel gradient called
P2SGrad (Probability-to-Similarity Gradient), which lever-
ages a cosine similarity instead of classiï¬cation probabil-
ity to control the gradients for updating neural network pa-
rameters. P2SGrad is adaptive and hyper-parameter free,
which makes training process more efï¬cient and faster. We
evaluate our P2SGrad on three face recognition bench-
marks, LFW [7], MegaFace [8], and IJB-C [16]. The re-
sults show that P2SGrad is stable in training, robust to
noise, and achieves state-of-the-art performance on all the
three benchmarks.

1. Introduction

Over the last few years, deep convolutional neural net-
works have signiï¬cantly boosted the face recognition accu-
racy. State-of-the-art approaches are based on deep neu-
ral networks and adopt the following pipeline:
training a
classiï¬cation model with different types of softmax losses
and use the trained model as a feature extractor to test un-
seen samples. Then the cosine similarities between testing
facesâ€™ features, are exploited to determine whether these
features belong to the same identity. Unlike other vision
tasks, such as object detection, where training and testing
have the same objectives and evaluation procedures, con-
ventional face recognition systems were trained with soft-
max losses but tested with cosine similarities.
In other
words, there is a gap between the softmax probability in
training and inner product similarity in testing.

This problem is not well addressed in the classical soft-
max cross-entropy loss function (softmax loss for short

in the remaining part), which mainly considers probabil-
ity distributions of training classes and ignores the test-
ing setup.
In order to bridge this gap, cosine softmax
losses [28, 13, 14] and their angular margin based vari-
ants [29, 27, 3] directly use cosine distances instead of in-
ner products as the input raw classiï¬cation scores, namely
logits. Specially, the angular margin based variants aim to
learn the decision boundaries with a margin between dif-
ferent classes. These methods improve the face recognition
performance in the challenging setup.

In spite of their successes, cosine-based softmax loss is
only a trade-off: the supervision signals for training are still
classiï¬cation probabilities, which are never evaluated dur-
ing testing. Considering the fact that the similarity between
two testing face images is only related to themselves while
the classiï¬cation probabilities are related to all the identi-
ties, cosine softmax losses are not the ideal training mea-
sures in face recognition.

This paper aims to address these problems from a differ-
ent perspective. Deep neural networks are generally trained
with Stochastic Gradient Descent (SGD) algorithms where
gradients play an essential role in this process. In addition
to the loss function, we focus on the gradients of cosine
softmax loss functions. This new perspective not only al-
lows us to analyze the relations and problems of previous
methods, but also inspires us to develop a novel form of
adaptive gradients, P2SGrad, which mitigates the problem
of training-testing mismatch and further improves the face
recognition performance in practice.

To be more speciï¬c, P2SGrad optimizes deep models by
well-designed gradients. Compared with the conventional
gradients in cosine-based softmax losses, P2SGrad uses co-
sine distances to replace the probabilities in the original gra-
dients. P2SGrad decouples gradients from hyperparameters
and the number of classes, and matches testing targets.

This paper mainly contributes in the following aspects:

1. We analyze the recent cosine softmax losses and their
angular-margin based variants from the perspective of
gradients, and propose a general formulation to unify
different cosine softmax cross-entropy losses;

2. With this uniï¬ed model, we propose an adaptive

9906

Training

Updating

Training Facial Images

Deep Face Model

Feature

Class. Prob. One-hot Ground Truth

Softmax

		ğ‘ƒ#

	ğ‘ƒ%

	ğ‘ƒ&
â€¦
	ğ‘ƒ$

With

0

1

0
â€¦
0

Cross  Entropy

ğ¿ $(

Feature Extractor

Feature Pairs

Cosine

Similarity

> ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘

Same 
Person

Different 
Person

< ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘

Testing Face Pairs

Testing

Figure 1. Pipeline of current face recognition system. In this general pipeline, deep face models trained on classiï¬cation tasks are treated
as feature extractors. Best viewed in color.

hyperparameter-free gradient method - P2SGrad for
training deep face recognition networks. This method
reserves advantages of using cosine distances in train-
ing but replaces classiï¬cation probabilities with cosine
similarities in the backward propagation;

3. We conduct extensive experiments on large-scale face
datasets. Experimental results show that P2SGrad out-
performs state-of-the-art methods on the same setup
and clearly improves the stability of the training pro-
cess.

2. Related Works

The accuracy improvements of face recognition [9, 6,
18, 25] enjoy the large-scale training data, and the im-
provements of neural network structures. Modern face
datasets contain a huge number of identities, such as
LFW [7], PubFig [10], CASIA-WebFace [32], MS1M [4]
and MegaFace [17, 8], which enable the effective training
of very deep neural networks. A number of recent studies
demonstrated that well-designed network architectures lead
to better performance, such as DeepFace [26], DeepID2,
3 [22, 23] and FaceNet [21].

In face recognition, feature representation normaliza-
tion, which restricts features to lie on a ï¬xed-radius hyper-
sphere, is a common operation to enhance modelsâ€™ ï¬nal per-
formance. COCO loss [13, 14] and NormFace [28] stud-
ied the effect of normalization through mathematical analy-
sis and proposed two strategies through reformulating soft-
max loss and metric learning. Coincidentally, L2-softmax
[20] also proposed a similar method. These methods obtain
the same formulation of cosine softmax loss from different
views.

Optimizing auxiliary metric loss function is also a pop-
ular choice for boosting performance. In the early years,
most face recognition approaches utilized metric loss func-
tions, such as triplet loss [30] and contrastive loss [2], which
use Euclidean margin to measure distance between features.
Taking advantages of these works, center loss [31] and

range loss [33] were proposed to reduce intra-class varia-
tions through minimizing distance within target classes [1].
Simply using Euclidean distance or Euclidean margin
is insufï¬cient to maximize the classiï¬cation performance.
To circumvent this difï¬culty, angular margin based softmax
loss functions were proposed and became popular in face
recognition. Angular constraints were added to traditional
softmax loss function to improve feature discriminativeness
in L-softmax [12] and A-softmax [11], where A-softmax
applied weight normalization but L-softmax [12] did not.
CosFace [29], AM-softmax [27] and ArcFace [3] also em-
braced the idea of angular margins and employed simpler as
well as more intuitive loss functions compared with afore-
mentioned methods. Normalization is applied to both fea-
tures and weights in these methods.

3. Limitations of cosine softmax losses

In this section we will discuss limitations caused by the
mismatch between training and testing of face recognition
models. We ï¬rst provide a brief review of the workï¬‚ow of
cosine softmax losses. Then we will reveal the limitations
of existing loss functions in face recognition from the per-
spective of forward and backward calculation respectively.

3.1. Gradients of cosine softmax losses

In face recognition tasks,

the cosine softmax cross-
entropy loss has an elegant two-part formulation, softmax
function and cross-entropy loss.

We discuss softmax function at ï¬rst. Assuming that the
vector ~xi denotes the feature representation of a face image,
the input of the softmax function is the logit fi,j , i.e.,

fi,j = s Â·

h~xi, ~Wji

k~xik2k ~Wjk2

= s Â· hË†xi, Ë†Wji = s Â· cos Î¸i,j ,

(1)

where s is a hyperparameter and fi,j is the classiï¬cation
score (logit) that ~xi is assigned to class j, and Wj is the
weight vector of class j. Ë†xi and Ë†Wj are normalized vec-
tors of xi and Wj respectively. Î¸i,j is the angle between

9907

feature xi and class weight Wj . The logits fi,j are then
input into the softmax function to obtain the probability
efi,j
Pi,j = Softmax(fi,j) =
k=1 efi,k , where C is the number
of classes and the output Pi,j can be interpreted as the prob-
ability of ~xi being assigned to a certain class j. If j = yi,
then Pi,yi is the class probability of ~xi being assigned to its
corresponding class yi.

PC

Then we discuss the cross-entropy loss associated with
the softmax function, which measures the divergence be-
tween the predicted probability Pi,yi and ground truth dis-
tributions as

ğ‘Š$%

ğœ• cos ğœƒ0,$%

ğœ•ğ‘Š$%

ğ’™ğ’Š

LCE(~xi) = âˆ’ log Pi,yi = âˆ’ log

,

(2)

ğœ• cos ğœƒ0 ,&	

ğœ•ğ‘Š&

efi,yi
k=1 efi,k

PC

where LCE(~xi) is the loss of input feature ~xi. The larger
probability Pi,yi is, the smaller loss LCE(~xi) is.

In order to decrease the loss LCE(~xi), the model needs
to enlarge Pi,yi and thus enlarge fi,yi . Then Î¸i,yi becomes
smaller.
In summary, cosine softmax loss function maps
Î¸i,yi to the probability Pi,yi and calculates the cross-entropy
loss to supervise the training.

In the backward propagation process, classiï¬cation prob-
abilities Pi,j play key roles for optimization. The gradient
of ~xi and ~Wj in cosine softmax losses are calculated as

(Pi,j âˆ’ âœ¶(yi = j)âˆ‡f (cos Î¸i,j) Â·

âˆ‚ cos Î¸i,j

âˆ‚~xi

,

=

C

Xj=1

âˆ‚LCE(~xi)

âˆ‚~xi

âˆ‚LCE(~xi)

âˆ‚ ~Wj

= (Pi,j âˆ’ âœ¶(yi = j)âˆ‡f (cos Î¸i,j) Â·

âˆ‚ cos Î¸i,j

âˆ‚ ~Wj

,

(3)
where the indicator function âœ¶(j = yi) returns 1 when j =
yi and 0 otherwise. âˆ‚ cos Î¸i,j
can be computed
respectively as:

and âˆ‚ cos Î¸i,j

âˆ‚ ~Wj

âˆ‚~xi

âˆ‚ cos Î¸i,j

âˆ‚~xi

âˆ‚ cos Î¸i,j

âˆ‚ ~Wj

=

=

1

k~xik2

1

k ~Wjk2

( Ë†Wj âˆ’ cos Î¸i,j Â· Ë†xi),

(Ë†xi âˆ’ cos Î¸i,j Â· Ë†Wj),

(4)

âˆ‚ ~Wj

where Ë†Wj and Ë†xi are unit vectors of ~Wj and ~xi, respec-
tively. âˆ‚ cos Î¸i,j
is visualized as the red arrow in Fig. 2. This
gradient vector is updating directions of class weights ~Wj .
Intuitively, we expect the updating of ~Wj makes ~Wyi close
to ~xi, and makes ~Wj for j 6= yi away from ~xi. Gradient
âˆ‚ cos Î¸i,j
is vertical to ~Wj and points toward ~xi. Thus it is

âˆ‚ ~Wj

the fastest and optiaml direction for updating ~Wj .
Then we consider the gradient âˆ‡f (cos Î¸i,j).

In con-
ventional cosine softmax losses [20, 28, 13], classiï¬cation
score f (cos Î¸i,j) = s Â· cos Î¸i,j and thus âˆ‡f (cos Î¸i,j) = s.
In angular margin-based cosine softmax losses [27, 29, 3],

ğ‘Š&, ğ’‹ â‰  ğ’šğ’Š

âˆ‚ cos Î¸i,j

âˆ‚ ~Wj

Figure 2. Gradient direction of
. Note this gradient is the
updating direction of ~Wj . The red pointed line shows that the
gradient of ~Wj is vertical to ~Wj itself and in the plane spanned by
~xi and ~Wj . This can be seen as the fastest direction for updating
~Wyi to be close to ~xi and for updating ~Wj , j 6= yi to be far away

from ~xi. Best viewed in color.

however, the gradient of fmargin(cos Î¸i,yi ) for j = yi de-
pends on where the margin parameter m is. For exam-
ple, in CosFace [29] f (cos Î¸i,yi ) = s Â· (cos Î¸i,yi âˆ’ m),
thus âˆ‡f (cos Î¸i,yi ) = s and in ArcFace [3] f (cos Î¸i,yi ) =
s Â· cos (Î¸i,yi + m), thus âˆ‡f (cos Î¸i,yi ) = s Â· sin (Î¸i,yi +m)
. In
general, gradient âˆ‡f (cos Î¸i,j) is always a scalar related to
parameters s, m and cos Î¸i,j .

sin Î¸i,yi

Based on the aforementioned discussions, we reconsider
, the ï¬rst

gradients of class weights ~Wj in Eq. (3). In âˆ‚LCE
âˆ‚ ~Wj
part (Pi,j âˆ’ âœ¶(yi = j) Â· âˆ‡f (cos Î¸i,j) is a scalar, which de-
cides the length of gradient, while the second part âˆ‚ cos Î¸i,j
is a vector which decides the direction of gradient. Since
the directions of gradients for various cosine softmax losses
remain the same, the essential difference of these cosine
softmax losses is the different lengths of gradients, which
signiï¬cantly affect the optimization of model. In the follow-
ing sections, we will discuss the suboptimal gradient length
caused by forward and backward process respectively.

âˆ‚ ~Wj

3.2. Limitations in probability calculation

In this section we discuss the limitations of the forward
calculation of cosine softmax losses in deep face networks
and focus on the classiï¬cation probability Pi,j obtained in
the forward calculation.

We ï¬rst revisit the relation between Pi,j and Î¸i,j . The
classiï¬cation probability Pi,j in Eq. (3) is a part of gradi-
ent length. Hence Pi,j signiï¬cantly affects the length of
gradient. Probability Pi,j and logit fi,j are positively corre-
lated. For all cosine softmax losses, logits fi,j measure Î¸i,j
between feature ~xi and class weight ~Wj . A larger Î¸i,j pro-

9908

Ï€
2

7Ï€
16

j
,
i
Î¸

3Ï€
8

5Ï€
16

Ï€
4

Average Î¸i,yi
Average Î¸i,j, j 6= yi

Iteration

Figure 3. The change of average Î¸i,j of each mini-batch when
training on WebFace dataset. (Red) average angles in each mini-
batch for non-corresponding classes, Î¸i,j for j 6= yi.
(Brown)
average angles in each mini-batch for corresponding classes, Î¸i,yi .

duces lower classiï¬cation probability Pi,j while a smaller
Î¸i,j produces higher Pi,j . It means that Î¸i,j affects gradient
length by its corresponding probability Pi,j . The equation
sets up a mapping relation between Î¸i,j and Pi,j and makes
Î¸i,j affects optimization. Above analysis is also the reason
why cosine softmax losses are effective on face recognition
tasks.

Since Î¸i,yi is the direct measurement of the generaliza-
tion but it can only indirectly affect gradient by correspond-
ing Pi,yi , setting a reasonable mapping relation between
Î¸i,yi and Pi,yi is crucial. However, there are two tricky
problems in current cosine softmax losses: (1) classiï¬cation
probability Pi,yi is sensitive to hyperparameter settings; (2)
the calculation of Pi,yi is dependent on class number, which
is not related to face recognition tasks. We will discuss these
problems below.

Pi,yi

is sensitive to hyperparameters.

The most
common hyperparameters in conventional cosine softmax
losses [20, 28, 13] and margin variants [3] are the scale pa-
rameter s and the angular margin parameter m. We will
analyze the sensitivity of probability Pi,yi to hyperparam-
eter s and m. For a more accurate analysis, we ï¬rst look
at the actual range of Î¸i,j . Fig. 3 exhibits how the average
Î¸i,j changes in training. Mathematically, Î¸i,j could be any
value in [0, Ï€]. In practice, however, the maximum Î¸i,j is
around Ï€
2 . The blue curve reveals that Î¸i,j for j 6= yi do
not change signiï¬cantly during training. The brown curve
reveals that Î¸i,yi is gradually reduced. Therefore we can
reasonably assume that Î¸i,j â‰ˆ Ï€
2 for j 6= yi and the range
of Î¸i,yi is [0, Ï€

2 ]. Then Pi,yi can be rewritten as

Pi,yi =

â‰ˆ

=

efi,yi
k=1 efi,k

efi,yi

PC
efi,yi +Pk6=yi

efi,yi

efi,yi +Pk6=yi

=

esÂ·cos Ï€/2

esÂ·cos Î¸i,k

efi,yi

efi,yi + (C âˆ’ 1)

,

(5)
where fi,yi is logit that ~xi is assigned to its corresponding
class yi, and C is class number.

Theoretically, we can give the correspondence between
probability Pi,yi and angle Î¸i,yi under different hyperpa-
rameter settings. In state-of-the-art angular margin based

i
y
,
i

P
y
t
i
l
i
b
a
b
o
r
P

1.0

0.8

0.6

0.4

0.2

0.0

0

s = 64, m = 0.2

s = 64, m = 0

s = 30, m = 0.5

s = 8, m = 0

s = 8, m = 0.5
cos Î¸i,yi

Ï€
16

Ï€
8

3Ï€
16

Ï€
4

5Ï€
16

3Ï€
8

7Ï€
16

Ï€
2

Figure 4. Probability Pi,yi curves w.r.t. the angle Î¸i,yi with differ-
ent hyperparameter settings.

Î¸i,yi âˆˆ (0, Ï€
2 )

j
,
i
Î¸

Ï€
2

7Ï€
16

3Ï€
8

5Ï€
16

Ï€
4

3Ï€
16

Ï€
8

Ï€
16

i
y
,
i

P
y
t
i
l
i
b
a
b
o
r
P

1.0

0.8

0.6

0.4

0.2

0.0

Average Î¸i,yi
Average Pi,yi

Figure 5. The change of probability Pi,yi and angle Î¸i,yi as the
iteration number increases with the hyperparameter setting s = 35
and m = 0.2. Best viewed in color.

Num. of Iteration

losses [3], logit fi,yi = s Â· cos (Î¸i,yi + m). Fig. 4 reveals
that different settings of s and m can signiï¬cantly affect the
relation between Î¸i,yi and Pi,yi . Apparently, both the green
curve and the purple curve are examples of unreasonable re-
lations. The former is so lenient that even a very larger Î¸i,yi
can produce a high Pi,yi â‰ˆ 1. The later is so strict that even
a very small Î¸i,yi can just produce a low Pi,yi . In short, for
a speciï¬c degree of Î¸i,yi , the difference of probability Pi,yi
under different settings is very large. This observation indi-
cates that probability Pi,yi is sensitive to parameters s and
m.

To further conï¬rm this conclusion, we take an example
of correspondences between Pi,yi and Î¸i,yi in real training.
In Fig. 5, the red curve represents the change of Pi,yi and the
blue curve represents the change of Î¸i,yi during the training
process. As we discussed above, Pi,yi â‰ˆ 1 can produce
very short gradients so that has little affection in updating.
This setting is not ideal because Pi,yi increases to 1 rapidly
but Î¸i,yi is still large. Therefore classiï¬cation probability
Pi,yi largely depends on the setting of hyperparameter.

Pi,yi contains class number. In closed-set classiï¬cation
problems, probabilities Pi,j become smaller as the growth
of class number C because each class is assigned more or
less probability (but not 0). This is reasonable in classiï¬ca-
tion tasks. However, this is not suitable for face recognition,
which is an open-set problem. Since Î¸i,yi is the direct mea-
surement of generalization of ~xi while Pi,yi is the indirect
measurement, we expect that they have a consistent seman-
tic meaning. But Pi,yi is related to class nubmer C while

9909

i
y
,
i

P
y
t
i
l
i
b
a
b
o
r
P

1.0

0.8

0.6

0.4

0.2

0.0

0

Class Number C = 10

Class Number C = 100

Class Number C = 1, 000

Class Number C = 10, 000

Class Number C = 100, 000
cos Î¸i,yi

Ï€
16

Ï€
8

3Ï€
16

Ï€
4

5Ï€
16

3Ï€
8

7Ï€
16

Ï€
2

Î¸i,yi âˆˆ (0, Ï€
2 )

Figure 6. Pi,yi with different class numbers. The hyperparameter
setting is ï¬xed to s = 15 and m = 0.5 for fair comparison. Best
viewed in color.

Î¸i,yi is not, which causes the mismatch between them.

As shown in Fig. 6, we can summarize that the class

number C is an important factor for Pi,yi .

From the above discussion, we reveal that limitations
exist in the forward calculation of cosine softmax losses.
Both hyperparameters and the class number, which are un-
related to face recognition tasks, can determine the proba-
bility Pi,yi , and thus affect the gradient length in Eq. (3).

3.3. Limitation in backward calculation of cosine

softmax losses

In this section, we discuss the limitations in the back-
ward calculation of the cosine softmax function, especially
the angular-margin based softmax losses [3].

We revisit gradient âˆ‡f (cos Î¸i,j) in Eq. (3). Besides
Pi,yi , the part of âˆ‡f (cos Î¸i,j) also affects the length of
gradient. Larger âˆ‡f (cos Î¸i,j) produce longer gradients
while smaller ones produce shorter gradients. So we expect
Î¸i,yi and values of âˆ‡f (cos Î¸i,j) to be positively correlated:
small Î¸i,yi for small âˆ‡f (cos Î¸i,j) and large Î¸i,yi for larger
âˆ‡f (cos Î¸i,j).

The logit fi,yi

is different in various cosine softmax
losses, and thus the speciï¬c form of âˆ‡f (cos Î¸i,j) is dif-
ferent. Generally, we focus on simple cosine softmax
losses [20, 28, 13] and state-of-the-art angular margin based
loss [3]. Their âˆ‡f (cos Î¸i,j) are visualized in Fig. 7, which
shows that, under the factor of âˆ‡f (cos Î¸i,j), the lengths of
gradients in conventional softmax cosine losses [20, 28, 13]
are constant. However in angular margin-based losses [3],
the lengths of gradients and Î¸i,yi are negatively correlated,
which is completely contrary to our expectations. More-
over, the correspondence between length of gradients in an-
gular margin-based loss [3] and Î¸i,yi becomes tricky: when
Î¸i,yi gradually reduced, Pi,yi tends to shorten length of gra-
dients but âˆ‡f (cos Î¸i,j) tends to elongate the length. There-
fore the geometric meaning of the gradient length becomes
unexplained in angular margin-based cosine softmax loss.

3.4. Summary

In the above discussion, we ï¬rst reveal that various
cosine softmax losses have the same updating direction.
Hence the main difference between the variants is their gra-
dient lengths. For the length of gradient, there are two
the probability Pi,yi in
scalars that determine its value:
the forward process and the gradient âˆ‡f (cos Î¸i,j). For
Pi,yi , we ï¬nd that it can easily lose its semantic mean-
ing with different hyperparameter settings and class num-
bers. For âˆ‡f (cos Î¸i,j), its value depends on the deï¬nition
of f (cos Î¸i,yi ).

In summary, from the perspective of gradient, the widely
used cosine softmax losses [20, 28, 13] and their angular
margin variants [3] cannot produce optimal gradient lengths
with well-explained geometric meanings.

4. P2SGrad: Change Probability to Similarity

in Gradient

In this section, we propose a new method, namely
P2SGrad, that determines the gradient length only by Î¸i,j
in training face recognition models. Formally, the gradient
length produced by P2SGrad is hyperparameter-free and not
related to the number of class C nor to a ad-hoc deï¬nition
of logit fi,yi . P2SGrad does not need a speciï¬ed formula-
tion of loss function because gradients is well-designed to
optimize deep models.

Since the main difference of state-of-the-art cosine soft-
max losses is the gradient length, reforming a reasonable
gradient length is an intuitive thought. In order to decouple
the length factor and direction factor of the gradients, we
rewrite Eq. (3) as

C

âˆ‡LCE(~xi) =

L(Pi,j, f (cos Î¸i,j)) Â· D( ~Wj, ~xi),

(6)

Xj=1

âˆ‡LCE( ~Wj) = L(Pi,j, f (cos Î¸i,j)) Â· D(~xi, ~Wj),

where the direction factors D( ~Wj, ~xi) and D(~xi, ~Wj) are
deï¬ned as

D( ~Wj, ~xi) =

D(~xi, ~Wj) =

1

k~xik2

1

k ~Wjk2

( Ë†Wj âˆ’ cos Î¸i,j Â· Ë†xi),

(Ë†xi âˆ’ cos Î¸i,j Â· Ë†Wj),

(7)

where Ë†Wj and Ë†xi are unit vectors of ~Wj and ~xi, respectively.
cos Î¸i,j is the cosine distances between feature ~xi and class
weights ~Wj . The direction factors will not be changed be-
cause they are the fastest changing directions, which are
speciï¬ed before. The length factor |L(Pi,j, f (cos Î¸i,j))| is
deï¬ned as

|L(Pi,j, f (cos Î¸i,j))| =((1 âˆ’ Pi,yi )|âˆ‡f (cos Î¸i,yi )|

Pi,j Â· |âˆ‡f (cos Î¸i,j)|

j = yi,
j 6= yi.

(8)

9910

ğ‘Š"#

ğ‘¥âƒ—&

ğœƒ&,"#

ğ‘Š"#

ğ‘¥âƒ—&

ğœƒ&,"#

Simple Cosine Softmax Loss

Angular Margin-based Loss

Figure 7. How âˆ‡f (cos Î¸i,j) affects the length of gradients. (Left) the correspondence between Î¸i,yi and âˆ‡f (cos Î¸i,j). The red curve
means âˆ‡f (cos Î¸i,j) is constant in conventional cosine softmax losses [20, 28, 13] while the blue curve means small a Î¸i,yi can produce a
very large âˆ‡f (cos Î¸i,j). (Right) each point refers to a feature ~xi and the vertical vector is weight ~Wyi . The Î¸i,yi is angle between each ~xi
and ~Wyi . The color from light to dark corresponds to the value of âˆ‡f (cos Î¸i,j) from small to large. Hence for the factor of âˆ‡f (cos Î¸i,j),

the dark points produce longer gradients than the light points. Best viewed in color.

The length factor |L(Pi,j, f (cos Î¸i,j))| depends on the prob-
ability Pi,j and âˆ‡f (cos Î¸i,j) and is what we aim to reform.
Since we expect that the new length is hyperparameter-
free, the cosine logit f (cos Î¸i,j) will not have hyperparam-
eters like s or m. Thus a constant âˆ‡f (cos Î¸i,j) should be
an ideal choice.

For the probability Pi,j , because it is hard to set a rea-
sonable mapping function between Î¸i,j and Pi,j , we can di-
rectly use cos Î¸i,j as a good alternative of Pi,j in the gradi-
ent length term. Firstly, they have the same theoretical range
of [0, 1] where Î¸i,j âˆˆ [0, Ï€
2 ]. Secondly, unlike Pi,j which is
adversely inï¬‚uenced by hyperparameter and the number of
class, cos Î¸i,j does not contain any of these. It means that
we do not need to select speciï¬ed parameters settings for
ideal correspondence between Î¸i,yi and Pi,yi . Moreover,
compared with Pi,j , cos Î¸i,j is a more natural supervision
because cosine similarities are used in the testing phase of
open-set face recognition systems while probabilities only
apply for close-set classiï¬cation tasks. Therefore, our re-
formed gradient length factor ËœL(cos Î¸i,j) can be deï¬ned as:

ËœL(cos Î¸i,j) = cos Î¸i,j âˆ’ âœ¶(j = yi),

(9)

where ËœL(cos Î¸i,j) is a function of cos Î¸i,j . The reformed
gradients ËœGP2SGrad could then be deï¬ned as

C

ËœGP2SGrad(~xi) =

ËœL(cos Î¸i,j) Â· D( ~Wj, ~xi),

(10)

Xj=1

ËœGP2SGrad( ~Wj) = ËœL(cos Î¸i,j) Â· D(~xi, ~Wj),

where âœ¶ is the indicator function. The full formulation can

be rewrite as

ËœGP2SGrad(~xi) =

C

Xj=1

(cos Î¸i,j âˆ’ âœ¶(j = yi)) Â·

âˆ‚ cos Î¸i,j

âˆ‚~xi

,

ËœGP2SGrad( ~Wj) = (cos Î¸i,j âˆ’ âœ¶(j = yi)) Â·

âˆ‚ cos Î¸i,j

âˆ‚ ~Wj

,

(11)
Although the analysis process is slightly complicated,
the formulation of P2SGrad is not only succinct but reason-
able. When j = yi, the proposed gradient length and Î¸i,j
are positively correlated, when j 6= yi, they are negatively
correlated. More importantly, gradient length in P2SGrad
only depends on Î¸i,j and thus ï¬ts the testing metric of face
recognition systems.

5. Experiments

In this section, we conduct a series of experiments to
evaluate the proposed P2SGrad. We ï¬rst verify advantages
of P2SGrad in some exploratory experiments by testing
the modelâ€™s performance on LFW [7]. Then we evaluate
P2SGrad on MegaFace [8] Challenge and IJBC 1:1 veriï¬-
cation [16] with the same training conï¬guration.

5.1. Exploratory Experiments

Preprocessing and training setting. We use CASIA-
WebFace [32] as training data and ResNet-50 as the neural
network architecture. Here WebFace [32] dataset is cleaned
and contains about 450k facial images. RSA [15] is ap-
plied to images to extract facial areas and then aligns the
faces similarity transformation. All images are resized to
144 Ã— 144. Also, we conduct pixel value normalization
by subtracting 127.5 and then dividing by 128. For all ex-
ploratory experiments, the size of a mini-batch is 512 in
every iteration.

9911

Avg. Î¸i,yi of l2-softmax.

Grad. Length of l2-softmax.

1.0
Ï€
2

7Ï€
16

3Ï€
8

5Ï€
16
0.8

Ï€
4

3Ï€
16

Ï€
8

Ï€
0.6
16

i
y
,
i
Î¸

.
g
v
A

0k

30k

60k

Iteration.

90k

Avg. Î¸i,yi of ArcFace.

Grad. Length of ArcFace.

i
y
,
i
Î¸

.
g
v
A

Ï€
2

0.4
7Ï€
16

3Ï€
8

5Ï€
16

Ï€
4
0.2

3Ï€
16

Ï€
8

Ï€
16

0.0

0k
0.0

h
t
g
n
e
L

.

d
a
r
G

h
t
g
n
e
L

.

d
a
r
G

1.2

1.0

0.8

0.6

0.4

0.2

0.0

1.2

1.0

0.8

0.6

0.4

0.2

0.0

i
y
,
i
Î¸

.
g
v
A

i
y
,
i
Î¸

.
g
v
A

Avg. Î¸i,yi of CosFace.

Grad. Length of CosFace.

Ï€
2

7Ï€
16

3Ï€
8

5Ï€
16

Ï€
4

3Ï€
16

Ï€
8

Ï€
16

0k

30k

60k

Iteration.

90k

Avg. Î¸i,yi of P2SGrad.

Grad. Length of P2SGrad.

Ï€
2

7Ï€
16

3Ï€
8

5Ï€
16

Ï€
4

3Ï€
16

Ï€
8

Ï€
16

h
t
g
n
e
L

.

d
a
r
G

h
t
g
n
e
L

.

d
a
r
G

1.2

1.0

0.8

0.6

0.4

0.2

0.0

1.2

1.0

0.8

0.6

0.4

0.2

0.0

30k

0.2

60k

90k
0.4

0k

0.6

30k

Iteration.

0.8

60k

Iteration.

90k

1.0

Figure 8. Curves of Î¸i,yi and gradient lengths w.r.t. iteration. Gradient lengths in existing cosine-based softmax losses (top-left, top-right,
bottom-left) rapidly decrease to nearly 0 while gradient length produced by P2SGrad (bottom-right) can match Î¸i,yi between xi and its
ground truth class yi. Best viewed in color.

Init. LR

10âˆ’1
10âˆ’2
10âˆ’3
10âˆ’4

NormFace CosFace ArcFace

Method

Ã—
âˆš
âˆš
âˆš

Ã—
Ã—
âˆš
âˆš

Ã—
Ã—
âˆš
âˆš

P2SGrad

âˆš
âˆš
âˆš
âˆš

Table 1. The sensitiveness of initial learning rates. This table
shows whether our P2SGrad and these cosine-based softmax loss
are trainable under different initial learning rates.

The change of gradient length and Î¸i,yi w.r.t.

iter-
ation. Since P2SGrad aims to set up a reasonable map-
ping from Î¸i,yi to the length of gradients, it is necessary
to visualize such mapping. In order to demonstrate the ad-
vancement of P2SGrad, we plot mapping curves of several
cosine-based softmax losses in Fig. 8. This ï¬gure clearly
shows that P2SGrad produces more optimal gradient length
according to the change of Î¸i,yi .

Robustness of initial learning rates. An important
problem of margin-based loss is that they are difï¬cult to
train with large learning rates. The implementation of L-
softmax [12] and A-softmax [11] use extra hyperparameters
to adjust the margin so that the models are trainable. Thus a
small initial learning rate is important for properly training
angular-margin-based softmax losses. In contrast, accord-
ing to Table. 1, our proposed P2SGrad is stable with large
learning rates.

Convergence rate. The convergence rate is important
for evaluating an optimization method. We evaluated the
trained modelâ€™s performance on Labeled Faces in the Wild
(LFW) dataset of several cosine-based softmax losses and
our P2SGrad method at different training periods. LFW
dataset is an academic test set for unrestricted face veriï¬-
cation. Its testing protocol contains about 13, 000 images
of about 1, 680 identities. There are 3, 000 positive matches

Î¸i,yi of l2-softmax
Î¸i,yi of CosFace
Î¸i,yi of ArcFace
Î¸i,yi of P2SGrad

Ï€
2

7Ï€
16

3Ï€
8

5Ï€
16

Ï€
4

3Ï€
16

Ï€
8

Ï€
16

i
y
,
i
Î¸

.
g
v
A

0k

30k

60k

Num. of Iteration

90k

Figure 9. The change of average Î¸i,yi w.r.t. iteration number. Î¸i,yi
represents the angle between xi and the weight vector of its ground
truth class yi. Curves by the proposed P2SGrad, l2-softmax loss
[20], CosFace [29] and ArcFace [3] are shown.

Method

l2-softmax [20]
CosFace [29]
ArcFace [3]
P2SGrad

Num. of Iteration

30k
81.50
83.63
85.32
91.25

60k
91.27
93.58
94.77
97.38

90k
97.92
99.05
99.47
99.82

Table 2. Convergence rates of P2SGrad and compared losses. With
the same number of iterations, P2SGrad leads to the best perfor-
mance.

and the same number of negative matches. Table. 2 shows
the results with the same training conï¬guration while Fig. 9
shows the decrease of average Î¸i,yi in P2SGrad is more
quickly than other losses. These results reveal that our pro-
posed P2SGrad can optimize neural network much faster.

9912

Method

Size of MegaFace Distractor

101

102

103

104

105

106

l2-softmax [20]
CosFace [29]
ArcFace [3]
P2SGrad

99.73% 99.49% 99.03% 97.85% 95.56% 92.05%
99.82% 99.68% 99.46% 98.57% 97.58% 95.50%
99.78% 99.65% 99.48% 98.87% 98.03% 96.88%
99.86% 99.70% 99.52% 98.92% 98.35% 97.25%

Table 3. Recognition accuracy on MegaFace. Inception-ResNet [24] models trained with different compared softmax loss and the same
cleaned WebFace [32] and MS1M [4] training data.

Method

VggFace [18]

Crystal Loss [19]
l2-softmax [20]
CosFace [29]
ArcFace [3]
P2SGrad

10âˆ’5

10âˆ’4

10âˆ’3

True Acceptance Rate @ False Acceptance Rate
10âˆ’2
10âˆ’6

10âˆ’1
95.64% 87.13% 74.79% 59.75% 43.69% 32.20%
99.06% 97.66% 95.63% 92.29% 87.35% 81.15% 71.37%
98.40% 96.45% 92.78% 86.33% 77.25% 62.61% 26.67%
99.01% 97.55% 95.37% 91.82% 86.94% 76.25% 61.72%
99.07% 97.75% 95.55% 92.13% 87.28% 82.15% 72.28%
99.03% 97.79% 95.58% 92.25% 87.84% 82.44% 73.16%

10âˆ’7

-

Table 4. TARs by different compared softmax losses on the IJB-C 1:1 veriï¬cation task. The same training data (WebFace [32] and MS1M
[4]) and Inception-ResNet [24] networks are used. Results of VggFace [18] and Crystal Loss [19] are from [19].

5.2. Evaluation on MegaFace

Preprocessing and training setting. Besides the men-
tioned WebFace [32] dataset, we add another public training
dataset, MS1M [4], which contains about 2.35M cleaned
and aligned images. Here we use Inception-ResNet [5, 24]
with a batch size of 512 for training.

Evaluation results. MegaFace 1 million Challenge [8]
is a public identiï¬cation benchmark to test the perfor-
mance of facial identiï¬cation algorithms. The distractor
in MegaFace contains about 1, 000, 000 images. Here we
follow the cleaned testing protocol in [3]. The results
of P2SGrad on MegaFace dataset are shown in Table 3.
P2SGrad exceeds other compared cosine-based losses on
MegaFace 1 million challenge with every size of distractor.

carefully designed gradients. Extensive experiments vali-
date the robustness and fast convergence of the proposed
method. Moreover, experimental results show that P2SGrad
achieves superior performance over state-of-the-art meth-
ods on several challenging face recognition benchmarks.

Acknowledgements. This work is supported in part by
SenseTime Group Limited, in part by the General Research
Fund through the Research Grants Council of Hong
Kong under Grants CUHK14202217, CUHK14203118,
CUHK14205615, CUHK14207814, CUHK14213616,
CUHK14208417, CUHK14239816,
in part by CUHK
Direct Grant, and in part by National Natural Science
Foundation of China (61472410) and the Joint Lab of
CAS-HK.

5.3. Evaluation on IJBC 1:1 veriï¬cation

References

Preprocessing and training setting. Same as 5.2.
Evaluation results. The IJB-C dataset [16] contains
about 3, 500 identities with a total of 31, 334 still facial
images and 117, 542 unconstrained video frames. The en-
tire IJB-C testing protocols are designed to test detection,
identiï¬cation, veriï¬cation and clustering of faces.
In the
1:1 veriï¬cation protocol, there are 19, 557 positive matches
and 15, 638, 932 negative matches. Therefore we test Ture
Acceptance Rates at very strict False Acceptance Rates.
Table. 4 exhibits that P2SGrad surpasses all other cosine-
based losses.

6. Conclusion

we comprehensively discussed the limitation of the for-
ward and backward processes in training deep model for
face recognition. To deal with the limitations, we pro-
posed a simple but effective gradient method, P2SGrad,
which is hyperparameter free and leads to better optimiza-
tion results. Unlike previous methods which focused on loss
functions, we improve the deep network training by using

[1] Peter N. Belhumeur, JoËœao P Hespanha, and David J. Krieg-
man. Eigenfaces vs. ï¬sherfaces: Recognition using class
speciï¬c linear projection.
IEEE Transactions on pattern
analysis and machine intelligence, 19(7):711â€“720, 1997.

[2] S Chopra, R Hadsell, and Y Lecun. Learning a similarity
metric discriminatively, with application to face veriï¬cation.
In Computer Vision and Pattern Recognition, 2005. CVPR
2005. IEEE Computer Society Conference on, pages 539â€“
546 vol. 1, 2005.

[3] Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface:
Additive angular margin loss for deep face recognition. arXiv
preprint arXiv:1801.07698, 2018.

[4] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and
Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for
large-scale face recognition.
In European Conference on
Computer Vision, pages 87â€“102. Springer, 2016.

[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770â€“778, 2016.

[6] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 2017.

9913

[22] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang.
Deep learning face representation by joint identiï¬cation-
veriï¬cation. In Advances in neural information processing
systems, pages 1988â€“1996, 2014.

[23] Yi Sun, Ding Liang, Xiaogang Wang, and Xiaoou Tang.
Deepid3: Face recognition with very deep neural networks.
arXiv preprint arXiv:1502.00873, 2015.

[24] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning.
In AAAI, vol-
ume 4, page 12, 2017.

[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, Andrew Rabinovich, et al. Going deeper with
convolutions. Cvpr, 2015.

[26] Yaniv Taigman, Ming Yang, Marcâ€™Aurelio Ranzato, and Lior
Wolf. Deepface: Closing the gap to human-level perfor-
mance in face veriï¬cation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
1701â€“1708, 2014.

[27] Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng. Ad-
ditive margin softmax for face veriï¬cation. arXiv preprint
arXiv:1801.05599, 2018.

[28] Feng Wang, Xiang Xiang, Jian Cheng, and Alan L Yuille.
Normface: l 2 hypersphere embedding for face veriï¬cation.
arXiv preprint arXiv:1704.06369, 2017.

[29] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li,
Dihong Gong, Jingchao Zhou, and Wei Liu. Cosface: Large
margin cosine loss for deep face recognition. arXiv preprint
arXiv:1801.09414, 2018.

[30] Kilian Q Weinberger and Lawrence K Saul. Distance met-
ric learning for large margin nearest neighbor classiï¬cation.
Journal of Machine Learning Research, 10(Feb):207â€“244,
2009.

[31] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A
discriminative feature learning approach for deep face recog-
nition. In European Conference on Computer Vision, pages
499â€“515. Springer, 2016.

[32] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn-
arXiv preprint

ing face representation from scratch.
arXiv:1411.7923, 2014.

[33] Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, and
Yu Qiao. Range loss for deep face recognition with long-
tailed training data. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5409â€“
5418, 2017.

[7] Gary B Huang, Manu Ramesh, Tamara Berg, and Erik
Learned-Miller. Labeled faces in the wild: A database for
studying face recognition in unconstrained environments.
Technical report, Technical Report 07-49, University of Mas-
sachusetts, Amherst, 2007.

[8] Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel
Miller, and Evan Brossard. The megaface benchmark: 1
million faces for recognition at scale. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4873â€“4882, 2016.

[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiï¬cation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097â€“1105, 2012.

[10] Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, and
Shree K Nayar. Attribute and simile classiï¬ers for face veri-
ï¬cation. In Computer Vision, 2009 IEEE 12th International
Conference on, pages 365â€“372. IEEE, 2009.

[11] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha
Raj, and Le Song. Sphereface: Deep hypersphere embedding
for face recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 1, 2017.

[12] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang.
Large-margin softmax loss for convolutional neural net-
works. In ICML, pages 507â€“516, 2016.

[13] Yu Liu, Hongyang Li, and Xiaogang Wang. Learning deep
features via congenerous cosine loss for person recognition.
arXiv preprint arXiv:1702.06890, 2017.

[14] Yu Liu, Hongyang Li, and Xiaogang Wang. Rethinking fea-
ture discrimination and polymerization for large-scale recog-
nition. arXiv preprint arXiv:1710.00870, 2017.

[15] Yu Liu, Hongyang Li, Junjie Yan, Fangyin Wei, Xiaogang
Wang, and Xiaoou Tang. Recurrent scale approximation for
object detection in cnn. In IEEE International Conference
on Computer Vision, 2017.

[16] Brianna Maze, Jocelyn Adams, James A Duncan, Nathan
Kalka, Tim Miller, Charles Otto, Anil K Jain, W Tyler
Niggel, Janet Anderson, Jordan Cheney, et al.
Iarpa janus
benchmarkâ€“c: Face dataset and protocol. In 11th IAPR In-
ternational Conference on Biometrics, 2018.

[17] Aaron Nech and Ira Kemelmacher-Shlizerman. Level play-
In 2017 IEEE
ing ï¬eld for million scale face recognition.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3406â€“3415. IEEE, 2017.

[18] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al.

Deep face recognition. In BMVC, volume 1, page 6, 2015.

[19] Rajeev Ranjan, Ankan Bansal, Hongyu Xu, Swami Sankara-
narayanan, Jun-Cheng Chen, Carlos D Castillo, and Rama
Chellappa. Crystal loss and quality pooling for uncon-
strained face veriï¬cation and recognition. arXiv preprint
arXiv:1804.01159, 2018.

[20] Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-
constrained softmax loss for discriminative face veriï¬cation.
arXiv preprint arXiv:1703.09507, 2017.

[21] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniï¬ed embedding for face recognition and clus-
tering. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 815â€“823, 2015.

9914

