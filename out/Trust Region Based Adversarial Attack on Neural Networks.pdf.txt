Trust Region Based Adversarial Attack on Neural Networks

Zhewei Yao1 Amir Gholami1 Peng Xu2 Kurt Keutzer1 Michael W. Mahoney1

1University of California, Berkeley 2 Stanford Univeristy

1{zheweiy, amirgh, keutzer, and mahoneymw}@berkeley.edu, 2pengxu@stanford.edu

Abstract

Deep Neural Networks are quite vulnerable to adver-
sarial perturbations. Current state-of-the-art adversar-
ial attack methods typically require very time consuming
hyper-parameter tuning, or require many iterations to
solve an optimization based adversarial attack. To ad-
dress this problem, we present a new family of trust
region based adversarial attacks, with the goal of com-
puting adversarial perturbations eﬃciently. We propose
several attacks based on variants of the trust region op-
timization method. We test the proposed methods on
Cifar-10 and ImageNet datasets using several diﬀerent
models including AlexNet, ResNet-50, VGG-16, and
DenseNet-121 models. Our methods achieve comparable
results with the Carlini-Wagner (CW) attack, but with
signiﬁcant speed up of up to 37×, for the VGG-16 model
on a Titan Xp GPU. For the case of ResNet-50 on Im-
ageNet, we can bring down its classiﬁcation accuracy to
less than 0.1% with at most 1.5% relative L∞ (or L2)
perturbation requiring only 1.02 seconds as compared to
27.04 seconds for the CW attack. We have open sourced
our method which can be accessed at [1].

1. Introduction

Deep Neural Networks (DNNs) have achieved impres-
sive results in many research areas, such as classiﬁca-
tion, object detection, and natural language processing.
However, recent studies have shown that DNNs are of-
ten not robust to adversarial perturbation of the input
data [8, 26]. This has become a major challenge for
DNN deployment, and signiﬁcant research has been
performed to address this. These eﬀorts can be broadly
classiﬁed into three categories: (i) research into ﬁnding
strategies to defend against adversarial inputs (which
has so far been largely unsuccessful); (ii) new attack
methods that are stronger and can break the proposed
defense mechanisms; and (iii) using attack methods as
form of implicit adversarial regularization for training
neural networks [23, 28, 29]. Our interest here is mainly

Figure 1: An example of DeepFool, CW, and our
TR attack on AlexNet, with L2 norm. Both CW and
TR perturbations are smaller in magnitude and more
targeted than DeepFool’s (2× smaller here). TR attack
obtains similar perturbation as CW, but 15× faster. In
the case of the VGG-16 network, we achieve an even
higher speedup of 37.5× (please see Fig. 4 for timings).

focused on ﬁnding more eﬀective attack methods that
could be used in the latter two directions. Such ad-

111350

versarial attack methods can be broadly classiﬁed into
two categories: white-box attacks, where the model
architecture is known; and black-box attacks, where the
adversary can only perform a ﬁnite number of queries
and observe the model behaviour. In practice, white-
box attacks are often not feasible, but recent work has
shown that some adversarial attacks can actually trans-
fer from one model to the other [15]. Therefore, precise
knowledge of the target DNN may actually not be es-
sential. Another important ﬁnding in this direction is
the existence of an adversarial patch, i.e., a small set of
pixels which, if added to an image, can fool the network.
This has raised important security concerns for appli-
cations such as autonomous driving, where addition
of such an adversarial patch to traﬃc signs could fool
the system [3].

Relatedly, ﬁnding more eﬃcient attack methods is
important for evaluating defense strategies, and this is
the main focus of our paper. For instance, the seminal
work of [5] introduced a new type of optimization based
attack, commonly referred to as CW (Carlini-Wagner)
attack, which illustrated that defensive distillation [18]
can be broken with its stronger attack. The latter
approach had shown signiﬁcant robustness to the fast
gradient sign attack method [8], but not when tested
against the stronger CW attack. Three metrics for an
eﬃcient attack are the speed with which such a per-
turbation can be computed, the magnitude or norm
of the perturbation that needs to be added to the in-
put to fool the network, and the transferability of the
attack to other networks. Ideally (from the perspec-
tive of the attacker), a stronger attack with a smaller
perturbation magnitude is desired, so that it could be
undetectable (e.g. an adversarial patch that is harder
to detect by humans).

In this work, we propose a novel trust region based
attack method. Introduced in [6, 25], trust region (TR)
methods form a family of numerical optimization meth-
ods for solving non-convex optimization problems [17].
The basic TR optimization method works by ﬁrst deﬁn-
ing a region, commonly referred to as trust region,
around the current point in the optimization landscape,
in which a (quadratic) model approximation is used
to ﬁnd a descent/ascent direction. The idea of using
this conﬁned region is due to the model approximation
error. In particular, the trust region method is designed
to improve upon vanilla ﬁrst-order and second-order
methods, especially in the presence of non-convexity.

We ﬁrst consider a ﬁrst-order TR method, which
uses gradient information for attacking the target DNN
model and adaptively adjusts the trust region. The
main advantage of ﬁrst-order attacks is their computa-
tional eﬃciency and ease of implementation. We show

that our ﬁrst-order TR method signiﬁcantly reduces
the over-estimation problem (i.e. requiring very large
perturbation to fool the network), resulting in up to
3.9× reduction in the perturbation magnitude, as com-
pared to DeepFool [16]. Furthermore, we show TR is
signiﬁcantly faster than the CW attack (up to 37×),
while achieving similar attack performance. We then
propose an adaptive TR method, where we adaptively
choose the TR radius based on the model approxima-
tion to further speed up the attack process. Finally, we
present the formulation for how our basic TR method
could be extended to a second-order TR method, which
could be useful for cases with signiﬁcant non-linear de-
cision boundaries, e.g., CNNs with Swish activation
function [20]. In more detail, our main contributions
are the following:

• We cast the adversarial attack problem into the
optimization framework of TR methods. This en-
ables several new attack schemes, which are easy
to implement and are signiﬁcantly more eﬀective
than existing attack methods (up to 3.9×, when
compared to DeepFool). Our method requires a
similar perturbation magnitude, as compared to
CW, but it can compute the perturbation signif-
icantly faster (up to 37×), as it does not require
extensive hyper-parameter tuning.

• Our TR-based attack methods can adaptively
choose the perturbation magnitude in every itera-
tion. This removes the need for expensive hyper-
parameter tuning, which is a major issue with the
existing optimization based methods.

• Our method can easily be extended to second-order
TR attacks, which could be useful for non-linear
activation functions. With fewer iterations, our
second-order attack method outperforms the ﬁrst-
order attack method.

Limitations. We believe that it is important for
every work to state its limitations (in general, but in
particular in this area). We paid special attention to re-
peat the experiments multiple times, and we considered
multiple diﬀerent DNNs on diﬀerent datasets to make
sure the results are general. One important limitation
of our approach is that a naïve implementation of our
second-order method requires computation of Hessian
matvec backpropogation, which is very expensive for
DNNs. Although the second-order TR attack achieves
better results, as compared to the ﬁrst-order TR attack,
this additional computational cost could limit its use-
fulness in certain applications. Moreover, our method
achieves similar results as CW attack, but signiﬁcantly
faster. However, if we ignore the strength of the attack,

11351

Figure 2: An example of DeepFool and TR attack on VGG-16, with L∞ norm. The ﬁrst pattern is the original
image. The second pattern is the image after TR attack. The ﬁnal two patterns are the perturbations generated by
DeepFool and TR. The TR perturbation is smaller than DeepFool’s (1.9× smaller). Also, the TR perturbation is
more concentrated around the butterﬂy.

then the DeepFool attack is faster than our method
(and CW’s for that matter). Although such comparison
may not be fair, as our attack is stronger. However,
this may be an important point for certain applications
where maximum speed is needed.

2. Background

In this section, we review related work on adversarial
attacks. Consider x ∈ Rn as input image, and y ∈ Rc
the corresponding label. Suppose M(x; θ) = ˆy is the
DNN’s prediction probabilities, with θ the model param-
eters and ˆy ∈ Rc the vector of probabilities. We denote
the loss function of a DNN as L(x, θ, y). Then, an ad-
versarial attack is aimed to ﬁnd a (small) perturbation,
∆x, such that:

arg max(M(x + ∆x; θ)) = arg max(ˆy) 6= arg max(y).

There is no closed form solution to analytically com-
pute such a perturbation. However, several diﬀerent
approaches have been proposed by solving auxiliary op-
timization or analytical approximations to solve for
the perturbation. For instance, the Fast Gradient
Sign Method (FGSM) [8] is a simple adversarial at-
tack scheme that works by directly maximizing the loss
function L(x, θ, y). It was subsequently extended to an
iterative FGSM [13], which performs multiple gradient
ascend steps to compute the adversarial perturbation,
and is often more eﬀective than FGSM in attacking
the network. Another interesting work in this direc-
tion is DeepFool, which uses an approximate analytical
method. DeepFool assumes that the neural network
behaves as an aﬃne multiclass classiﬁer, which allows
one to ﬁnd a closed form solution. DeepFool is based
on “projecting” perturbed inputs to cross the decision
boundary (schematically shown in Fig. 3), so that its

classiﬁcation is changed, and this was shown to outper-
form FGSM. However, the landscape of the decision
boundary of a neural network is not linear. This is the
case even for ReLU networks with the softmax layer.
Even before the softmax layer, the landscape is piece-
wise linear, but this cannot be approximated with a
simple aﬃne transformation. Therefore, if we use the
local information, we can overestimate/underestimate
the adversarial perturbation needed to fool the network.
The seminal work of [5] introduced the so-called CW
attack, a more sophisticated way to directly solve for
the ∆x perturbation. Here, the problem is cast as an
optimization problem, where we seek to minimize the
distance between the original image and the perturbed
one, subject to the constraint that the perturbed input
would be misclassiﬁed by the neural network. This work
also clearly showed that defensive distillation, which at
the time was believed to be a robust method to defend
against adversaries, is not robust to stronger attacks.
One major disadvantage of the CW attack is that it is
very sensitive to hyper-parameter tuning. This is an
important problem in applications where speed is impor-
tant, as ﬁnding a good/optimal adversarial perturbation
for a given input is very time consuming. Addressing
this issue, without sacriﬁcing attack strength, is a goal
of our work.

On another direction, adversarial training has been
used as a defense method against adversarial at-
tacks [23]. In particular, by using adversarial examples
during training, one can obtain models that are more
robust to attacks (but still not foolproof). This adver-
sarial training was further extended to ensemble adver-
sarial training [27], with the goal of making the model
more robust to black box attacks. Other approaches
have also been proposed to detect/defend against ad-
versarial attacks [14, 18]. However, it has recently been

11352

Decision Boundary

Figure 3: Schematic illustration of the decision bound-
aries for a classiﬁcation problem. Points mapped to the
hashed region, are classiﬁed with the same label.

shown that, with a stronger attack method, defense
schemes such as distillation or obfuscated gradients can
be broken [2, 4, 5].

A ﬁnal important application of adversarial attacks
is to train neural networks to obtain improved general-
ization, even in non-adversarial environments. Multi-
ple recent works have shown that adversarial training
(speciﬁcally, training with mixed adversarial and clean
data) can be used to train a neural network from scratch
in order to achieve a better ﬁnal generalization perfor-
mance [22, 23, 28, 29]. In particular, the work of [29]
empirically showed that using adversarial training would
result in ﬁnding areas with “ﬂatter” curvature. This
property has recently been argued to be an important
parameter for generalization performance [11]. Here,
the speed with which adversarial perturbations can be
computed is very important since it appears in the inner
loop of the training process, and the training needs to
be performed for many epochs.

3. Trust Region Adversarial Attack

Let us denote the output of the DNN before the
softmax function to be z = Z(x; θ) ∈ Rc. Therefore, we
will have:

M(x; θ) = sof tmax(cid:0)Z(x; θ)(cid:1) = ˆy.

Denote yt to be the true label of x, and zt = arg max z
to be the prediction output of M(x; θ). For clariﬁcation,
note that zt is only the same as yt if the neural network
makes the correct classiﬁcation. An adversarial attack
seeks to ﬁnd ∆x that fools the DNN, that is:

arg min
k∆xkp

arg max Z(x + ∆x; θ) 6= yt,

(1)

where k · kp denotes the Lp norm of a vector.
It is
often computationally infeasible to solve (1) exactly.

Algorithm 1: Trust Region Attack

input

output

: Image x0, label y, initial radius ǫ0,
threshold σ1 and σ2, radius
adjustment rate η
: Adversarial Image

∆x = 0, j = 0;
Using scheme to choose the attacking index i;

// Initialization

// Index Selection
while arg max Z(xj) = arg max y do

t,ii + 1

∆xtmp = arg mink∆xj k≤ǫj mj(∆xj) =
arg mink∆xj k≤ǫj h∆xj, gj
xj+1 = clip(xj + ∆xtmp, min(x), max(x));
// Update
j+1
ρ = (z
t −z
if ρ > σ1 then

2 h∆xj, Hj

j+1
i
mj (∆xj )

j+1
t −z

j+1
i

)−(z

;

)

// Compute Ratio

t,i∆xji

ǫj+1 = min{ηǫj, ǫmax}

else if ρ < σ2 then

ǫj+1 = min{ǫj/η, ǫmin}

else

ǫj+1 = ǫj

j = j + 1

Therefore, a common approach is to approximately
solve (1) [5, 8, 26]. To do so, the problem can be
formulated as follows:

max

k∆xkp≤ǫ

J (x + ∆x, θ, y),

(2)

where ǫ constrains the perturbation magnitude, and J
can be either the loss function (L) or more generally
another kernel [5]. In the case of DeepFool (DF) attack,
this problem is solved by approximating the decision
boundary by a linear aﬃne transformation. For such a
decision boundary, the perturbation magnitude could be
analytically computed by just evaluating the gradient
at the current point. However, for neural networks
this approximation could be very inaccurate, that is it
could lead to over/under-estimation of the perturbation
along a sub-optimal direction. The smallest direction
would be orthogonal to the decision boundary, and this
cannot be computed by a simple aﬃne transformation,
as the decision boundary is non-linear (see Fig. 3 for
illustration). This is obvious for non-linear activation
functions, but even in the case of ReLU, the model
behaves like a piece-wise linear function (before the
softmax layer, and actually non-linear afterwards). This
approach cannot correctly ﬁnd the orthogonal direction,
even if we ignore the non-linearity of the softmax layer.
To address this limitation, we instead use TR meth-
ods, which are well-known for solving non-convex opti-
mization problems [25]. The problem of ﬁnding adver-

11353

Figure 4: The two subﬁgures show, for various neural networks, the time to compute the adversarial attack
(x-axis) and the perturbation needed by that attack method to fool an image (y-axis), corresponding to ImageNet
results in Table 3. On the left, the y-axis is plotted for average perturbation; and on the right, for the worst case
perturbation. An attack that achieves smaller perturbation in shorter time is preferred. Diﬀerent colors represent
diﬀerent models, and diﬀerent markers illustrate the diﬀerent attack methods. Observe that our TR and TR Adap
methods achieve similar perturbations as CW but with signiﬁcantly less time (up to 37.5×).

sarial perturbation using TR is deﬁned as follows:

max

k∆xj kp≤ǫj

mj(∆xj) = h∆xj, gj

t,ii +

1
2

h∆xj, Hj

t,i∆xji,

i

) with gj

t,i and Hj

(3)
where ǫj is the TR radius at jth iteration, mj is the
approximation of the kernel function of f (xj−1) =
(zj−1
t − zj−1
t,i denoting the corre-
sponding gradient and Hessian, and xj = x+Pj−1
i=1 ∆xi.
Note that the subscript means the index of maximal
z, i.e. arg max z. The main idea of the TR method is
to iteratively select the trusted radius ǫj to ﬁnd the
adversarial perturbation within this region such that
the probability of an incorrect class becomes maximum.
TR adjusts this radius by measuring the approxima-
tion of the local model mj(sj) to the actual function
value f (xj+1) − f (xj). In particular, we increase the
trusted radius if the approximation of the function is
accurate (measured by ρ = f (x
> σ1 with a
typical value of σ1 = 0.9). In such a case, the trusted
radius is increased for the next iterations by a factor
of η > 1 (ǫj+1 = ηǫt). However, when the local model
mj(sj) is a poor approximation of f (xj+1) − f (xj),
i.e., ρ = f (x
< σ2 (with a typical σ2 = 0.5),
we decrease the trusted radius for the next iteration
ǫj+1 = ǫt/η. Otherwise, we keep the same ǫj for ǫj+1.
Typically, a threshold is also used for lower and upper
bounds of ǫj. Using this approach, the TR attack can
iteratively ﬁnd an adversarial perturbation to fool the
network. See Alg. 1 for details.

j+1)−f (x
mj (sj )

j+1)−f (x
mj (sj )

j )

j )

Note that for cases where all the activations of
the DNN are ReLU, the Hessian is zero almost ev-

erywhere [29, Theorem 1], and we actually do not need
the Hessian. This means the landscape of zt − zi is
piece-wise linear, i.e., we could omit Hj
t,i in (3). How-
ever, for non-linear activation functions, we need to
keep the Hessian term (since when the NN has smooth
activation functions, the Hessian is not zero). For these
cases, the problem of ﬁnding the adversarial pertur-
bation becomes a Quadratic Constrained Quadratic
Programming (QCQP) problem. It is quadratic con-
straint due to the fact that the norm of the perturbation
is limited by the TR radius, ηj, and the quadratic pro-
gramming arises from the non-zero Hessian term. We
use Lanczos algorithm to solve the QCQP problem. In
this approach, the solution is iteratively found in a
Krylov subspace formed by the Hessian operator.

4. Performance of the Method

To test the eﬃcacy of the TR attack method and
to compare its performance with other approaches, we
perform multiple experiments using diﬀerent models on
Cifar-10 [12] and ImageNet [7] datasets. In particular,
we compare to DeepFool [16], iterative FGSM [8, 13],
and the Carlini-Wagner (CW) attack [5].

As mentioned above, the original TR method adap-
tively selects the perturbation magnitude. Here, to test
how eﬀective the adaptive method performs, we also
experiment with a case where we set the TR radius to
be a ﬁxed small value and compare the results with the
original adaptive version. We refer to the ﬁxed radius
version as "TR Non-Adap" and the adaptive version as
"TR Adap". Furthermore, the metric that we use for
performance of the attack is the relative perturbation,

11354

0102030405060Per Image Attack Time (s)0.150.200.250.30Average Pertubation (%)AlexNetVGG16ResNet50DenseNet121CWDPTR Non-AdapTR Adap0102030405060Per Image Attack Time (s)1.01.52.02.53.03.54.04.5Worst Pertubation (%)AlexNetVGG16ResNet50DenseNet121CWDPTR Non-AdapTR AdapTable 1: Average perturbations / worst case perturbations are reported of diﬀerent models on Cifar-10 for best
class attack.. Lower values are better. The ﬁrst set of rows show L2 attack and the second shows L∞ attack.

Model

Accuracy

ρ2

DeepFool

CW

ρ2

TR Non-Adap

TR Adap

ρ2

ρ2

AlexLike
AlexLike-S
ResNet
WResNet

85.78
86.53
92.10
94.77

1.67% / 11.5% 1.47% / 9.70% 1.49% / 9.13% 1.49% / 9.09%
1.74% / 11.0% 1.57% / 8.59% 1.57% / 9.48% 1.57% / 9.46%
0.80% / 5.60% 0.62% / 3.12% 0.66% / 3.97% 0.66% / 3.96%
0.89% / 5.79% 0.66% / 4.51% 0.73% / 4.43% 0.72% / 4.34%

DeepFool

FGSM

TR Non-Adap

TR Adap

Model

Accuracy

ρ∞

ρ∞

ρ∞

ρ∞

AlexLike
AlexLike-S
ResNet
WResNet

85.78
86.53
92.10
94.77

1.15% / 6.85% 1.40% / 16.44% 1.05% / 5.45% 1.03% / 5.45%
1.18% / 6.01% 1.45% / 14.88% 1.09% / 4.76% 1.07% / 4.73%
0.60% / 3.98% 0.85% / 4.35% 0.56% / 3.18% 0.50% / 3.35%
0.66% / 3.34% 0.85% / 3.30% 0.56% / 2.67% 0.54% / 2.69%

deﬁned as follows:

4.1. Setup

ρp =

k∆xkp
kxkp

,

(4)

where ∆x is the perturbation needed to fool the testing
example. The perturbation is chosen such that the
accuracy of the model is reduced to less than 0.1%. We
report both the average perturbation as well as the
highest perturbation required to fool a testing image.
To clarify this, the highest perturbation is computed
after all of testing images (50K in ImageNet and 10K in
Cifar-10) and then ﬁnding the the highest perturbation
magnitude that was needed to fool a correctly classiﬁed
example. We refer to this case as worst case perturba-
tion. Ideally we would like this worst case perturbation
to be bounded and close to the average cases.

Table 2: Average perturbations / worst case perturba-
tions are reported of diﬀerent models on Cifar-10 for
hardest class attack. Lower values are better. The ﬁrst
set of rows show L2 attack and the second shows L∞
attack.

DeepFool

TR Non-Adap

TR Adap

Model

ρ2

ρ2

ρ2

AlexLike
4.36% /18.9% 2.47% /13.4% 2.47% /13.4%
AlexLike-S 4.70% /17.7% 2.63% /14.4% 2.62% /14.2%
1.71% /8.01% 0.99% /4.76% 0.99% /4.90%
ResNet
1.80% /8.74% 1.05% /6.23% 1.08% /6.23%
WResNet

Model

ρ∞

ρ∞

ρ∞

AlexLike
2.96% /12.6% 1.92% /9.99% 1.86% /10.0%
AlexLike-S 3.12% /12.2% 1.98% /8.19% 1.92% /8.17%
ResNet
1.34% /9.65% 0.77% /4.70% 0.85% /5.44%
1.35% /6.49% 0.81% /3.77% 0.89% /3.90%
WResNet

We consider multiple diﬀerent neural networks includ-
ing variants of (wide) residual networks [9, 30], AlexNet,
VGG16 [24], and DenseNet from [10]. We also test with
custom smaller/shallower convolutional networks such
as a simple CNN [29, C1] (refer as AlexLike with ReLU
and AlexLike-S with Swish activation). To test the
second order attack method we run experiments with
AlexNet-S (by replacing all ReLUs with Swish func-
tion activation function [19]), along with a simple MLP
(3072 → 1024 → 512 → 512 → 256 → 10) with Swish
activation function.

By deﬁnition, an adversarial attack is considered
successful if it is able to change the classiﬁcation of the
input image. Here we perform two types of attacks. The
ﬁrst one is where we compute the smallest perturbation
needed to change the target label. We refer to this as
best class attack. This means we attack the class with:

arg min

j

zt − zj

k∇x(zt − zj)k

.

Intuitively, this corresponds to perturbing the input to
cross the closest decision boundary (Fig. 3). On the
other hand, we also consider perturbing the input to
the class whose decision boundary is farthest away:

arg max

j

zt − zj

k∇x(zt − zj)k

.

Furthermore, we report two perturbation metrics of
average perturbation, computed as:

ρp =

1
N

N

X

i=1

k∆xikp
kxikp

,

11355

Table 3: Average perturbations / worst case perturbations are reported of diﬀerent models on ImageNet for best
class attack. Lower values are better. The ﬁrst set of rows show L2 attack and the second shows L∞ attack.

Model

Accuracy

ρ2

DeepFool

CW

ρ2

TR Non-Adap

TR Adap

ρ2

ρ2

AlexNet
VGG16
ResNet50
DenseNet121

56.5
71.6
76.1
74.4

0.20% / 4.1% 0.31% / 1.8% 0.17% / 2.5% 0.18% / 3.3%
0.14% / 4.4% 0.16% / 1.1% 0.12% / 1.2% 0.12% / 3.8%
0.17% / 3.0% 0.19% / 1.1% 0.13% / 1.5% 0.14% / 2.3%
0.15% / 2.5% 0.20% / 1.5% 0.12% / 1.3% 0.13% / 1.7%

DeepFool

FGSM

TR Non-Adap

TR Adap

Model

Accuracy

ρ∞

ρ∞

ρ∞

ρ∞

AlexNet
VGG16
ResNet50
DenseNet121

56.5
71.5
76.1
74.4

0.14% / 4.3% 0.16% / 4.7% 0.13% / 1.4% 0.13% / 3.6%
0.11% / 4.0% 0.18% / 5.1% 0.10% / 1.4% 0.10% / 3.4%
0.13% / 3.2% 0.18% / 3.7% 0.11% / 1.3% 0.11% / 2.7%
0.11% / 2.3% 0.15% / 4.1% 0.10% / 1.1% 0.10% / 1.8%

along with worst perturbation, computed as:

ρp = max{

k∆xikp
kxikp

}N
i=1.

For comparison, we also consider the following attack
methods:

• Iterative FGSM from [8, 13], where the following
formula is used to compute adversarial perturba-
tion, after which the perturbation is clipped in
range (min(x), max(x)):

xj+1 = xj + ǫ sign(∇xL(xj, θ, y)),

• DeepFool (DF) from [16]. We follow the same
implementation as [16]. For the hardest class test,
the target class is set the same as our TR method.

• CW attack from [5]. We use the open source code

from [21]1.

Finally, we measure the time to fool an input image
by averaging the attack time over all the testing exam-
ples. The measurements are performed on a Titan Xp
GPU with an Intel E5-2640 CPU.

4.2. Cifar-10

We ﬁrst compare diﬀerent attacks of various neural
network models on Cifar-10 dataset, as reported in Ta-
ble 1. Here, we compute the average and worst case
perturbation for best class attack. For L2 attack, we can
see that TR Non-Adap can achieve comparable pertur-
bation as CW, with both TR and CW requiring smaller
perturbation than DeepFool. An important advantage

1https://github.com/bethgelab/foolbox

Table 4: Average perturbations / worst case perturba-
tions are reported of diﬀerent models on ImageNet for
hardest class attack (on the top 100 prediction classes).
Lower values are better. The ﬁrst set of rows show L2
attack and the second shows L∞ attack.

Model

AlexNet
VGG16
ResNet50
DenseNet

Model

DeepFool

TR Non-Adap

TR Adap

ρ2

ρ2

ρ2

0.74% /8.7% 0.39% /5.0% 0.39% /5.0%
0.45% /5.4% 0.27% /3.6% 0.27% /3.8%
0.52% /5.8% 0.31% /4.2% 0.31% /4.2%
0.48% /5.7% 0.29% /3.8% 0.29% /3.8%

ρ∞

ρ∞

ρ∞

AlexNet
VGG16
ResNet50
DenseNet

0.53% /9.9% 0.31% /7.5% 0.33% /9.1%
0.36% /11.6% 0.25% /5.1% 0.26% /6.8%
0.43% /6.6% 0.28% /3.7% 0.30% /4.6%
0.38% /6.4% 0.24% /4.5% 0.27% /5.7%

of the TR attack is its speed, as compared to CW attack,
which is illustrated in Fig. ?? (please see appendix).
Here we plot the time spent to fool one input image
versus average perturbation for all L2 attack methods
on diﬀerent models. It can be clearly seen that, with
similar perturbations, the time to get the adversarial
examples is: TR < CW. Note that DeepFool is also very
fast but requires much larger perturbations than TR
attack and CW. Also note that the TR Adap method
achieves similar results, with slightly slower speed and
slightly larger perturbation. This is because the adap-
tive method has not been tuned any way, whereas for
the non adaptive version we manually tuned ǫ. TR
Adap does not require tuning, as it automatically ad-
just the TR radius. The slight performance degradation
is due to the relaxed σ1 and σ2 parameters, which could

11356

Table 5: Second order and ﬁrst order comparison on MLP and AlexNet with Swish activation function on Cifar-10.
The corresponding baseline accuracy without adversarial perturbation is 62.4% and 76.6%, respectively. As expected,
the second order TR attack achieves better results as compared to ﬁrst-order with ﬁxed iterations. However, the
second-order attack is signiﬁcantly more expensive, due to the overhead of solving QCQP problem.

Iter

MLP
MLP

1

2

3

4

5

6

7

8

9

10

TR First
TR Second 47.84 33.37 21.49

22.24 13.76 8.13 4.59 2.41 1.31 0.63 0.27
7.39 4.16 2.17 1.09 0.49 0.20

47.63

33.7

13.3

51.51 28.17 12.45
AlexNet TR First
AlexNet TR Second 50.96 26.97 10.73

5.53
4.11

2.61 1.33 0.82 0.66 0.51 0.46
1.79 0.91 0.67 0.54 0.47 0.44

be made more conservative as a trade-oﬀ for speed. But
we did not tune these parameters beyond the default, to
give a realistic performance for the non-tuned version.
Another important test is to measure the perturba-
tion needed to fool the network to the hardest target
class. This is important in that ﬂipping a pedestrian
to a cyclist may be easier than ﬂipping it to become a
car. In Table 2, we report the hardest class attack on
Cifar-10. Note that our methods are roughly 1.5 times
better than DeepFool in all cases. Particularly, For L2
attack on WResNet, our worst case is 3.9 times better
than DeepFool in terms of perturbation magnitude.

4.3. ImageNet Result

We observe similar trends on ImageNet. We report
diﬀerent attacks on various models on ImageNet in
Table 3. Note that TR and CW require signiﬁcantly
smaller perturbation for the worst case as compared to
DeepFool. However, TR is signiﬁcantly faster than CW.
The timing results are shown in Fig. 4. For instance in
the case of VGG-16, TR attack is 37.5× faster than
CW which is signiﬁcant. An example perturbation
with AlexNet is shown in Fig. 1 (for which TR is 15×
faster). As one can see, CW and TR perturbations
are smaller than DeepFool (2× in this case), and more
targeted around the object. For L∞ methods, our
TR Non-Adap and TR Adap are consistently better
than FGSM and DeepFool in both average and worst
cases. Particularly, for worst cases, TR is roughly two
times better than the other methods. An example
perturbation of DeepFool and TR Non-Adap with L∞
on VGG16 is shown in Fig. 2. It can be clearly seen that,
TR perturbation is much smaller than DeepFool (1.9×
in this case), and more targeted around the objective.

4.4. Second order method

As mentioned in Section 3, the ReLU activation func-
tion does not require Hessian computation. However,
for non-linear activation functions including Hessian
information is beneﬁcial, although it may be very ex-
pensive. To test this hypothesis, we consider two models

with Swish activation function. We ﬁx the TR radius
(set to be 1 for all cases) of our ﬁrst and second order
methods, and gradually increase the iterations. Table 5
shows the results for MLP and AlexNet models. It can
be seen that second order TR out-performs the ﬁrst
order TR method in all iterations. Particularly, for
two and three iterations on AlexNet, TRS can drop
the model accuracy 1.2% more as compared to the ﬁrst
order TR variant. However, the second order based
model is more expensive than the ﬁrst order model,
mainly due to the overhead associated with solving the
QCQP problem. There is no closed form solution for
this problem because the problem is non-convex and
the Hessian can contain negative spectrum. Develop-
ing a computationally eﬃcient method for this is an
interesting next direction.

5. Conclusions

We have considered various TR based methods for ad-
versarial attacks on neural networks. We presented the
formulation for the TR method along with results for
our ﬁrst/second-order attacks. We considered multiple
models on Cifar-10 and ImageNet datasets, including
variants of residual and densely connected networks.
Our method requires signiﬁcantly smaller perturbation
(up to 3.9×), as compared to DeepFool. Furthermore,
we achieve similar results (in terms of average/worst
perturbation magnitude to fool the network), as com-
pared to the CW attack, but with signiﬁcant speed up
of up to 37.5×. For all the models considered, our at-
tack method can bring down the model accuracy to less
than 0.1% with relative small perturbation (in L2/L∞
norms) of the input image. Meanwhile, we also tested
the second order TR attack by backpropogating the Hes-
sian information through the neural network, showing
that it can ﬁnd a stronger attack direction, as compared
to the ﬁrst order variant.

11357

References

[1] https://github.com/amirgholami/trattack, November

2018.

[2] Anish Athalye, Nicholas Carlini, and David Wagner.
Obfuscated gradients give a false sense of security: Cir-
cumventing defenses to adversarial examples. arXiv
preprint arXiv:1802.00420, 2018.

[3] Tom B Brown, Dandelion Mané, Aurko Roy, Martín
Abadi, and Justin Gilmer. Adversarial patch. arXiv
preprint arXiv:1712.09665, 2017.

[4] Nicholas Carlini and David Wagner. Adversarial exam-
ples are not easily detected: Bypassing ten detection
methods. In Proceedings of the 10th ACM Workshop on
Artiﬁcial Intelligence and Security, pages 3–14. ACM,
2017.

[5] Nicholas Carlini and David Wagner. Towards evaluat-
ing the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pages 39–57.
IEEE, 2017.

[6] Andrew R Conn, Nicholas IM Gould, and Ph L Toint.

Trust region methods, volume 1. Siam, 2000.

[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. Imagenet: A large-scale hierarchi-
cal image database. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on,
pages 248–255. Ieee, 2009.

[14] Jan Hendrik Metzen, Tim Genewein, Volker Fischer,
and Bastian Bischoﬀ. On detecting adversarial pertur-
bations. arXiv preprint arXiv:1702.04267, 2017.

[15] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
Omar Fawzi, and Pascal Frossard. Universal adversarial
perturbations. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 86–94.
IEEE, 2017.

[16] Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and
Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In Proceedings
of 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), number EPFL-CONF-
218057, 2016.

[17] J. Nocedal and S. Wright. Numerical Optimization.

Springer, New York, 2006.

[18] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh
Jha, and Ananthram Swami. Distillation as a defense to
adversarial perturbations against deep neural networks.
In 2016 IEEE Symposium on Security and Privacy
(SP), pages 582–597. IEEE, 2016.

[19] Prajit Ramachandran, Barret Zoph, and Quoc V Le.
Swish: a self-gated activation function. arXiv preprint
arXiv:1710.05941, 2017.

[20] Prajit Ramachandran, Barret Zoph, and Quoc V Le.
arXiv preprint

Searching for activation functions.
arXiv:1710.05941, 2018.

[8] Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial ex-
amples. International Conference on Learning Repre-
sentations (arXiv:1412.6572), 2015.

[21] Jonas Rauber, Wieland Brendel, and Matthias Bethge.
Foolbox v0. 8.0: A python toolbox to benchmark the
robustness of machine learning models. arXiv preprint
arXiv:1707.04131, 2017.

[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016.

[22] Swami Sankaranarayanan, Arpit Jain, Rama Chellappa,
and Ser Nam Lim. Regularizing deep networks using
eﬃcient layerwise adversarial training. arXiv preprint
arXiv:1705.07819, 2017.

[10] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and
Kilian Q Weinberger. Densely connected convolutional
networks. In CVPR, volume 1, page 3, 2017.

[11] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge
Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Gen-
eralization gap and sharp minima. arXiv preprint
arXiv:1609.04836, 2016.

[12] Alex Krizhevsky and Geoﬀrey Hinton. Learning mul-
tiple layers of features from tiny images. Technical
report, Citeseer, 2009.

[13] Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
Adversarial examples in the physical world. arXiv
preprint arXiv:1607.02533, 2016.

[23] Uri Shaham, Yutaro Yamada, and Sahand Negahban.
Understanding adversarial training: Increasing local
stability of supervised models through robust optimiza-
tion. Neurocomputing, 2018.

[24] Karen Simonyan and Andrew Zisserman. Very deep
convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.

[25] Trond Steihaug. The conjugate gradient method and
trust regions in large scale optimization. SIAM Journal
on Numerical Analysis, 20(3):626–637, 1983.

[26] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv
preprint arXiv:1312.6199, 2013.

11358

[27] Florian Tramèr, Alexey Kurakin, Nicolas Papernot,
Ian Goodfellow, Dan Boneh, and Patrick McDaniel.
Ensemble adversarial training: Attacks and defenses.
arXiv preprint arXiv:1705.07204, 2017.

[28] Zhewei Yao, Amir Gholami, Kurt Keutzer, and
Michael W. Mahoney. Large batch size training of
neural networks with adversarial training and second-
order information. arXiv preprint arXiv:1810.01021,
2018.

[29] Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and
Michael W Mahoney. Hessian-based analysis of large
batch training and robustness to adversaries. Neural
Information Processing Systems (NIPS’18), 2018.

[30] Sergey Zagoruyko and Nikos Komodakis. Wide residual

networks. arXiv preprint arXiv:1605.07146, 2016.

11359

