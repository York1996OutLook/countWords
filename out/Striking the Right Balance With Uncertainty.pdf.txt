Striking the Right Balance with Uncertainty

Salman Khanâˆ— Munawar Hayatâˆ—

Syed Waqas Zamir

Jianbing Shenâ€ 

Ling Shao

Inception Institute of Artiï¬cial Intelligence, UAE

firstname.lastname@inceptioniai.org

Figure 1: Imbalance learning with Bayesian
uncertainty estimates. (a) We enforce class-
level margin penalty based on class uncer-
tainty. This pushes boundaries further away
from rare classes B and C. (b) We also con-
sider sample-level uncertainty that is mod-
eled as a Gaussian distribution. The learned
margins consider the conï¬dence level of
classiï¬er to re-adjust boundaries (i.e., pro-
vide more room to uncertain samples). This
improves the generalization ability of the
proposed model for imbalanced classes.

Abstract

1. Introduction

Learning unbiased models on imbalanced datasets is a
signiï¬cant challenge. Rare classes tend to get a concen-
trated representation in the classiï¬cation space which ham-
pers the generalization of learned boundaries to new test
examples. In this paper, we demonstrate that the Bayesian
uncertainty estimates directly correlate with the rarity of
classes and the difï¬culty level of individual samples. Sub-
sequently, we present a novel framework for uncertainty
based class imbalance learning that follows two key in-
sights: First, classiï¬cation boundaries should be extended
further away from a more uncertain (rare) class to avoid
over-ï¬tting and enhance its generalization. Second, each
sample should be modeled as a multi-variate Gaussian dis-
tribution with a mean vector and a covariance matrix de-
ï¬ned by the sampleâ€™s uncertainty. The learned boundaries
should respect not only the individual samples but also their
distribution in the feature space. Our proposed approach
efï¬ciently utilizes sample and class uncertainty informa-
tion to learn robust features and more generalizable classi-
ï¬ers. We systematically study the class imbalance problem
and derive a novel loss formulation for max-margin learn-
ing based on Bayesian uncertainty measure. The proposed
method shows signiï¬cant performance improvements on six
benchmark datasets for face veriï¬cation, attribute predic-
tion, digit/object classiï¬cation and skin lesion detection.

âˆ—Equal contribution, â€ Corresponding author

Objects, events, actions and visual concepts appear with
varying frequencies in real world imagery [38]. This of-
ten leads to highly skewed datasets where a few abundant
classes outnumber several rare classes in a typical long-
tail data distribution. The low amount of training data for
infrequent classes makes it challenging to learn optimal
classiï¬cation boundaries in the feature space. Existing ap-
proaches to tackle class imbalance either modify data distri-
bution [42, 7, 22] or introduce appropriate costs to re-weight
class errors [25, 1, 40]. The popular data-level approaches
are prone to over-ï¬tting while the cost-sensitive learning
requires careful choice of weights for successful training.
Despite an overwhelming success of deep neural networks
on computer vision problems, learning from highly imbal-
anced sets is still an open problem for deep learning [24].

This paper proposes a new direction towards learn-
ing balanced representations using deep neural networks
(Fig. 1). We use a principled approach to integrate Bayesian
uncertainty estimates for class imbalance learning at two
distinct levels, i.e., category-level and individual sample-
level. Our approach is based on the observation that rare
classes have higher uncertainty in the prediction space and
the associated classiï¬er conï¬dence levels are low. There-
fore, the uncertainty estimates can be used to expand deci-
sion regions for less frequent classes so that classiï¬erâ€™s gen-
eralization to new test examples is improved. This concept
is illustrated in Fig. 1. Since all samples within a class do

1103

Class CClass BClass ABoundaries learnedusingsoft-max lossBoundaries learnedwith our cost functionSample distributionClass density distributionClass A SampleClass B SampleClass uncertaintynot have a uniform difï¬culty level, our approach also op-
timizes margins with respect to the uncertainty associated
with individual samples. The basic intuition for both cases
is the same: a classiï¬er should assign larger regions to more
uncertain (rare) samples/classes.
Related work: State-of-the-art deep imbalance learning
methods mainly propose novel objective functions [17].
Khan et al. [25] presented a cost-sensitive loss for CNNs
where class-speciï¬c weights were automatically learned.
Huang et al. [21] suggested a combination of triplet and
quintuplet losses to preserve local class structures. A mod-
iï¬ed softmax was proposed in [31] to maximize the angu-
lar margin, thus avoiding class imbalance. Quite recently,
a meta-learning approach in [40] used selective instances
for training on imbalanced sets. These methods have their
respective limitations, e.g., [25] only considers class-level
costs, [21] is not differentiable and requires heavy pre-
processing for quintuplet creation, [31] can only maximize
margin on the hypersphere surface and [40] used an addi-
tional validation set to assign sample weights. Concurrent
to this work, [10] re-weights the loss by the inverse effective
number of samples to learn balanced representations.
Contributions: Our approach is distinct in two ways: (a)
this is the ï¬rst work to link class imbalance with Bayesian
uncertainty estimates [16], that have shown great promise
on other tasks [23, 15, 52], and (b) we incorporate both class
and sample-level conï¬dence estimates to appropriately re-
shape learned boundaries. The paper therefore introduces
(1) A principled margin-
the following major novelties.
enforcing formulation for softmax loss, underpinned by the
Bayesian uncertainty estimates. (2) Sample modeling using
multi-variate Gaussian distributions. The class boundaries
are optimized to respect second order moments which im-
proves generalization. (3) A fully differentiable loss formu-
lation that can be easily plugged into existing architectures
and used alongside other regularization techniques.

2. The Imbalance Problem

We begin with an in depth analysis of the imbalance
problem and draw several insights which lead to our pro-
posed framework. We base our analysis around softmax
loss, which is the most popular objective function for clas-
siï¬cation. For brevity, we consider a simplistic case of bi-
nary classiï¬cation with two classes A and B in the training
set denoted by image-label pairs: D = {xk, yk}K
k=1. The
goal is to learn an optimal set of â€˜representative vectorsâ€™
(wA, wB for classes A and B, respectively) that lead to
minimal empirical loss on set D. The Class-Representative
Vectors (CRV) deï¬ne a loss-minimizing hyper-plane â€˜wâ€™,
which is the boundary between two classes, i.e., given a
feature projection â€˜f â€™ corresponding to an input image x,
f âˆˆ w iff (wA âˆ’ wB)f = 0 (ignoring unit biases). The
class imbalance problem exists when the class frequencies

Figure 2: Illustration for the class imbalance problem. True
class distributions are shown in green and red. Unbalanced
distributions lead to a skewed classiï¬cation boundary that is
biased towards the minority class.

Ï„A, Ï„B are greatly mismatched in the set D. As illustrated in
Fig. 2, in such cases, the hypothesis (w) learned on D using
a softmax loss can be biased towards the minority class and
signiï¬cantly different from the ideal separator (wâˆ—). Next,
we breakdown the imbalance problem and explain underly-
ing reasons.

2.1. Bias due to Empirical Loss Minimization

We consider wâˆ— to be an optimal boundary obtained by
loss minimization with respect to the actual hidden distribu-
tions PA and PB of classes, i.e.:

wâˆ— = arg min
w

LP (w), where,

LP (w) =ZRw

B

PA(f )df +ZRw

A

PB(f )df ,

(1)

A , Rw

and Rw
B denote the classiï¬cation regions for classes A
and B, respectively. Given D, the empirical loss calculated
on the training set is:

LD(w) =#{xk|f k âˆˆ DA âˆ§ f k âˆˆ Rw
B }+
#{xk|f k âˆˆ DB âˆ§ f k âˆˆ Rw
A }

(2)

Further, assume that the normalized class frequencies Ï„A
and Ï„B are related as Ï„A + Ï„B = 1. Then, the expected
empirical loss for any hypothesis w is:

E[LD(w)] = Ï„AZRw

B

PA(f )df + Ï„BZRw

A

PB(f )df .

(3)

Note that Ï„A 6= Ï„B 6= 0.5 due to class imbalance and typ-
ically |Ï„A âˆ’ Ï„B| > 0.5 in practical cases where a signiï¬-
cant imbalance ratio exists. Next, we show that when large
imbalance exists, the learned classiï¬cation boundaries are
biased towards minority classes.

Theorem 1. For high imbalanced ratios, minimization of
empirical loss results in a hypothesis Ë†w that is highly likely
to be biased towards the minority class â€˜zâ€™ such that Rwâˆ—
z >
R Ë†w
z . In other words, the classiï¬cation region induced by the
optimal separator is larger than the one induced by empiri-
cally learned boundary.

104

ğ’˜ğ’˜âˆ—Ideal separatorBiased separatorğ‘¤ğ´ğ‘¤ğµğ‘¤ğ´ğ‘¤ğµğ’˜âˆ—Ideal separatorBalanced Class Distributions in Training SetImbalanced Class Distributions in Training SetProof. According to Eq. 3, due to the imbalanced propor-
tion among classes, wâˆ— is more likely to incur higher empir-
ical error than an alternate hypothesis based on an empirical
loss, i.e., for any Ë†w : Rz

Ë†w, it is more likely that:

wâˆ— >Rz

LD(wâˆ—) > LD( Ë†w) because Ï„AZRw
Ï„BZRw

PB(f )df > Ï„AZR Ë†w

B

B

A

âˆ—

PA(f )df + Ï„BZR Ë†w

A

PA(f )df +

âˆ—

PB(f )df .

(4)

z > R Ë†w

Then, for a signiï¬cant imbalance ratio such that Ï„z << Ï„rest,
it directly follows that Rwâˆ—
z . Intuitively, this is a
natural implication of imbalanced class distribution which
forces the classiï¬er to shift Ë†w closer to minority classes be-
cause it reduces empirical error. The likelihood of classiï¬er
bias is directly proportional to the imbalance rate.

A common strategy to tackle data imbalance is through
the introduction of cost-sensitive loss functions [25]. We
brieï¬‚y elaborate on the effect of these losses next and ex-
plain why this solution is sub-optimal.

2.2. Cost sensitive Loss

From Eq. 4, one simple solution seems to be the in-
troduction of costs to re-weight the minority class errors.
Existing cost-sensitive losses (particularly those based on
deep-networks [25]) adopt this idea and assign score-level
penalty to the minority class predictions. This means that
the classiï¬er is forced to correctly classify training samples
belonging to minority classes. This approach has certain
limitations. (1) Appropriately tuning class speciï¬c costs is a
challenging task as it requires domain-knowledge with costs
usually ï¬xed at the beginning and not dynamically changed
during the course of training. (2) A more stringent caveat is
that such costs do not affect the learned boundaries Ë†w if the
training samples are separable [53, 25]. Further, when the
classes are non-separable, minority class representation in
the dataset is directly proportional to its mis-classiï¬cation
(3) Generally,
these costs are not applied at test time and therefore class-
boundaries are effectively unchanged. In summary, while
this practice enforces the classiï¬er to more accurately clas-
sify training samples belonging to minority classes, it does
not enhance generalization capability of the learned model.
This can be understood from the relation for generalization
error and Fig. 2. For an empirical distribution Q for classes
A and B, the generalization error (expected loss on the test
set T ) is:

probability, i.e.,: Ï„z âˆ RR Ë†w

Pz(f )df .

rest

E[LT (w)] = Ï„ â€²

AZRw

B

QA(f )df + Ï„ â€²

BZRw

A

QB(f )df ,

(5)

where Ï„ â€²
B are the normalized frequencies on the test set.
The paper aims to overcome these existing limitations by

A, Ï„ â€²

Figure 3: Top: One-dimensional Gaussian process regres-
sion using maximul-likelihood estimation. The lack of ob-
servations results in higher conï¬dence levels. Bottom: The
uncertainty estimates for imbalanced CIFAR-10 dataset.
The uncertainty is higher for classes with less representa-
tion.

proposing a new loss formulation that seeks to simultane-
ously extend minority class boundaries and enforce mar-
gin constraints on less represented classes to achieve bet-
ter generalization performance. We provide details of our
technique in the next section.

3. Bayesian Uncertainty Estimates

Bayesian models can provide uncertainty estimates
alongside output predictions. Given an input, the uncer-
tainty estimates correspond to the conï¬dence level for each
outcome predicted by the model. We hypothesize that the
conï¬dence-level of predictions is directly related to the
class representation in the training set. As illustrated in
Fig. 3, under-represented classes in the training set lead to
higher uncertainty and bigger conï¬dence intervals. In con-
trast, well-represented classes are associated with less un-
certainty and compact conï¬dence intervals.

We use deep CNNs with dropout to obtain Bayesian un-
certainty estimates. It has been proved that dropout-based
deep networks provide an approximation to Gaussian pro-
cess [16]. A Gaussian process is a Bayesian technique be-
cause it constructs a prior distribution over a family of func-
tions F [39]. This distribution is updated conditioned on
observations, i.e., all the functions that are consistent with
the labels are retained. At inference time, an output is ob-

105

0246810x10505101520f(x)Regression: Point Density and Uncertaintyf(x)=xcos(x)ObservationsPrediction95% confidence intervalC1C2C3C4C5C6C7C8C9C10Class Numbers0123ScoresClassification: Class Frequency and UncertaintyUncertaintyFrequencytained from each of the functions and expectation is com-
puted to generate the ï¬nal prediction. The variance of these
outputs gives an uncertainty estimate. In the following, we
ï¬rst provide an overview of dropout and then describe un-
certainty computation using dropout.

Dropout: Dropout was originally proposed as a regu-
larization measure for deep neural networks [46]. During
training, a sub-network is sampled from the full network
by randomly dropping a set of neurons.
In this manner,
each neuron is activated with a ï¬xed probability â€˜pâ€™. At
test time, full model is used for prediction and the output
activations are multiplied with the probability p to obtain
expectation. Suppose the network parameters are denoted
by Î˜ = {Î¸1, . . . , Î¸L} for a total of L network layers. Then,
by applying masks m generated using i.i.d binary distribu-
tions, we can obtain N samples all corresponding to differ-
ent network conï¬gurations Ë†Î˜ that form an ensemble M:

M ={ Ë†Î˜i : i âˆˆ [1, N ]}, where, Ë†Î˜i = Î˜ â—¦ mi,
mi ={ml : l âˆˆ [1, L]}, s.t., ml âˆ¼ Bernoulli(p)

(6)

Uncertainty: For each input xk, N model conï¬gura-
tions are applied to obtain a set of outputs {Ë†y}. The ex-
pected output is calculated using Monte Carlo estimate for
i=1 Ë†y(x; Ë†Î˜i), where
ï¬rst moment (Eq(y|x)[y]): y â‰ˆ 1
q denotes an output distribution that approximates the in-
tractable posterior distribution of deep Gaussian process.
The uncertainty is calculated using the second moment
(Vq(y|x)[y]) through Monte Carlo estimation:

N PN

u â‰ˆ Ï„ âˆ’1ID +

1
N

N

Xi=1

Ë†yT Ë†y âˆ’ Eq(y|x)[y]T Eq(y|x)[y],

(7)

where Ï„ is the model precision (a function of weight decay)
and IC âˆˆ RCÃ—C is an identity matrix where C denotes the
number of classes.

4. Uncertainty based Max-margin Learning

The softmax loss can be computed for a given feature f

and its true class label y as follows:

Lsm = âˆ’ log(cid:16) exp(wT
Pj exp(wT

y f )

j f )(cid:17),

(8)

where j âˆˆ [1, C] (C is the number of classes). In the above
loss formulation, we include the last fully connected layer
within softmax loss which will be useful for our analysis
later on. Further, for the sake of brevity, we do not mention
bias in Eq. 8. Note that the dot-product wT
y f can also be ex-
pressed as wT
y f = kwykkf k cos(Î±y). Therefore, if a class
z is rare in the training set, for an input feature belonging to
this class, the softmax loss enforces:

kwzkkf k cos(Î±z) > kwrestkkf k cos(Î±rest).

(9)

Intuitively, we would like to impose a large margin on more
uncertain classes. Our experiments show that the class un-
certainty is inversely proportional to its frequency in the
training set, i.e., rare classes are more uncertain (Fig. 3).
To improve generalization performance, we can impose a
more strict constraint for uncertain classes:

kwzkkf k cos(mzÎ±z) > kwrestkkf k cos(mrestÎ±rest),

where, m = max(1, âŒŠ0.5uyâŒ‹), uz > urest, 0 â‰¤ Î±z â‰¤ Ï€
uz
and uz âˆˆ R+. This implies that the classiï¬er will try to
separate rare classes by a more rigorous margin. The margin
maximizing softmax loss [32] is deï¬ned as:

Lâ€²

sm = âˆ’ log(cid:16) exp(kwykkf kÏˆ(Î±y))
Pj exp(kwjkkf kÏˆ(Î±j))(cid:17),

where Ïˆ(Â·) is a continuous and monotonically decreasing
function in the range [0, Ï€]:

(10)

Ïˆ(Î±j) =((âˆ’1)r cos(mÎ±j)âˆ’2r Î±j=y âˆˆh rÏ€

cos(Î±j)

j 6= y,

m i
m , (r+1)Ï€

where r âˆˆ [0, mâˆ’1] is an integer. The gradient back-
propagation requires relations in terms of w and f , there-
fore we substitute cos(mÎ±j) with its expansion in terms of
Chebyshev polynomials of the ï¬rst kind (Tm), i.e.,

cos(mÎ±j) =

âŒŠm/2âŒ‹

Xt=0 (cid:16) m

2t (cid:17)(cos2(Î±j) âˆ’ 1)m cos(Î±j)mâˆ’2t.

Here, cos(Î±j) is substituted with
kwj kkf k . This gives us the
max-margin formulation in terms of differentiable relations.

wT

j f

5. Sample-level Uncertainty Modeling

Although uncertainty driven class-level margin enforce-
ment is important, not all samples in a class have equal
difï¬culty level. The samples that can potentially be mis-
classiï¬ed have larger uncertainties. We therefore propose a
mechanism to incorporate sample-level uncertainty for im-
balanced learning. Existing classiï¬cation networks only use
the mean representation from a distribution of samples to
represent each training example. Inspired by [51], we pro-
pose to represent a single sample as a function of its ï¬rst and
second order moments. To this end, consider that the deep
feature representations of input media is randomly sampled
from a multi-variate Gaussian distribution: f âˆ¼ N (Âµf , Î£f ),
where Âµf and Î£f , respectively denote mean and covariance
of the features. The softmax loss can be computed for a
given feature f and its true class label y using Eq. 8.

For an input feature to be correctly classiï¬ed, its pro-
jection on the true class vector should give a maximum re-
sponse (Eq. 9). In contrast, an example is classiï¬ed into a

106

wrong category k if the following condition holds true:

6. Experiments

âˆƒj âˆˆ [1, C] s.t., wT

y f < wT

j f ,

j 6= y.

(11)

6.1. Datasets

We are interested in quantifying the probability of misclas-
siï¬cation taking into account the distribution of each sam-
ple. It can provide a measure of conï¬dence for loss esti-
mates computed on the training samples. Direct computa-
tion of the misclassiï¬cation probability using softmax loss
in Eq. 8 is intractable and can only be approximated. There-
fore, we introduce a simpler error function that models the
essential loss behavior.

E(f ) = wT

j f âˆ’ wT

y f ,

j 6= y.

(12)

The formulation can be used to exactly compute the mis-
classiï¬cation probability as we will show next. Since the
error function is a linear transformation of input feature
f âˆ¼ N (Âµf , Î£f ), the resulting error distribution is also a
uni-variate Gaussian variable. The ï¬rst and second order
statistics of the error distribution can be given in terms of
Âµf , Î£f as follows:

ÂµE = E[E(f )] = (wj âˆ’ wy)T Âµf
E = E[(E(f ) âˆ’ ÂµE )2] = (wjâˆ’wy)T Î£f (wjâˆ’wy) (13)
Ïƒ2

Now, the misclassiï¬cation probability for a feature f can
be linked with error distribution because E(f ) > 0 denotes
a misclassiï¬cation. The complementary cumulative proba-
bility distribution function (CCDF) Ë†FE is given as follows:

Ë†FE (0) =P(E(f ) > 0) = 1 âˆ’ P(E(f ) < 0)

=1 âˆ’ P(cid:16) E(f ) âˆ’ ÂµE

ÏƒE

E(f ) âˆ’ ÂµE

< âˆ’

ÂµE

ÏƒE(cid:17)

âˆ¼ N (0, 1)

if z =

ÏƒE

Ë†FE (0) =1 âˆ’ Fz(âˆ’

ÂµE
ÏƒE

) =

1

2(cid:16)1 + erfh ÂµE
p2Ïƒ2

Ei(cid:17),

(14)

where Fz denotes the cumulative probability distribution
function (CDF). The probability estimates are then used to
re-weight the loss values such that uncertainty is incorpo-
rated. The function Ïˆ(Â·) is modiï¬ed as follows to obtain an
improved loss function in Eq. 10:

Ïˆ(Î±j) =(Ë†FE (0)((âˆ’1)r cos(mÎ±j) âˆ’ 2r) Î±j=y âˆˆ h rÏ€

cos(Î±j)

j 6= y

m i
m , (r+1)Ï€

(15)

The loss deï¬ned above enforces a margin (m) between
output predictions weighted by the cumulative probability
(Ë†FE (0)). A higher uncertainty means a stricter margin based
penalty for a class j. The modiï¬ed loss function becomes
equal to the original softmax loss when the uncertainty is
zero and m = 1.

Face Recognition: Facial recognition datasets commonly
exhibit large-imbalance which poses a signiï¬cant challenge
for classiï¬er learning.
Following [12], we use VGG2
dataset [5] with 3,141,890 images of 8,631 subjects to
train our deep network. We evaluate the trained model on
four large-scale datasets namely Labeled Faces in the Wild
(LFW) [27] and YouTube Faces (YTF) [57], AgeDB [37]
and Celebrities in Frontal Proï¬le (CFP) [41]. LFW [27]
contains 13,233 web-collected images belonging to 5,749
different identities, with large variations in pose, expression
and illumination. We follow the standard protocol of â€˜un-
restricted with labeled outside dataâ€™. YTF [57] has 3,425
sequences of 195 subjects. We follow the standard evalu-
ation protocol on 5,000 video pairs. AgeDB [37] dataset
has 12,240 images of 440 subjects. The test set is divided
into four groups with different year gaps (5, 10, 20 and 30
years). We only report the performance on the most chal-
lenging subset, AgeDB-30. CFP [41] has 500 subjects in to-
tal, each with 10 frontal and 4 proï¬le images. In this paper,
we only evaluate on the most challenging subset CFP-FP.
Skin Lesion Classiï¬cation: Edinburgh Dermoï¬t Image
Library (DIL) consists of 1,300 high quality skin lesion
images based on diagnosis from dermatologists and der-
matopathologists. There are 10 types of lesions identiï¬ed
in this dataset including melanomas, seborrhoeic keratosis
and basal cell carcinomas. The number of images in each
category varies between 24 and 331 (mean 130, median 83).
Similar to [3], we report results with 3-fold cross validation.
Digit/Object Classiï¬cation: We evaluate on imbalanced
MNIST and CIFAR-10 datasets for generic digit and object
classiï¬cation. Standard MNIST consists of 70,000 images
of handwritten digits (0-9). Out of these, 60,000 images
are used for training (600/class) and the remaining 10,000
for testing (100/class). CIFAR-10 contains 60,000 images
belonging to 10 classes (6,000 images/class). The standard
train/test split for each class is âˆ¼83.3%/16.7% images. We
evaluate our approach on the standard split as well as on
an artiï¬cially created imbalanced split. To imbalance the
training distribution, we randomly drop 90% of the samples
for half of the classes.
Attribute Prediction: We use the large-scale CelebA
dataset [33] for (multi-label) facial attribute prediction task.
This dataset consists of 202,599 images belonging to 10,177
human identities. Each image is annotated with a diverse
set of 40 binary attributes. There exists a signiï¬cant im-
balance in the training set with ratios up to 1:43. Following
the standard protocol [33], we use 152,770 images for train-
ing, 10,000 for validation, and remaining 19,867 for testing.
For evaluation, we report Balanced Classiï¬cation Accuracy
(BCA) deï¬ned as: BCA = 0.5Ã— tp
, where tp
Np

+ 0.5Ã— tn
Nn

107

and tn, respectively denote true positives and true nega-
tives, and Np and Nn are total number of positive and neg-
ative samples. This evaluation metric is more suitable for
multi-label imbalanced learning tasks since it gives equal
weight to both majority and minority classes. Other evalu-
ation metrics used in the literature [33] which deï¬ne accu-
racy as tp+tn
Np+Nn

can be biased towards majority classes.

6.1.1

Implementation Details

The uncertainty estimates are applied progressively during
training. We start with standard softmax (m = 1), fol-
lowed by class-level uncertainty based max-margin learn-
ing and ï¬nally sample-level uncertainty modeling during
the last 10 epochs. The compute intensive sample-level un-
certainty estimates are therefore only done for few epochs.
The proposed strategy can be related with curriculum learn-
ing, since it starts with a simple task by considering a bal-
anced class distribution, and gradually introduces harder
tasks by expanding or shrinking classiï¬cation boundaries of
different classes based upon their uncertainty estimates. For
attribute prediction on CelebA dataset, the training times
required for standard softmax and ours are âˆ¼3.4 and 4.6
hours, respectively, on a Dell Precision 7920 machine with
TitanXp GPU. In our experiments, we ï¬xed m = 3 since
it gives relatively better results. Experiments on imbal-
anced CIFAR-10 for m = 2, 3, 4 achieve an accuracy of
80.2%, 80.6%, 80.5%, respectively. Values of N â‰¥ 5 give
stable uncertainty estimates. We ï¬xed N = 10 for the
optimal trade-off between reliable uncertainty and com-
pute efï¬ciency. An ablation study on different values of
N = 5, 10, 20, 40 on imbalanced CIFAR-10 results in re-
spective accuracies of 80.4%, 80.6%, 80.6%, 80.7%.

For Skin Lesion detection, we deploy ResNet-18 back-
bone with two fully connected layers (with intermediate
rectiï¬ed linear units non-linearities and dropout) inserted
after the global pooling layer. For face veriï¬cation tasks,
we train Squeeze and Excitation (SE) networks [20] with
ResNet-50 backbone. The face images are pre-processed to
112 Ã— 112 using multi-task cascaded CNN [61]. After the
network is trained on VGG-2 dataset, we use features ex-
tracted after global pooling layer for face veriï¬cation evalu-
ations. On imbalanced MNIST dataset, we use the same set-
tings as in [25] to enable direct comparison with the recently
proposed imbalanced learning technique [25]. For experi-
ments on imbalanced CIFAR-10 dataset, we extract features
from VGG16 [44] pre-trained on ImageNet [11]. A simple
neural network with two hidden layers (512 neurons each)
with dropouts is trained on the extracted features to get un-
certainty estimates and perform classiï¬cation. Training a
network on VGG extracted features enables us to compare
against traditional imbalanced learning techniques. Specif-
ically, data level under-sampling & over-sampling methods

Methods using
non-public data

Novel Loss
Functions

Imbalance
Learning

Methods

LFW YTF

DeepFace [49]
FaceNet [43]
Web-scale [50]
DeepID2+ [48]
Baidu [30]
Center Face [56]
Marginal Loss [13]
Noisy Softmax [8]

Softmax+Contrastive [47]
Triplet Loss [43]
Large-Margin Softmax [32]
Center Loss [56]
SphereFace [31]
CosFace [55]

Range Loss [63]
Augmentation [36]
Center Inv. Loss [58]
Feature transfer [60]
LMLE [21]

97.35
99.63
98.37
99.47
99.13
99.28
99.48
99.18

98.78
98.70
99.10
99.05
99.42
99.33

99.52
98.06
99.12
99.37
99.51

91.4
95.4

-

93.2

-

94.9
96.0
94.9

93.5
93.4
94.0
94.4
95.0
96.1

93.7

-

93.9

-

95.8

This Paper

99.71

97.3

Table 1: Face Veriï¬cation Performance on LFW and YTF
datasets. We trained our model on VGG2 dataset. Most
methods in the ï¬rst cell use large-scale outside data that are
not publicly available. The second cell includes novel loss
functions. The state-of-the-art imbalanced learning meth-
ods are in the last group.

in Table 5 are used on VGG features, followed by the two
layered NN for classiï¬cation. For attribute prediction on
CelebA dataset, we train a model with ResNet-50 backbone
and two fully connected layers with dropout inserted after
the global pooling and the ï¬rst fully connected layers. The
model is trained to minimize sum of binary cross entropy
losses, using relatively smaller learning rates for the layers
before the global pooling layer and larger rates for the layers
inserted afterwards.

6.2. Results and Comparisons

Face Veriï¬cation: We compare our approach with 20
recent and top-performing methods on LFW and YTF
datasets (Table 1). We divide these methods into three cate-
gories: (a) methods that use large amounts of non-publicly
available data sources to train their models, (b) methods
that design novel loss functions for face veriï¬cation and (c)
methods that deal with data imbalance. We note that the per-
formances on both LFW and YTF are currently saturated
with many recent methods already surpassing human per-
formance. Our method achieves competitive performance
on both these datasets. Note that some of the compared
methods used as much as 200M images [43] and an ensem-
ble of 25 models [48] for training. Further evaluations on
additional datasets show that the proposed method achieves
veriï¬cation accuracies of 97.0% and 94.4% on CFP-FP and

108

Methods

Imbalanced

Performances

Split

Exp. 1 (5-classes) Exp. 2 (10-classes)

Hierarchical-KNN [3]
Hierarchical-Bayes [2]
Flat-KNN [3]

Baseline CNN [25]
CoSen CNN [25]

This paper

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

74.3 Â± 2.5%
69.6 Â± 0.4%
69.8 Â± 1.6%

75.2 Â± 2.7%
80.2 Â± 2.5%
95.7 Â± 1.2%

68.8 Â± 2.0%
63.1 Â± 0.6%
64.0 Â± 1.3%

69.5 Â± 2.3%
72.6 Â± 1.6%
86.9 Â± 0.7%

Table 2: Experimental results for Skin Lesion Classiï¬cation
on the DIL dataset.

Methods

Imbalanced Split Performances

Deeply Supervised Nets [29]
Generalized Pooling Func. [28]
Maxout NIN [6]

Baseline CNN [25]
CoSen CNN [25]
This paper

âœ—

âœ—

âœ—

âœ“

âœ“

âœ“

99.6%
99.7%
99.8%

97.1%
98.4%
98.7%

Table 3: Results for digit classiï¬cation on MNIST.

AgeDB-30 datasets, respectively.
Skin Lesion Detection: The results for skin lesion detec-
tion are listed in Table 2. We perform a 3-fold cross val-
idation to report the results. Compared to a recent cost-
sensitive CNN approach [25], we obtain an impressive ab-
solute performance gain of 15.5% and 14.3% on Experi-
ments 1 (5 classes) and 2 (10 classes), respectively.
Digit Classiï¬cation: For hand-written digit classiï¬cation,
we report our results on an imbalanced split of MNIST
dataset in Table 3. For the sake of comparison, we also re-
port some representative methods on the original balanced
split of MNIST. However, the two set of techniques are not
directly comparable since the bottom group uses âˆ¼ 45%
less training data. Our technique outperforms other imbal-
ance learning approaches.
Attribute Prediction: We report the multi-label attribute
prediction results on CelebA dataset in Table 4. This dataset
is particularly challenging as it exhibits a high imbalance
with majority-to-minority ratio up to 1:43. We compare
with nine recent state-of-the-art methods. These methods
include techniques that speciï¬cally focus on class imbal-
ance learning (Table 4, right block). Our approach performs
signiï¬cantly better compared to both normal and class im-
balance learning methods. Speciï¬cally, we achieve top-
most accuracy in 23/40 attributes and second-best accuracy
in other 8/40 classes.
In particular, since our method fo-
cuses on assigning a larger classiï¬cation region to under-
represented classes, we achieve more pronounced boost for
the case of highly imbalanced classes. For example, out of
the top 50% most imbalanced classes (with fewer samples),
we achieve best performance in 16/20 (80%) cases. Quali-
tative examples for attribute prediction are shown in Fig. 4.
Object Classiï¬cation: We compare against a number of
popular imbalanced learning approaches including both

l
e
v
e
l

e
c
n
a
l
a
b
m

I

]
3
4
[

N
N
k
-
t
e
l
p
i
r
T

]
7
4
[
2
D
I
p
e
e
D

]
2
6
[

a
d
n
a
P

]
4
3
[

t
e
N
A

]
4
1
[
g
n
i
l
p
m
a
s
-
r
e
v
O

]
4
1
[
g
n
i
l
p
m
a
s
-
r
e
d
n
U

]
9
[

.
j
d
A
d
l
o
h
s
e
r
h
T

]
9
1
[

e
v
i
t
i
s
n
e
s
-
t
s
o
C

]
1
2
[
E
L
M
L

r
e
p
a
p
s
i

h
T

83 85
92
92
91
86 89
91

87 78
1
93 96 89
2
98 97 89
2
97 95 92
3
89 84
5
99 99 94
8
96 88
11 88 95
18 77 78 81 73
22 61 66 67 63
69 66
22 61 67
23 73 77 76 77
90 83
26 82 84
26 55 56 57 62
27 68 72 78 73
84 76
28 75 78
29 63 66 69 65
30 76 85 83 79
30 63 67 70 74
83 75
31 69 77
93 88
33 82 87
35 81 92 90 91
35 81 91 90 90
36 68 74 82 78
38 50 51 59 70
38 47 51 57 64
85
39 66 76 81
42 60 67 70 81
43 73 85 79 83
44 82 88 95 92
44 64 68 76 86
44 73 84 86 90
44 64 65 70 81
44 71 81 79 89
45 43 50 56 74
45 84 90 90 90
45 60 64 68 83
46 63 69 77 81
46 72 79 85 90
46 57 63 61 88
48 75 74 73

76 74 73

66 61 60

77 78 69 78
89 87 89 89
90 90 88 90
92 91 89 91
84 80 83 85
95 90 95 93
87 89 89 89
70 70 77 75
72 64
63 58
72 65
67 63
79 70 76 78
84 80 86 85
61 61
73 76 76 74
75 80 24 75
73 67
66 61
82 76 81 84
73 71
76 70 76 76
88 88 15 88
90 88 93 90
90 85 92 89
84 79 82
80 75
71 59
71 66 62
71 65 59
65 61
85 82 82 84 82
83 81 76
82 79
79 80 76 82
91 85 95 91
90 82 82
89 85 89 86
83 78 81 82 79
90 80 89
90 88
78 76 59
76 68
89 90 95 90
84 80 83
82 78
90 88
90 60 86 88 73
93 93 90

88 77
96 93
99 90
99 94
92 83
99 98
98 91
83 84
68 67
72 72
79 83
92 90
69
80 79
87 72
73 81
87 86
83
83 89
96 85
98 95
99 95
87
76
75
91
86
90 92
98 99
92 78 89
95 96
87
95
87
99 97
85
89
91 94
91
95

84 74
85 80 80
91 90

93 92 79

Attributes

Attractive
Mouth Open
Smiling
Wear Lipstick
High Cheekbones
Male
Heavy Makeup
Wavy Hair
Oval Face
Pointy Nose
Arched Eyebrows
Black Hair
Big Lips
Big Nose
Young
Straight Hair
Brown Hair
Bags Under Eyes
Wear Earrings
No Beard
Bangs
Blond Hair
Bushy Eyebrows
Wear Necklace
Narrow Eyes
5 oâ€™clock Shadow
Receding Hairline
Wear Necktie
Eyeglasses
Rosy Cheeks
Goatee
Chubby
Sideburns
Blurry
Wear Hat
Double Chin
Pale Skin
Gray Hair
Mustache
Bald

Overall

-

72 77 80 81

82 78 79 82

84 87

Table 4: Multi-label attribute prediction results on CelebA
dataset. The compared methods are divided into two cat-
egories (a) left: methods without class imbalance learning
and (b) right: methods that focus on imbalance learning.

data-level (e.g., SMOTE NN [7], ADASYS NN [18]) and
algorithm-level imbalance removal techniques (e.g., cost-
sensitive SVM [4], NearMiss [35]), on imbalanced CIFAR-
10 dataset (by retaining only 10% of the samples for 50%
classes). Table 5 summarizes our results in terms of a di-

109

Methods

Accuracy

Precision

Performances
F1

Recall

G-Mean

IBA

Cost-sen SVM [4]
NearMiss [35]
SMOTE NN [7]
ADASYN NN [18]
Under-samp. Clustering [59]

Neighborhood Cleaning [26]
Instance Hardness [45]
This Paper

34.2
63.5
76.3
76.4
75.7

76.7
67.8
80.6

60.9Â±33.6
66.1Â±14.9
80.5Â±12.6
80.4Â±12.2
78.3Â±12.2
79.2Â±10.4
76.3Â±19.8
80.8Â±6.1

34.3Â±32.4
63.5Â±19.7
76.3Â± 22.1
76.4Â±21.1
75.7Â±10.3
76.7Â±18.6
67.8Â±18.7
80.6Â±9.3

30.5Â±23.0
62.8Â±14.2
75.0Â±12.3
75.2Â±10.6
75.9Â±6.9
75.9Â±8.6
67.6Â±10.8
80.4Â±6.3

69.7Â±29.4
79.2Â±9.5
88.3Â±6.4
88.2Â±6.0
87.0Â±6.8
85.7Â±10.2
79.9Â±11.1
88.7Â±5.2

55.5Â±30.6
61.8Â±14.8
77.1Â±11.9
76.9Â±11.4
74.8Â±12.2
73.2Â±18.1
63.4Â±17.4
77.6Â±9.6

deviation

Table 5: Performance on
CIFAR-10.
imbalanced
Standard
on
class-speciï¬c performance
is reported for each metric.
Our method performs bet-
ter compared to others on
a diverse set of evaluation
metrics.

Figure 4: Sample attribute predictions on the CelebA.

Method (â†“)

CNN + Softmax loss

Accuracy

97.2

Dropout Rate (â†’)

0.3

0.5

0.7

CNN + UMM loss
CNN + UMM + SUM (Ours)

98.0
98.3

98.3
98.7

98.2
98.7

Table 6:
Ablation
study on
imbalanced
MNIST
dataset.

verse range of metrics (e.g., accuracy, F1, G-mean, IBA,
precision, recall). These metrics provide a comprehensive
view on the performances of ours and other imbalanced
techniques and are more suitable for imbalanced learning
scenarios. Our results show that the proposed uncertainty
based technique consistently performs better than the tra-
ditional imbalance removal methods. Particularly, for the
case of evaluation metrics that give equal importance to
rare classes (e.g., recall and F1 measure), our approach
achieves signiï¬cant performance boost of 3.9 and 4.5, re-
spectively. Furthermore, the deviation of individual class
scores is much lower compared to other techniques.
Ablation Study: We experiment with different variants of
our approach in Table 6. Speciï¬cally, we report the per-
formance of a simple baseline model with softmax loss and
compare it with the uncertainty-based margin (UMM) en-
forcement and sample-level uncertainty modeling (SUM).
We note a progression in performance from UMM to
UMM+SUM. This is because the SUM penalizes hard ex-
amples which further helps in improving generalizability.
We also study the effect of changing the dropout rate on our
uncertainty-based approach. Increasing the dropout rate be-
yond 0.5 does not help, while a smaller rate of 0.2 or 0.3
results in a performance drop.
Other margin enforcing losses with uncertainty: We also
experiment with recent variants of softmax loss that explic-
itly enforce margin constraints during classiï¬cation along

Loss Type

Settings Original with Uncer.
n

m

Softmax (Baseline)
Additive Margin [54]
Arc Margin [12]
Large Margin [32]
Sphere Product [31]

-
30
30
-
-

-

0.35
0.4
4
4

76.8
78.3
78.1
79.4
76.2

-

78.9
78.4
79.9
77.5

Table 7: The behavior of other recent margin-based loss
formulations with uncertainty. The accuracy values are re-
ported for imbalanced CIFAR-10 dataset. â€˜nâ€™, â€˜mâ€™ stands
for feature norm and margin, respectively.

with uncertainty estimates (Table 7). These methods in-
clude Additive Margin Softmax [54], Angular (Arc) Margin
Softmax [12], Large Margin Softmax [32] and SphereFace
[31]. As these loss functions have generally been proposed
for face veriï¬cation, we already compare with them in Ta-
ble 1. However, here our main goal is to analyze if the un-
certainty estimates help in learning better boundaries for the
rare classes. To this end, we use exactly the same features
for all techniques and use uncertainty estimates in place
of their original parameter m settings. Since our applica-
tion is different from face veriï¬cation, we note a relatively
lower performance from SphereFace and a higher perfor-
mance from Large Margin Softmax. Overall, the uncer-
tainty scores help in achieving discriminativeness between
difï¬cult classes and gives better performance in all cases.

7. Conclusion

We present a new approach to address class imbalance
problem, underpinned by the Bayesian uncertainty esti-
mates. We demonstrate that the classiï¬er conï¬dence levels
are directly associated with: (a) the difï¬culty-level of indi-
vidual samples and (b) the scarcity of the training data for
under-represented classes. Our proposed approach utilizes
uncertainty to enforce larger classiï¬cation regions for rare
classes and challenging training samples. This results in
better generalization of learned classiï¬er to new samples for
less frequent classes. We achieve signiï¬cant performance
gains on several datasets for face veriï¬cation, attribute pre-
diction, object/digit classiï¬cation and skin lesion detection.

110

Male Bags Under Eyes Mouth Open YoungBushy Eyebrows Black Hair Heavy MakeupAttractiveArched EyebrowsOval FaceEyeglassesBig Lips GoateeBig NoseReceding HairlineGray Hair High Cheekbones Oval FaceSmiling Wavy HairBig NoseChubby Double Chin EyeglassesNo BeardBangsNo BeardStraight Hair YoungBlack Hair References

[1] R. Akbani, S. Kwek, and N. Japkowicz. Applying support
vector machines to imbalanced datasets. In European con-
ference on machine learning, pages 39â€“50. Springer, 2004.

[2] L. Ballerini, R. B. Fisher, B. Aldridge, and J. Rees. Non-
melanoma skin lesion classiï¬cation using colour image data
in a hierarchical k-nn classiï¬er.
In Biomedical Imaging
(ISBI), 2012 9th IEEE International Symposium on, pages
358â€“361. IEEE, 2012.

[3] L. Ballerini, R. B. Fisher, B. Aldridge, and J. Rees. A color
and texture based hierarchical k-nn approach to the classi-
ï¬cation of non-melanoma skin lesions.
In Color Medical
Image Analysis, pages 63â€“86. Springer, 2013.

[4] P. Cao, D. Zhao, and O. Zaiane. An optimized cost-sensitive
svm for imbalanced data learning.
In Paciï¬c-Asia Confer-
ence on Knowledge Discovery and Data Mining, pages 280â€“
292. Springer, 2013.

[5] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman.
Vggface2: A dataset for recognising faces across pose and
age.
In International Conference on Automatic Face and
Gesture Recognition, 2018.

[6] J.-R. Chang and Y.-S. Chen. Batch-normalized maxout net-

work in network. arXiv preprint arXiv:1511.02583, 2015.

[7] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer. Smote: synthetic minority over-sampling tech-
nique. Journal of artiï¬cial intelligence research, 16:321â€“
357, 2002.

[8] B. Chen, W. Deng, and J. Du. Noisy softmax: Improving
the generalization ability of dcnn via postponing the early
softmax saturation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5372â€“
5381, 2017.

[9] J. Chen, C.-A. Tsai, H. Moon, H. Ahn, J. Young, and C.-
H. Chen. Decision threshold adjustment in class prediction.
SAR and QSAR in Environmental Research, 17(3):337â€“352,
2006.

[10] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie. Class-
balanced loss based on effective number of samples. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2019.

[11] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei.
ImageNet: A large-scale hierarchical image database.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 248â€“255, 2009.

[12] J. Deng, J. Guo, and S. Zafeiriou. Arcface: Additive an-
gular margin loss for deep face recognition. arXiv preprint
arXiv:1801.07698, 2018.

[13] J. Deng, Y. Zhou, and S. Zafeiriou. Marginal loss for deep
face recognition.
In 2017 IEEE Conference on Computer
Vision and Pattern Recognition Workshops (CVPRW), pages
2006â€“2014. IEEE, 2017.

[14] C. Drummond and R. C. Holte. C4. 5, class imbalance, and
cost sensitivity: Why under-sampling beats over-sampling.
ICML Workshops, 2003.

[16] Y. Gal and Z. Ghahramani. Dropout as a bayesian approxi-
mation: Representing model uncertainty in deep learning. In
international conference on machine learning, pages 1050â€“
1059, 2016.

[17] M. Hayat, S. Khan, W. Zamir, J. Shen, and L. Shao. Max-
margin class imbalanced learning with gaussian afï¬nity.
arXiv preprint arXiv:1901.07711, 2019.

[18] H. He, Y. Bai, E. A. Garcia, and S. Li. Adasyn: Adaptive
synthetic sampling approach for imbalanced learning.
In
Neural Networks, 2008. IJCNN 2008.(IEEE World Congress
on Computational Intelligence). IEEE International Joint
Conference on, pages 1322â€“1328. IEEE, 2008.

[19] H. He and E. A. Garcia. Learning from imbalanced data.
IEEE Transactions on knowledge and data engineering,
21(9):1263â€“1284, 2009.

[20] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 7, 2017.

[21] C. Huang, Y. Li, C. Change Loy, and X. Tang. Learning
In Pro-
deep representation for imbalanced classiï¬cation.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5375â€“5384, 2016.

[22] P. Jeatrakul, K. W. Wong, and C. C. Fung. Classiï¬cation
of imbalanced data by combining the complementary neural
network and smote algorithm. In International Conference
on Neural Information Processing, pages 152â€“159. Springer,
2010.

[23] A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning us-
ing uncertainty to weigh losses for scene geometry and se-
mantics. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.

[24] S. Khan, H. Rahmani, S. A. A. Shah, and M. Bennamoun. A
guide to convolutional neural networks for computer vision.
Synthesis Lectures on Computer Vision, 8(1):1â€“207, 2018.

[25] S. H. Khan, M. Hayat, M. Bennamoun, F. Sohel, and
R. Togneri. Cost sensitive learning of deep feature represen-
tations from imbalanced data. IEEE Transactions on Neural
Networks and Learning Systems, 2017.

[26] J. Laurikkala.

Improving identiï¬cation of difï¬cult small
classes by balancing class distribution.
In Conference on
Artiï¬cial Intelligence in Medicine in Europe, pages 63â€“66.
Springer, 2001.

[27] G. B. H. E. Learned-Miller. Labeled faces in the wild: Up-
dates and new reporting procedures. Technical Report UM-
CS-2014-003, University of Massachusetts, Amherst, May
2014.

[28] C.-Y. Lee, P. W. Gallagher, and Z. Tu. Generalizing pooling
functions in convolutional neural networks: Mixed, gated,
and tree. In Proceedings of the 19th International Confer-
ence on Artiï¬cial Intelligence and Statistics, pages 464â€“472,
2016.

[29] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. 2015.

[30] J. Liu, Y. Deng, T. Bai, Z. Wei, and C. Huang. Targeting ulti-
mate accuracy: Face recognition via deep embedding. arXiv
preprint arXiv:1506.07310, 2015.

[15] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner.
Detecting adversarial samples from artifacts. arXiv preprint
arXiv:1703.00410, 2017.

[31] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
Sphereface: Deep hypersphere embedding for face recogni-
tion.

111

[32] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin soft-
max loss for convolutional neural networks. In International
Conference on Machine Learning, pages 507â€“516, 2016.

[33] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face at-
tributes in the wild. In Proceedings of the IEEE International
Conference on Computer Vision, pages 3730â€“3738, 2015.

[34] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face
attributes in the wild. In Proceedings of International Con-
ference on Computer Vision (ICCV), 2015.

[35] I. Mani and I. Zhang. knn approach to unbalanced data
distributions: a case study involving information extraction.
In Proceedings of workshop on learning from imbalanced
datasets, volume 126, 2003.

[36] I. Masi, A. T. Trn, T. Hassner, J. T. Leksut, and G. Medioni.
Do we really need to collect millions of faces for effective
face recognition?
In European Conference on Computer
Vision, pages 579â€“596. Springer, 2016.

[37] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kot-
sia, and S. Zafeiriou. Agedb: the ï¬rst manually collected, in-
the-wild age database. In Proceedings of IEEE Intl Conf. on
Computer Vision and Pattern Recognition (CVPR-W 2017),
Honolulu, Hawaii, June 2017.

[38] S. Rahman, S. Khan, and F. Porikli. Zero-shot object de-
tection: Learning to simultaneously recognize and localize
novel concepts. arXiv preprint arXiv:1803.06049, 2018.

[39] C. E. Rasmussen. Gaussian processes in machine learn-
ing. In Advanced lectures on machine learning, pages 63â€“71.
Springer, 2004.

[40] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to
reweight examples for robust deep learning. In International
Conference on Machine Learning, 2018.

[41] C. C. V. P. R. C. D. J. S. Sengupta, J.C. Cheng. Frontal to
proï¬le face veriï¬cation in the wild. In IEEE Conference on
Applications of Computer Vision, February 2016.

[42] J. A. SÂ´aez, J. Luengo, J. Stefanowski, and F. Herrera. Smoteâ€“
ipf: Addressing the noisy and borderline examples problem
in imbalanced classiï¬cation by a re-sampling method with
ï¬ltering. Information Sciences, 291:184â€“203, 2015.

[43] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniï¬ed embedding for face recognition and clustering.
In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 815â€“823, 2015.

[44] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations (ICLR), 2015.

[45] M. R. Smith, T. Martinez, and C. Giraud-Carrier. An in-
stance level analysis of data complexity. Machine learning,
95(2):225â€“256, 2014.

[46] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: a simple way to prevent neu-
ral networks from overï¬tting. Journal of Machine Learning
Research, 15(1):1929â€“1958, 2014.

[47] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning
face representation by joint identiï¬cation-veriï¬cation.
In
Advances in neural information processing systems, pages
1988â€“1996, 2014.

[48] Y. Sun, X. Wang, and X. Tang. Deeply learned face repre-
sentations are sparse, selective, and robust. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 2892â€“2900, 2015.

[49] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriï¬-
cation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1701â€“1708, 2014.

[50] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Web-
scale training for face identiï¬cation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2746â€“2754, 2015.

[51] C. Tzelepis. Maximum Margin Learning Under Uncertainty.

PhD thesis, Queen Mary University of London, 2018.

[52] C. Tzelepis, V. Mezaris, and I. Patras. Linear maximum mar-
gin classiï¬er for learning from uncertain data. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 2017.
[53] B. C. Wallace, K. Small, C. E. Brodley, and T. A. Trikali-
In Data Mining (ICDM),
nos. Class imbalance, redux.
2011 IEEE 11th International Conference on, pages 754â€“
763. IEEE, 2011.

[54] F. Wang, J. Cheng, W. Liu, and H. Liu. Additive margin
softmax for face veriï¬cation. IEEE Signal Processing Let-
ters, 25(7):926â€“930, 2018.

[55] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li,
and W. Liu. Cosface: Large margin cosine loss for deep face
recognition.
In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.

[56] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discrimina-
tive feature learning approach for deep face recognition. In
European Conference on Computer Vision, pages 499â€“515.
Springer, 2016.

[57] L. Wolf, T. Hassner, and I. Maoz. Face recognition in uncon-
strained videos with matched background similarity. In Com-
puter Vision and Pattern Recognition (CVPR), 2011 IEEE
Conference on, pages 529â€“534. IEEE, 2011.

[58] Y. Wu, H. Liu, J. Li, and Y. Fu. Deep face recognition with
center invariant loss.
In Proceedings of the on Thematic
Workshops of ACM Multimedia 2017, pages 408â€“414. ACM,
2017.

[59] S.-J. Yen and Y.-S. Lee. Cluster-based under-sampling ap-
proaches for imbalanced data distributions. Expert Systems
with Applications, 36(3):5718â€“5727, 2009.

[60] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker. Fea-
ture transfer learning for deep face recognition with long-tail
data. arXiv preprint arXiv:1803.09014, 2018.

[61] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection
and alignment using multitask cascaded convolutional net-
works. IEEE Signal Processing Letters, 23(10):1499â€“1503,
2016.

[62] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. Bourdev.
Panda: Pose aligned networks for deep attribute modeling. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1637â€“1644, 2014.

[63] X. Zhang, Z. Fang, Y. Wen, Z. Li, and Y. Qiao. Range loss
for deep face recognition with long-tailed training data. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 5409â€“5418, 2017.

112

