End-to-End Efï¬cient Representation Learning

via Cascading Combinatorial Optimization

Department of Computer Science and Engineering, Seoul National University, Seoul, Korea

Yeonwoo Jeong, Yoonsung Kim, Hyun Oh Song

{yeonwoo, yskim227, hyunoh}@mllab.snu.ac.kr

Abstract

We develop hierarchically quantized efï¬cient embedding
representations for similarity-based search and show that
this representation provides not only the state of the art per-
formance on the search accuracy but also provides several
orders of speed up during inference. The idea is to hierar-
chically quantize the representation so that the quantization
granularity is greatly increased while maintaining the accu-
racy and keeping the computational complexity low. We also
show that the problem of ï¬nding the optimal sparse com-
pound hash code respecting the hierarchical structure can
be optimized in polynomial time via minimum cost ï¬‚ow in an
equivalent ï¬‚ow network. This allows us to train the method
end-to-end in a mini-batch stochastic gradient descent set-
ting. Our experiments on Cifar100 and ImageNet datasets
show the state of the art search accuracy while providing sev-
eral orders of magnitude search speedup respectively over
exhaustive linear search over the dataset.

1. Introduction

Learning the feature embedding representation that pre-
serves the notion of similarities among the data is of great
practical importance in machine learning and vision and
is at the basis of modern similarity-based search [21, 23],
veriï¬cation [26], clustering [2], retrieval [25, 24], zero-shot
learning [31, 5], and other related tasks. In this regard, deep
metric learning methods [2, 21, 23] have shown advances
in various embedding tasks by training deep convolutional
neural networks end-to-end encouraging similar pairs of data
to be close to each other and dissimilar pairs to be farther
apart in the embedding space.

Despite the progress in improving the embedding repre-
sentation accuracy, improving the inference efï¬ciency and
scalability of the representation in an end-to-end optimiza-
tion framework is relatively less studied. Practitioners de-
ploying the method on large-scale applications often resort
to employing post-processing techniques such as embedding
thresholding [1, 32] and vector quantization [27] at the cost
of the loss in the representation accuracy. Recently, Jeong

& Song [11] proposed an end-to-end learning algorithm for
quantizable representations which jointly optimizes the qual-
ity of the convolutional neural network based embedding
representation and the performance of the corresponding
sparsity constrained compound binary hash code and showed
signiï¬cant retrieval speedup on ImageNet [20] without com-
promising the accuracy.

In this work, we seek to learn hierarchically quantizable
representations and propose a novel end-to-end learning
method signiï¬cantly increasing the quantization granularity
while keeping the time and space complexity manageable
so the method can still be efï¬ciently trained in a mini-batch
stochastic gradient descent setting. Besides the efï¬ciency
issues, however, naively increasing the quantization granu-
larity could cause severe degradation in the search accuracy
or lead to dead buckets hindering the search speedup.

To this end, our method jointly optimizes both the sparse
compound hash code and the corresponding embedding rep-
resentation respecting a hierarchical structure. We alternate
between performing cascading optimization of the optimal
sparse compound hash code per each level in the hierarchy
and updating the neural network to adjust the corresponding
embedding representations at the active bits of the compound
hash code.

Our proposed learning method outperforms both the re-
ported results in [11] and the state of the art deep metric
learning methods [21, 23] in retrieval and clustering tasks on
Cifar-100 [13] and ImageNet [20] datasets while, to the best
of our knowledge, providing the highest reported inference
speedup on each dataset over exhaustive linear search.

2. Related works

Embedding representation learning with neural networks
has its roots in Siamese networks [4, 9] where it was trained
end-to-end to pull similar examples close to each other and
push dissimilar examples at least some margin away from
each other in the embedding space. [4] demonstrated the idea
could be used for signature veriï¬cation tasks. The line of
work since then has been explored in wide variety of practical
applications such as face recognition [26], domain adaptation

111379

[22], zero-shot learning [31, 5], video representation learning
[28], and similarity-based interior design [2], etc.

Another line of research focuses on learning binary ham-
ming ranking [29, 33, 19, 14] representations via neural
networks. Although comparing binary hamming codes is
more efï¬cient than comparing continuous embedding repre-
sentations, this still requires the linear search over the entire
dataset which is not likely to be as efï¬cient for large scale
problems. [7, 16] seek to vector quantize the dataset and
back propagate the metric loss, however, it requires repeat-
edly running k-means clustering on the entire dataset during
training with prohibitive computational complexity.

We seek to jointly learn the hierarchically quantizable
embedding representation and the corresponding sparsity
constrained binary hash code in an efï¬cient mini-batch based
end-to-end learning framework. Jeong & Song [11] moti-
vated maintaining the hard constraint on the sparsity of hash
code to provide guaranteed retrieval inference speedup by
only considering ks out of d buckets and thus avoiding linear
search over the dataset. We also explicitly maintain this con-
straint, but at the same time, greatly increasing the number of
representable buckets by imposing an efï¬cient hierarchical
structure on the hash code to unlock signiï¬cant improvement
in the speedup factor.

3. Problem formulation

Consider the following hash function

r(x) = argmin
hâˆˆ{0,1}d

âˆ’f (x; Î¸)âŠºh

under the constraint that khk1 = ks. The idea is to optimize
the weights in the neural network f (Â·; Î¸) : X â†’ Rd, take
ks highest activation dimensions, activate the corresponding
dimensions in the binary compound hash code h, and hash
the data x âˆˆ X into the corresponding active buckets of a
hash table H. During inference, a query xq is given, and
all the hashed items in the ks active bits set by the hash
function r(xq) are retrieved as the candidate nearest items.
Often times [27], these candidates are reranked based on
the euclidean distance in the base embedding representation
f (Â·; Î¸) space.

i

is Pi6=q Pr(hâŠº

Given a query hq, the expected number of retrieved items
hq 6= 0). Then, the expected speedup factor
[11] (SUF) is the ratio between the total number of items
and the expected number of retrieved items. Concretely, it
becomes (Pr(hâŠº
i
case d â‰« ks, this ratio approaches d/ks

hq 6= 0))âˆ’1 = (1 âˆ’ (cid:0)dâˆ’ks

ks(cid:1))âˆ’1. In

ks (cid:1)/(cid:0) d

2.

Now, suppose we design a hash function r(x) so that
the function has total dim(r(x)) = dk (i.e. exponential in
some integer parameter k > 1) indexable buckets. The ex-
pected speedup factor [11] approaches dk/k2
s which means
the query time speedup increases linearly with the number
of buckets. However, naively increasing the bucket size

for higher speedup has several major downsides. First, the
hashing network has to output and hold dk activations in the
memory at the ï¬nal layer which can be unpractical in terms
of the space efï¬ciency for large scale applications. Also, this
could also lead to dead buckets which are under-utilized and
degrade the search speedup. On the other hand, hashing the
items uniformly at random among the buckets could help
to alleviate the dead buckets but this could lead to a severe
drop in the search accuracy.

Our approach to this problem of maintaining a large num-
ber of representable buckets while preserving the accuracy
and keeping the computational complexity manageable is to
enforce a hierarchy among the optimal hash codes in an efï¬-
cient tree structure. First, we use dim(f (x)) = dk number
of activations instead of dk activations in the last layer of the
hash network. Then, we deï¬ne the unique mapping between
the dk activations to dk buckets by the following procedure.

Denote the hash code as eh = [h1, . . . , hk] âˆˆ {0, 1}dÃ—k

where khvk1 = 1 âˆ€v 6= k and khkk1 = ks. The superscript
denotes the level index in the hierarchy. Now, suppose we
construct a tree T with branching factor d, depth k where
the root node has the level index of 0. Let each dk leaf
node in T represent a bucket indexed by the hash function
r(x). Then, we can interpret each hv vector to indicate the
branching from depth v âˆ’ 1 to depth v in T . Note, from

the construction of eh, the branching is unique until level

k âˆ’ 1, but the last branching to the leaf nodes is multi-way
because ks bits are set due to the sparsity constraint at level
k. Figure 1 illustrates an example translation from the given
hash activation to the tree bucket index for k = 2 and ks = 2.
Concretely, the hash function r(x) : RdÃ—k â†’ {0, 1}dk
can
be expressed compactly as Equation (1).

k

Ov=1

r(x) =

argmin

hv

âˆ’ (f (x; Î¸)v)âŠº hv

(1)

subject to khvk1 =(1

ks

âˆ€v 6= k

v = k

and hv âˆˆ {0, 1}d

two vectors. The following section discusses how to ï¬nd

whereN denotes the tensor multiplication operator between
the optimal hash code eh and the corresponding activation

f (x; Î¸) = [f (x; Î¸)1, . . . , f (x; Î¸)k] âˆˆ RdÃ—k respecting the
hierarchical structure of the code.

4. Methods

To compute the optimal set of embedding representations
and the corresponding hash code, the embedding representa-
tions are ï¬rst required in order to infer which ks activations
to set in the hash code, but to learn the embedding repre-
sentations, it requires the hash code to determine which
dimensions of the activations to adjust so that similar items
would get hashed to the same buckets and vice versa. We
take the alternating minimization approach iterating over
computing the sparse hash codes respecting the hierarchical
quantization structure and updating the network parameters

211380

elements as possible for a pair of hash codes from different
classes in the current level v. The last term also makes sure,
in the event that the second term becomes zero, the hash code
still respects orthogonality among dissimilar items. This can
occur when the hash code for all the previous levels was
computed perfectly splitting dissimilar pairs into different
branches and the second term becomes zero.

minimize
h1 ,...,hn

n

X

âˆ’(f (xi; Î¸)v)âŠº hi

+ X

i Qâ€² hj
hâŠº

+ X

i P â€² hj
hâŠº

|

}

}

|

i=1

{z

{z

unary term

orthogonality

(i,j)âˆˆN

sibling penalty

i = hw

(i,j)âˆˆSv
{z
}
|
(3)
j , âˆ€w = 1, . . . , v âˆ’ 1(cid:9)
where S v = (cid:8)(i, j) âˆˆ N | hw

denotes the set of pairs of siblings at level v in T , and
Qâ€², P â€² encodes the pairwise cost for the sibling and the
orthogonality terms respectively. However, optimizing Equa-
tion (3) is NP-hard in general even in the simpler case of
ks = 1, k = 1, d > 2 [3, 11]. Inspired by [11], we use
the average embedding of each class within the minibatch
p = 1
cv

m Pi:yi=p f (xi; Î¸)v âˆˆ Rd as shown in Equation (4).

zp

âŠºQzq +Xp6=q

zp

âŠºP zq

minimize
z1,...,znc

âˆ’(cv

nc

Xp=1
|

p)âŠºzp + X(p,q)âˆˆSv
{z

âˆ€v 6= k

p6=q

z

v = k

:=Ë†g(z1,...,znc )

, zp âˆˆ {0, 1}d, âˆ€p,

subject to kzpk =(1
z = (cid:8)(p, q) | zw

ks

p = zw

q , âˆ€w = 1, . . . , v âˆ’ 1(cid:9), nc

where S v
is the number of unique classes in the minibatch, and
we assume each class has m examples in the minibatch
in
(i.e.
accordance with the deep metric learning problem setting
[21, 23, 11], we assume we are given access to the label
adjacency information only within the minibatch.

npairs [23] minibatch construction). Note,

}

(4)

The objective in Equation (4) upperbounds the objective
in Equation (3) (denote as g(Â·; Î¸)) by a gap M (Î¸) which
depends only on Î¸. Concretely, rewriting the summation in
the unary term in g, we get

nc

hâŠº

Xp Xi:yi=p
+ X(i,j)âˆˆSv
p)âŠº hi + X(i,j)âˆˆSv
Xp Xi:yi=p
{z

:=M (Î¸)

(cv

nc

â‰¤

nc

Xp Xi:yi=p

âˆ’(cv

+ maximize

Ë†h1,...,Ë†hn

|

(5)

i P â€²hj
hâŠº

i Qâ€²hj + X(i,j)âˆˆN

hâŠº

i Qâ€²hj + X(i,j)âˆˆN

i P â€²hj
hâŠº

p âˆ’ f (xi; Î¸)v)âŠº Ë†hi

.

Minimizing the upperbound in Equation (5) over h1, . . . , hn
is identical to minimizing the objective Ë†g(z1, . . . , znc ) in

}

311381

Figure 1: Example hierarchical structure for k = 2 and
ks = 2. (Left) The hash code for each embedding represen-
tation [f (xi; Î¸)1, f (xi; Î¸)2] âˆˆ R2d. (Right) Corresponding
activated hash buckets out of total d2 buckets.

indexed at the given hash codes per each mini-batch. Sec-
tion 4.1 and Section 4.3 formalize the subproblems in detail.

4.1. Learning the hierarchical hash code

Given a set of continuous embedding representation
{f (xi; Î¸)}n
i=1, we wish to compute the optimal binary hash
code {h1, . . . , hn} so as to hash similar items to the same
buckets and dissimilar items to different buckets. Further-
more, we seek to constrain the hash code to simultaneously
maintain the hierarchical structure and the hard sparsity con-
ditions throughout the optimization process. Suppose items
xi and xj are dissimilar items, in order to hash the two items
to different buckets, at each level of T , we seek to encourage
the hash code for each item at level v, hv
j to differ.
To achieve this, we optimize the hash code for all items per
each level sequentially in cascading fashion starting from
1, . . . , h1
the ï¬rst level {h1
1, . . . , hk
n}
as shown in Equation (2).

n} to the leaf nodes {hk

i and hv

âˆ’(f (xi; Î¸)v)âŠº hv
i

(2)

n

Xi=1

k

Xv=1
|

hv
i

minimize
1:n,...,h1
hk
1:n

+

k

Xv=2 X(i,j)âˆˆN
|

subject to khv

unary term

vâˆ’1

{z
Yw=1

sibling penalty

{z
i k =(1

ks

âˆ€v 6= k

v = k

âŠºQâ€²hv
j

1(hw

i = hw
j )

+

hv
i

âŠºP â€²hv
j

}

}

k

Xv=1 X(i,j)âˆˆN
|
{z

orthogonality

}

where N denotes the set of dissimilar pairs of data and 1(Â·)
denotes the indicator function. Concretely, given the hash
codes from all the previous levels, we seek to minimize the
following discrete optimization problem in Equation (3),
subject to the same constraints as in Equation (2), sequen-
tially for all levels1 v âˆˆ {1, . . . , k}. The unary term in the
objective encourages selecting as large elements of each em-
bedding vector as possible while the second term loops over
all pairs of dissimilar siblings and penalizes for their orthog-
onality. The last term encourages selecting as orthogonal

1In Equation (3), we omit the dependence of v for all h1, . . . , hn to

avoid the notation clutter.

, hv

i âˆˆ {0, 1}d, âˆ€i,

g(h1, . . . , hn; Î¸) =

âˆ’(f (xi; Î¸)v)âŠº hi

0ğ‘â„1â„2ğ‘ğ‘ğ‘‘âˆ’100â‹®â‹®10â‹®â‹®01â‹®â‹®01â‹®â‹®00â„0â„1[0]â‹¯â‹¯â‹¯â‹¯â‹¯ğ‘Ÿ(ğ‘¥)0â‹¯â‹¯0â‹¯1â‹¯1â‹¯0â‹¯0â‹¯0ğ‘ğ‘‘ğ‘ğ‘‘+ğ‘ğ‘ğ‘‘+ğ‘ğ‘ğ‘‘+ğ‘‘âˆ’1ğ‘‘2âˆ’1â„1[ğ‘]â„1[ğ‘‘âˆ’1]â„2[0]â„2[ğ‘]â„2[ğ‘]â„2[ğ‘‘âˆ’1]â‹¯â‹¯Equation (4) since each example j in class i shares the same
class mean embedding vector ci. Absorbing the factor m
into the cost matrices i.e. Q = mQâ€² and P = mP â€², we
arrive at the upperbound minimization problem deï¬ned in
Equation (4). In the upperbound problem Equation (4), we
consider the case where the pairwise cost matrices are di-
agonal matrices of non-negative values. Theorem 1 in the
following subsection proves that ï¬nding the optimal solution
of the upperbound problem in Equation (4) is equivalent to
ï¬nding the minimum cost ï¬‚ow solution of the ï¬‚ow network
Gâ€² illustrated in Figure 2. Section B in the supplementary
material shows the running time to compute the minimum
cost ï¬‚ow (MCF) solution is approximately linear in nc and
d. On average, it takes 24 ms and 53 ms to compute the MCF
solution (discrete update) and to take a gradient descent step
with npairs embedding [23] (network update), respectively
on a machine with 1 TITAN-XP GPU and Xeon E5-2650.

4.2. Equivalence of the optimization problem to

minimum cost ï¬‚ow

Theorem 1. The optimization problem in Equation (4) can
be solved exactly in polynomial time by ï¬nding the minimum
cost ï¬‚ow solution on the ï¬‚ow network Gâ€™.

Proof. Suppose we construct a vertex set A =
{a1, . . . , anc} and partition A into {Ar}l
r=0 with the par-
tition of {1, . . . , nc} from equivalence relation S v
2. Here,
z
we will deï¬ne A0 as a union of subsets of size 1 (i.e. each el-
ement in A0 is a singleton without a sibling), and A1, . . . , Al
as the rest of the subsets (of size greater than or equal to2).

Concretely, |A| = nc and A = Sl

r=0 Ar.

Then, we construct l + 1 set of complete bipartite graphs
{Gr = (Ar âˆª Br, Er)}l
r=0 where we deï¬ne gr = |Ar| and
|Br| = d âˆ€r. Now suppose we construct a directed graph Gâ€²
by directing all edges Er from Ar to Br, attaching source
s to all vertices in Ar, and attaching sink t to all vertices in

B0. Formally, Gâ€² = (cid:16)Sl

r=0 (Ar âˆª Br) âˆª {s, t}, Eâ€²(cid:17). The

edges in Eâ€² inherit all directed edges from source to vertices
in Ar, edges from vertices in B0 to sink, and {Er}l
r=0. We
also attach gr number of edges for each vertex br,q âˆˆ Br to
b0,q âˆˆ B0 and attach nc number of edges from each vertex
b0,q âˆˆ B0 to t. Concretely, Eâ€² is

{(s, ap)|ap âˆˆ A} âˆª

l

[

r=0

Er âˆª

l

[

r=1

{(br,q , b0,q)i}gr âˆ’1

i=0 âˆª {(b0,q , t)j }ncâˆ’1
j=0 .

Edges incident to s have capacity u(s, ap) = ks and
cost v(s, ap) = 0 for all ap âˆˆ A. The edges between
ap âˆˆ Ar and br,q âˆˆ Br have capacity u(ap, br,q) = 1 and
cost v(ap, br,q) = âˆ’cp[q]. Each edge i âˆˆ {0, . . . , gr âˆ’ 1}
between br,q âˆˆ Br and b0,q âˆˆ B0 has capacity

u(cid:0)(br,q, b0,q)i(cid:1) = 1 and cost u(cid:0)(br,q, b0,q)i(cid:1) = 2Î±i.
capacity u(cid:16)(b0,q, t)j(cid:17) = 1 and cost v(cid:16)(b0,q, t)j(cid:17) = 2Î²j.

Each edge j âˆˆ {0, . . . , nc âˆ’ 1} between b0,q âˆˆ B0 and t has

2Deï¬ne (p, q) âˆˆ S v

z â‡â‡’ ap, aq âˆˆ Ar, âˆ€r â‰¥ 1

Figure 2 illustrates the ï¬‚ow network Gâ€². The amount of ï¬‚ow
from source to sink is ncks. The ï¬gure omits the vertices in
A0 and the corresponding edges to B0 to avoid the clutter.

Now we deï¬ne the ï¬‚ow {fz(e)}eâˆˆEâ€² for each edge in-
dexed both by ï¬‚ow conï¬guration zp âˆˆ z1:nc where zp âˆˆ
{0, 1}d, kzpk1 = ks âˆ€p and e âˆˆ Eâ€² below in Equation (6).

(i) fz(s, ap) = ks, (ii) fz(ap, br,q) = zp[q]

(iii) fz(cid:0)(br,q, b0,q)i(cid:1) =(1
(iv) fz(cid:16)(b0,q, t)j(cid:17) =(1

0 otherwise

âˆ€i <Pp:apâˆˆAr
âˆ€j <Pnc

zp[q]

p=1

0 otherwise

zp[q]

(6)

To prove the equivalence of computing the minimum cost
ï¬‚ow solution and ï¬nding the minimum binary assignment
in Equation (4), we need to show (1) that the ï¬‚ow deï¬ned
in Equation (6) is feasible in Gâ€² and (2) that the minimum
cost ï¬‚ow solution of the network Gâ€² and translating the
computed ï¬‚ows to {zp} in Equation (4) indeed minimizes
the discrete optimization problem. We ï¬rst proceed with the
ï¬‚ow feasibility proof.

It is easy to see the capacity constraints are satisï¬ed by
construction in Equation (6) so we prove that the ï¬‚ow conser-
vation conditions are met at each vertices. First, the output
p=1 ks = ncks
is equal to the input ï¬‚ow. For each vertex ap âˆˆ A, the
amount of input ï¬‚ow is ks and the output ï¬‚ow is the same

ï¬‚ow from the source PapâˆˆA fz(s, ap) = Pnc
fz(ap, br,q) = Pd
Pbr,qâˆˆBr
ï¬‚ow as yr,q = PapâˆˆAr
Pp:apâˆˆAr

For r > 0, for each vertex br,q âˆˆ Br, denote the input
zp[q].
=
i=0 fz((br,q, b0,q)i)
The second term vanishes

fz(ap, br,q) = Pp:apâˆˆAr

ï¬‚ow is Pgrâˆ’1

zp[q] = kzk1 = ks.

zp[q] = yr,q.

output

The

q=1

because of Equation (6) (iii).

The last ï¬‚ow conservation condition is to check
the connections from each vertex b0,q âˆˆ B0 to the
sink. Denote the input ï¬‚ow at the vertex as y0,q =
zp[q]. The output
zp[q] = y0,q which is
identical to the input ï¬‚ow. Therefore, the ï¬‚ow construction
in Equation (6) is feasible in Gâ€².

zp[q] + Pl
r=1 yr,q = Pnc
j=0 fz((b0,q, t)j) = Pnc

Pp:apâˆˆA0
ï¬‚ow isPncâˆ’1

p=1

p=1

The second part of the proof is to check the optimal-
ity conditions and show the minimum cost ï¬‚ow ï¬nds
the minimizer of Equation (4). Denote, {fo(e)}eâˆˆEâ€²
as the minimum cost ï¬‚ow solution of the network Gâ€²

denote the optimal ï¬‚ow from ap âˆˆ Ar to br,q âˆˆ
Br, fo(ap, bq) as zâ€²
By optimality of the ï¬‚ow,

which minimizes the total cost PeâˆˆEâ€² v(e)fo(e). Also
{fo(e)}eâˆˆEâ€² , PeâˆˆEâ€² v(e)fo(e) â‰¤ PeâˆˆEâ€² v(e)fz(e) âˆ€z.
Pnc
p=1 âˆ’cp

the lhs of the inequality is equal
T zâ€²

r=1Pp16=p2âˆˆ{p|apâˆˆAr} Î±zâ€²

p + Pl

By Lemma 1,

to
p2 +

p[q].

T zâ€²

p1

411382

Arâˆ’1

Ar

...

...

Input flow

ncks

s

ks, 0

ap

...

...

Ar+1

Brâˆ’1

Br

1, âˆ’cp[0]

1, âˆ’cp[q]

1, âˆ’cp[d]

Br+1

...

br,1
...
br,q
...
br,d

...

...

...

1, 2(gr âˆ’ 1)Î±

...
1, 0
...

...

B0

b0,1
...
b0,q
...
b0,d

1, 2(nc âˆ’ 1)Î²

...
...
...
1, 0
...
...

unary term

sibling penalty

orthogonality

Output flow

t

ncks

Figure 2: Equivalent ï¬‚ow network diagram Gâ€² corresponding to the discrete optimization Equation (4). Edge labels show the
capacity and the cost respectively.

p1

p2 .

T zâ€²

Î²zâ€²

Pp16=p2
the rhs of the inequality is equal to Pnc
T zp2 + Pp16=p2
Pl
r=1Pp16=p2âˆˆ{p|apâˆˆAr} Î±zp1

Additionally, Lemma 2 shows
T zp +
T zp2 .

p=1 âˆ’cp
Î²zp1

Finally, âˆ€{z}

nc

X

p=1

âˆ’cp

T zâ€²

p +

l

X

r=1

X

p16=p2âˆˆ{p|apâˆˆAr }

Î±zâ€²

p1

T

zâ€²

p2 + X
p16=p2

Î²zâ€²

p1

T

zâ€²

p2

â‰¤

nc

X

p=1

âˆ’cp

T zp +

l

X

r=1

X

p16=p2âˆˆ{p|apâˆˆAr }

Î±zp1

T zp2 + X
p16=p2

Î²zp1

T zp2 .

This shows computing the minimum cost ï¬‚ow solution on
Gâ€² and converting the ï¬‚ows to zâ€™s, we can ï¬nd the minimizer
of the objective in Equation (4).

Lemma 1. Given the minimum cost ï¬‚ow {fo(e)}eâˆˆEâ€² of the

network Gâ€², the total cost of the ï¬‚ow is PeâˆˆEâ€² v(e)fo(e) =
Pnc
p=1 âˆ’cp
Pp16=p2
Î²zâ€²

r=1Pp16=p2âˆˆ{p|apâˆˆAr} Î±zâ€²

p + Pl

T zâ€²

T zâ€²

T zâ€²

p2 .

p2 +

p1

p1

Proof. Proof in section A.2 of the supplementary material.

Lemma 2. Given a feasible ï¬‚ow {fz(e)}eâˆˆEâ€² of the net-

work Gâ€², the total cost of the ï¬‚ow is PeâˆˆEâ€² v(e)fz(e) =
Pnc
p=1 âˆ’cp
Pp16=p2

r=1Pp16=p2âˆˆ{p|apâˆˆAr} Î±zp1

T zp + Pl

T zp2 +

T zp2 .

Î²zp1

Proof. Proof in section A.2 of the supplementary material.

4.3. Learning the embedding representation given

the hierarchical hash codes

1, . . . , zv

Given a set of binary hash codes for the mean embed-
dings {zv
nc}, âˆ€v = 1, . . . , k computed from Equa-
tion (4), we can derive the hash codes for all n examples
in the minibatch, hv
p âˆ€i : yi = p and update the
i
network weights Î¸ given the hierarchical hash codes in
turn. The task is to update the embedding representations,
{f (xi; Î¸)v}n
i=1, âˆ€v = 1, . . . , k, so that similar pairs of data

:= zv

have similar embedding representations indexed at the acti-
vated hash code dimensions and vice versa. Note, In terms
of the hash code optimization in Equation (4) and the bound
in Equation (5), this embedding update has the effect of
tightening the bound gap M (Î¸).

i âˆ¨ hv

k(cid:0)hv

We employ the state of the art deep metric learning
algorithms (denote as â„“metric(Â·)) such as triplet loss with
semi-hard negative mining [21] and npairs loss [23] for
this subproblem where the distance between two exam-
ples xi and xj at hierarchy level v is deï¬ned as dv
ij =

j(cid:1)âŠ™(f (xi; Î¸)v âˆ’ f (xj; Î¸)v) k1. Utilizing the log-

ical OR of the two binary masks, in contrast to independently
indexing the representation with respective masks, to index
the embedding representations helps prevent the pairwise
distances frequently becoming zero due to the sparsity of the
code. Note, this formulation in turn accommodates the back-
propagation gradients to ï¬‚ow more easily. In our embedding
representation learning subproblem, we need to learn the rep-
resentations which respect the tree structural constraint on
the corresponding hash code h = [h1, . . . , hk] âˆˆ {0, 1}dÃ—k
where khvk1 = 1 âˆ€v 6= k and khkk1 = ks. To this end, we
decompose the problem and compute the embedding loss
per each hierarchy level v separately.

Furthermore, naively using the similarity labels to deï¬ne
similar pairs versus dissimilar pairs during the embedding
learning subproblem could create a discrepancy between the
hash code discrete optimization subproblem and the embed-
ding learning subproblem leading to contradicting updates.
Suppose two examples xi and xj are dissimilar and both had
the highest activation at the same dimension o and the hash
code for some level v was identical i.e. hv
j [o] = 1.
Enforcing the metric learning loss with the class labels, in
this case, would lead to increasing the highest activation for
one example and decreasing the highest activation for the
other example. This can be problematic for the example with
decreased activation because it might get hashed to another
occupied bucket after the gradient update and this can repeat

i [o] = hv

511383

causing instability in the optimization process.

However, if we relabel the two examples so that they are
treated as the same class as long as they have the same hash
code at the level, the update wouldnâ€™t decrease the activations
for any example, and the sibling term (the second term) in
Equation (4) would automatically take care of splitting the
two examples in the next subsequent levels.

To this extent, we apply label remapping as follows.
yv
i = remap(hv
i ), where remap(Â·) assigns arbitrary unique
labels to each unique conï¬guration of hv
i . Concretely,
remap(hv
i = yv
j . Finally, the
embedding representation learning subproblem aims to solve
Equation (7) given the hash codes and the remapped labels.
Section C in the supplementary material includes the abla-
tion study of label remapping.

i ) = remap(hv

j ) â‡â‡’ yv

minimize

Î¸

k

Xv=1

â„“metric ({f (xi; Î¸)v}n

i=1; {hv

i }n

i=1, {yv

i }n

i=1)

(7)

Following the protocol in [11], we use the Tensorï¬‚ow
implementation of deep metric learning algorithms in
tf.contrib.losses.metric_learning.

5. Implementation details

Algorithm 1 Learning algorithm

input Î¸emb
initialize Î¸f = [Î¸b, Î¸d]

b

(pretrained metric learning base model); Î¸d, k

for t = 1, . . . , MAXITER do

Sample a minibatch {xi} and initialize S1
for v = 1, Â· Â· Â· , k do

z = âˆ…

Update the ï¬‚ow network Gâ€² by computing class cost vectors

p = 1
cv

mPi:yi=p f (xi; Î¸f )v

Compute the hash codes {hv
Update S v+1
z and {hv
i }
Remap the label to compute yv

given S v

z

i } via minimum cost ï¬‚ow on Gâ€²

end for
Update the network parameter given the hash codes

Î¸f â† Î¸f âˆ’ Î·(t)âˆ‚Î¸f

â„“metric(Î¸f ; hv

1:nc

, yv

1:nc )

k

Xv=1

Update stepsize Î·(t) â† ADAM rule [12]

end for

output Î¸f (ï¬nal estimate);

Network architecture For fair comparison, we follow
the protocol in [11] and use the NIN [15] architecture (de-
note the parameters Î¸b) with leaky relu [30] with Ï„ = 5.5
as activation function and train Triplet embedding network
with semi-hard negative mining [21], Npairs network [23]
from scratch as the base model, and snapshot the network
weights (Î¸emb
) of the learned base model. Then we replace
the last layer in (Î¸emb
) with a randomly initialized dk di-
mensional fully connected projection layer (Î¸d) and ï¬netune
the hash network (denote the parameters as Î¸f = [Î¸b, Î¸d]).
Algorithm 1 summarizes the learning procedure.

b

b

b

b

Hash table construction and query We use the learned
hash network Î¸f and apply Equation (1) to convert xi into
the hash code h(xi; Î¸f ) and use the base embedding net-
work Î¸emb
to convert the data into the embedding represen-
tation f (xi; Î¸emb
). Then, the embedding representation is
hashed to buckets corresponding to the ks set bits in the hash
code. During inference, we convert a query data xq into the
hash code h(xq; Î¸f ) and into the embedding representation
f (xq; Î¸emb
). Once we retrieve the union of all bucket items
indexed at the ks set bits in the hash code, we apply a rerank-
ing procedure [27] based on the euclidean distance in the
embedding space.

b

Evaluation metrics Following the evaluation protocol
in [11], we report our accuracy results using precision@k
(Pr@k) and normalized mutual information (NMI) [17] met-
rics. Precision@k is computed based on the reranked order-
ing (described above) of the retrieved items from the hash
table. We evaluate NMI, when the code sparsity is set to
ks = 1, treating each bucket as an individual cluster. We
report the speedup results by comparing the number of re-
trieved items versus the total number of data (exhaustive
linear search) and denote this metric as SUF.

6. Experiments

We report our results on Cifar-100 [13] and ImageNet
[20] datasets and compare against several baseline methods.
First baseline methods are the state of the art deep metric
learning models [21, 23] performing an exhaustive linear
search over the whole dataset given a query data (denote
as â€˜Metricâ€™). Next baseline is the Binarization transform
[1, 32] where the dimensions of the hash code corresponding
to the top ks dimensions of the embedding representation
are set (denote as â€˜Thâ€™). Then we perform vector quantiza-
tion [27] on the learned embedding representation from the
deep metric learning methods above on the entire dataset
and compute the hash code based on the indices of the ks
nearest centroids (denote as â€˜VQâ€™). Another baseline is the
quantizable representation in [11](denote as [11]). In both
Cfar-100 and ImageNet, we follow the data augmentation
and preprocessing steps in [11] and train the metric learn-
ing base model with the same settings in [11] for fair com-
parison. In Cifar-100 experiment, we set (d, k) = (32, 2)
and (d, k) = (128, 2) for the npairs network and the triplet
network, respectively.
In ImageNet experiment, we set
(d, k) = (512, 2) and (d, k) = (256, 2) for the npairs net-
work and the triplet network, respectively. In ImageNetSplit
experiment, we set (d, k) = (64, 2). We also perform LSH
hashing [10] baseline and Deep Cauchy Hashing [6] baseline
which both generate n-bit binary hash codes with 2n buckets
and compare against other methods when ks = 1 (denote as
â€˜LSHâ€™ and â€˜DCHâ€™, respectively). For the fair comparison,
we set the number of buckets, 2n = dk.

611384

test

train

test

train

Triplet

Npairs

ks

1

2

3

4

Method

Metric

LSH
DCH

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

SUF

1.00

138.83
96.13
41.21
22.78
97.67
97.67

14.82
5.63
76.12
98.38

7.84
2.83
42.12
94.55

4.90
1.91
16.19
92.18

Pr@1

Pr@4

Pr@16

56.78

55.99

53.95

52.52
56.26
54.82
56.74
57.63
58.42

56.55
56.78
57.30
58.39

56.78
56.78
56.97
58.19

56.84
56.77
57.11
58.52

48.67
55.65
52.88
55.94
57.16
57.88

55.62
56.00
56.70
57.51

55.91
55.99
56.25
57.42

56.01
55.99
56.21
57.79

39.71
54.26
48.03
53.77
55.76
56.58

52.90
53.99
55.19
56.09

53.64
53.95
54.40
56.02

53.86
53.94
54.20
56.22

SUF

1.00

135.64
89.60
43.19
40.35
97.77
97.28

15.34
6.94
78.28
97.20

8.04
2.96
44.36
93.69

5.00
1.97
16.52
91.27

Pr@1

Pr@4

Pr@16

62.64

61.91

61.22

60.45
61.06
61.56
62.54
63.85
64.73

62.41
62.66
63.60
64.35

62.66
62.62
62.87
63.60

62.66
62.62
62.81
64.20

58.10
60.80
60.24
61.78
63.40
64.63

61.68
61.92
63.19
63.91

61.88
61.92
62.22
63.35

61.94
61.91
62.14
63.95

54.00
60.81
58.23
60.98
63.39
64.69

60.89
61.26
63.09
63.81

61.16
61.22
61.84
63.32

61.24
61.22
61.58
63.63

SUF

1.00

29.74
41.59
12.72
34.86
54.85
101.1

5.09
6.08
16.20
69.48

3.10
2.66
7.25
57.09

2.25
1.66
4.51
49.43

Pr@1

Pr@4

Pr@16

57.05

55.70

53.91

53.55
57.23
54.95
56.76
58.19
58.28

56.52
57.13
57.27
57.60

56.97
57.01
57.15
57.56

57.02
57.03
57.15
57.75

50.75
56.25
52.60
55.35
57.22
57.79

55.28
55.74
55.98
56.98

55.56
55.69
55.81
56.70

55.64
55.70
55.77
56.79

43.03
54.45
47.16
53.75
55.87
56.92

53.04
53.90
54.42
55.82

53.76
53.90
54.10
55.41

53.88
53.91
54.01
55.50

SUF

1.00

30.75
40.49
13.65
31.35
54.90
97.47

5.36
5.44
16.51
69.91

3.21
2.36
7.32
58.62

2.30
1.55
4.52
50.80

Pr@1

Pr@4

Pr@16

61.78

60.63

59.73

59.87
61.59
60.80
61.22
63.11
63.06

61.65
61.82
61.98
62.19

61.75
61.78
61.90
62.30

61.78
61.78
61.81
62.43

58.34
60.77
59.49
60.24
62.29
62.62

60.50
60.56
60.93
61.71

60.66
60.62
60.80
61.44

60.66
60.62
60.69
61.65

55.35
60.12
57.27
59.34
61.94
62.44

59.50
59.70
60.15
61.27

59.73
59.73
59.96
60.91

59.75
59.73
59.77
61.01

Table 1: Results with Triplet network with hard negative mining and Npairs network. Querying test data against a hash table
built on test set and a hash table built on train set on Cifar-100.

Triplet

Npairs

Triplet

Npairs

Method

SUF

Pr@1 Pr@4 Pr@16

SUF

Pr@1 Pr@4 Pr@16

ks Metric

1.00

10.90

9.39

1

2

3

8.86
LSH
164.25
9.82
DCH 140.77
18.81
10.20
146.26 10.37
221.49 11.00
590.41 10.91

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

Th
VQ
[11]
Ours

10.82
6.33
10.88
32.83
60.25
11.10
533.86 11.14

10.87
3.64
10.90
13.85
27.16
11.20
477.86 11.21

7.23
8.43
8.58
8.84
9.59
9.58

9.30
9.33
9.64
9.72

9.38
9.38
9.55
9.72

7.45

5.04
6.44
6.50
6.90
7.83
7.85

7.32
7.39
7.73
7.96

7.42
7.44
7.60
7.94

1.00

15.73 13.75

11.08

112.31
220.52

1.74

451.42
478.46
952.49

11.71
8.98
13.87 11.77
15.06 12.92
15.20 13.27
16.95 15.27
17.00 15.53

1.18

15.70 13.69
15.62 13.68
116.26
116.61
16.40 14.49
1174.35 17.22 15.57

1.07
55.80
53.98

15.73 13.74
15.74 13.74
16.24 14.32
1297.98 17.09 15.37

5.56
8.99
9.92
10.96
13.06
13.54

10.96
11.15
12.00
13.63

11.07
11.12
11.73
13.39

Table 2: Results with Triplet network with hard negative
mining and Npairs [23] Network. Querying ImageNet val
data against hash table built on val set.

6.1. Cifar 100

Cifar-100 [13] dataset has 100 classes. Each class has
500 images for train and 100 images for test. Given a query
image from test, we experiment the search performance
both when the hash table is constructed from train and from
test. The batch size is set to 128 in Cifar-100 experiment.
We ï¬netune the base model for 70k iterations and decayed
the learning rate to 0.3 of previous learning rate after 20k
iterations when we optimize our methods. Table 1 shows
the results from the triplet network and the npairs network
respectively. The results show that our method not only
outperforms search accuracies of the state of the art deep
metric learning base models but also provides the superior
speedup over other baselines.

Cifar-100

train

test

62.94
86.11
68.20
76.85
89.11
89.95

53.11
68.88
54.95
62.68
68.95
69.64

ImageNet

val

37.90
45.55
31.62
45.47
48.52
61.21

Cifar-100

train

test

43.80
80.74
51.46
80.25
84.90
86.80

37.45
65.62
44.32
66.69
68.56
71.30

ImageNet

val

36.00
50.01
15.20
53.74
55.09
65.49

LSH
DCH

Th
VQ
[11]
Ours

Table 3: Hash table NMI for Cifar-100 and Imagenet.

ks

1

2

3

Method

Metric

LSH
Th

VQ-train
VQ-test

[11]
Ours

Th

VQ-train
VQ-test

[11]
Ours

Th

VQ-train
VQ-test

[11]
Ours

SUF

1.00

33.75
10.98
54.30
57.44
56.35
78.23

4.55
15.29
16.43
15.99
71.14

2.79
7.80
8.20
7.24
84.04

Pr@1

Pr@4

Pr@16

21.55

18.49
20.25
20.15
20.59
21.35
21.46

21.27
21.51
21.58
22.12
22.12

21.53
21.56
21.58
22.18
21.97

19.11

15.50
17.22
18.10
18.31
18.49
18.88

18.86
19.03
18.93
19.21
18.63

19.11
19.11
19.09
19.40
18.87

16.06

11.14
13.66
14.85
15.32
15.32
15.67

15.68
15.88
15.94
15.95
15.34

15.99
16.03
16.06
16.10
15.56

Table 4: Results with Triplet network with hard negative
mining. Querying ImageNet val set in Ctest against hash
table built on val set in Ctest.

6.2. ImageNet

ImageNet ILSVRC-2012 [20] dataset has 1, 000 classes
and comes with train (1, 281, 167 images) and val set
(50, 000 images). We use the ï¬rst nine splits of train set

711385

â„([8]

â„([14]

â„([12]

â„([6]

â„([21]

]

0

[

"

â„

â„"

[2]

â„([9]

(cid:1)

â„([6]

Root

depth 1

(cid:1)

depth 2

â„"

[11]

]

0

1

[

"

â„

â„([29]

â„([2]

â„([5]

â„([20]

â„([11]

Figure 3: Visualization of the examples mapped by our
trained three level hash codes [h(1), h(2)] on Cifar-100. Each
parent node (denoted as depth 1) is color coded in red, yellow,
blue, and green in cw order. Each color coded box (denoted
as depth 2) shows examples of the hashed items in each child
node.

to train our model, the last split of train set for validation,
and use validation dataset to test the query performance.
We use the images downsampled to 32 Ã— 32 from [8]. We
ï¬netune npairs base model and triplet base model as in [11]
and add a randomly initialized fully connected layer to learn
hierarchical representation. Then, we train the parameters
in the newly added layer with other parameters ï¬xed. When
we train with npairs loss, we set the batch size to 1024 and
train for 15k iterations decaying the learning rate to 0.3 of
previous learning rate after each 6k iterations. Also, when
we train with triplet loss, we set the batch size to 512 and
train for 30k iterations decaying the learning rate of 0.3 of
previous learning rate after each 10k iterations. Our results
in Table 2 show that our method outperforms the state of
the art deep metric learning base models in search accuracy
while providing up to 1298Ã— speedup over exhaustive linear
search. Table 3 compares the NMI metric and shows that
the hash table constructed from our representation yields
buckets with signiï¬cantly better class purity on both datasets
and on both the base metric learning methods.

ageNet ILSVRC-2012 dataset. Then, we split the two sub-
classes of each 119 super-class into Ctrain and Ctest, where
Ctrain âˆ© Ctest = âˆ…. Section D in the supplementary material
shows the class names in Ctrain and Ctest. We use the images
downsampled to 32 Ã— 32 from [8]. We train the models
with triplet embedding on Ctrain and test the models on Ctest.
The batch size is set to 200 in ImageNetSplit dataset. We
ï¬netune the base model for 50k iterations and decayed the
learning rate to 0.3 of previous learning rate after 40k iter-
ations when we optimize our methods. We also perform
vector quantization with the centroids obtained from Ctrain
(denote as â€˜VQ-trainâ€™) and Ctest (denote as â€˜VQ-testâ€™), re-
spectively. Table 4 shows our method preserves the accuracy
without compromising the speedup factor.

Note, in all our experiments in Tables 1 to 4, while all the
baseline methods show severe degradation in the speedup
over the code compound parameter ks, the results show
that the proposed method robustly withstands the speedup
degradation over ks. This is because our method 1) greatly
increases the quantization granularity beyond other base-
line methods and 2) hashes the items more uniformly over
the buckets.
In effect, indexing multiple buckets in our
quantized representation does not as adversarially effect
the search speedup as other baselines. Figure 3 shows a
qualitative result with npairs network on Cifar-100, where
d = 32, k = 2, ks = 1. As an interesting side effect, our
qualitative result indicates that even though our method does
not use any super/sub-class labels or the entire label infor-
mation during training, optimizing for the objective in Equa-
tion (2) naturally discovers and organizes the data exhibiting
a meaningful hierarchy where similar subclasses share com-
mon parent nodes.

7. Conclusion

We have shown a novel end-to-end learning algorithm
where the quantization granularity is signiï¬cantly increased
via hierarchically quantized representations while preserv-
ing the search accuracy and maintaining the computational
complexity practical for the mini-batch stochastic gradient
descent setting. This not only provides the state of the art
accuracy results but also unlocks signiï¬cant improvement in
inference speedup providing the highest reported inference
speedup on Cifar100 and ImageNet datasets respectively.

6.3. ImageNetSplit

Acknowledgements

In order to test the generalization performance of our
learned representation against previously unseen classes,
we performed an experiment on ImageNet where the set of
classes for training and testing are completely disjoint. Each
class in ImageNet ILSVRC-2012 [20] dataset has super-
class based on WordNet [18]. We select 119 super-classes
which have exactly two sub-classes in 1000 classes of Im-

This work was partially supported by Kakao, Kakao
Brain and Basic Science Research Program through
the National Research Foundation of Korea (NRF)
(2017R1E1A1A01077431). Hyun Oh Song is the corre-
sponding author.

811386

[22] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learn-
ing transferrable representations for unsupervised domain
adaptation. In NIPS, 2016. 2

[23] K. Sohn. Improved deep metric learning with multi-class

n-pair loss objective. In NIPS, 2016. 1, 3, 4, 5, 6, 7

[24] H. O. Song, S. Jegelka, V. Rathod, and K. Murphy. Deep

metric learning via facility location. In CVPR, 2017. 1

[25] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016. 1

[26] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriï¬ca-
tion. In CVPR, 2014. 1

[27] J. Wang, T. Zhang, J. Song, N. Sebe, and H. T. Shen. A survey
on learning to hash. arXiv preprint arXiv:1606.00185, 2016.
1, 2, 6

[28] X. Wang and A. Gupta. Unsupervised learning of visual

representations using videos. In ICCV, 2015. 2

[29] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan. Supervised hashing
for image retrieval via image representation learning. In AAAI,
2014. 2

[30] B. Xu, N. Wang, T. Chen, and M. Li. Empirical evaluation of
rectiï¬ed activations in convolutional network. arXiv preprint
arXiv:1505.00853, 2015. 6

[31] Y. Yuan, K. Yang, and C. Zhang. Hard-aware deeply cascaded

embedding. In ICCV, 2017. 1, 2

[32] A. Zhai, D. Kislyuk, Y. Jing, M. Feng, E. Tzeng, J. Donahue,
Y. L. Du, and T. Darrell. Visual discovery at pinterest. In
Proceedings of the 26th International Conference on World
Wide Web Companion, 2017. 1, 6

[33] F. Zhao, Y. Huang, L. Wang, and T. Tan. Deep semantic
In

ranking based hashing for multi-label image retrieval.
CVPR, 2015. 2

References

[1] P. Agrawal, R. Girshick, and J. Malik. Analyzing the perfor-
mance of multilayer neural networks for object recognition.
In ECCV, 2014. 1, 6

[2] S. Bell and K. Bala. Learning visual similarity for product
design with convolutional neural networks. In SIGGRAPH,
2015. 1, 2

[3] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy
minimization via graph cuts. IEEE Transactions on pattern
analysis and machine intelligence, 2001. 3

[4] J. Bromley, I. Guyon, Y. Lecun, E. Sackinger, and R. Shah.
Signature veriï¬cation using a "siamese" time delay neural
network. In NIPS, 1994. 1

[5] M. Bucher, S. Herbin, and F. Jurie.

Improving semantic
embedding consistency by metric learning for zero-shot clas-
sifï¬cation. In ECCV, 2016. 1, 2

[6] Y. Cao, M. Long, B. Liu, and J. Wang. Deep cauchy hashing
for hamming space retrieval. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
6

[7] Y. Cao, M. Long, J. Wang, H. Zhu, and Q. Wen. Deep
quantization network for efï¬cient image retrieval. In AAAI,
2016. 2

[8] P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled
variant of imagenet as an alternative to the cifar datasets.
arXiv preprint arXiv:1707.08819, 2017. 8

[9] R. Hadsell, S. Chopra, and Y. Lecun. Dimensionality re-
duction by learning an invariant mapping. In CVPR, 2006.
1

[10] P. Jain, B. Kulis, and K. Grauman. Fast image search for

learned metrics. In CVPR, 2008. 6

[11] Y. Jeong and H. O. Song. Efï¬cient end-to-end learning for

quantizable representations. In ICML, 2018. 1, 2, 3, 6, 7, 8

[12] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 6

[13] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-100 (canadian

institute for advanced research). 2009. 1, 6, 7

[14] Q. Li, Z. Sun, R. He, and T. Tan. Deep supervised discrete

hashing. In NIPS, 2017. 2

[15] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR,

abs/1312.4400, 2013. 6

[16] S. Liu and H. Lu. Learning deep representations with diode
loss for quantization-based similarity search. In IJCNN, 2017.
2

[17] C. D. Manning, P. Raghavan, and H. Schutze. Introduction to
Information Retrieval. Cambridge university press, 2008. 6
[18] G. A. Miller. Wordnet: a lexical database for english. Com-

munications of the ACM, 1995. 8

[19] M. Norouzi, D. J. Fleet, and R. R. Salakhutdinov. Hamming

distance metric learning. In NIPS, 2012. 2

[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,
and L. Fei-Fei. ImageNet Large Scale Visual Recognition
Challenge. IJCV, 2015. 1, 6, 7, 8

[21] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniï¬ed embedding for face recognition and clustering. In
CVPR, 2015. 1, 3, 5, 6

911387

