Iterative Projection and Matching: Finding Structure-preserving

Representatives and Its Application to Computer Vision

Alireza Zaeemzadeh‚àó, Mohsen Joneidi‚àó, Nazanin Rahnavard, and Mubarak Shah

{zaeemzadeh, joneidi, nazanin}@eecs.ucf.edu, shah@crcv.ucf.edu

University of Central Florida

Abstract

Data selection task is not trivial and possibly implies ad-

The goal of data selection is to capture the most struc-
tural information from a set of data. This paper presents
a fast and accurate data selection method, in which the se-
lected samples are optimized to span the subspace of all
data. We propose a new selection algorithm, referred to as
iterative projection and matching (IPM), with linear com-
plexity w.r.t.
the number of data, and without any pa-
rameter to be tuned. In our algorithm, at each iteration,
the maximum information from the structure of the data is
captured by one selected sample, and the captured infor-
mation is neglected in the next iterations by projection on
the null-space of previously selected samples. The compu-
tational efÔ¨Åciency and the selection accuracy of our pro-
posed algorithm outperform those of the conventional meth-
ods. Furthermore, the superiority of the proposed algorithm
is shown on active learning for video action recognition
dataset on UCF-101; learning using representatives on Im-
ageNet; training a generative adversarial network (GAN)
to generate multi-view images from a single-view input on
CMU Multi-PIE dataset; and video summarization on UTE
Egocentric dataset.

1. Introduction

Thanks to recent advances in computing, deep learning
based systems, which employ very large numbers of inputs,
have been developed in the last decade. However, process-
ing/labeling/communication of a large number of input data
has remained challenging. Therefore, novel machine learn-
ing methods that make the best use of a signiÔ¨Åcantly less
amount of data are of great interest. For example, active
learning (AL) [26] aims at addressing this problem by train-
ing a model using a small number of labeled data, testing on
the trained model, and then querying the labels of some se-
lected data, which then are used for training a new model.
In this context, preserving the underlying structure of data
by a succinct format is an essential concern.

‚àóindicates shared Ô¨Årst authorship.

K(cid:1) possibili-

dressing an NP-hard problem (i.e., there are(cid:0)M

ties of choosing K distinct sample out of M available ones).
This means that an optimal solution cannot be efÔ¨Åciently
computed when the number of available data becomes ex-
cessively large. A convex relaxation of the original NP-hard
problem has been suggested in terms of the D-optimal and
A-optimal solutions [1, 23].
In addition to convex relax-
ation, a sub-modular cost function as the criterion of selec-
tion, allows us to employ much faster greedy optimization
methods for selection [36]. The stochastic implementation
of D-optimal solution is referred to volume sampling (VS),
which is a fast and well-studied method. VS selects each
subset of data, which are organized in the rows of a ma-
trix, with probability proportional to the determinant (vol-
ume) of the reduced matrix. Moreover, QR decomposition
with column pivoting (QRCP) and convex hull-based selec-
tion methods have been suggested for optimal data selection
[10, 9]. All the mentioned methods aim to select the most
diverse subset of data in an optimal sense. However, these
methods do not guarantee that the un-selected samples are
well-covered by the selected ones. Further, outliers are se-
lected with a high probability using such algorithms due to
their diversity, unless preprocessed by outlier detection al-
gorithms [35]. Authors in [22] address this problem via a
two-phase algorithm. There are some other efforts for out-
lier rejection in the selection procedure [34, 42]. However,
the outlier and inlier data are not well-deÔ¨Åned and these
methods are not consistent with general data.

There is another more effective approach for subset se-
lection, which chooses data such that the selected samples
are able to approximate the rest of data accurately. This se-
lection problem is formulated using a convex optimization
problem and referred as sparse modeling representative se-
lection (SMRS) algorithm [12]. The same goal is pursued
by dissimilarity-based sparse subset selection (DS3), which
is based on simultaneous sparse recovery for Ô¨Ånding data
representatives [11]. Representative approaches, such as
SMRS and DS3, provide more suitable subset rather than
selecting some diverse samples. However, their computa-

15414

tional burden is not tractable for large datasets. Moreover,
SMRS and DS3 algorithms utilize some parameters in their
implementation, which makes their Ô¨Åne tuning difÔ¨Åcult.

In order to address above issues, we propose a novel
representative-based selection method, referred to as Iter-
ative Projection and Matching (IPM). In our algorithm, at
each iteration the maximum information from structure of
the data is captured by one selected sample, and the cap-
tured information is neglected in the next iterations by pro-
jection on the null-space of previously selected samples. In
summary, this paper makes the following contributions:

‚Ä¢ The complexity of IPM is linear w.r.t. number of orig-

inal data. Hence, IPM is tractable for larger datasets.

‚Ä¢ IPM has no parameters for Ô¨Åne tuning, unlike some
existing methods [11, 12]. This makes IPM dataset-
and problem-independent.

if T ‚äÇ {1, 2, . . . , M } is any subset with cardinality K, cho-
sen with probability proportional to det(ATAT

T ), then1,

E{kA ‚àí œÄT(A)k2

F } ‚â§ (K + 1)kA ‚àí AK k2
F ,

(1)

where, œÄT(A) is a matrix representing projection of rows
of A on to the span of selected rows indexed by T. E in-
dicates expectation operator w.r.t. all the combinatorial se-
lection of K rows of A out of M . AK is the best rank-K
approximation of A that can be obtained by singular value
decomposition and k.k2
F is the Frobenius norm. VS is not
a deterministic selection algorithm, as it gives a probabil-
ity of selection for any subset of samples, and for which
only a loose upper bound for the expectation of projection
error is guaranteed. In contrast, in this paper a determinis-
tic algorithm is proposed based on direct minimization of
projection error using a new optimization mechanism.

‚Ä¢ Robustness of the proposed solution is investigated

2.2. Representative Selection

theoretically.

‚Ä¢ The superiority of the proposed algorithm is shown in

different computer vision applications.

2. Problem Statement and Related Work

Let a1, a2, . . . , aM ‚àà RN be M given data points of
dimension N . We deÔ¨Åne an M √ó N matrix, A, such that
aT
m is the mth row of A, for m = 1, 2, . . . , M . The goal
is to reduce this matrix into a K √ó N matrix, AR, based
on an optimality metric. In this section, we introduce some
related work on matrix subset selection and data selection.

2.1. Selection Based on Diversity

Consider a large system of equations y = Aw, which
can be interpreted as a simple linear classiÔ¨Åer in which y is
the vector of labels, A represents the training data and w is
the classiÔ¨Åer weights. An optimal sense for data selection
is to reduce this system of equations to a smaller system,
yR = AR ÀÜw, such that the reduced subsystem estimates the
same classiÔ¨Åer as the original system, i.e., the estimation
error of ÀÜw is minimized [6].

A typical selection objective is to minimize EŒΩ{kw ‚àí
ÀÜwk2}, where EŒΩ is expectation w.r.t. noise distribution of
w ‚àí ÀÜw. This criterion is referred as A-optimal design in
the literature of optimization.
It is an NP hard problem,
which can be solved via convex relaxation with computa-
tional complexity of O(M 3) [23].

However, there are other criteria which have interesting
properties. For example D-optimal design optimizes the de-
terminant of a reduced matrix [23]. There are several other
efforts in this area [7, 8, 27, 13, 21]. Inspired by D-optimal
design, volume sampling (VS), which has received lots of
attention, considers a selection probability for each subset
of data, which is proportional to the determinant (volume)
of the reduced matrix [27, 32, 6]. VS theory expresses that

A method for sampling from a set of data is proposed by
Elhamifar et. al. based on sparse modeling representative
selection (SMRS) [12]. Their proposed cost function for
data selection is the error of projecting all the data onto the
subspace spanned by the selected data. Mathematically, the
optimization problem in [12] can be written as,

kA ‚àí œÄT(A)k2
F .

argmin
|T|=K

(2)

This is an NP-hard problem. Their main contribution is
solving this problem via convex relaxation. However, there
is no guarantee that convex relaxation provides the best ap-
proximation for an NP-hard problem. Furthermore, such
methods that try to solve the selection problem via convex
programming are usually too computationally intensive for
large datasets [12, 11, 31, 29]. In this paper, we propose a
new fast algorithm for solving Problem (2).

Dissimilarity-based Sparse Subset Selection (DS3) algo-
rithm selects a subset of data based on pairwise distance of
all data to some target points [11]. DS3 considers a source
dataset and its goal is to encode the target data according to
pairwise dissimilarity between each sample of source and
target datasets. This algorithm can be interpreted as the
non-linear implementation of SMRS algorithm [11].

3. Iterative Projection and Matching (IPM)

In this section, an iterative and computationally efÔ¨Åcient
algorithm is proposed for approximating the solution to the
NP-hard selection problem (2). The proposed algorithm it-
eratively Ô¨Ånds the best direction on the unit sphere2, and
then from the available samples in dataset selects the sam-
ple with the smallest angle to the found direction.

1AT is the selected rows of A indexed by set T.
2In unit sphere, every point corresponds to a unique direction.

5415

Projection of all the data on to the subspace spanned
by the K rows of A, indexed by T, i.e., œÄT(A), can be
expressed by a rank-K factorization, U V T , where U ‚àà
RM√óK , V T ‚àà RK√óN , and V T includes the K rows of A,
indexed by T, and normalized to have unit length. There-
fore, optimization problem (2) can be restated as

argmin

U ,V

kA ‚àí U V T k2

F s.t. vk ‚àà A,

(3)

where, A = { Àúa1, Àúa2, . . . , ÀúaM }, Àúam = am/kamk2, and
vk is the kth column of V . It should be noted that V T is
restricted to be a collection of K normalized rows of A,
while there is no constraint on U . Assume we are to select
one sample at a time, which is the best representation of
all data. Since Problem (3) involves a combinatorial search
and is not easy to tackle, let us modify (3) into two con-
secutive problems. The Ô¨Årst sub-problem relaxes the con-
straint vk ‚àà A in (3) to a moderate constraint kvk = 1,
and the second sub-problem reimposes the underlying con-
straint. These sub-problems are formulated as

(u, v) =argmin

kA ‚àí uvT k2

F s.t. kvk = 1,

u,v

m(1) =argmax

|vT Àúam|.

m

(4a)

(4b)

Here m(1) is the index of the Ô¨Årst selected data point and
am(1) is the selected sample. Subproblem (4a) is equiva-
lent to Ô¨Ånding the Ô¨Årst right singular vector of A. The con-
straint kvk = 1 keeps v on the unit sphere to remove scale
ambiguity between u and v. Moreover, the unit sphere is
a superset for A and keeps the modiÔ¨Åed problem close to
the recast problem (3). After solving for v (which is not
necessarily one of our data points), we Ô¨Ånd the data point
that matches v the most (makes the smallest angle with v)
in (4b).

After selecting the Ô¨Årst data point (am(1) ), we project
all data points onto the null space of the selected sample.
This forms a new matrix A(I ‚àí Àúam(1) ÀúaT
m(1) ), where I is an
identity matrix. We solve (4) with this new matrix to Ô¨Ånd
the second data point. This process will continue until we
select K data points. It should be noted that the null space
of selected sample(s) indicates a subspace that the selected
sample(s) cannot span. Therefore, the next selected data is
obtained by only searching in this null space.

Algorithm 1 shows the steps of the proposed iterative
projection and matching (IPM) algorithm, in which m(k)
denotes the index of the selected data at the kth iteration.
IPM is a low-complexity algorithm with no parameters to
be tuned. These features in addition to its superior perfor-
mance (as will be shown in many scenarios in Section 4)
make IPM very desirable for a wide range of applications.

Time complexity order of computing the Ô¨Årst singular
component of an M √ó N matrix is O(M N ) [4]. As the pro-

Figure 1: A toy example that illustrates the Ô¨Årst iteration of IPM.
(Left) The most matched sample with the Ô¨Årst right singular vector,
v, is selected. (Right) The rest of samples are projected on the null
space of the selected sample in order to continue selection in the
lower dimensional subspace.

posed algorithm only needs the Ô¨Årst singular component for
each selection, its time complexity is O(KN M ), which is
much faster than convex relaxation-based algorithms with
complexity O(M 3) [23]. Moreover, IPM performs faster
than K-medoids algorithm, whose complexity is of order
O(KN (M ‚àí K)2) [41]. It is also worthwhile to mention
that the condition that needs to be satisÔ¨Åed for a good per-
formance is K ‚â§ N < M . This ensures that the calculated
singular vector is reliable and not impacted by noise. This
condition is satisÔ¨Åed in subset selection scenarios, where
the dataset is large, the number of selected samples is a lot
less than the number of samples (K ‚â™ M ), and we have
freedom over the dimension of the samples/features (N ).

Algorithm 1 Iterative Projection and Matching Algorithm

Require: A and K

Output: AT

1: Initialization:

A(1) ‚Üê‚àí A
T = {}
for k = 1, ¬∑ ¬∑ ¬∑ , K

v ‚Üê‚àí Ô¨Årst right singular-vector of A(k) by solving (4a)
2:
3: m(k) ‚Üê‚àí index of the most correlated data with v (4b)
4:
5: A(k+1) ‚Üê‚àí A(k)(I ‚àí Àúa

T ‚Üê‚àí T ‚à™ m(k)

m(k) ) (null space projection)

m(k) ÀúaT

end

3.1. A Lower Bound on Maximum Correlation

In this section, we will derive a lower bound on the max-
imum of the absolute value of the correlation coefÔ¨Åcient
between data points a1, a2, . . . , aM and v, when data are
normalized on the unit sphere. Figure 1 shows an intuitive
example for one iteration of the proposed algorithm. First,
the leading singular vector is computed, and then the most
correlated sample in the dataset is matched with the com-
puted singular vector. Next, all data are projected onto the
null space of the matched sample. The projected data are
ready to perform one more iteration, if needed. These iter-
ations are terminated either by reaching the desired number

5416

 ùëé1 ùëé2 ùëé3 ùëé4ùë£SelectedSampleNull Space of  ùëé3of selected samples or a given threshold of residual energy.
Next, we present a lemma that guarantees the existence of a
highly correlated sample with the Ô¨Årst right singular vector,
illustrating the fact that the selected sample will not be too
bad.

Lemma 1 Let a1, a2, . . . , aM ‚àà RN be M given data
points of dimension N . Let A denote an M √ó N matrix
m being its mth row for m = 1, 2, . . . , M . Let œÉ1, u
with aT
and v denote the Ô¨Årst singular value, the corresponding left
and right singular vectors of A, respectively. Then, there
exists at least one data point such that the absolute value
of its inner product with v is greater than or equal to œÉ1‚àöM
.
Hence, max

|vT am| ‚â• œÉ1‚àöM

.

m

The following proposition states a lower bound on the
maximum of the absolute value of the correlation between
data points a1, a2, . . . , aM and v, when data are normal-
ized on the unit sphere. First, let us deÔ¨Åne the following
measure.

DeÔ¨Ånition 1 Rank-oneness measure (ROM) of a rank R
matrix A with singular values œÉ1, œÉ2, . . . , œÉR is deÔ¨Åned as

ROM (A) =r œÉ2

PR

1

r=1 œÉ2

r

= œÉ1
kAkF

.

Proposition 1 Assume the rows of A are normalized to lie
on the unit sphere. There exists at least one data point,
i, such that the correlation coefÔ¨Åcient between ai and the
Ô¨Årst right singular vector of A is greater than or equal to
ROM (A).

3.2. Robustness to Perturbation

Data selection algorithms are vulnerable to outlier sam-
ples. Since outlier samples are more spread in the space
of data, their span covers a wider subspace. However, the
spanned subspace by outliers may not be a proper repre-
sentative subspace. DS3 adds a penalty to the cost func-
tion in order to reject outliers [11]. Our proposed algorithm
computes the Ô¨Årst singular vector as the leading direction
in each iteration. We show here that this direction is the
most robust spectral component against changes in the data.
First consider the autocorrelation matrix of data deÔ¨Åned as,

m=1 amaT
m.

C =PM

Eigenvectors of this matrix are equal to right singular
vectors of A. Adding a new row in A does not change the
size of matrix C, but perturbs this matrix. The following
lemma shows the robustness of eigenvectors of C against
perturbations.

Lemma 2 Assume square matrix C and its spectrum
[Œªi, vi]. Then, the following inequality holds,

k‚àÇvik2 ‚â§sXj6=i

1

(Œªi ‚àí Œªj)2 k‚àÇCkF .

tor of a square matrix is deÔ¨Åned by, si ,qPj6=i

DeÔ¨Ånition 2 The sensitivity coefÔ¨Åcient of the ith eigenvec-
(Œªi‚àíŒªj )2 .
It is easy to show that s1 < s2. Based on Lemma 2 and
this deÔ¨Ånition the following proposition suggests a condi-
tion to satisfy s1 < si, ‚àÄi ‚â• 2.

1

Proposition 2 Assume square matrix C and its spectrum
[Œªi, vi], where the gap between consecutive eigenvalues is
decreasing. Then, s1 < si, ‚àÄi ‚â• 2.

The proofs of Propositions and Lemmas in this section
are presented in the supplementary material. Moreover, the
results of Proposition 1 and 2 are also veriÔ¨Åed in supple-
mentary material.

4. Applications of IPM

To validate our theoretical investigation and to empiri-
cally demonstrate the behavior and effectiveness of the pro-
posed selection technique, we have performed extensive
sets of experiments considering several different scenarios.
We divide our experiments into three different subsections.
In Section 4.1, we use our algorithm in the active learning
setting and show that IPM is able to reduce the labelling
cost signiÔ¨Åcantly, by selecting the most informative unla-
beled samples. Next, in Section 4.2, we show the effective-
ness of IPM in selecting the most informative representa-
tives, by training the classiÔ¨Åer using only a few representa-
tives from each class. Lastly, in Section 4.3, the application
of IPM for video summarization is exhibited. In addition,
we investigate the robustness and other performance met-
rics, such as projection error and running time, of different
selection methods and verify our theoretical results in the
supplementary material3.

4.1. Active Learning

Active learning aims at addressing the costly data label-
ing problem by iteratively training a model using a small
number of labeled data, and then querying the labels of
some selected data, using an acquisition function.

In active learning, the model is initially trained using a
small set of labeled data (the initial training set). Then, the
acquisition function selects a few points from the pool of
unlabeled data, asks an oracle (often a human expert) for
the labels, and adds them to the training set. Next, a new
model is trained on the updated training set. By repeating
these steps, we can collect the most informative samples,
which often result in signiÔ¨Åcant reductions in the labeling
cost. Now, the fundamental question in active learning is:
Given a Ô¨Åxed labeling budget, what are the best unlabeled
data instances to be selected for labeling for the best perfor-
mance?

3Code for IPM is available at cwnlab.eecs.ucf.edu/ipm/

5417

Mean samples/class

2

3

4

5

6

7

8

Random
Spectral Clustering
K-medoids
OMP
DS3 [11]
Uncertainty [15]
IPM
IPM + Uncertainty

60.1 ¬± 0.7
62.3 ¬± 1.9
60.1 ¬± 2.2
64.2 ¬± 0.6
64.0 ¬± 1.5
59.5 ¬± 0.4
64.6 ¬± 0.7
64.3 ¬± 0.4

65.1 ¬± 1.2
66.9 ¬± 1.1
65.3 ¬± 1.0
66.6 ¬± 0.7
66.5 ¬± 0.7
66.7 ¬± 1.6
68.7 ¬± 0.3
69.4 ¬± 0.8

68.2 ¬± 1.7
68.1 ¬± 0.7
68.4 ¬± 1.6
70.8 ¬± 1.5
67.8 ¬± 1.2
69.4 ¬± 1.7
72.2 ¬± 1.0
72.8 ¬± 1.0

69.9 ¬± 1.4
68.9 ¬± 0.3
69.2 ¬± 0.5
71.7 ¬± 0.4
68.3 ¬± 0.5
71.5 ¬± 1.5
73.4 ¬± 0.9
73.8 ¬± 0.9

71.7 ¬± 0.6
70.8 ¬± 0.9
72.3 ¬± 0.7
74.3 ¬± 0.7
69.6 ¬± 1.1
73.9 ¬± 0.3
74.3 ¬± 0.4
76.2 ¬± 1.0

73.0 ¬± 0.6
71.0 ¬± 2.2
73.6 ¬± 0.4
74.3 ¬± 0.3
70.9 ¬± 1.3
75.5 ¬± 0.7
74.7 ¬± 1.4
76.3 ¬± 0.3

74.8 ¬± 0.5
71.6 ¬± 0.1
74.5 ¬± 0.6
75.4 ¬± 0.2
71.9 ¬± 0.9
75.9 ¬± 1.1
75.3 ¬± 0.6
77.9 ¬± 0.2

Table 1: ClassiÔ¨Åcation accuracy (%) for action recognition on UCF-101, at different active learning cycles. The initial training set (cycle
1) is the same for all the methods. The accuracy for cycle 1 is 54.2% and the accuracy using the full training set (9537 samples) is 82.23%.

In many active learning frameworks, new data points are
selected based on the model uncertainty. However, the ef-
fect of such selection only kicks in after the size of the train-
ing set is large enough, so we can have a reliable uncertainty
measure. In this section, we show that the proposed selec-
tion method can effectively Ô¨Ånd the best representatives of
the data and outperforms several recent uncertainty-based
and algebraic selection methods.

In particular, we study IPM for active learning of video
action recognition, using the 3D ResNet18 architecture4, as
described in [20]. The experiments are run on UCF-101
human action dataset [37], and the network is pretrained on
Kinetics-400 dataset [24]. We provide the results on split 1.
To ensure that at least one sample per class exists in the
training set, for the initial training, one sample per class is
selected randomly and the fully-connected layer of the clas-
siÔ¨Åer is Ô¨Åne tuned. Then, at each active learning cycle, one
sample per class is selected, without the knowledge of the
labels, and added to the training set. Next, using the updated
training set, the fully connected layer of the network is Ô¨Åne
tuned for 60 epochs, using learning rate of 10‚àí1, weight de-
cay of 10‚àí3, and batch size of 24 on 2 GPUs. Rest of the
implementation and training settings are the same as [20].
Note that, in this experiment, Ô¨Åne-tuning is only performed
to train the fully connected layer, because it achieved the
best accuracy during the preliminary investigation for very
small training sets, which is the scope of this experiment.

The selection is performed on the convolutional features
extracted from the last convolutional layer of the network.
Table 1 shows the accuracy of the trained network at each
active learning cycle for different selection methods. The
high computational complexity of DS3 prevents its imple-
mentation on all the data5 [11]. So, we provide the results
for DS3 only for the clustered version, meaning that one
sample per cluster is selected using DS3 (clusters are ob-
tained using spectral clustering). For spectral clustering
results, the extracted features are clustered into 101 clus-
ters, and one sample from each cluster is selected randomly.
Furthermore, OMP, which stands for Orthogonal Matching

4We use the code provided by the authors at https://github.

com/kenshohara/3D-ResNets-PyTorch

Pursuit, selects the samples that are most correlated with
the null space of the selected samples [40, 2]. The OMP
approach is very sensitive to the outliers. Random outliers
have low correlation with the samples and therefore a high
correlation with the null space of the selected samples.

For uncertainty-based selection, Bayesian active learn-
ing [15, 14] is utilized. For that, a dropout unit with pa-
rameter 0.2 is added before the fully-connected layer and
the uncertainty measure is computed by using 10 forward
iterations (following the implementation in [14]). In our ex-
periments, we use variation ratio6 as the uncertainty metric,
which is shown to be the most reliable metric among sev-
eral well-known metrics [15]. Also, for a fair comparison,
the initial training set is the same for all the experiments at
each run.

It is evident that, during the Ô¨Årst few cycles, since the
classiÔ¨Åer is not able to generate reliable uncertainty score,
uncertainty-based selection does not lead to a performance
gain.
In fact, random selection outperforms uncertainty-
based selection. On the other hand, IPM is able to select the
critical samples. In the Ô¨Årst few active learning cycles, IPM
is constantly outperforming other methods, which translates
into signiÔ¨Åcant reductions in labeling cost for applications
such as video action recognition.

As the classiÔ¨Åer is trained with more data, it is able to
provide us with better uncertainty scores. Thus to enjoy the
beneÔ¨Åts of both IPM and uncertainty-based selection, we
can use a compound selection criterion. For the extremely
small datasets, samples should be selected only using IPM.
However, as we collect more data, the uncertainty score
should be integrated into the decision making process. Our
proposed selection algorithm, unlike other methods, easily
lends itself to such modiÔ¨Åcation. At each selection iteration,
instead of selecting the most correlated data with v (line 3
in Algorithm 1), we can select the samples based on the
following criterion:

m‚àó = arg max

Œ± |vT Àúam| + (1 ‚àí Œ±) q(am),

m

where q(.) is an uncertainty measure, e.g. variation ratios.
Parameter Œ± determines the relative importance of the IPM

5We use the code provided by the authors at http://www.ccs.

6Variation ratio of x is deÔ¨Åned as 1 ‚àí maxy p(y|x). which measures

neu.edu/home/eelhami/codes.htm

lack of conÔ¨Ådence.

5418

0.95

3

10

1

10

(a) The Ô¨Årst row is obtained by K-medoids and the second and the
third row show the selection of DS3 and IPM, respectively.

0.8

5

6

7

8

9

10

-1

10

1000

2000

(b) Angles of the selected images. K-medoids selects 8 different
angles. DS3 algorithm selects from 7 angles and our proposed
IPM selects the maximum possible 10 distinguished angles.

Figure 2: Selection of 10 representatives out of 520 images of a
subject and their corresponding angles.

metric versus the uncertainty metric. To gradually increase
the impact of q(.), as the model becomes more reliable, we
start by setting Œ± = 1 and multiply it by decay rate of 0.95 at
each active learning cycle. This compound selection criteria
leads to better results for larger dataset sizes.

4.2. Learning Using Representatives

In this experiment, we consider the problem of learn-
ing using representatives. We Ô¨Ånd the best representatives
for each class and use this reduced training set for learn-
ing. Finding representatives reduces the computation and
storage requirements, and can even be used for tasks such
as clustering. In the ideal case, if we collect the samples
that contain enough information about the distribution of
the whole dataset, the learning performance would be very
close to the performance using all the data.

4.2.1 Finding Representatives for Multi-PIE Dataset

Here, we present our experimental results on CMU Multi-
PIE Face Database [17]. We use 249 subjects from the Ô¨Årst
session with 13 poses, 20 illuminations, and two expres-
sions. Thus, there are 13√ó20√ó2 images per subject. Figure
2a shows 10 selected images from 520 images of a subject.
As it can be seen, the results of K-medoids and DS3 algo-
rithms are concentrated on side views, while our selection
provides images from more diverse angles. Figure2b high-
lights this by showing the angles of selected images of each
algorithm. IPM selects from 10 different angles, while the
selected images by DS3 and K-medoids contain repetitious
angles. Figure 3 shows the performance of different selec-
tion algorithms in terms of normalized projection error and
running time. It is evident that our proposed approach Ô¨Ånds
a better minimizer for Problem deÔ¨Åned in equation (2) and
is able to do so in orders of magnitude less time.

Figure 3: Performance of different methods for minimizing the
cost function of representative selection in equation (2).
(Left)
The ratio of projection error using selection algorithms to projec-
tion error of random selection for selecting K representatives from
each subject, averaged over all the subjects. (Right) Running time
of different algorithms versus number of input samples.

Figure 4: Multi-view face generation results for a sample subject
in testing set using CR-GAN [39]. The network is trained on re-
duced training set (9 images per subject) using random selection
(Ô¨Årst row), K-medoids (second row), DS3 [11] (third row), and
IPM (fourth row). The Ô¨Åfth row shows the results generated by
the network trained on all the data (360 images per subject). IPM-
reduced dataset generates closest results to the complete dataset.

4.2.2 Representatives To Generate Multi-view Images

Using GAN

Next, to investigate the effectiveness of the proposed se-
lection, we use the selected samples to train a generative
adversarial network (GAN) to generate multi-view images
from a single-view input. For that, the GAN architecture
proposed in [39] is employed. Following the experiment
setup in [39], only 9 poses between œÄ
6 are consid-
ered. Furthermore, the Ô¨Årst 200 subjects are for training
and the rest are for testing. Thus, the total size of the train-
ing set is 72, 000, 360 per subject. All the implementation
details are same as [39], unless otherwise is stated7.

6 and 5œÄ

We select only 9 images from each subject (1800 to-
tal), and train the network with the reduced dataset for 300
epochs using the batch size of 36. Figure 4 shows the gener-
ated images of a subject in the testing set, using the trained
network on the reduced dataset, as well as using the com-
plete dataset. The network trained on samples selected by
IPM (fourth row) is able to generate more realistic images,
with fewer artifacts, compared to other selection methods

7We use the code provided by the authors at https://github.

com/bluer555/CR-GAN

5419

0120ùúã/6ùúã/3ùúã/22ùúã/3ùúã5ùúã/6K-medoidsDS3IPMMethod
9 images / subject
360 images / subject

Random K-Medoids
0.5616

0.5993

DS3

0.6022

IPM
0.553

0.5364

Table 2: Identity dissimilarities between real and generated im-
ages by network trained on reduced (using different selection
methods) and complete dataset.

Samples / Class

1

Random
K-medoids
OMP
DS3[11]
IPM

54.6
61.0
51.1
60.8
65.3

2

64.7
67.7
64.6
69.1
72.6

3

69.2
69.4
70.7
74.0
74.9

4

70.5
70.9
72.8
75.2
77.6

5

72.9
71.7
73.0
74.8
77.0

6

74.0
72.0
74.5
75.3
78.5

Table 3: Accuracy (%) of ResNet18 on UCF-101 dataset, trained
using only the representatives selected by different methods. The
accuracy using the full training set (9537 samples) is 82.23%.

(rows 1-3). Furthermore, compared to the results using all
the data (row 5), it is clear that IPM-reduced dataset gen-
erates the closest results to the complete dataset. This is
because, as demonstrated in Figure 2, samples selected by
IPM cover more angles of the subject, leading better train-
ing of the GAN. See supplementary material for further ex-
periments and sample outputs.

For a quantitative performance investigation, we evalu-
ate the identity similarities between the real and generated
images. For that, we feed each pair of real and generated
images to a ResNet188, trained on MS-Celeb-1M dataset
[18], and obtain 256-dimensional features. ‚Ñì2 distances of
features correspond to the face dissimilarity. Table 2 shows
the normalized ‚Ñì2 distances between the real and generated
images, averaged over all the images in the testing set. Our
method outperforms other selection methods in this met-
ric as well. Thus, from Figure 4 (qualitative) and Table 2
(quantitative), we can conclude that the IPM-reduced train-
ing set contains more information about the complete set,
compared to other selection methods.

4.2.3 Finding Representatives for UCF-101 Dataset

Here, similar to Section 4.1, we use a 3D ResNet18 clas-
siÔ¨Åer pretrained on Kinetics-400 dataset, and the selection
algorithms are performed on feature space generated by the
output of the last convolutional layer. To Ô¨Ånd the represen-
tatives, we use the selection methods to sequentially Ô¨Ånd
the most informative representatives from each class. Af-
ter selecting the representatives, the fully connected layer
of the network is Ô¨Ånetuned in the same manner as described
in Section 4.1. Table 3 shows the performance of different
selection methods for different numbers of representatives
per class. As more samples are collected, the performance
gap among different methods, including random, decreases.
This is expected, since Ô¨Ånding only one representative for
each class is a much more difÔ¨Åcult task, compared to choos-
ing many, e.g. 6, representatives.

8We use the naive ResNet18 architecture as described in [3].

2 Samples

5 Samples

10 Samples

i

s
d
o
d
e
m
K

-

3
S
D

M
P

I

(a)

(b)

Figure 5:
t-SNE visualization [25] of two randomly selected
classes of UCF-101 dataset and their representatives selected by
different methods. (a) Decision function learned by using all the
data. The goal of selection is to preserve the structure with only a
few representatives. (b) Decision function learned by using 2 (Ô¨Årst
column), 5 (second column), and 10 (third column) representatives
per class, using K-medoids (Ô¨Årst row), DS3 [11] (second row), and
IPM (third row). IPM can capture the structure of the data better
using the same number of selected samples.

Images per Class

1

5

10

Random
K-Medoids
IPM

(0.08%)

(0.4%)

(0.8%)

3.18
11.78
12.50

8.71
17.01
21.69

12.97
17.56
25.26

50

(4%)

25.61
26.86
30.77

Table 4: Top-1 classiÔ¨Åcation accuracy (%) on ImageNet, using
selected representatives from each class. Accuracy using all the
labeled data ( 1.2M samples) is 46.86%. Numbers in () show the
size of the selected representatives as a % of the full training set.

Using only one representative selected by IPM, we can
achieve a classiÔ¨Åcation accuracy of 65.3%, which is more
than 10% improvement compared to random selection and
more than 4% improvement compared to other competitors.
Figure 5 shows the t-SNE visualization [25] of the selec-
tion process for two randomly selected classes of UCF-101.
To visualize the structure of the data, the contours represent
the decision function of an SVM trained in this 2D space.
Selection is performed on the original 512-dimensional fea-
ture space. This experiment illustrates that each IPM sam-
ple contains new structural information, as the selected sam-
ples are far away from each other in the t-SNE space, com-
pared to other methods. Moreover, it is evident that as
we collect more samples, the structure of the data is bet-
ter captured by the samples selected by IPM, compared to
other methods selecting the same number of representatives.
The decision boundaries of the classiÔ¨Åer trained on 5 IPM-
selected samples look very similar to the boundaries learned
from all the data. This leads to signiÔ¨Åcant accuracy im-
provements, as already discussed and exhibited in Table 3.

4.2.4 Finding Representatives for ImageNet

In this section, we use ImageNet dataset [5] to show the
effectiveness of IPM in selecting the representatives for im-

5420

Method

F-measure

Recall

Selection Methods (Unsupervised)
Random
Uniform
K-medoids
DS3
IPM

26.30
28.68
30.11
30.13
31.53

23.73
25.76
27.30
27.34
29.09

Supervised Summarization Methods
SeqDPP [16]
Submod-V [19]
Submod-V+ [33]

28.87
29.35
34.15

26.83
27.43
31.59

Table 5: F-measure and recall scores using ROUGE-SU metric
for UT Egocentric video summarization task. Results are reported
for several supervised and unsupervised methods.

age classiÔ¨Åcation task. For that, Ô¨Årst, we extract features
from images in an unsupervised manner, using the method
proposed in [43]. We then perform selection in the learned
128-dimensional space and perform k-nearest neighbors (k-
NN) using the learned similarity metric, following the ex-
periments in [43]9. Here, we show that we can learn the
feature space and the similarity metric in an unsupervised
manner, as there is no shortage of unlabeled data, and use
only a few labeled representatives to classify the data.

Due to the volume of this dataset, selection methods
based on convex-relaxation, such as DS3 [11] and SMRS
[12], fail to select class representatives in a tractable time
(as discussed before and shown in Figure 3 for Multi-PIE
dataset). Table 4 shows the top-1 classiÔ¨Åcation accuracy for
the testing set using k-NN. Using less than 1% of the labels,
we can achieve an accuracy of more than 25%, showing the
potential beneÔ¨Åts of the proposed approach for dataset re-
duction. ClassiÔ¨Åcation accuracy of k-NN, using the learned
similarity metric, reÔ¨Çects the representativeness of the se-
lected samples, thus highlighting the fact that IPM-selected
samples preserve the structure of the data fairly well.

4.3. Video Summarization

In this section, we evaluate the performance of the pro-
posed selection algorithm on the video summarization task.
The goal is to select key frames/clips and create a video
summary, such that it contains the most essential contents
of the video. We evaluate our approach on UT Egocentric
(UTE) dataset [45, 30]. It contains 4 Ô¨Årst-person videos of
3-5 hours of daily activities, recorded in an uncontrolled
environment. Authors in [44] have provided text annota-
tions for each 5-second segment of the video, as well as
human-provided reference text summaries for each video.
Following [33, 19, 44], the performance is evaluated in text
domain. For that, a text summary is created by concatenat-
ing the text annotations associated with the selected clips.
The generated summaries are compared with the reference

9We use the feature space generated by the ResNet50 backbone,
as provided in https://github.com/zhirongw/lemniscate.
pytorch

summaries using the ROUGE metric [28]. As in prior work,
we report f-measure and recall using the ROUGE-SU score
with the same parameters as in [33, 19, 44].

Table 5 provides the results for two-minute-long sum-
maries (24 5-second samples), generated by different meth-
ods. To generate results using K-medoids, DS3, and IPM,
we use 1024-dimensional feature vectors extracted using
GoogleNet [38], as described in [46]. Then, the features
are clustered into 24 clusters using K-means and one sam-
ple is selected from each cluster using different selection
techniques. The results are the mean results over all the 4
videos and over 100 runs. Furthermore, for the supervised
methods, the results are as reported in [33]. The proposed
unsupervised selection method, IPM, is the closest competi-
tor to the state-of-art supervised method proposed in [33],
outperforming other unsupervised methods and some of the
supervised methods. These supervised methods split the
dataset into training, and testing sets and use reference text
or video summaries of the training set to learn to summarize
the videos from the test set. This experiment demonstrates
the strength of IPM and the potential beneÔ¨Åts of employing
it in more advanced unsupervised or supervised schemes.

5. Conclusions

A novel data selection algorithm, referred to as Iterative
Projection and Matching (IPM) is presented, that selects the
most informative data points in an iterative and greedy man-
ner. Interestingly, we show that our greedy approach, with
linear complexity wrt the dataset size, is able to outperform
state-of-the-art methods, which are based on convex relax-
ation, in several performance metrics such as projection er-
ror and running time. Furthermore, the effectiveness and
compatibility of our approach are demonstrated in a wide
array of applications such as active learning, video summa-
rization, and learning from representatives. This motivates
us to further investigate the potential beneÔ¨Åts and applica-
tions of IPM in other computer vision problems.

6. Acknowledgements

This research is based upon work supported in parts
by the National Science Foundation under Grants No.
1741431 and CCF-1718195 and the OfÔ¨Åce of the Direc-
tor of National Intelligence (ODNI), Intelligence Advanced
Research Projects Activity (IARPA), via IARPA R&D Con-
tract No. D17PC00345. The views, Ô¨Åndings, opinions, and
conclusions or recommendations contained herein are those
of the authors and should not be interpreted as necessar-
ily representing the ofÔ¨Åcial policies or endorsements, either
expressed or implied, of the NSF, ODNI, IARPA, or the
U.S. Government. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.

5421

References

[1] S. Boyd and L. Vandenberghe. Convex Optimization. Cam-

bridge University Press, New York, NY, USA, 2004.

[2] T. T. Cai and L. Wang. Orthogonal Matching Pursuit for
Information Theory,

Sparse Signal Recovery With Noise.
IEEE Transactions on, 57(7):4680‚Äì4688, 7 2011.

[3] K. Cao, Y. Rong, C. Li, X. Tang, and C. Change Loy.
Pose-Robust Face Recognition via Deep Residual Equivari-
ant Mapping. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.

[4] P. Comon and G. H. Golub. Tracking a few extreme singular
values and vectors in signal processing. Proceedings of the
IEEE, 78(8):1327‚Äì1343, 1990.

[5] J. Deng, W. Dong, R. Socher, L.-J. Li, Kai Li, and Li Fei-
Fei. ImageNet: A large-scale hierarchical image database.
In 2009 IEEE Conference on Computer Vision and Pattern
Recognition, pages 248‚Äì255. IEEE, 6 2009.

[6] M. Derezinski and M. Warmuth. Subsampling for Ridge Re-
gression via Regularized Volume Sampling. In International
Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages
716‚Äì725, 2018.

[7] A. Deshpande and L. Rademacher. EfÔ¨Åcient volume sam-
pling for row/column subset selection.
In Foundations of
Computer Science (FOCS), 2010 51st Annual IEEE Sympo-
sium on, pages 329‚Äì338. IEEE, 2010.

[8] A. Deshpande, L. Rademacher, S. Vempala, and G. Wang.
Matrix approximation and projective clustering via volume
sampling. In Proceedings of the seventeenth annual ACM-
SIAM symposium on Discrete algorithm, pages 1117‚Äì1126.
Society for Industrial and Applied Mathematics, 2006.

[9] S. Ding, X. Nie, H. Qiao, and B. Zhang. A fast algorithm
of convex hull vertices selection for online classiÔ¨Åcation.
IEEE transactions on neural networks and learning systems,
29(4):792‚Äì806, 2018.

[10] J. A. Duersch and M. Gu. Randomized QR with column piv-
oting. SIAM Journal on ScientiÔ¨Åc Computing, 39(4):C263‚Äì
C291, 2017.

[11] E. Elhamifar, G. Sapiro, and S. S. Sastry. Dissimilarity based
sparse subset selection. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 38(11):2182‚Äì2197, 2016.

[12] E. Elhamifar, G. Sapiro, and R. Vidal. See all by looking
at a few: Sparse modeling for Ô¨Ånding representative objects.
In 2012 IEEE Conference on Computer Vision and Pattern
Recognition, pages 1600‚Äì1607. IEEE, 2012.

[13] A. K. Farahat, A. Elgohary, A. Ghodsi, and M. S. Kamel.
Greedy column subset selection for large-scale data sets.
Knowledge and Information Systems, 45(1):1‚Äì34, 2015.

[14] Y. Gal and Z. Ghahramani. Dropout as a Bayesian Approxi-
mation: Representing Model Uncertainty in Deep Learning.
In PMLR, 2016.

[15] Y. Gal, R. Islam, and Z. Ghahramani. Deep Bayesian Ac-
tive Learning with Image Data. In D. Precup and Y. W. Teh,
editors, Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pages 1183‚Äì1192, International Conven-
tion Centre, Sydney, Australia, 2017. PMLR.

[16] B. Gong, W.-L. Chao, K. Grauman, and F. Sha. Diverse
Sequential Subset Selection for Supervised Video Summa-
rization.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 27, pages 2069‚Äì2077.
Curran Associates, Inc., 2014.

[17] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.
Multi-PIE. Image and Vision Computing, 28(5):807‚Äì813, 5
2010.

[18] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. MS-Celeb-
1M: Challenge of Recognizing One Million Celebrities in
the Real World. Electronic Imaging, 2016(11):1‚Äì6, 2 2016.

[19] M. Gygli, H. Grabner, and L. Van Gool. Video summariza-
tion by learning submodular mixtures of objectives. In 2015
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3090‚Äì3098. IEEE, 6 2015.

[20] K. Hara, H. Kataoka, and Y. Satoh. Can Spatiotemporal 3D
CNNs Retrace the History of 2D CNNs and ImageNet? Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 18‚Äì22, 2017.

[21] M. Joneidi, A. Zaeemzadeh, and N. Rahnavard. Dynamic
Sensor Selection for Reliable Spectrum Sensing via E-
optimal Criterion. In 2017 IEEE 14th International Confer-
ence on Mobile Adhoc and Sensor Systems, Orlando, 2017.
IEEE Computer Society.

[22] M. Joneidi, A. Zaeemzadeh, B. Shahrasbi, G.-J. Qi, and
N. Rahnavard. E-optimal Sensor Selection for Compressive
Sensing-based Purposes.
IEEE Transactions on Big Data,
page To Appear in, 2018.

[23] S. Joshi and S. Boyd. Sensor Selection via Convex Optimiza-
tion. IEEE Transactions on Signal Processing, 57(2):451‚Äì
462, 2 2009.

[24] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier,
S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Nat-
sev, M. Suleyman, and A. Zisserman. The Kinetics Human
Action Video Dataset. 5 2017.

[25] Laurens van der Maaten. Visualizing Data using t-SNE. An-

nals of Operations Research, 2014.

[26] D. D. Lewis and W. A. Gale. A sequential algorithm for
training text classiÔ¨Åers. In Proceedings of the 17th annual
international ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 3‚Äì12. Springer-
Verlag New York, Inc., 1994.

[27] C. Li, S. Jegelka, and S. Sra. Polynomial time algorithms for
dual volume sampling. In Advances in Neural Information
Processing Systems, pages 5038‚Äì5047, 2017.

[28] C. Y. Lin. Rouge: A package for automatic evaluation of
summaries. Annual Meeting of the Association for Compu-
tational Linguistics, 2004.

[29] H. Liu, Y. Liu, and F. Sun. Robust Exemplar Extraction Us-
ing Structured Sparse Coding. IEEE Transactions on Neural
Networks and Learning Systems, 26(8):1816‚Äì1821, 8 2015.

[30] Z. Lu and K. Grauman. Story-Driven Summarization for
Egocentric Video. In 2013 IEEE Conference on Computer
Vision and Pattern Recognition, pages 2714‚Äì2721. IEEE, 6
2013.

5422

[46] K. Zhang, W. L. Chao, F. Sha, and K. Grauman. Video sum-
marization with long short-term memory. In Lecture Notes
in Computer Science (including subseries Lecture Notes in
ArtiÔ¨Åcial Intelligence and Lecture Notes in Bioinformatics),
2016.

[31] J. Meng, H. Wang, J. Yuan, and Y.-P. Tan. From Keyframes
to Key Objects: Video Summarization by Representative Ob-
ject Proposal Selection. In 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 1039‚Äì
1048. IEEE, 6 2016.

[32] A. Nikolov, M. Singh, and U. T. Tantipongpipat. Propor-
tional Volume Sampling and Approximation Algorithms for
A-Optimal Design. arXiv preprint arXiv:1802.08318, 2018.
[33] B. A. Plummer, M. Brown, and S. Lazebnik. Enhanc-
ing Video Summarization via Vision-Language Embedding.
In 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1052‚Äì1060. IEEE, 7 2017.

[34] M. Rahmani and G. K. Atia. Robust and Scalable Col-
In ICCV

umn/Row Sampling from Corrupted Big Data.
Workshops, pages 1818‚Äì1826, 2017.

[35] M. Sedghi, G. Atia, and M. Georgiopoulos. Robust Manifold
IEEE Signal Processing

Learning via Conformity Pursuit.
Letters, 26(3):425‚Äì429, 3 2019.

[36] M. Shamaiah, S. Banerjee, and H. Vikalo. Greedy sensor se-
lection: Leveraging submodularity. In Decision and Control
(CDC), 2010 49th IEEE Conference on, pages 2572‚Äì2577,
12 2010.

[37] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A Dataset
of 101 Human Actions Classes From Videos in The Wild. 12
2012.

[38] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In 2015 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
1‚Äì9. IEEE, 6 2015.

[39] Y. Tian, X. Peng, L. Zhao, S. Zhang, and D. N. Metaxas. CR-
GAN: Learning Complete Representations for Multi-view
Generation. In Proceedings of the Twenty-Seventh Interna-
tional Joint Conference on ArtiÔ¨Åcial Intelligence, pages 942‚Äì
948, California, 7 2018. International Joint Conferences on
ArtiÔ¨Åcial Intelligence Organization.

[40] J. A. Tropp and A. C. Gilbert. Signal Recovery From Ran-
dom Measurements Via Orthogonal Matching Pursuit. IEEE
Transactions on Information Theory, 53(12):4655‚Äì4666, 12
2007.

[41] P. A. Vijaya, M. N. Murty, and D. K. Subramanian. Leaders‚Äì
Subleaders: An efÔ¨Åcient hierarchical clustering algorithm for
large data sets. Pattern Recognition Letters, 25(4):505‚Äì513,
2004.

[42] H. Wang, Y. Kawahara, C. Weng, and J. Yuan. Representa-
tive selection with structured sparsity. Pattern Recognition,
63:268‚Äì278, 2017.

[43] Z. Wu, Y. Xiong, X. Y. Stella, and D. Lin. Unsupervised
Feature Learning via Non-Parametric Instance Discrimina-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018.

[44] S. Yeung, A. Fathi, and L. Fei-Fei. VideoSET: Video Sum-

mary Evaluation through Text. 6 2014.

[45] Yong Jae Lee, J. Ghosh, and K. Grauman. Discovering im-
portant people and objects for egocentric video summariza-
tion. In 2012 IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 1346‚Äì1353. IEEE, 6 2012.

5423

