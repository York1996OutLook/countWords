Compressing Convolutional Neural Networks via

Factorized Convolutional Filters

Tuanhui Li1 Baoyuan Wu2‚àó Yujiu Yang1‚àó Yanbo Fan2 Yong Zhang2 Wei Liu2

1Graduate School at Shenzhen, Tsinghua University 2Tencent AI Lab

lth17@mails.tsinghua.edu.cn, wubaoyuan1987@gmail.com,yang.yujiu@sz.tsinghua.edu.cn,

{fanyanbo0124,zhangyong201303}@gmail.com, wl2223@columbia.edu

Abstract

This work studies the model compression for deep con-
volutional neural networks (CNNs) via Ô¨Ålter pruning. The
workÔ¨Çow of a traditional pruning consists of three sequen-
tial stages: pre-training the original model, selecting the
pre-trained Ô¨Ålters via ranking according to a manually de-
signed criterion (e.g., the norm of Ô¨Ålters), and learning the
remained Ô¨Ålters via Ô¨Åne-tuning. Most existing works fol-
low this pipeline and focus on designing different ranking
criteria for Ô¨Ålter selection. However, it is difÔ¨Åcult to con-
trol the performance due to the separation of Ô¨Ålter selection
and Ô¨Ålter learning. In this work, we propose to conduct Ô¨Ål-
ter selection and Ô¨Ålter learning simultaneously, in a uniÔ¨Åed
model. To this end, we deÔ¨Åne a factorized convolutional
Ô¨Ålter (FCF), consisting of a standard real-valued convolu-
tional Ô¨Ålter and a binary scalar, as well as a dot-product op-
erator between them. We train a CNN model with factorized
convolutional Ô¨Ålters (CNN-FCF) by updating the standard
Ô¨Ålter using back-propagation, while updating the binary
scalar using the alternating direction method of multipli-
ers (ADMM) based optimization method. With this trained
CNN-FCF model, we only keep the standard Ô¨Ålters corre-
sponding to the 1-valued scalars, while all other Ô¨Ålters and
all binary scalars are discarded, to obtain a compact CNN
model. Extensive experiments on CIFAR-10 and ImageNet
demonstrate the superiority of the proposed method over
state-of-the-art Ô¨Ålter pruning methods.

1. Introduction

Many popular deep convolutional neural networks have
emerged in recent years, e.g., VGGNet [32] and ResNet
[10], etc. These models show promising results on many
visual tasks, such as image classiÔ¨Åcation [18, 36, 37, 39],

‚àóindicates corresponding authors. This work was done when Tuanhui

Li was an intern at Tencent AI Lab.

semantic segmentation [2, 26], object detection [6], object
tracking [22, 43] or visual reasoning [34, 42]. However,
the model sizes and computation complexities of these deep
models also grow exponentially. For example, ResNet-152
[10] contains about 60 million parameters and 11.3 billion
FLOPS, which precludes the application of these models
to mobile systems. A feasible approach to tackle this dif-
Ô¨Åculty is model compression, whose goal is to reduce the
parameters while keeping the model performance as much
as possible.

Many seminal works have been developed in the liter-
ature of model compression for deep convolutional neural
networks. They can be generally partitioned to four cate-
gories, including pruning [8, 12, 21, 23, 27], low-rank fac-
torization [16, 19, 45], weight quantiÔ¨Åcation [24, 25] and
compact network design [13, 28], respectively. In this work,
we focus on the pruning approach, and we refer the readers
to [4] for more details about other categories. SpeciÔ¨Åcally,
we focus on the Ô¨Ålter-level pruning (Ô¨Ålter pruning), which
prunes the output channel of a Ô¨Ålter tensor. A typical work-
Ô¨Çow of the Ô¨Ålter pruning is demonstrated in Fig. 1 (top). It
consists of three sequential stages, including the training of
the original model, pruning Ô¨Ålters according to a manually
designed ranking criterion, and Ô¨Åne-tuning the model with
remained Ô¨Ålters. Many existing works focused on design-
ing different ranking criteria. However, most of these crite-
ria depend on the weights values themselves or the results
(e.g., the classiÔ¨Åcation accuracy) of the pre-trained original
model. A typical criterion is the assumption that the Ô¨Ålters
with small ‚Äôweight‚Äô norms have small contributions to the
model, thus they can be pruned [21]. However, to the best of
our knowledge, we have never found a rigorous veriÔ¨Åcation
of this assumption. What is more important, the manually
designed ranking criterion only depends on the pre-trained
original model, rather than the followed Ô¨Åne-tuning process
of the pruned model. The efÔ¨Åcacy of the ranking criterion

3977

Figure 1. Overview of the workÔ¨Çow of Ô¨Ålter pruning on layer l (1 ‚â§ l ‚â§ L), where the dotted green cubes indicate the pruned Ô¨Ålters.
(Top): Traditional pruning consists of three sequential stages: pre-training, selecting Ô¨Ålters according to a ranking criterion, and Ô¨Åne-tuning.
(Bottom): Our method conducts the Ô¨Ålter learning and Ô¨Ålter selection jointly, through training factorized convolutional Ô¨Ålters.

can be only measured according to the Ô¨Åne-tuning result. It
may take many iterations of pruning and Ô¨Åne-tuning to Ô¨Ånd
a good ranking criterion.

In this work, we propose to conduct Ô¨Ålter learning and
Ô¨Ålter selection jointly, in a uniÔ¨Åed optimization framework.
To this end, we design a novel type of Ô¨Ålter, dubbed fac-
torized convolutional Ô¨Ålter (FCF). As shown in Fig. 1 (bot-
tom), a FCF consists of a standard convolutional Ô¨Ålter Wl
i
(l is the layer index and i is the Ô¨Ålter index), and a bi-
nary scalar vl
i ‚àà {0, 1}, as well as a dot-product operator
between them. After training the CNN model with fac-
torized convolutional Ô¨Ålters (CNN-FCF), the standard Ô¨Ål-
ters corresponding to 0-valued binary scalars and all binary
scalars in other FCFs are directly abandoned, to construct
a compact CNN model (see the green Ô¨Ålters in Fig. 1 (bot-
tom)). However, due to the binarity of vl
i, the standard back-
propagation with gradient descent algorithm cannot be di-
rectly adopted to train CNN-FCF. To tackle this difÔ¨Åculty,
inspired by the general integer programming framework,
i.e., ‚Ñìp-Box ADMM [38], the binary vector vl (including
all binary scalars in layer l) is equivalently reformulated as
a continuous vector, being subjected to the intersected con-
straint space between the box constraint and the ‚Ñì2-sphere
constraint. Consequently, we propose a novel training al-
gorithm for CNN-FCF by inserting the alternating direction
method of multipliers (ADMM) [3] algorithm into the back-
propagation framework. SpeciÔ¨Åcally, Wl is updated by the
standard gradient descent algorithm, while vl is updated by
the ADMM algorithm. We compress the popular ResNet
models with different layers on two benchmark datasets, in-
cluding CIFAR-10 [17] and ImageNet LSVRC-2012 [31]
(ImageNet for clarity). Experimental results verify the com-
petitive performance of the proposed method, compared to
the state-of-the-art Ô¨Ålter pruning methods.

The main contributions of this work are three-fold. (1)
We propose to conduct Ô¨Ålter learning and Ô¨Ålter selection
jointly in a uniÔ¨Åed optimization framework, through train-
ing CNNs with factorized convolutional Ô¨Ålters. (2) We pro-
pose a novel algorithm to train CNN-FCF by inserting the
ADMM algorithm into the standard gradient-based back-
propagation framework. (3) Extensive experiments on pop-
ular ResNet models and benchmark datasets demonstrate
the efÔ¨Åcacy of the proposed method.

2. Related Work

In this section, we brieÔ¨Çy review the pruning-based com-
pressing approach, which can be generally partitioned to
two levels, including weight-level and Ô¨Ålter-level pruning.

Weight-level pruning (weight pruning for clarity) is an
unstructured pruning method that prunes some entries in
each Ô¨Ålter.
It was Ô¨Årstly proposed in optimal brain dam-
age [20] and optimal brain surgeon [9], which pruned the
weights according to the second order derivatives of the loss
function. Recently, Han et al. [8] proposed to remove the
weights with small values below the threshold. Guo et al.
[7] proposed an interactive method, which is composed of
pruning and splicing. Zhang et al. [44] formulated the prun-
ing problem into a non-convex optimization problem and
combined the weights with cardinality constraints. How-
ever, weight pruning can only produce a sparse network.
Consequently, the memory and computational cost of the
compressed model are not signiÔ¨Åcantly reduced.

Filter-level pruning (Ô¨Ålter pruning for clarity) is a struc-
tured pruning method that prunes the whole channel of Ô¨Ål-
ters. Thus, it can reduce more parameters and computation
costs than the weight pruning method. (1) Some works cal-
culate the importance score for each Ô¨Ålter according to a
manually designed evaluation criterion. A commonly used
metric is the norm of Ô¨Ålters [11, 21], based on the assump-

3978

ùêñ1ùëôùêñ1ùëôùêñ1ùëôùêñ1ùëôùë£ùê∂ùëôùëô‚®ÄùêØùëô0.2361.4250.395ùë£1ùëôInitial convolutional filtersOptimized convolutional filtersOptimized factorized convolutional filters0.962ùë£2ùëôùë£3ùëôTrain‡µùùêØùëô‚àà0,1ùê∂ùëôùüè‚ä§ùêØùëô=ùëòùëôscoreRankPruneFine-tuneFine-tuned convolutional filters‚Ä¶Joint trainPruneInitial factorized convolutional filtersPruned convolutional filtersùë£ùê∂ùëôùëô=1ùë£1ùëô=1ùë£2ùëô=0ùë£3ùëô=0ùêñùëôùêñùëôùêñ2ùëôùêñ3ùëôùêñùê∂ùëôùëô‚Ä¶ùêñ1ùëôùêñ1ùëôùêñ1ùëôùêñ1ùëôùêñ1ùëôùêñùëôùêñ2ùëôùêñ3ùëôùêñùê∂ùëôùëô‚Ä¶ùêñ1ùëôùêñ1ùëôùêñ1ùëôùêñùëôùêñ2ùëôùêñ3ùëôùêñùê∂ùëôùëô‚Ä¶ùêñ2ùëôùêñùê∂ùëôùëô‚Ä¶ùêñ1ùëôùêñ1ùëôùêñ1ùëôùêñ1ùëôùêñùëôùêñ2ùëôùêñ3ùëôùêñùê∂ùëôùëô‚Ä¶ùêñ1ùëô‚®ÄùêØùëô‚Ä¶ùêñ1ùëôùêñ2ùëôùêñ3ùëôùêñ1ùëôùêñùëôùêñùê∂ùëôùëô‚Ä¶ùêñ1ùëôùêñùëôùêñùê∂ùëôùëôùêñ1ùëô‚Ä¶[40] and Ye et al.

tion that Ô¨Ålters with small norms do not contribute much
to the model performance. Similar to Ô¨Ålter norm, Hu et al.
[14] proposed to rank the Ô¨Ålters with the average percentage
of zeros of the corresponding output feature maps. Yu et al.
[41] proposed to obtain the importance score of other layers
via back-propagating the Ô¨Ålter scores of the Ô¨Ånal response
layer. (2) Regularization constraints are also usually used in
many studies for pruning. Some studies introduced group
lasso with Ô¨Ålters to directly derive sparse Ô¨Ålters [1, 35].
[23] imposed ‚Ñì1 constraint
Liu et al.
to the scaling factors of BN layers, and the magnitudes of
these scaling factors are used as the Ô¨Ålter scores. Huang
et al. [15] introduced the scaling factors with a ‚Ñì1 regular-
ization for the selection of different micro-structures such
as residual blocks. (3) Some works proposed to minimize
the reconstruction error between the original model and the
pruned model. He et al. [12] formulated the reconstruc-
tion error using lasso regression to retain the representative
Ô¨Ålters. Luo et al. [27] proposed to minimize the reconstruc-
tion error based on greedy search. Inspired by optimal brain
damage, Dong et al. [5] performed Taylor expansion on the
reconstruction error function, and determined the parame-
ter importance according to the second order derivatives.
Zhuang et al. [46] proposed to seek discriminative chan-
nels by minimizing both reconstruction error loss and addi-
tional loss. The main commonality of above related works
is that the Ô¨Ålter selection is somewhat separated with Ô¨Ålter
learning. In contrast, the proposed method conducts Ô¨Ålter
selection and Ô¨Ålter learning simultaneously.

3. Model Compression

3.1. Convolutional Filter
dataset

training

consists

of N samples
Given
{xi, yi}N
i=1, with xi being input feature and yi being
ground-truth label of xi. Considering a CNN model with L
layers, we use Wl ‚àà RC l√óN l√óW l√óH l
to represent the Ô¨Ål-
ters of the l-th convolutional layer, where (C l, N l, W l, H l)
are the number of output channels, the number of input
channels, the width of kernel and the height of kernel,
respectively. The convolutional operation of layer l is
formulated as

Rl

out = Conv(Rl

in, Wl),

(1)

where Conv denotes the convolutional operation. Rl
RN √óN l√óW l
the input and output responses of layer l, respectively.

out ‚àà RN √óC l√óW l

in ‚àà
out indicate

in and Rl

√óH l

√óH l

out

in

3.2. Factorized Convolutional Filter

To facilitate the Ô¨Ålter selection, we propose a novel Ô¨Ålter,
dubbed factorized convolutional Ô¨Ålter (FCF), by associating
i ‚àà RN l√óW l√óH l
a binary scalar vl
(l is layer index and i is Ô¨Ålter index), through a dot-product
operator. It is formulated as follows:

i ‚àà {0, 1} to each Ô¨Ålter Wl

Rl

out = Conv(Rl

in, Wl ‚äô vl),

where vl = [. . . ; vl
i ‚äô vl
[. . . ; Wl
cating that vl

i; . . .] ‚àà {0, 1}C l
i; . . .] ‚àà RC l√óN l√óW l√óH l
i is multiplied to every element in Wl
i.

, with Wl

(2)
and Wl ‚äô vl =
i indi-

i ‚äô vl

3.3. Training CNNs with Factorized Convolutional

Filters

Joint training.
In this section, we present how to con-
duct Ô¨Ålter learning and Ô¨Ålter selection jointly, based on
the factorized convolutional Ô¨Ålters. Let W = {Wl}L
l=1,
v = {vl}L
l=1, we denote f (xi; W, v) as the output proba-
bility of a CNN model with factorized convolutional Ô¨Ålters
(CNN-FCF). Then, the objective function of training CNN-
FCF is formulated as follows:

arg min

W,v

1
N

N

Xi=1

‚Ñì(cid:0)yi, f (xi; W, v)(cid:1)

s.t. 1‚ä§vl = kl, vl ‚àà {0, 1}C l

, ‚àÄ l ‚àà {1, 2, ..., L},

(3)

where kl ‚àà {1, 2, . . . , C l} denotes the number of remained
Ô¨Ålters in layer l after pruning. The loss function ‚Ñì can be
speciÔ¨Åed by any loss function that could be used to train
standard CNNs. In our experiments for image classiÔ¨Åcation,
we adopt the cross entropy loss. Due to the binary constraint
on v, Problem (3) cannot be directly optimized by contin-
uous optimization algorithm (e.g., the gradient-based back-
propagation algorithm). Thus, we propose a novel continu-
ous optimization algorithm, dubbed back-propagation with
ADMM, as detailed in Section 4.

Filter pruning. Given a trained CNN-FCF model through
optimizing Problem (3), one can obtain 1) a compact CNN
model by pruning the Ô¨Ålters Wl
i corresponding to zero-
valued vl
i in each layer (see Fig. 1 (bottom)), or 2) a sparse
CNN model by setting zeros to the Ô¨Ålters Wl
i correspond-
ing to the zero-valued vl
i. The choice of this two types of
models depends on the model architecture and the pruning
strategy, which will be detailed in Section 5.2.

4. Optimization

4.1. Continuous Reformulation

To tackle the binary constraint in Problem (3), we Ô¨Årstly
transform the binary constraint to continuous constraints us-
ing the following technique proposed in [38], as follows:

vl ‚àà {0, 1}C l

‚áî vl ‚àà Sb ‚à© vl ‚àà Sp,

(4)

denotes a box constraint, and Sp =

where Sb = [0, 1]C l
2 = C l

nvl : kvl ‚àí 1

2 k2

4 o indicates a ‚Ñì2-sphere constraint.

Furthermore, following the ‚Ñìp-Box ADMM algorithm [38],
we introduce two additional variables to split the continuous
constraints, so that they can be satisÔ¨Åed alternatively. Con-
sequently, Problem (3) can be equivalently reformulated as

3979

the following continuous problem:

arg min
W,v,z1,z2

1
N

N

Xi=1

‚Ñì(cid:0)yi, f (xi; W, v)(cid:1)

s.t. vl = zl

1, vl = zl
2,
zl
2 ‚àà Sp,

zl
1 ‚àà Sb,

1‚ä§zl

1 = kl,

‚àÄ l ‚àà {1, 2, ..., L},

(5)

1}L
where z1 = {zl
after we shorten 1

l=1 and z2 = {zl

2}L

l=1. For clarity, here-

N PN

i=1 ‚Ñì(cid:0)yi, f (xi; W, v)(cid:1) as L(W, v).

4.2. Back propagation with ADMM

is continuous,

Although Problem (5)

the back-
propagation algorithm with the standard gradient-based op-
timizer (e.g., stochastic gradient descent (SGD) [30] ) can‚Äôt
be directly used to optimize the constrained problem. We
propose a new algorithm by inserting the ADMM algo-
rithm into the standard back-propagation training frame-
work. SpeciÔ¨Åcally, considering layer l, the parameters are
updated by the following alternative steps:

‚Ä¢ Given the Ô¨Åxed vl, the parameter Wl can be updated

using gradient-based optimizer (see Section 4.2.1);

‚Ä¢ Given the Ô¨Åxed Wl, the variables vl, zl

1, zl

2 are up-

dated by the ADMM algorithm (see Section 4.2.2).

This algorithm is outlined in Algorithm 1. To facilitate the
following derivations of the proposed algorithm, we decom-
pose the output function f (xi; W, v) with respect to the
parameters of layer l, as follows:

out; {Wj}L

j=l+1, {vj}L

j=l+1(cid:17),

Rl

out = Conv(Rl

f (xi; W, v) = f1(cid:16)Rl
in = f2(cid:16)xi; {Wj}l‚àí1

Rl

Ô£±Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥

where Rl
of layer l, respectively.

in and Rl

in, Wl ‚äô vl),

j=1, {vj}l‚àí1

j=1(cid:17),

(6)
out denote the input and output response

4.2.1 Given vl, Solving Wl Using Gradient Descent

Wl can be updated using the standard gradient descent,

Wl = Wl ‚àí Œ∑W

‚àÇL(W, v)

‚àÇWl

,

(7)

where Œ∑W denotes the learning rate. Using the chain rule
and the decomposition in Eq. (6), we have

Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥

‚àÇL(W, v)

‚àÇWl

=

‚àÇL
‚àÇf1

√ó

‚àÇf1
‚àÇRl

√ó

out

‚àÇRl
Wl

,

out
‚àÇWl ‚äô vl

out

out

‚àÇRl
Wl =

‚àÇRl
‚àÇWl ‚äô vl √ó

‚àÇRl
‚àÇWl ‚äô vl √ó Vl,
where Vl is expanded from vl, and has the same shape as
Wl. All the terms can be easily computed as did in training
standard CNNs.

‚àÇWl =

out

Algorithm 1 Back-propagation with ADMM
Input: Training data {xi, yi}N

i=1 and initial Wl, vl, ‚àÄ l ‚àà

{1, 2, ..., L}.

Output: ÀÜWl.
1: while not converged do
2:

for l = 1 to L do

3:

4:

Given vl, update Wl using gradient descent (see
Section 4.2.1);
Given Wl, update vl using ADMM (see Section
4.2.2).

end for
5:
6: end while
7: if vl

i = 1 then
ÀÜWl
i = Wl
8:
9: end if
10: return ÀÜWl

i (l is layer index and i is Ô¨Ålter index).

4.2.2 Given Wl, Solving vl Using ADMM

With the Ô¨Åxed Wl and parameters of all other layers, Prob-
lem (5) with respect to vl, zl
2 can be solved by the alter-
nating direction method of multipliers (ADMM) [3] algo-
rithm. Following the standard ADMM procedure, we Ô¨Årstly
present the augmented Lagrangian function, as follows:

1, zl

L(Wl, vl, zl

1, zl

2, ul
1) + h2(zl

1, ul
2) + (ul

2) = L(W, v)

1)‚ä§(vl ‚àí zl
1)

+ h1(zl

(8)

+ (ul

2)‚ä§(vl ‚àí zl

2) +

1k2

2 + kvl ‚àí zl

2k2

œÅl

2hkvl ‚àí zl

2i,

1 : 1‚ä§zl

2) = I(zl

1) = I(zl

1 ‚àà Sb ‚à© {zl

where h1(zl
1 = kl}) and
h2(zl
2 ‚àà Sp) are indicator functions. I(a) = 0
if a is true, otherwise I(a) = ‚àû. ul
are dual
variables, and œÅl > 0 is a penalty parameter. Then, in the
following, we iteratively update (zl
2, vl) by minimizing
the Lagrangian during each training iteration, and update
(ul
1, zl
Update (zl
sub-problems:

2): They are updated by solving the following

2) by gradient ascent.

2 ‚àà RC l

1, ul

1, ul

1, zl

( zl

1 = arg minzl
zl
2 = arg minzl

1‚ààSc

1‚ààSp

1

2 kzl
2 kzl

2 ‚àí (vl + ul
2 ‚àí (vl + ul

1k2
2k2

œÅl )‚ä§zl
1,
œÅl )‚ä§zl
2,

1

1

2

(9)

1 : 1‚ä§zl

where Sc = Sb ‚à© {zl
1 = kl}. The Ô¨Årst sub-problem
is a standard quadratic program (QP) problem, which can
be globally optimized by any off-the-shelf QP solver. In our
experiments, we adopt the OSQP solver [33]. The second
sub-problem can be globally optimized by projecting the
solution of the unconstrained problem onto the ‚Ñì2-sphere
(i.e., Sp), as presented in [38].
Update vl: Due to the loss term L(W, v), the sub-problem

3980

with respect to vl doesn‚Äôt have a closed form solution. In-
stead, we adopt the gradient descent algorithm, as follows:

vl = vl ‚àí Œ∑v

where,

‚àÇL(Wl, vl, zl
‚àÇvl

1, zl

2, ul

1, ul
2)
,

(10)

‚àÇL
‚àÇvl =
‚àÇL(W, v)

‚àÇL(W, v)

+

√ó

‚àÇC
‚àÇvl

,

‚àÇf1
‚àÇRl

‚àÇvl

=

‚àÇL
‚àÇf1

√ó

‚àÇRl
vl

out

,

out
‚àÇWl ‚äô vl

out

‚àÇRl
‚àÇWl ‚äô vl √ó
1 + ul

‚àÇvl
2 + œÅl(2vl ‚àí zl

1 ‚àí zl

2).

Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

‚àÇvl

‚àÇRl

out

vl =
‚àÇC
‚àÇvl = ul
Update (ul

1, ul

2):

=

‚àÇRl
‚àÇWl ‚äô vl √ó Wl,

out

(ul

ul

1 = ul
2 = ul

1 + œÅl(vl ‚àí zl
2 + œÅl(vl ‚àí zl

1),
2).

(11)

Besides, to accelerate the convergence process, we increase
œÅl by ¬µl after each iteration, i.e., œÅl ‚Üê ¬µlœÅl, and set an
upper bound œÅl
max to avoid early stopping. They will be
speciÔ¨Åed in experiments.

4.2.3 Complexity Analysis

analyze
in Algorithm 1.

the

computational

layer

Here we
of
the forward Rl
Eq. 2)

l

complexity
The complexity of
in, Wl ‚äô vl)
(see
in).
The complex-

out = Conv(Rl
inH l

is O(N C lN lW lH lW l
the backward ‚àÇRl
outW lH l).

outH l

out

Wl

(see Section 4.2.1)

ity of
is
O(N C lN lW l
They are same with the
complexities of forward and backward pass in standard
convolutional layer. The additional complexity is mainly
1, which is O((C l)3). Considering
from the QP solver for zl
the performance enhancement from the joint training, we
believe that such an extra training cost is acceptable.

5. Experiments

5.1. Experimental Settings

Dataset. Our experiments are conducted on two bench-
including CIFAR-10 [17] and ImageNet
mark datasets,
[31]. CIFAR-10 has 50k training images and 10k validation
images, which is annotated by 10 classes. ImageNet con-
tains 1.28 million training images and 50k validation im-
ages of 1000 classes.
Models and compared methods. We evaluate the pro-
posed method on the ResNet [10] architecture with different
layers. On CIFAR-10, we compress ResNet-20, ResNet-
32, ResNet-56 and ResNet-110, comparing with state-of-
the-art methods, inlcuding SNLI [40], SFP [11], Pruning
[21], NISP [41]. On ImageNet, we compress ResNet-34

and ResNet-50 to compare with NISP [41], SFP [11], Prun-
ing [21], ThiNet [27], SSS [15] and Channel Pruning [12].
Evaluation metrics. We adopt three metrics to evaluate
the compressed model, including Params.‚Üì%, FLOPs‚Üì%,
and Acc.‚Üì%. Params.‚Üì% denotes the ratio of pruned pa-
rameters from the original model. FLOPs‚Üì% represents the
ratio of decreased computational cost compared to the orig-
inal model. Acc.‚Üì% indicates the reduced accuracy com-
pared to the accuracy of the orginal model. A better com-
pressing performance corresponds the higher Params.‚Üì%
and FLOPs‚Üì%, while the lower Acc.‚Üì%.
Implementation details. In the Ô¨Årst stage, the joint train-
ing process of W and v using the proposed algorithm of
back-propagation with ADMM for at most 30 epochs. The
learning rates of Œ∑W (see Eq. 7) and Œ∑v (see Eq. 10) are
initialized as 0.1, and they are divided by 10 after every 10
epochs.
In terms of the hyper-parameters of the ADMM
part (see Section 4.2.2), we adopt same settings for all lay-
ers. SpeciÔ¨Åcally, for l-th layer, vl is initialized as 1 to in-
clude all Ô¨Ålters at the very beginning, (ul
2) are
initialized as 0. On ImageNet, œÅl is initialized as 0.001;
¬µl, œÅl
max and the batch-size are set as 1.001, 6 and 256, re-
spectively. On CIFAR-10, œÅl is initialized as 0.01; ¬µl, œÅl
max
and the batch-size are set as 1.01, 6 and 128, respectively.
Besides the maximum number of epochs (i.e., 30), we also
set another stopping criterion, i.e., kvl ‚àí zl
2 6 10‚àí4 and
kvl ‚àí zl
2 6 10‚àí4, ‚àÄ l ‚àà {1, 2, . . . , L}. Then, we binarize
vl to exactly 0 or 1. Theoretically speaking, if vl converges
exactly to 0 or 1, the trained CNN-FCF model will be the
output. However, due to the numerical reason, there are still
small changes on vl after binarization, i.e., Wl
i| or
Wl
i ‚àí 0|. In Section 5.5, we will study the inÔ¨Çuence of
these small differences. To alleviate the potential inÔ¨Çuence
of these small differences, we also conduct Ô¨Åne-tuning on
the compact CNN model in the second stage. In the Ô¨Åne-
tuning process, we adopt SGD with 0.9 momentum, and the
intial learning rate is 0.01. The weight decay factor is set as
0.0001. On ImageNet, we Ô¨Åne-tune for 90 epochs with the
batch-size 256, and the learning rate is divided by 10 every
30 epochs. On CIFAR-10, we Ô¨Åne-tune for 150 epochs with
the batch-size 128, and the learning rate is divided by 10
every 60 epochs. All experiments are implemented using
PyTorch [29].

i ‚äô |1 ‚àí vl

i ‚äô |vl

1, ul

1, zl

2, zl

1k2

2k2

5.2. Pruning Strategies and Pruning Ratios

Pruning strategies. In the experiments, we Ô¨Ånd that many
existing Ô¨Ålter pruning works (e.g., [21], [41]) not only prune
the output channels of the parameter tensor, but also prune
the input channels. Compared to the pruning only on the
output channels, this strategy can remove useless parame-
ters and reduce unnecessary computations, leading to higher
Param.‚Üì% and FLOPs‚Üì%.
In the following, we present
brief deÔ¨Ånitions of these two pruning strategies.

3981

‚Ä¢ Single-channel pruning. As shown in Fig. 2 (top),
single-channel pruning only prunes the output chan-
nels in Wl and Wl+1.

‚Ä¢ Pair-channel pruning. However, as shown in Fig. 2
(bottom), when a Ô¨Ålter in Wl is pruned, the corre-
sponding response in Rl
out will be removed (see the
green dashed part of Rl
out in Fig. 2). Consequently,
the Ô¨Ålter at the input channel of Wl+1 that corresponds
to this removed response should also be removed (see
the cube with green dashed lines in Fig. 2 (bottom)).

Remark. As most existing works change the model archi-
tecture via pruning the Ô¨Ålters before Ô¨Åne-tuning, they have
to adopt the pair-channel pruning strategy to match the out-
put channel of Wl and the input channel of Wl+1. Be-
sides, if there is a short-cut connection (e.g., ResNet), then
the last layer in each block cannot be pruned in many exist-
ing works. Besides, as most entries in the pruned Ô¨Ålters are
likely to be non-zero, the Ô¨Åne-tuning is always required to
recover the performance after pruning.

i corresponding to zero-valued vl

In contrast, as we don‚Äôt change the model architecture
when training CNN-FCF, all above limitations don‚Äôt exist
in our method. Our pruning method is much more Ô¨Çexi-
ble than existing works. The training process of CNN-FCF
can be seen as the single-channel pruning strategy. How-
ever, given a trained CNN-FCF model, we can also adopt
the pair-channel pruning strategy to set Ô¨Ålters‚Äô parameters
w.r.t. the pruned input channels to 0 for each layer. Given
a trained CNN-FCF model, we can obtain a compact or
sparse CNN models depending on the original model struc-
tures: 1) If there is no short-cut, we can directly prune the
Ô¨Ålter Wl
i to obtain a com-
pact CNN model. One further point should be pointed out
is that if there is a batch-normalization (BN) layer (e.g.,
ResNet), then the mean and variance values of BN will be
modiÔ¨Åed after pruning, leading to the change of responses.
Thus, it also requires Ô¨Åne-tuning to resume the model per-
formance, as did in most existing works. 2) If there is a
short-cut and we also perform Ô¨Ålter pruning (i.e., set a prun-
ing ratio using the cardinality constraint) for the last layer
in each block, then we cannot directly prune the Ô¨Ålters as
did above. The reason is that the remained Ô¨Ålters in two
layers connected by the short-cut cannot be aligned.
In-
stead, we obtain a sparse CNN model, by setting zeros to
the convolutional Ô¨Ålters corresponding to zero-valued vl
i.
When using this sparse CNN model for inference, we de-
sign a squeeze-convolution-expand procedure. SpeciÔ¨Åcally,
for each sparse convolutional tensor, we Ô¨Årstly squeeze it
by removing zero-valued Ô¨Ålters to obtain a small dense ten-
sor, then conduct convolution using this small tensor, Ô¨Ånally
expand the obtained feature map to the original shape by
Ô¨Ålling zeros.
In practice, the computational costs of the
squeeze and expand steps are negligible compared to the
convolution step.

Figure 2. (Top): Single-channel pruning only prunes the output
channels of Wl and Wl+1. (Bottom): Pair-channel pruning not
only prunes the output channels of Wl+1, but also prunes its input
channels corresponding to the pruned output channels in Wl.

Model

ResNet-20

ResNet-56

Method
SNLI [40]
SFP [11]
CNN-FCF
SNLI [40]
CNN-FCF
SFP [11]

Params.‚Üì% FLOPs‚Üì% Ref.%1 Acc.‚Üì%
37.22
‚Äì
42.75
67.83
68.44
‚Äì
42.71
69.46

92.00
92.20
92.20
92.00
92.20
92.63
92.43
92.43
93.04
93.04
93.59
‚Äì
93.14
93.14
93.53
93.53
93.68
‚Äì
93.58
93.58
1 Ref. denotes the accuracy of the pre-trained original model.
2 ‚Äì means that the metric value is not reported in the compared method.
3 Negative value means the pruned model accuracy is higher than the Ref.

ResNet-32 CNN-FCF
CNN-FCF
Pruning-A [21] 9.40
Pruning-B [21] 13.70
SFP [11]
NISP [41]
CNN-FCF
CNN-FCF
Pruning-A [21] 2.30
Pruning-B [21] 32.40
SFP [11]
NISP [41]
CNN-FCF
CNN-FCF

‚Äì2
42.20
41.60
‚Äì
68.91
41.50
42.21
70.21
10.40
27.60
41.10
43.61
42.78
70.90
15.90
38.60
40.80
43.78
43.08
70.81

1.10
1.37
1.07
3.20
2.67
0.55
0.25
1.69
-0.06
-0.02
-0.19
0.03
-0.243
1.22
0.02
0.23
-0.18
0.18
-0.09
0.62

‚Äì
42.60
43.09
69.74

‚Äì
43.25
43.19
69.51

ResNet-110

Table 1. Comparison results on CIFAR-10.

Pruning ratios. We also notice that some existing works
(e.g., ThiNet [27] and NISP [41]) adopted the same pruning
ratio for all pruned convolutional layers, especially when
there is short-cut in the model (e.g., ResNet). However, we
believe that different layers of one CNN model have dif-
ferent contributions to the model performance. Thus, the
ratios of parameter redundancy of different layers should
be different. We will evaluate the inÔ¨Çuence of the same or
different pruning ratios in experiments (see Section 5.4).

5.3. Comparisons with State of the art Methods

In this section, to compare with state-of-the-art meth-
ods (e.g., ThiNet [27] and NISP [41]), our method which
denoted as CNN-FCF also adopts the pair-channel pruning
strategy and same pruning ratios for all layers.

Results on CIFAR-10. As shown in Table 1, we compress
ResNet-20, ResNet-32, ResNet-56 and ResNet-110 with
two pruning ratios, including 43% and 69%. On ResNet-
20, at the pruning ratio of about 43%, our method gives
the smallest accuracy loss (i.e., 1.07% Acc.‚Üì%).
In con-

3982

ùêñùëôùêëùëúùë¢ùë°ùëôùêñùëô+1ùê∂ùëôùëÅùëôùê∂ùëôùê∂ùëô+1ùëÅùëô+1(=ùê∂ùëô)‚Ä¶‚Ä¶‚Ä¶‚Ä¶Model

Method

Top5‚Üì%

5.4. Analysis of Pruning Strategies and Ratios

Pruning [21]
NISP [41]
CNN-FCF
SFP [11]
NISP [41]
CNN-FCF
CNN-FCF
CNN-FCF
SSS [15]
NISP [41]
CNN-FCF
ThiNet [27]
SFP [11]
NISP [41]
Channel pruning [12] ‚Äì
CNN-FCF
CNN-FCF
CNN-FCF

10.80
27.14
27.05
‚Äì
43.68
42.19
55.80
67.24
27.06
27.12
26.70
33.72
‚Äì
43.82

Params.‚Üì% FLOPs‚Üì% Top1
Ref.%
73.23
‚Äì
73.30
73.92
‚Äì
73.30
73.30
73.30
76.12
‚Äì
76.15
75.30
76.15
‚Äì
‚Äì
76.15
76.15
76.15

24.20
27.32
26.83
41.10
43.76
41.38
54.87
66.05
31.08
27.31
29.24
36.79
41.80
44.01
50.00
46.05
57.10
66.17

42.41
52.52
61.01

Top1‚Üì% Top5
Ref.%
‚Äì
‚Äì
91.42
91.62
‚Äì
91.42
91.42
91.42
92.86
‚Äì
92.87
92.20
92.87
‚Äì
92.20
92.87
92.87
92.87

1.06
0.28
-0.25
2.09
0.92
0.51
1.97
3.59
1.94
0.21
-0.35
1.27
1.54
0.89
‚Äì
0.47
1.60
2.62

‚Äì
-
-0.08
1.29
‚Äì
0.47
1.22
2.11
0.95
‚Äì
-0.26
0.09
0.81
‚Äì
1.40
0.19
0.69
1.37

ResNet-34

ResNet-50

Table 2. Comparison results on ImageNet. ‚ÄúTop1 Ref.%‚Äù denotes
the top1 accuracy of the original model.

trast, SNLI [40] gives the slightly higher accuracy loss (i.e.,
1.1% Acc.‚Üì%), while the pruned ratio of parameters is less
than ours, i.e., 37.22 vs. 42.75 of Params.‚Üì%. The pruned
ratio of parameters by SFP [11] is similar with ours, but its
accuracy loss is higher, i.e., 1.37 vs. 1.07 of Acc.‚Üì%. At
the pruning ratio of about 69%, SNLI also performs worse
than our method, with the higher accuracy loss, i.e., 3.2
vs. 2.67 of Acc.‚Üì%. On other ResNet models with dif-
ferent layers, our method also show very competitive per-
formance, compared to other methods. Moreover, when
comparing the compressing performance of our method on
ResNet models with different layers, an interesting obser-
vation is that the accuracy loss decreases along with the in-
creasing of the number of layers, at the same pruning ratio.
SpeciÔ¨Åcally, at the pruning ratio of about 43%, the values
of Acc.‚Üì% of our method are 1.07, 0.25, ‚àí0.24, ‚àí0.09, on
ResNet models with 20, 32, 56 and 110 layers, respectively;
at the pruning ratio of about 69%, the corresponding values
are 2.67, 1.69, 1.22, 0.62. It demonstrates that the ResNet
models with deeper layers have the larger redundancy on
the image classiÔ¨Åcation task on CIFAR-10.

Results on ImageNet. As shown in Table 2, we compress
ResNet-34 and ResNet-50 with four pruning ratios, includ-
ing about 27%, 43%, 55%, 67%. We present the losses on
both top1 and top5 accuracy. On ResNet-34, at the prun-
ing ratio of about 27%, the Top1‚Üì% value of our method is
‚àí0.25, while that of NISP [41] is 0.28. At the pruning ratio
of about 43%, Param.‚Üì% and FLOPs‚Üì% of our method are
slightly lower than NISP, but our Top1‚Üì% is also lower than
that of NISP, i.e., 0.51 vs. 0.92. At the pruning ratios of
about 55% and 67%, the values of Top1‚Üì% of our method
are only 1.97 and 3.59, respectively; the values of Top5‚Üì%
of our method are only 1.22 and 2.11, respectively. On
ResNet-50, our method also shows very competitive per-
formance, compared to state-of-the-art methods. Moreover,
similar to the results on CIFAR-10, we observe that there is
also a large proportion of redundant parameters in ResNet,
even for the image classiÔ¨Åcation task on ImageNet.

i = |Wl

As demonstrated in Section 5.2, here we evaluate the in-
Ô¨Çuence if we allocate distinct pruning ratios for different
layers. Inspired by [21, 23], we try a simple allocation of
pruning ratios as follows: (1) Calculating the score of each
Ô¨Ålter using the ‚Ñì1-norm, i.e., sl
i|1; (2) Storing all
Ô¨Ålter scores sl
i to a vector s, and sorting s in an ascending
order; (3) Assume that the overall pruning ratio is p, then
the number of Ô¨Ålters to be pruned is Np = p|s|. We set
the Np-th value of s as a global threshold, denoted as sp;
(4) Counting the number of sl
i exceeding sp in each layer,
which is recorded as kl
i (see Problem (3)). For comparison,
we also evaluate the identical ratio for all layers. After train-
ing the CNN-FCF with same or distinct pruning ratios, we
can choose single-channel or pair-channel pruning strategy
before Ô¨Åne-tuning. Thus, there are four settings, including
identical-ratio with single-channel, identical-ratio with pair-
channel, distinct-ratio with single-channel and distinct-ratio
with pair-channel, respectively. The results are shown in Ta-
ble 3. Given the same level of Params.‚Üì% on same dataset
and model, the accuracy loss of distinct ratios is always
much lower than that of identical ratio.
It demonstrates
that distinct ratios obtain much better compressing perfor-
mance. However, the FLOPs‚Üì% value of distinct ratios is
always lower than that of identical ratio. The reason is that
in the setting of distinct ratios, the ratio allocation strategy
described above assigns more pruning ratios to the higher
layers (i.e., closer to the output layer), where the #FLOPs is
smaller than that of the lower layers, as the corresponding
feature maps become smaller. In the future, we will explore
other allocation strategies to achieve more reductions of
#FLOPs. In comparison between single-channel and pair-
channel pruning, given the same level of Params.‚Üì%, the
same dataset and model, pair-channel pruning always gives
lower accuracy loss. The reason is that there are some use-
less parameters in the single-channel pruning. In summary,
the setting of training the CNN-FCF model with distinct
pruning ratio, and with the pair-channel pruning strategy
for Ô¨Åne-tuning, obtains the best compressing performance.

5.5. Analysis of Different Compressing Stages

In this section, we analyze the inÔ¨Çuence of different
stages of a compression procedure of the proposed method,
including the joint optimization of W and v in the CNN-
FCF model, pruning the Ô¨Ålter corresponding to the 0-valued
vl
i, and Ô¨Åne-tuning. Besides, we also compare with two
baselines, including pruning the pre-trained CNN model
randomly (denoted as Random in Table 4 and Fig. 3), and
pruning using the ranking (i.e., ranking the Ô¨Ålters in each
layer using the ‚Ñì1 norm in descent order, and pruning the
lower-ranked Ô¨Ålters with a pre-deÔ¨Åned ratio. This setting
is denoted as Ranking in Table 4 and Fig. 3). We evalu-
ate above methods for compressing the ResNet-56 model

3983

(a) single-channel pruning

Dataset

Model

Params.‚Üì% FLOPs‚Üì% Top1‚Üì% Top5‚Üì%

CIFAR-10

ImageNet

ResNet56-I
ResNet56-D
ResNet56-I
ResNet56-D
ResNet34-I
ResNet34-D
ResNet34-I
ResNet34-D

43.18
42.66
69.14
70.17
27.33
27.23
42.42
43.71

43.15
32.02
71.09
53.08
26.96
16.31
42.13
22.88

0.42
-0.14
2.33
0.75
0.59
-0.03
2.25
0.87

‚Äì
‚Äì
‚Äì
‚Äì

0.36
0.16
1.40
0.50

(b) pair-channel pruning

Dataset

Model

Params.‚Üì% FLOPs‚Üì% Top1‚Üì% Top5‚Üì%

CIFAR-10

ImageNet

ResNet56-I
ResNet56-D
ResNet56-I
ResNet56-D
ResNet34-I
ResNet34-D
ResNet34-I
ResNet34-D

43.09
41.97
69.74
69.57
27.05
27.38
42.19
43.72

42.78
33.99
70.90
55.67
26.83
22.21
41.38
28.42

-0.24
-0.16
1.22
0.64
-0.25
-0.28
0.51
0.39

‚Äì
‚Äì
‚Äì
‚Äì

-0.08
-0.08
0.47
0.28

Table 3. Results of CNN-FCF with identical and distinct prun-
ing ratios. ResNet56-I denotes pruning ResNet-56 using identical
pruning ratio for each layer, while ResNet56-D indicates distinct
ratios for different layers.

Ref.%1 Optim.%2 Pruning%3 Fine-tuning%4
93.14
93.14
93.14
93.14

Method
Random
91.14
92.09
Ranking
92.35
CNN-FCF-S
CNN-FCF-P
92.89
1 Ref.% indicates the accuracy of the original model.
2 Optim.% denotes the accuracy of the optimized CNN-FCF

10.00
10.00
91.44
30.17

‚Äì
‚Äì
91.92
92.36

model.

3 Pruning% denotes the accuracy after pruning.
4 Fine-tuning% denotes the accuracy after Ô¨Åne-tuning.

Table 4. Compressing Performance of ResNet-56 on CIFAR-10,
with the pruning ratio of 45% parameters.

on CIFAR-10, with the overall pruning ratio of 45% param-
eters and the identical pruning ratio of all layers. The joint
optimization of CNN-FCF takes at most 150 epochs. As
shown in Table 4, all methods starts from the same check-
point of ResNet-56, of which the accuracy is 93.14%. In
terms of the Random method, after pruning 45% parame-
ters from the pre-trained model, the accuracy drops to 10%;
after Ô¨Åne-tuning, the accuracy resumes to 91.14%. In terms
of the Ranking method, the accuracy also drops to 10% after
pruning, and it achieves 92.09% after Ô¨Åne-tuning. It demon-
strates that the Ô¨Åne-tuning is crucial for these two baselines,
and Ranking keeps better Ô¨Ålters than Random. CNN-FCF-
S denotes our method with single-channel pruning. After
the joint optimization of W and v in CNN-FCF, the accu-
racy achieves 91.92%, which is very close to 93.14% of the
pre-trained CNN model. After the single-channel pruning,
there is a slight drop of accuracy, i.e., 0.48%. As analyzed
in Implementation details of Section 5.1, it is due to the nu-
merical difference between the converged v and the bina-
rized v. After Ô¨Åne-tuning, the accuracy resumes to 92.35%.
CNN-FCF-P denotes our method with pair-channel prun-

Figure 3. The accuracy curves of Ô¨Åne-tuning of four methods.
As the accuracies of all methods excluding CNN-FCF-S are small
in the Ô¨Årst 5 epochs, we ignore them to highlight the accuracy
differences in Ô¨Ånal epochs. Please refer to Section 5.5 for details.

ing. After the joint optimization, the accuracy achieves
92.36%, which is higher than that of CNN-FCF-S. The rea-
son is that although the overall pruning ratios are same
(i.e., 45%), the Ô¨Ålter cardinality (i.e., kl) of CNN-FCF-P
is larger than that of CNN-FCF-S, as it will prune more pa-
rameters in the pruning stage. However, after pair-channel
pruning, the accuracy signiÔ¨Åcantly drops to 30.17%. The
reason is that the pruning on the input channels of Ô¨Ålters
signiÔ¨Åcantly changes the trained CNN-FCF model. But the
accuracy achieves 92.89% after Ô¨Åne-tuning, which is high-
est among all compared methods. We also show the Ô¨Åne-
tuning curves of above four methods in Fig. 3. The above
comparisons demonstrate that: (1) The proposed compress-
ing method based on CNN-FCF performs better than base-
lines; (2) The joint training based on CNN-FCF produces
a very good compressed model, even without Ô¨Åne-tuning;
(3) When adopting the pair-channel pruning strategy, Ô¨Åne-
tuning is useful to resume the model performance.

6. Conclusion

This work presented a novel model compression method
for deep CNNs, of which the core idea is conducting Ô¨Ålter
learning and Ô¨Ålter selection jointly. To this end, we deÔ¨Åned
a novel type of Ô¨Ålters, dubbed factorized convolutional Ô¨Ålter
(FCF), consisting of a standard convolutional Ô¨Ålter and a bi-
nary scalar, as well as a dot-product operator between them.
To train a CNN model with FCF, we proposed to insert the
ADMM algorithm into the back-propagation framework via
gradient descent. Given a trained CNN-FCF model, a com-
pact or a sparse standard CNN model can be obtained by
only keeping the standard Ô¨Ålters corresponding to the 1-
valued scalars. Experiments on compressing ResNet mod-
els demonstrate the superiority of the proposed method over
state-of-the-art Ô¨Ålter pruning methods.

Acknowledgement The involvements of Yujiu Yang and
Tuanhui Li
in this work were supported in part by
the National Key Research and Development Program
of China (No.2018YFB1601102), and Shenzhen special
fund for the strategic development of emerging industries
(No.JCYJ20170412170118573).

3984

0520406080100120140epoch0.800.820.840.860.880.900.92accuracyRandomRankingCNN-FCF-SCNN-FCF-PReferences

[1] Jose M Alvarez and Mathieu Salzmann. Learning the num-
ber of neurons in deep networks. In Advances in Neural In-
formation Processing Systems, pages 2270‚Äì2278, 2016. 3

[2] Linchao Bao, Baoyuan Wu, and Wei Liu. Cnn in mrf: Video
object segmentation via inference in a cnn-based higher-
order spatio-temporal mrf. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5977‚Äì5986, 2018. 1

[3] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
Jonathan Eckstein, et al. Distributed optimization and sta-
tistical learning via the alternating direction method of mul-
tipliers. Foundations and Trends R(cid:13) in Machine learning,
3(1):1‚Äì122, 2011. 2, 4

[4] Jian Cheng, Pei-song Wang, Gang Li, Qing-hao Hu, and
Han-qing Lu. Recent advances in efÔ¨Åcient computation
of deep convolutional neural networks. Frontiers of Infor-
mation Technology & Electronic Engineering, 19(1):64‚Äì77,
2018. 1

[5] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to
prune deep neural networks via layer-wise optimal brain sur-
geon.
In Advances in Neural Information Processing Sys-
tems, pages 4857‚Äì4867, 2017. 3

[6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation.
In Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
580‚Äì587, 2014. 1

[7] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic net-
work surgery for efÔ¨Åcient dnns. In Advances in Neural In-
formation Processing Systems, pages 1379‚Äì1387, 2016. 2

[8] Song Han, Huizi Mao, and William J Dally. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding.
In International
Conference on Learning Representations, 2016. 1, 2

[9] Babak Hassibi and David G Stork. Second order derivatives
for network pruning: Optimal brain surgeon.
In Advances
in Neural Information Processing Systems, pages 164‚Äì171,
1993. 2

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of IEEE Conference on Computer Vision and Pattern
Recognition, pages 770‚Äì778, 2016. 1, 5

[11] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi
Yang. Soft Ô¨Ålter pruning for accelerating deep convolutional
neural networks. International Joint Conferences on ArtiÔ¨Å-
cial Intelligence, 2018. 2, 5, 6, 7

[12] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In Proceedings
of IEEE International Conference on Computer Vision, 2017.
1, 3, 5, 7

[13] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: EfÔ¨Åcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017. 1

[14] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung
Tang. Network trimming: A data-driven neuron pruning ap-
proach towards efÔ¨Åcient deep architectures. arXiv preprint
arXiv:1607.03250, 2016. 3

[15] Zehao Huang and Naiyan Wang. Data-driven sparse struc-
arXiv preprint

ture selection for deep neural networks.
arXiv:1707.01213, 2017. 3, 5, 7

[16] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim
Choi, Lu Yang, and Dongjun Shin. Compression of deep
convolutional neural networks for fast and low power mobile
applications. arXiv preprint arXiv:1511.06530, 2015. 1

[17] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Cite-
seer, 2009. 2, 5

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiÔ¨Åcation with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems, pages 1097‚Äì1105, 2012. 1

[19] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Os-
eledets, and Victor Lempitsky. Speeding-up convolutional
neural networks using Ô¨Åne-tuned cp-decomposition. arXiv
preprint arXiv:1412.6553, 2014. 1

[20] Yann LeCun, John S Denker, and Sara A Solla. Optimal
brain damage. In Advances in Neural Information Process-
ing Systems, pages 598‚Äì605, 1990. 2

[21] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning Ô¨Ålters for efÔ¨Åcient convnets. In In-
ternational Conference on Learning Representations, 2017.
1, 2, 5, 6, 7

[22] Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, and Ming-
Hsuan Yang. Target-aware deep tracking.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2019. 1

[23] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
Shoumeng Yan, and Changshui Zhang. Learning efÔ¨Åcient
convolutional networks through network slimming. In Pro-
ceedings of IEEE International Conference on Computer Vi-
sion, pages 2755‚Äì2763, 2017. 1, 3, 7

[24] Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei
Liu, and Kwang-Ting Cheng. Bi-real net: Binarizing deep
network towards real-network performance. arXiv preprint
arXiv:1811.01335, 2018. 1

[25] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu,
and Kwang-Ting Cheng. Bi-real net: Enhancing the perfor-
mance of 1-bit cnns with improved representational capabil-
ity and advanced training algorithm. In Proceedings of the
European Conference on Computer Vision, pages 722‚Äì737,
2018. 1

[26] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3431‚Äì3440, 2015. 1

[27] Jian-Hao Luo, Hao Zhang, Hong-Yu Zhou, Chen-Wei Xie,
Jianxin Wu, and Weiyao Lin. Thinet: Pruning cnn Ô¨Ålters
for a thinner net. IEEE transactions on pattern analysis and
machine intelligence, 2018. 1, 3, 5, 6, 7

3985

[42] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-
Seng Chua. Visual translation embedding network for visual
relation detection. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5532‚Äì
5540, 2017. 1

[43] Tianzhu Zhang, Bernard Ghanem, Si Liu, and Narendra
Ahuja. Robust visual tracking via multi-task sparse learn-
ing. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2042‚Äì2049, 2012. 1

[44] Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wu-
jie Wen, Makan Fardad, and Yanzhi Wang. A systematic
dnn weight pruning framework using alternating direction
method of multipliers. In Proceedings of the European Con-
ference on Computer Vision, 2018. 2

[45] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
Accelerating very deep convolutional networks for classiÔ¨Å-
cation and detection. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 38(10):1943‚Äì1955, 2016. 1

[46] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu,
Yong Guo, Qingyao Wu, Junzhou Huang, and Jinhui Zhu.
Discrimination-aware channel pruning for deep neural net-
works. In Advances in Neural Information Processing Sys-
tems, 2018. 3

[28] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
ShufÔ¨Çenet v2: Practical guidelines for efÔ¨Åcient cnn archi-
tecture design. Proceedings of the European Conference on
Computer Vision, 2018. 1

[29] Adam Paszke, Sam Gross, Soumith Chintala, and Gregory
Chanan. Pytorch: Tensors and dynamic neural networks in
python with strong gpu acceleration, 2017. 5

[30] Herbert Robbins and Sutton Monro. A stochastic approxi-
mation method. In Herbert Robbins Selected Papers, pages
102‚Äì109. Springer, 1985. 4

[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211‚Äì252, 2015. 2, 5

[32] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations, 2015.
1

[33] B. Stellato, G. Banjac, P. Goulart, A. Bemporad, and S.
Boyd. OSQP: An operator splitting solver for quadratic pro-
grams. ArXiv e-prints, Nov. 2017. 4

[34] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo,
and Wei Liu. Learning to compose dynamic tree structures
for visual contexts. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2019. 1

[35] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and
Hai Li. Learning structured sparsity in deep neural net-
works. In Advances in Neural Information Processing Sys-
tems, pages 2074‚Äì2082, 2016. 3

[36] Baoyuan Wu, Weidong Chen, Yanbo Fan, Yong Zhang,
Jinlong Hou, Junzhou Huang, Wei Liu, and Tong Zhang.
Tencent ml-images: A large-scale multi-label
image
database for visual representation learning. arXiv preprint
arXiv:1901.01703, 2019. 1

[37] Baoyuan Wu, Weidong Chen, Peng Sun, Wei Liu, Bernard
Ghanem, and Siwei Lyu. Tagging like humans: Diverse and
distinct image annotation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
7967‚Äì7975, 2018. 1

[38] Baoyuan Wu and Bernard Ghanem. lp-box admm: A versa-
tile framework for integer programming. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2018. 2, 3, 4
[39] Baoyuan Wu, Fan Jia, Wei Liu, and Bernard Ghanem. Di-
verse image annotation.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2559‚Äì2567, 2017. 1

[40] Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethink-
ing the smaller-norm-less-informative assumption in channel
pruning of convolution layers. In International Conference
on Learning Representations, 2018. 3, 5, 6, 7

[41] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I
Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and
Larry S Davis. Nisp: Pruning networks using neuron impor-
tance score propagation. In Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition, 2018. 3, 5, 6,
7

3986

