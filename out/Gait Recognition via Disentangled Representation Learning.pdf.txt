Gait Recognition via Disentangled Representation Learning

Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming Liu

Michigan State University

{zhang835, tranluan, yinxi1, atoumyou, liuxm}@msu.edu

Jian Wan, Nanxin Wang

Ford Research and Innovation Center

{jwan1, nwang1}@ford.com

Abstract

Gait, the walking pattern of individuals, is one of the
most important biometrics modalities. Most of the exist-
ing gait recognition methods take silhouettes or articulated
body models as the gait features. These methods suffer from
degraded recognition performance when handling confound-
ing variables, such as clothing, carrying and view angle. To
remedy this issue, we propose a novel AutoEncoder frame-
work to explicitly disentangle pose and appearance features
from RGB imagery and the LSTM-based integration of pose
features over time produces the gait feature. In addition, we
collect a Frontal-View Gait (FVG) dataset to focus on gait
recognition from frontal-view walking, which is a challeng-
ing problem since it contains minimal gait cues compared to
other views. FVG also includes other important variations,
e.g., walking speed, carrying, and clothing. With exten-
sive experiments on CASIA-B, USF and FVG datasets, our
method demonstrates superior performance to the-state-of-
the-arts quantitatively, the ability of feature disentanglement
qualitatively, and promising computational efﬁciency.

Figure 1: We propose to a novel CNN-based model, termed Gait-
Net, to automatically learn the disentangled gait feature from a
walking video, as opposed to handcrafted GEI, or skeleton-based
features. While many conventional gait databases study side-view
imagery, we collect a new gait database where both gallery and
probe are captured in frontal-views.

1. Introduction

Biometrics measures people’s unique physical and behav-
ioral characteristics to recognize the identity of an individual.
Gait [35], the walking pattern of an individual, is one of the
biometrics modalities, e.g., face, ﬁngerprint, and iris. Gait
recognition has the advantage that it can operate at a distance
without user cooperation. Also, it is difﬁcult to camouﬂage.
Due to these advantages, gait recognition is applicable to
many applications such as person identiﬁcation, criminal
investigation, and healthcare.

As other recognition problems in vision, the core of gait
recognition lies in extracting gait-related features from the
video frames of a walking person, where the prior approaches
are categorized into two types: appearance-based and model-
based methods. The appearance-based methods such as Gait

Energy Image (GEI) [20] take the averaged silhouette image
as the gait feature. While having a low computational cost
and can handle low-resolution imagery, it can be sensitive
to variations such as clothes change, carrying, view angles
and walking speed [37, 5, 46, 6, 24, 1]. The model-based
method ﬁrst performs pose estimation and takes articulated
body skeleton as the gait feature. It shows more robustness
to those variations but at a price of a higher computational
cost and dependency on pose estimation accuracy [17, 2].

It is understandable that the challenge in designing a
gait feature is the necessity of being invariant to the ap-
pearance variation due to clothing, viewing angle, carrying,
etc. Therefore, our desire is to disentangle the gait feature
from the visual appearance of the walking person. For both
appearance-based or model-based methods, such disentan-

4710

Table 1: Comparison of existing gait databases and our collected FVG database.

Dataset

#Subjects

#Videos Environment

Resolution

Format

Variations

CASIA-B
USF
OU-ISIR-LP
OU-ISIR-LP-Bag
FVG (our)

124
122
4, 007
62, 528

226

13, 640
1, 870

−

−

2, 856

Indoor
Outdoor
Indoor
Indoor
Outdoor

320×240
720×480
640×480
1, 280×980
1, 920×1, 080

RGB
RGB

View, Clothing, Carrying
View, Ground Surface, Shoes, Carrying, Time

Silhouette View
Silhouette Carrying

RGB

View, Walking Speed, Carrying, Clothing, Background, Time

glement is achieved by manually handcrafting the GEI or
body skeleton, since neither has color information. However,
we argue that these manual disentanglements may lose cer-
tain or create redundant gait information. E.g., GEI learns
the average contours over time, but not the dynamic of how
body parts move. For body skeleton, under carrying con-
dition, certain body joints such as hands may have ﬁxed
positions, and hence are redundant information to gait.

To remedy the issues in handcrafted features, as shown
in Fig. 1, this paper aims to automatically disentangle the
pose/gait features from appearance features, and use the
former for gait recognition. This disentanglement is realized
by designing an autoencoder-based CNN, GaitNet, with
novel loss functions. For each video frame, the encoder
estimates two latent representations, pose feature (i.e., frame-
based gait feature) and appearance feature, by employing two
loss functions: 1) cross reconstruction loss enforces that the
appearance feature of one frame, fused with the pose feature
of another frame, can be decoded to the latter frame; 2) gait
similarity loss forces a sequence of pose features extracted
from a video sequence, of the same subject to be similar even
under different conditions. Finally, the pose features of a
sequence are fed into a multi-layer LSTM with our designed
incremental identity loss to generate the sequence-based gait
feature, where two of which can use the cosine distance as
the video-to-video similarity metric.

Furthermore, most prior work [20, 46, 33, 12, 2, 7, 13]
often choose the walking video of the side view, which
has the richest gait information, as the gallery sequence.
However, practically other view angles, such as the frontal
view, can be very common when pedestrians toward or away
from the surveillance camera. Also, the prior work [40,
10, 11, 34] that focuses on frontal view are often based on
RGB-D videos, which have richer depth information than
RGB videos. Therefore, to encourage gait recognition from
the frontal-view RGB videos that generally has the minimal
amount of gait information, we collect a high-deﬁnition
(HD,1080p) frontal-view gait database with a wide range of
variations. It has three frontal-view angles where the subject
walks from left 45◦, 0◦, and right 45◦ off the optical axes
of the camera. For each of three angles, different variants
are explicitly captured including walking speed, clothing,
carrying, clutter background, etc.

The contributions of this work are the following:
1) We propose an autoencoder-based network, GaitNet,
with novel loss functions to explicitly disentangle the pose

features from visual appearance and use multi-layer LSTM
to obtain aggregated gait feature.

2) We introduce a frontal-view gait database, named
FVG, including various variations of viewing angles, walk-
ing speeds, carrying, clothing changes, background and time
gaps. This is the ﬁrst HD gait database, with a nearly doubled
number of subjects than prior RGB gait databases.

3) Our proposed method outperforms state of the art on

three benchmarks, CASIA-B, USF, and FVG datasets.

2. Related Work

Gait Representation. Most prior works are based on two
types of gait representations. In appearance-based meth-
ods, gait energy image (GEI) [20] or gait entropy image
(GEnI) [5] are deﬁned by extracting silhouette masks. Specif-
ically, GEI uses an averaged silhouette image as the gait
representation for a video. These methods are popular in the
gait recognition community for their simplicity and effective-
ness. However, they often suffer from sizeable intra-subject
appearance changes due to covariates such as clothing, car-
rying, views, and walking speed. On the other hand, model-
based methods [17] ﬁt articulated body models to images
and extract kinematic features such as 2D body joints. While
they are robust to some covariates such as clothing and speed,
they require a relatively higher image resolution for reliable
pose estimation and higher computational costs.

In contrast, our approach learns gait information from
raw RGB video frames which contain the richer information,
thus with higher potential of extracting discriminative gait
features. The most relevant work to ours is [12], which learns
gait features from RGB images via Conditional Random
Field. Compared to [12], our CNN-based approach has
the advantage of being able to leverage a large amount of
training data and learning more discriminative representation
from data with multiple covariates. This is demonstrated by
our extensive comparison with [12] in Sec. 5.2.1.
Gait Databases. There are many classic gait databases such
as SOTON Large dataset [39], USF [37], CASIA-B [23],
OU-ISIR [32], TUM GAID [23] and etc. We compare our
FVG database with the most widely used ones in Tab. 1.
CASIA-B is a large multi-view gait database with three vari-
ations: view angle, clothing, and carrying. Each subject is
captured from 11 views under three conditions: normal walk-
ing (NM), walking in coats (CL) and walking while carrying
bags (BG). For each view, 6, 2 and 2 videos are recorded
from normal, coats and bags conditions. USF database has

4711

Figure 2: Overall architecture of our proposed approach, with three novel loss functions.

122 subjects with ﬁve variations, totaling 32 conditions for
each subject. It contains two view angles (left and right), two
ground surface (grass and concrete), shoes change, carrying
condition and time. While OU-ISIR-LP and OU-ISIR-LP-
Bag are large datasets, we can not leverage them as only the
silhouette is publicly released.

Unlike those databases, our FVG database focuses on
the frontal view, with 3 different near frontal-view angles
towards the camera, and other variations including walking
speed, carrying, clothing, cluttered background and time.

Disentanglement Learning.
Besides model-based ap-
proaches [43, 42, 31] representing data with semantic latent
vectors; data-driven disentangled representation learning
approaches are gaining popularity in computer vision com-
munity. DrNet [14] disentangles content and pose vectors
with a two-encoders architecture, which removes content
information in the pose vector by generative adversarial
training. The work of [3] segments foreground masks of
body parts by 2D pose joints via U-Net [36] and then trans-
forms body parts to desired motion with adversarial training.
Similarly, [15] utilizes U-net and Variational Auto Encoder
(VAE) to disentangle an image into appearance and shape.
DR-GAN [44, 45] achieves state-of-the-art performances on
pose-invariant face recognition by explicitly disentangling
pose variation with a multi-task GAN [19].

Different from [14, 3, 15], our method has only one en-
coder to disentangle the appearance and gait information,
through the design of novel loss functions without the need
for adversarial training. Unlike DR-GAN [45], our method
does not require adversarial training, which makes training
more accessible. Further, pose labels are used in DR-GAN
training so as to disentangle identity feature from the pose.
However, to disentangle gait and appearance feature from
the RGB information, there is no gait nor appearance label to
be utilized for our method, since the type of walking pattern

or clothes cannot be deﬁned as discrete classes.

3. Proposed Approach

Let us start with a simple example. Assuming there are
three videos, where videos 1 and 2 capture subject A wear-
ing t-shirt and long down coat respectively, and in video
3 subject B wears the same long down coat as in video 2.
The objective is to design an algorithm, from which the gait
features of video 1 and 2 are the same, while those of video 2
and 3 are different. Clearly, this is a challenging objective, as
the long down coat can easily dominate the feature extraction,
which would make videos 2 and 3 to be more similar than
videos 1 and 2 in the latent space of gait features. Indeed the
core challenge, as well as the objective, of gait recognition
is to extract gait features that are discriminative among sub-
jects, but invariant to different confounding factors, such as
viewing angles, walking speeds and appearance.

Our approach to achieve this objective is via feature dis-
entanglement - separating the gait feature from appearance
information for a given walking video. As shown in Fig. 2,
the input to our model is a video frame, with background
removed using any off-the-shelf pedestrian detection and seg-
mentation method [21, 9, 8]. An encoder-decoder network,
with carefully designed loss functions, is used to disentangle
the appearance and pose features for each video frame. Then,
a multi-layer LSTM explores the temporal dynamics of pose
features and aggregates them to a sequence-based gait fea-
ture for the identiﬁcation purpose. In this section, we ﬁrst
present the feature disentanglement, followed by temporal
aggregation, and ﬁnally implementation details.

3.1. Appearance and Pose Feature Disentanglement

For the majority of gait recognition datasets, there is a
limited appearance variation within each subject. Hence,
appearance could be a discriminate cue for identiﬁcation
during training as many subjects can be easily distinguished

4712

by their clothes. Unfortunately, any networks or feature
extractors relying on appearance will not generalize well on
the test set or in practice, due to potentially diverse clothing
or appearance between two videos of the same subject.

This limitation on training sets also prevents us from
learning good feature extractors if solely relying on identiﬁ-
cation objective. Hence we propose to learn to disentangle
the gait feature from the visual appearance in an unsuper-
vised manner. Since a video is composed of frames, dis-
entanglement should be conducted on the frame level ﬁrst.
Because there is no dynamic information within a video
frame, we aim to disentangle the pose feature from the vi-
sual appearance for a frame. The dynamics of pose features
over a sequence will contribute to the gait feature. In other
words, we view the pose feature as the manifestation of
video-based gait feature at a speciﬁc frame.

To this end, we propose to use an encoder-decoder net-
work architecture with carefully designed loss functions to
disentangle the pose feature from appearance feature. The
encoder, E, encodes a feature representation of each frame,
I, and explicitly splits it into two parts, namely appearance
fa and pose fg features:

fa, fg = E(I).

(1)

These two features are expected to fully describe the original
input image. As they can be decoded back to the original
input through a decoder D:

(2)

eI = D(fa, fg).

We now deﬁne the various loss functions deﬁned for learning
the encoder, E, and decoder D.

Cross Reconstruction Loss. The reconstructedeI should

be close to the original input I. However, enforcing self-
reconstruction loss as in typical auto-encoder can’t ensure
the appearance fa learning appearance information across
the video and fg representing pose information in each frame.
Hence we propose the cross reconstruction loss, using an
appearance feature f t1
g of
another one to reconstruct the latter frame:

a of one frame and pose feature f t2

,

(3)

Lxrecon =(cid:13)(cid:13)D(f t1

g ) − It2(cid:13)(cid:13)2

2

a , f t2

where It is the video frame at the time step t.

The cross reconstruction loss, on one hand, can play a
role as the self-reconstruction loss to make sure the two
features are sufﬁciently representative to reconstruct video
frames. On the other hand, as we can pair a pose feature of
a current frame to the appearance feature of any frame in
the same video to reconstruct the same target, it enforces the
appearance features to be similar across all frames.
Gait Similarity Loss. The cross reconstruction loss pre-
vents the appearance feature fa to be over-represented, con-
taining pose variation that changes between frames. How-
ever, appearance information may still be leaked into pose

feature fg. In an extreme case, fa is a constant vector while
fg encodes all the information of a video frame. To make fg
“cleaner”, we leverage multiple videos of the same subject.
Extra videos can introduce the change in appearance. Given
two videos of the same subject with length n1, n2 in two
different conditions c1, c2. Ideally, c1, c2 should contain
difference in the person’s appearance, i.e., cloth changes.
While appearance changes, the gait information should be
consistent between two videos. Since it’s almost impos-
sible to enforce similarity on fg between video frames as
it requires precise frame-level alignment; we enforce the
similarity between two videos’ averaged pose features:

Lgait-sim =(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

1
n1

n1Xt=1

f (t,c1)
g

−

1
n2

n2Xt=1

f (t,c2)
g

2

2

.

(4)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

3.2. Gait Feature Learning via Aggregation

Even when we can disentangle appearance and pose in-
formation for each video frame, the current feature fg only
contains the walking pose of the person in a speciﬁc instance,
which can share similarity with another speciﬁc instance of
a very different person. Here, we are looking for discrim-
inative characteristics in a person walking pattern. There-
fore, modeling its temporal change is critical. This is where
temporal modeling architectures like the recurrent neural
network or long short-term memory (LSTM) work best.

Speciﬁcally, in this work, we utilize a multi-layer LSTM
structure to explore spatial (e.g., the shape of a person) and
mainly, temporal (e.g., how the trajectory of subjects’ body
parts changes over time) information on pose features. As
shown in Fig. 2, pose features extracted from one video
sequence are feed into a 3-layer LSTM. The output of the
LSTM is connected to a classiﬁer C, in this case, a linear
classiﬁer is used, to classify the subject’s identity.

Let ht be the output of the LSTM at time step t, which is

accumulative after feeding t pose features fg into it:

ht = LSTM(f 1

g , f 2

g , ..., f t

g).

(5)

Now we deﬁne the loss function for LSTM. A trivial
option for identiﬁcation is to add the classiﬁcation loss on
top of the LSTM output of the ﬁnal time step:

Lid-single = − log(Ck(hn)),

(6)

which is the negative log likelihood that the classiﬁer C
correctly identiﬁes the ﬁnal output hn as its identity label k.
Identiﬁcation with Averaged Feature. By the nature of
LSTM, the output ht is greatly affected by its last input f t
g.
Hence the LSTM output, ht, can be varied across time steps.
With a desire to obtain a gait feature that can be robust to the
stopping instance of a walking cycle, we propose to use the

4713

averaged LSTM output as our gait feature for identiﬁcation:

f t
gait =

hs.

1
t

tXs=1

The identiﬁcation loss can be rewritten as:

Lid-avg = − log(Ck(f n

gait))

= − log Ck  1

n

hs!! .

nXs=1

(7)

(8)

Incremental Identity Loss. LSTM is expected to learn that
the longer the video sequence, the more walking information
it processes then the more conﬁdent it identiﬁes the subject.
Instead of minimizing the loss on the ﬁnal time step, we
propose to use all the intermediate outputs of every time step
weighted by wt:

Lid-inc-avg =

1
n

nXt=1

−wt log Ck  1

t

hs!! .

tXs=1

(9)

To this end, the overall training loss function is:

L = Lid-inc-avg + λrLxrecon + λsLgait-sim.

(10)

The entire system, encoder-decoder, and LSTM are
jointly trained. Updating E to optimize Lid-inc-avg also helps
to further generate pose feature that has identity information
and on which LSTM is able to explore temporal dynamics.
At the test time, the output f t
gait of LSTM is the gait feature of
the video and used as the identity feature representation for
matching. The cosine similarity score is used as the metric.

Figure 3: Examples of FVG Dataset. (a) Samples of the near
frontal middle, left and right walking view angles in session 1 (SE1)
of the ﬁrst subject (S1). SE3-S1 is the same subject in session 3.
(b) Samples of slow and fast walking speed for another subject in
session 1. Frames in top red boxes are slow and in the bottom red
box are fast walking. Carrying bag sample is shown below. (c)
samples of changing clothes and with cluttered background from
one subject in session 2.

Sigmoid activation to bring the value into [0, 1] range as the
input. The classiﬁcation part is a stacked 3-layer LSTM [18],
which has 256 hidden units in each of cells.

Adam optimizer [27] is used with the learning rate of
0.0001, and the momentum of 0.9. For each batch, we use
video frames from 32 different clips. Since video lengths
are varied, a random crop of 20-frame sequence is applied;
all shorter videos are discarded. For Eqn. 9, we set wt =
t2 while other options such as wt = 1 also yield similar
performance. The λr and λs (Eqn. 10) are set to 0.1 and
0.005 in all experiments.

3.3. Implementation Details

4. Front-View Gait Database

Segmentation and Detection. Our network receives video
frames with the person of interest segmented. The fore-
ground mask is obtained from the state-of-the-art detection,
Mask R-CNN [21]. Instead of using a zero-one mask by
hard thresholding, we keep the soft mask returned by the
network, where each pixel indicates the probability of being
a person. This is partially due to the difﬁculty in choosing a
threshold. Also, it prevents the loss in information due to the
mask estimation error. We use a bounding box with a ﬁxed
ratio of width: height = 1 : 2 with the absolute height and
center location given by the Mask R-CNN network. Input is
obtained by pixel-wise multiplication between the mask and
RGB values which is then resized to 32 × 64.
Network hyperparameter. Our encoder-decoder network
is a typical CNN. Encoder consisting of 4 stride-2 convo-
lution layers following by Batch Normalization and Leaky
ReLU activation. The decoder structure is an inverse of
the encoder, built from transposed convolution, Batch Nor-
malization and Leaky ReLU layers. The ﬁnal layer has a

Collection. To facilitate the research of gait recognition
from frontal-view angles, we collect the Front-View Gait
(FVG) database in a course of two years 2017 and 2018.
During the capturing, we place the camera (Logitech C920
Pro Webcam or GoPro Hero 5) on a tripod at the height
of 1.5 meter. We ask each of 226 subjects to walk toward
the camera 12 times starting from around 16 meters, which
results in 12 videos per subject. The videos are captured
at 1, 080 × 1, 920 resolution with the average length of 10
seconds. The height of human in the video ranges from 101
to 909 pixels. These 12 walks have the combination of three
angles toward the camera (−45◦, 0◦, 45◦ off the optical axes
of the camera), and four variations.

FVG is collected in three sessions. In session 1, in 2017,
videos from 147 subjects are collected with four variations
(normal walking, slow walking, fast walking, and carrying
status). In session 2, in 2018, videos from additional 79
subjects are collected. Variations are normal, slow or fast
walking speed, clothes or shoes change, and twilight or clus-

4714

Figure 4: Synthesized frames on CASIA-B by decoding the vari-
a and f
ous combination of f
g. Left and right parts are two examples.
For each example, f
a is extracted from images in the ﬁrst column
and f
g is extracted from images in the ﬁrst row. 0 vector has the
same dimension as f

a, accordingly.

g or f

tered background. Finally in session 3, we collect repeated
12 subjects in year 2018 for extreme challenging test with
the same setup as section 1. The purpose is to test how
time gaps affect gait, along with changes in cloth/shoes or
walking speed. Fig. 3 shows exemplar images from FVG.
Protocols. Different from prior gait databases, subjects in
FVG are walking toward the camera, which creates a great
challenge on exploiting gait information as the difference
in consecutive frames can be much smaller than side-view
walking. We focus our evaluation on variations that are
challenging, e.g., different appearance, carrying a bag, or are
not presented in other databases, e.g., cluttered background,
along with view angles.

To benchmark research on FVG, we deﬁne 5 evaluation
protocols, among which there are two commonalities: 1)
the ﬁrst 136 and rest 90 subjects are used for training and
testing respectively; 2) the video 2, the normal frontal-view
walking, is used as the gallery. The 5 protocols differ in their
speciﬁc probe data, which cover the variations of Walking
Speed (WS), Carrying Bag (CB), Changing Clothes (CL),
Cluttered Background (CBG), and all variations (All). At
the top part of Fig. 6, we list the detailed probe set for all 5
protocols. E.g., for the WS protocol, the probes are video
4 − 9 in session 1 and video 4 − 6 in session 2.

5. Experiments

Databases. We evaluate the proposed approach on three
gait databases, CASIA-B [47], USF [37] and FVG. As men-
tioned in Sec. 2, CASIA-B, and USF are the most widely
used gait databases, making the comparison with prior work
easier. We compare our method with [46, 12, 29, 30] on
these two databases, by following the respective experimen-
tal protocols of the baselines. These are either the most

Figure 5: Synthesized frames on CASIA-B by decoding f
a and
f
g from different variations (NM vs. CL). Left and right parts
are two examples. For each example, f
a is extracted from the
most left column image (CL) and f
g is extracted from the top
row images (NM). Top row synthesized images are generated with
model trained without Lgait-sym loss, bottom row is with the loss. To
show the differences, details in generated images are magniﬁed.

Table 2: Ablation study on our disentanglement loss and classiﬁ-
cation loss. By removing or replacing with other loss functions,
Rank-1 recognition rate on cross NM and CL condition degrades.

Disentanglement Loss Classiﬁcation Loss Rank 1

-

Lxrecon

Lxrecon + Lgait-sim
Lxrecon + Lgait-sim
Lxrecon + Lgait-sim
Lxrecon + Lgait-sim

Lid-inc-avg
Lid-inc-avg
Lid-inc-avg

Lid-avg
Lid-single
Lid-ae [41]

56.0
60.2
85.6
62.6
26.0
71.2

recent and state-of-art work or classic gait recognition meth-
ods. The OU-ISIR database [32] is not evaluated, and related
methods [33] are not compared since our work consumes
RGB video input, but OU-ISIR only releases silhouettes.

5.1. Ablation Study

Feature Visualization. To aid on understanding our fea-
tures, we randomly pair fa, fg features from different images
and visualize the resultant paired feature by feeding it into
our learned decoder D. As shown in Fig. 4, each result is
generated by paring the appearance fa in the ﬁrst column,
and the pose fg in the ﬁrst row. The synthesized images
show that indeed fa contributes all the appearance informa-
tion, e.g., cloth, color, texture, contour, as they are consistent
across each row. Meanwhile, fg contributes all the pose
information, e.g., position of hand and feet, which share
similarity across columns. We also visualize features fa, fg
individually by forcing the other feature to be a zero vector 0.
Without fg, the reconstructed image still shares appearance
similarity with fa input but does not show a clear walking
pose. Meanwhile, when removing fa, the reconstructed im-
age still mimics the pose of fg’s input.
Disentanglement with Gait Similarity Loss. With the
cross reconstruction loss, the appearance feature fa can be

4715

Table 3: Recognition accuracy cross views under NM on CASIA-B dataset. One single GaitNet module is trained for all the view angles.

Methods

0◦

18◦

36◦

54◦

72◦

108◦

126◦

144◦

162◦

180◦ Average

13
CPM [12]
16
GEI-SVR [29]
18
CMCC [28]
8
ViDP [26]
STIP+NN [30] −
18
LB [46]
38
L-CRF [12]
68
GaitNet (our)

14
22
24
12
−
36
75

74

17
35
41
45
−
67.5
68
88

27
62
63
95
66
96
80
100
− 84.0
99.5
93
98
93
91
99

65
95
95
100
86.4
99.5
99
98

22
65
68
81
−
92
93
84

20
38
41
50
−
66
67
75

15
20
21
15
−
36
76
76

10
13
13
8
−
18
39
65

24.1
42.0
43.9
45.4

−

56.9
67.8
81.8

Table 4: Comparison on CASIA-B with cross view and conditions.
Three models are trained for NM-NM, NM-BG, NM-CL. Average
accuracies are calculated excluding probe view angles.

Gallery NM #1-4

Probe NM #5-6

CCA [4]
ViDP [26]
LB [46]
GaitNet (our)

Probe BG #1-2

LB-subGEI [46]
GaitNet (our)

Probe CL #1-2

LB-subGEI [46]
GaitNet (our)

0◦

−

−

82.6
91.2

0◦

64.2
83.0

0◦

37.7
42.1

54◦

−
64.2
94.3
95.6

54◦

76.9
86.6

54◦

61.1
70.7

0◦-180◦

90◦

−

60.4
87.4
92.6

36◦-144◦

126◦ Mean

54◦

90◦

126◦ Mean

−

65.0
94.0
96.0

−

−

89.6
93.9

66.0
87.0
98.0
99.1

66.0
87.7
98.0
99.0

67.0
89.3
99.2
99.2

66.3
88.0
98.4
99.1

90◦

126◦ Mean

54◦

90◦

126◦ Mean

63.1
74.8

76.9
85.8

70.3
82.6

89.2
90.0

84.3
85.6

91.0
92.7

88.2
89.4

90◦

126◦ Mean

54◦

90◦

126◦ Mean

54.6
70.6

59.1
69.4

53.1
63.2

77.3
80.0

74.5
81.2

74.5
79.4

75.4
80.2

enforced to represent static information that shares across
the video. However, as discussed, the feature fg can be
spoiled or even encode the whole video frame. Here we
show the need for the gait similarity loss Lgait-sym on the
feature disentanglement. Fig. 5 shows the cross visualization
of two different models learned with and without Lgait-sym.
Without Lgait-sym the decoded image shares some appearance
characteristic, e.g., cloth style, contour, with fg. Meanwhile
with Lgait-sym, appearance better matches with fa.

Joints Location as Pose Feature.
In literature, there is a
large amount of effort in human pose estimation [17]. Ag-
gregating joint locations over time could be a good candidate
for gait features. Here we compare our framework with a
baseline, named PE-LSTM, using pose estimation results as
the input to the same LSTM as ours. Using state-of-the-art
pose estimator [16], we extract 14 joints’ locations and feed
to the LSTM. This network achieves the recognition accu-
racy of 65.4% TDR at 1% FAR on the ALL protocol of FVG
dataset, where our method outperforms it with 81.2%. This
result demonstrates that our pose feature fg does explore
more discriminate feature than the joints’ locations alone.

Loss Function’s Impact on Performance. As the sys-
tem consists of multiple loss functions, here we analyze the
effect of each loss function on the ﬁnal recognition perfor-
mance. Tab. 2 reports the recognition accuracy of different
variants of our framework on CASIA-B dataset under NM
and CL. We ﬁrst explore the effects of different disentangle-
ment losses. Using Lid-inc-avg as the classiﬁcation loss, we
train different variants of our framework: a baseline with-
out any disentanglement losses, a model with Lxrecon, and

our full model with both Lxrecon and Lgait-sim. The baseline
achieves the accuracy of 56.0%. Adding the Lxrecon slightly
improves the performance to 60.2%. By combining with
Lgait-sim, our model signiﬁcantly improves the performance
to 85.6%. Between Lxrecon and Lgait-sim, the gait similarity
loss plays a more critical role as Lxrecon is mainly designed to
constrain the appearance feature fa, which does not directly
involve identiﬁcation.

Using the combination, Lxrecon and Lgait-sim, we bench-
mark different options for classiﬁcation loss as presented
in Sec. 3.1, as well as the autoencoder loss by Srivastava et
al. [41]. The model using the conventional identity loss on
the ﬁnal LSTM output Lid-single achieves the rank-1 accuracy
of 26.0%. Using the average output of LSTM as identity
feature, Lid-average, shows to improve the performance to
62.6%. The autoencoder loss [41] achieves a good perfor-
mance, 71.2%. However, it is still far from our proposed
incremental identity loss Lid-inc-avg’s performance.

5.2. Evaluation on Benchmark Datasets
5.2.1 CASIA-B

Since various experimental protocols have been deﬁned on
CASIA-B, for a fair comparison, we strictly follow the re-
spective protocols in the baseline methods. Following [46],
Protocol 1 uses the ﬁrst 74 subjects for training and rest 50
for testing, regarding variations of NM (normal), BG (carry-
ing bag) and CL (wearing a coat) with crossing view angles
of 0◦, 54◦, 90◦, and 126◦. Three models are trained for
comparison in Tab. 4. For the detailed protocol, please refer
to [46]. Here we mainly compare our performance to Wu
et al. [46], along with other methods [26]. Under multiple
view angles and cross three variations, our method (GaitNet)
achieves the best performance on all comparisons.

Recently, Chen et al. [12] propose new protocols to unify
the training and testing where only one single model is being
trained for each protocol. Protocol 2 focuses on walking
direction variations, where all videos used are in NM. The
training set includes videos of ﬁrst 24 subjects in all view
angles. The rest 100 subjects are for testing. The gallery is
made of four videos at 90◦ view for each subject. Videos
from remaining view angles are the probe. The rank 1 recog-
nition accuracy are reported in Tab. 3. Our GaitNet achieves
the best average accuracy of 81.8% across ten view angles,
with signiﬁcant improvement on extreme views. E.g., at

4716

Table 5: Comparison with [12] and [46] under different walking
conditions on CASIA-B by accuracies. One single GaitNet model
is trained with all gallery and probe views and the two conditions.

Probe Gallery

54
54
90
90
126
126

36
72
72
108
108
144

Mean

GaitNet (our)

L-CRF [12]

LB [46]

RLTDA [25]

BG

91.6
90.0
95.6
87.4
90.1
93.8

91.4

CL

BG

CL

BG

CL

BG

87.0
90.0
94.2
86.5
89.8
91.2

93.8
91.2
94.4
89.2
92.5
88.1

89.8

91.5

59.8
72.5
88.5
85.7
68.8
62.5

73.0

92.7
90.4
93.3
88.9
93.3
86.0

49.7
62.0
78.3
75.6
58.1
51.4

80.8
71.5
75.3
76.5
66.5
72.3

90.8

62.5

73.8

CL

69.4
57.8
63.2
72.1
64.6
64.2

65.2

Table 6: Deﬁnition of FVG protocols and performance comparison.
Under each of the 5 protocols, the ﬁrst/second columns indicate the
indexes of videos used in gallery/probe.

Index of Gallery & Probe videos

Session 1
Session 2
Session 3

Variation

2
2
−

4-9
4-6
−

2
−

−

10-12

−

−

−
2
−

−
7-9
−

−
2
−

−

10-12

−

2
2
−

1,3-12
1,3-12
1-12

WS

CB

CL

CBG

All

TDR@FAR

1%

5%

1%

5%

1%

5%

1%

5%

1%

PE-LSTM
GEI [20]
GEINet [38]
DCNN [1]
LB [46]
GaitNet (our)

79.3
9.4
15.5
11.0
53.4
91.8

87.3
19.5
35.2
23.6
73.1
96.6

59.1
6.1
11.8
5.7
23.1
74.2

78.6
12.5
24.7
12.7
50.3
85.1

55.4
5.7
6.5
7.0
23.2
56.8

67.5
13.2
16.7
15.9
38.5
72.0

61.6
6.3
17.3
8.1
56.1
92.3

72.2
16.7
35.2
20.9
74.3
97.0

65.4
5.8
13.0
7.9
40.7
81.2

5%

74.1
16.1
29.2
19.0
61.6
87.8

view angles of 0◦, and 180◦, the improvement margins are
30% and 26% respectively. This shows that GaitNet learns a
better view-invariant gait feature than other methods.

Protocol 3 focuses on appearance variations. Training sets
have videos under BG and CL. There are 34 subjects in total
with 54◦ to 144◦ view angles. Different test sets are made
with the different combination of view angles of the gallery
and probe as well as the appearance condition (BG or CL).
The results are presented in Tab. 5. We have comparable
performance with the state-of-art method L-CRF [12] on BG
subset while signiﬁcantly improving the performance on CL
subset. Note that due to the challenge of CL protocol, there
is a signiﬁcant performance gap between BG and CL for all
methods except ours, which is yet another evidence that our
gait feature has strong invariance to all major gait variations.
Across all evaluation protocols, GaitNet consistently out-
performs state of the art. This shows the superior of GaitNet
on learning a robust representation under different varia-
tions. It is contributed to our ability to disentangle pose/gait
information from other static variations.

5.2.2 USF

The original protocol of USF [37] does not deﬁne a training
set, which is not applicable to our method, as well as [46],
that require data to train the models. Hence following the
experiment setting in [46], we randomly partition the dataset
into the non-overlapping training and test sets, each with half
of the subjects. We test on Probe A, deﬁned in [46], where
the probe is different from the gallery by the viewpoint. We
achieve the identiﬁcation accuracy of 99.5 ± 0.2%, which

Table 7: Runtime (ms per frame) comparison on FVG dataset.

Methods

Pre-processing

Inference Total

PE-LSTM
GEINet [38]
DCNN [1]
LB [46]
GaitNet (our)

22.4
0.5
0.5
0.5
0.5

0.1
1.5
1.7
1.3
1.0

22.5
2.0
2.2
1.8
1.5

is better than the reported 96.7 ± 0.5% of LB network [46],
and 94.7 ± 2.2% of multi-task GAN [22].

5.2.3 FVG

Given that FVG is a newly collected database and no re-
ported performance from prior work, we make the efforts
to implement 4 classic or state-of-the-art methods on gait
recognition [20, 38, 1, 46]. For each of 4 methods and our
GaitNet, one model is trained with the 136-subject training
set and tested on all 5 protocols.

As shown in Tab. 6, our method shows state-of-the-art
performance compared with other methods, including the
recent CNN-based methods. Among 5 protocols, CL is the
most challenging variation as in CASIA-B. Comparing with
all different methods GEI based methods suffer from frontal
view due to the lack of walking information.

5.3. Runtime Speed

System efﬁciency is an essential metric for many vision
systems including gait recognition. We calculate the efﬁ-
ciency while each of the 5 methods processing one video of
USF dataset on the same desktop with GeForce GTX 1080
Ti GPU. As shown in Tab. 7, our method is signiﬁcantly
faster than the pose estimation method because of 1) efﬁ-
ciency of Mask R-CNN; 2) an accurate, yet slow, version of
AlphaPose [16] is required for gait recognition.

6. Conclusions

This paper presents an autoencoder-based method termed
GaitNet that can disentangle appearance and gait feature rep-
resentation from raw RGB frames, and utilize a multi-layer
LSTM structure to further explore temporal information to
generate a gait representation for each video sequence. We
compare our method extensively with the state of the arts
on CASIA-B, USF, and our collected FVG datasets. The
superior results show the generalization and promising of
the proposed feature disentanglement approach. We hope
that in the future, this disentanglement approach is a viable
option for other vision problems where motion dynamics
needs to be extracted while being invariant to confounding
factors, e.g., expression recognition with invariance to facial
appearance, activity recognition with invariance to clothing.

Acknowledgement

This work was supported with funds from the Ford-MSU
Alliance program.

4717

References

[1] Munif Alotaibi and Ausif Mahmood. Improved Gait recogni-
tion based on specialized deep convolutional neural networks.
Computer Vision and Image Understanding (CVIU), 164:103–
110, 2017. 1, 8

[2] Gunawan Ariyanto and Mark S Nixon. Marionette mass-
spring model for 3D gait biometrics. In International Confer-
ence on Biometrics (ICB), 2012. 1, 2

[3] Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fredo Du-
rand, and John Guttag. Synthesizing Images of Humans in
Unseen Poses. In Computer Vision and Pattern Recognition
(CVPR), 2018. 3

[4] Khalid Bashir, Tao Xiang, and Shaogang Gong. Cross-View
Gait Recognition Using Correlation Strength. In British Ma-
chine Vision Conference (BMVC), 2010. 7

[5] Khalid Bashir, Tao Xiang, and Shaogang Gong. Gait Recogni-
tion Using Gait Entropy Image. In International Conference
on Imaging for Crime Detection and Prevention (ICDP), 2010.
1, 2

[6] Khalid Bashir, Tao Xiang, and Shaogang Gong. Gait recogni-
tion without subject cooperation. Pattern Recognition Letters,
31(13):2052–2060, 2010. 1

[7] Aaron F Bobick and Amos Y Johnson. Gait Recognition
In Computer

Using Static, Activity-Speciﬁc Parameters.
Vision and Pattern Recognition (CVPR), 2001. 2

[8] Garrick Brazil and Xiaoming Liu. Pedestrian Detection with
In Computer Vision and

Autoregressive Network Phases.
Pattern Recognition (CVPR), 2019. 3

[9] Garrick Brazil, Xi Yin, and Xiaoming Liu.

Illuminating
Pedestrians via Simultaneous Detection and Segmentation. In
International Conference on Computer Vision (ICCV), 2017.
3

[10] Pratik Chattopadhyay, Aditi Roy, Shamik Sural, and Jayanta
Mukhopadhyay. Pose Depth Volume extraction from RGB-D
streams for frontal gait recognition. Journal of Visual Com-
munication and Image Representation, 25(1):53–63, 2014.
2

[11] Pratik Chattopadhyay, Shamik Sural, and Jayanta Mukherjee.
Frontal Gait Recognition From Incomplete Sequences Using
RGB-D Camera. IEEE Transactions on Information Forensics
and Security, 9(11):1843–1856, 2014. 2

[12] Xin Chen, Jian Weng, Wei Lu, and Jiaming Xu. Multi-Gait
Recognition Based on Attribute Discovery. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
40(7):1697–1710, 2018. 2, 6, 7, 8

[13] David Cunado, Mark S Nixon, and John N Carter. Automatic
extraction and description of human gait models for recogni-
tion purposes. Computer Vision and Image Understanding,
90(1):1–41, 2003. 2

[14] Emily L Denton et al. Unsupervised Learning of Disentan-
In Neural Information

gled Representations from Video.
Processing Systems (NeurIPS), 2017. 3

[16] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
RMPE: Regional Multi-Person Pose Estimation. In Inter-
national Conference on Computer Vision (ICCV), 2017. 7,
8

[17] Yang Feng, Yuncheng Li, and Jiebo Luo. Learning Effective
Gait Features Using LSTM. In International Conference on
Pattern Recognition (ICPR), 2016. 1, 2, 7

[18] Felix A Gers, J¨urgen Schmidhuber, and Fred Cummins.
Learning to forget: continual prediction with LSTM. Neural
Computation, 1999. 5

[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative Adversarial Nets.
In Neural
Information Processing Systems (NeurIPS), 2014. 3

[20] Ju Han and Bir Bhanu. Individual Recognition Using Gait
Energy Image. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI), 28(2):316–322, 2006. 1, 2, 8

[21] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask R-CNN. In International Conference on Com-
puter Vision (ICCV), 2017. 3, 5

[22] Yiwei He, Junping Zhang, Hongming Shan, and Liang Wang.
Multi-Task GANs for View-Speciﬁc Feature Learning in Gait
Recognition. IEEE Transactions on Information Forensics
and Security, 14(1):102–113, 2019. 8

[23] Martin Hofmann, J¨urgen Geiger, Sebastian Bachmann, Bj¨orn
Schuller, and Gerhard Rigoll. The TUM Gait from Audio,
Image and Depth (GAID) database: Multimodal recognition
of subjects and traits. Journal of Visual Communication and
Image Representation, 25(1):195–206, 2014. 2

[24] Md Altab Hossain, Yasushi Makihara, Junqiu Wang, and
Yasushi Yagi. Clothing-invariant gait identiﬁcation using part-
based clothing categorization and adaptive weight control.
Pattern Recognition, 43(6):2281–2291, 2010. 1

[25] Haifeng Hu. Enhanced Gabor Feature Based Classiﬁcation
Using a Regularized Locally Tensor Discriminant Model for
Multiview Gait Recognition. IEEE Transactions on Circuits
and Systems for Video Technology, 23(7):1274–1286, 2013. 8

[26] Maodi Hu, Yunhong Wang, Zhaoxiang Zhang, James J Little,
and Di Huang. View-Invariant Discriminative Projection for
Multi-View Gait-Based Human Identiﬁcation. IEEE Trans-
actions on Information Forensics and Security, 8(12):2034–
2045, 2013. 7

[27] Diederik P Kingma and Jimmy Ba. Adam: A Method for
Stochastic Optimization. arXiv preprint arXiv:1412.6980,
2014. 5

[28] Worapan Kusakunniran. Recognizing Gaits on Spatio-
Temporal Feature Domain. IEEE Transactions on Informa-
tion Forensics and Security, 9(9):1416–1423, 2014. 7

[29] Worapan Kusakunniran, Qiang Wu, Jian Zhang, and Hong-
dong Li. Support Vector Regression for Multi-View Gait
Recognition based on Local Motion Feature Selection. In
Computer Vision and Pattern Recognition (CVPR), 2010. 6, 7

[15] Patrick Esser, Ekaterina Sutter, and Bj¨orn Ommer. A Varia-
tional U-Net for Conditional Appearance and Shape Genera-
tion. In Computer Vision and Pattern Recognition (CVPR),
2018. 3

[30] Worapan Kusakunniran, Qiang Wu, Jian Zhang, Hongdong Li,
and Liang Wang. Recognizing Gaits Across Views Through
Correlated Motion Co-Clustering.
IEEE Transactions on
Image Processing, 23(2):696–709, 2014. 6, 7

4718

[46] Zifeng Wu, Yongzhen Huang, Liang Wang, Xiaogang Wang,
and Tieniu Tan. A Comprehensive Study on Cross-View
Gait Based Human Identiﬁcation with Deep CNN.
IEEE
Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 39(2):209–226, 2017. 1, 2, 6, 7, 8

[47] Shiqi Yu, Daoliang Tan, and Tieniu Tan. A Framework for
Evaluating the Effect of View Angle, Clothing and Carrying
Condition on Gait Recognition. In International Conference
on Pattern Recognition (ICPR), 2006. 6

[31] Feng Liu, Dan Zeng, Qijun Zhao, and Xiaoming Liu. Disen-
tangling Features in 3D Face Shapes for Joint Face Recon-
struction and Recognition. In Computer Vision and Pattern
Recognition (CVPR), 2018. 3

[32] Yasushi Makihara, Hidetoshi Mannami, Akira Tsuji, Md Al-
tab Hossain, Kazushige Sugiura, Atsushi Mori, and Yasushi
Yagi. The OU-ISIR Gait Database Comprising the Tread-
mill Dataset.
IPSJ Transactions on Computer Vision and
Applications, 4:53–62, 2012. 2, 6

[33] Yasushi Makihara, Atsuyuki Suzuki, Daigo Muramatsu, Xi-
ang Li, and Yasushi Yagi. Joint Intensity and Spatial Metric
Learning for Robust Gait Recognition. In Computer Vision
and Pattern Recognition (CVPR), 2017. 2, 6

[34] Athira M Nambiar, Paulo Lobato Correia, and Lu´ıs Ducla
Soares. Frontal Gait Recognition Combining 2D and 3D Data.
In ACM Workshop on Multimedia and Security, 2012. 2

[35] Mark S Nixon, Tieniu Tan, and Rama Chellappa. Human

Identiﬁcation Based on Gait. 2010. 1

[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net:
Convolutional Networks for Biomedical Image Segmentation.
In International Conference on Medical Image Computing
and Computer-Assisted Intervention (MICCAI), 2015. 3

[37] Sudeep Sarkar, P Jonathon Phillips, Zongyi Liu, Isidro Rob-
ledo Vega, Patrick Grother, and Kevin W Bowyer. The Hu-
man ID Gait Challenge Problem: Data Sets, Performance,
and Analysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI), 27(2):162–177, 2005. 1, 2, 6,
8

[38] Kohei Shiraga, Yasushi Makihara, Daigo Muramatsu, Tomio
Echigo, and Yasushi Yagi. GEINet: View-Invariant Gait
Recognition Using a Convolutional Neural Network. In Inter-
national Conference on Biometrics (ICB), 2016. 8

[39] Jamie D Shutler, Michael G Grant, Mark S Nixon, and John N
Carter. On a Large Sequence-Based Human Gait Database.
In Applications and Science in Soft Computing. 2004. 2

[40] Sabesan Sivapalan, Daniel Chen, Simon Denman, Sridha Srid-
haran, and Clinton Fookes. Gait Energy Volumes and Frontal
Gait Recognition using Depth Images. In International Joint
Conference on Biometrics (IJCB), 2011. 2

[41] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-
nov. Unsupervised Learning of Video Representations using
LSTMs. In International Conference on Machine Learning
(ICML), 2015. 6, 7

[42] Luan Tran, Feng Liu, and Xiaoming Liu. Towards High-
ﬁdelity Nonlinear 3D Face Morphable Model. In Computer
Vision and Pattern Recognition (CVPR), June 2019. 3

[43] Luan Tran and Xiaoming Liu. Nonlinear 3D Face Morphable
Model. In Computer Vision and Pattern Recognition (CVPR),
June 2018. 3

[44] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled Repre-
sentation Learning GAN for Pose-Invariant Face Recognition.
In Computer Vision and Pattern Recognition (CVPR), 2017.
3

[45] Luan Tran, Xi Yin, and Xiaoming Liu. Representation Learn-
ing by Rotating Your Faces. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI), 2018. 3

4719

