Deep Sketch-Shape Hashing with Segmented 3D Stochastic Viewing

Jiaxin Chen1âˆ—, Jie Qin1âˆ—â€ , Li Liu1, Fan Zhu1, Fumin Shen2, Jin Xie3 and Ling Shao1

1Inception Institute of Artiï¬cial Intelligence (IIAI), Abu Dhabi, UAE

2University of Electronic Science and Technology of China, Chengdu, China

3Nanjing University of Science and Technology, Nanjing, China

{firstname.lastname}@inceptioniai.org, fumin.shen@gmail.com, csjxie@njust.edu.cn

Abstract

Sketch-based 3D shape retrieval has been extensively
studied in recent works, most of which focus on improv-
ing the retrieval accuracy, whilst neglecting the efï¬ciency.
In this paper, we propose a novel framework for efï¬cient
sketch-based 3D shape retrieval, i.e., Deep Sketch-Shape
Hashing (DSSH), which tackles the challenging problem
from two perspectives. Firstly, we propose an intuitive 3D
shape representation method to deal with unaligned shapes
with arbitrary poses. Speciï¬cally, the proposed Segmented
Stochastic-viewing Shape Network models discriminative
3D representations by a set of 2D images rendered from
multiple views, which are stochastically selected from non-
overlapping spatial segments of a 3D sphere. Secondly,
Batch-Hard Binary Coding (BHBC) is developed to learn
semantics-preserving compact binary codes by mining the
hardest samples. The overall framework is jointly learned
by developing an alternating iteration algorithm. Extensive
experimental results on three benchmarks show that DSSH
improves both the retrieval efï¬ciency and accuracy remark-
ably, compared to the state-of-the-art methods.

1. Introduction

Recently, sketch-based 3D shape retrieval has drawn a
signiï¬cant amount of attention from the computer vision
community [41, 14, 26, 50, 58, 12, 53, 48, 37, 7], owing
to the succinctness of free-hand sketches and the increasing
demands from real applications. This task aims to search
for semantically relevant 3D shapes queried by 2D sketches,
which is very challenging due to the large divergences be-
tween the two modalities.

Numerous efforts have been devoted to this task, and
they typically aim at improving the retrieval accuracy by
learning discriminative representations for both sketches
and shapes [48, 53, 7] or developing ranking/distance met-
rics robust to cross-modality variations [50, 15, 12, 27, 37].

Figure 1. (a) 2D sketches; (b) Sampling 2D images from the
aligned 3D shape; (c) Sampling 2D images from the unaligned
3D shape; (d) Our segmented stochastic sampling strategy.

In general, one of the critical issues in this task is how to
model the meshed surface of a 3D shape. Most state-of-
the-art methods adopt the projection-based model [45], in
which a 3D shape is projected into a set of rendered 2D im-
ages. Through aborative observations on the existing 3D
shape datasets, we ï¬nd that most shapes are stored in the
upright position, as also pointed out by [50, 11]. Hence, if
we select the rendering views horizontally (see Fig. 1 (b)),
we can always obtain informative 2D images to learn robust
representations. The above strategy is commonly adopted
by most existing approaches. However, realistic 3D shapes
often lack alignment with arbitrary poses1. In such cases,
conventional methods may fail to acquire useful 2D im-
ages. For instance, as shown in Fig. 1 (c), if the 3D shape
is stored horizontally, the rendered 2D images will hardly
contain any useful information. Note that sketches are often
drawn from the side view, so the retrieval task will become
intractable given the signiï¬cant discrepancies between the
sketches and 2D images rendered from 3D shapes.

In this work, we ï¬rst propose a novel stochastic sampling
method, namely Segmented Stochastic-viewing Shape Net-
work (S3N), to tackle the above challenge of 3D shape rep-
resentation. S3N randomly samples rendering views from
the sphere around a 3D shape. Concretely, it ï¬rst divides the

1Though shapes can be aligned beforehand, 3D shape alignment is non-

âˆ— indicates equal contributions; â€  indicates corresponding author.

trivial and will induce considerable computational time additionally.

791

ğ’šğ’™ğ’›ğ’šğ’›ğ’™ğŸoğŸğŸ–ğŸoğŸğŸ•ğŸoğŸoğŸğŸ–ğŸoğŸğŸ•ğŸoğ“¢1ğ“¢2ğ“¢3ğ“¢4ğ“¢1ğ“¢2ğ“¢3ğ“¢4â€¦+âˆâ€¦â€¦â€¦(ğš)(ğ›)(ğœ)(ğ)ğŸğŸğŸoğŸoğŸ‘ğŸoğŸ”ğŸoğŸ—ğŸoğŸğŸ“ğŸoğŸğŸ–ğŸoğŸğŸğŸoğŸğŸ’ğŸoğŸ‘ğŸ‘ğŸoğŸ‘ğŸğŸoğŸğŸğŸoğŸoğŸ‘ğŸoğŸ”ğŸoğŸ—ğŸoğŸğŸ“ğŸoğŸğŸ–ğŸoğŸğŸğŸoğŸğŸ’ğŸoğŸ‘ğŸ‘ğŸoğŸ‘ğŸğŸoğ“¢ğ’Œ:ğ’ğ©ğšğ­ğ¢ğšğ¥ğ¬ğğ ğ¦ğğ§ğ­ğ¨ğŸğ­ğ¡ğğ¬ğ©ğ¡ğğ«ğUnaligned 3D shapeAligned 3D shapeMulti-view sampling in the horizontal planeğ¬ğšğ¦ğ©ğ¥ğ¢ğ§ğ ğŸğ§ğğ¬ğšğ¦ğ©ğ¥ğ¢ğ§ğ ğŸ‘ğ«ğğ¬ğšğ¦ğ©ğ¥ğ¢ğ§ğ ğŸğ¬ğ­Stochastic sampling from each segmentMulti-view sampling in the horizontal planeSketchesğ’™ğ’›ğ’šsphere into K non-overlapping segments, and then stochas-
tically samples one view from each spatial segment. Fur-
thermore, an attention network is proposed to exploit the
importance of different views. Despite its simplicity, the
proposed strategy has the following advantages: 1) K is
typically set to a small value (e.g., 4). A 3D shape is
thus represented by a set of limited 2D images, making
the sampling procedure computationally efï¬cient. 2) Since
the spatial segments are non-overlapping and the views are
sampled randomly, S3N avoids sampling completely non-
informative views (see Fig. 1 (d)). 3) If we sample a 3D
shape multiple times, a sequence of K rendering views will
be generated. Therefore, given sufï¬cient sampling times,
S3N can capture as comprehensive information as possible,
resulting in much more discriminative representations.

In addition, most existing sketch-based 3D shape re-
trieval approaches require high computational costs and
large memory load. As a result, they are not capable of pro-
viding real-time responses for efï¬cient retrieval, especially
when dealing with large-scale 3D shape data. There is thus
a pressing need for 3D shape retrieval systems that can store
a large number of 3D shapes with low memory costs, whilst
accomplishing fast and accurate retrieval. Moreover, with
the prevalence of portable/wearable devices, which have
limited computational capabilities and storage space, the
demands for real-time applications in handling large-scale
data is rising. To deal with this, inspired by recent advances
in binary coding, we aim to project high-dimensional rep-
resentations to low-dimensional Hamming space, where the
distances between the binary codes of sketches and shapes
can be computed extremely fast using XOR operations. To
this end, a Batch-Hard Binary Coding (BHBC) scheme is
proposed to learn semantics-preserving discriminative bi-
nary codes by mining the hardest positive/negative samples.

Finally, by jointly learning the above two modules and
the Sketch Network (as shown in Fig. 2), we propose a novel
framework, i.e., Deep Sketch-Shape Hashing (DSSH), for
efï¬cient and accurate sketch-based 3D shape retrieval. Our
main contributions are three-fold:

1) We propose a novel binary coding approach for efï¬-
cient sketch-based 3D shape retrieval. DSSH learns to em-
bed both sketches and shapes into compact binary codes,
which can signiï¬cantly reduce memory storage and boost
computational efï¬ciency. To the best of our knowledge,
this is the ï¬rst work that addresses the efï¬ciency issue of
sketch-based 3D shape retrieval, whilst achieving competi-
tive accuracies with the state-of-the-art methods.

2) A new projection-based method (i.e., S3N) is pro-
posed for learning effective 3D representations, even when
3D shapes lack alignment. S3N represents a 3D shape as a
set of 2D images rendered from segmented stochastic ren-
dering views. Furthermore, S3N incorporates an attention
network that exploits the importance of different views.

3) A novel binary coding strategy (i.e., BHBC) is de-
veloped to learn discriminative binary codes by mining the
hardest samples across different modalities. More impor-
tantly, BHBC, S3N, and the Sketch Network are learned
jointly via an alternative iteration optimization algorithm to
fulï¬ll the ultimate goal of efï¬cient and accurate retrieval.

2. Related Work

Sketch-based 3D Shape Retrieval. A lot of efforts have
been devoted to 3D shape retrieval [31, 4, 52, 2, 45, 39, 54,
3, 11, 1]. Since free-hand sketches are more convenient to
acquire than 3D models, sketch-based 3D shape retrieval
has attracted more and more attention in the last few years.
Existing works mainly focus on learning modality-
speciï¬c representations for sketches and 3D shapes [28,
26, 55, 56, 58, 27], or designing effective matching meth-
ods across modalities [15, 55, 25, 27]. Recently, a vari-
ety of deep learning based approaches have been proposed
for joint representation learning and matching [50, 58, 12,
54, 48, 7, 37].
In [50, 12, 53], discriminative represen-
tations for both sketches and 3D shapes were learned by
two Siamese CNNs. In [58], the cross-domain neural net-
works with pyramid structures were presented to mitigate
cross-domain divergences. [7] addressed the same issue by
developing a Generative Adversarial Networks based deep
adaptation model. In [37], 3D shape representations were
learned by PointNet [38], which was jointly trained with a
deep sketch network through semantic embedding. How-
ever, all the above works focused on improving the match-
ing accuracy, ignoring the time costs and memory load.
Learning-based Hashing. Hashing/binary coding [22, 35,
17, 34, 29, 13, 24, 42, 9, 20, 8, 40, 6, 32] has been exten-
sively studied recently, due to its promising performance
in processing large-scale data. Among various approaches,
cross-modality hashing [5, 23, 30, 19, 33], which learns
semantics-correlated binary codes for two heterogeneous
modalities, is the most relevant to our work. However, most
of these methods focus on text or sketch-based image re-
trieval tasks.
[16] discussed the semi-supervised hashing
based 3D model retrieval, by applying the existing ITQ [17]
method. Nevertheless, all of the above hashing approaches
are not speciï¬cally designed for the sketch-based 3D shape
retrieval, and thus neglect to explore the intrinsic relation-
ships between hand-drawn sketches and 3D shapes.

3. Deep Sketch-Shape Hashing

As illustrated in Fig. 2, our DSSH is composed of two
branches of networks, i.e., the Sketch Network (SN) and
the Segmented Stochastic-viewing Shape Network (S3N).
Speciï¬cally, SN extracts high-dimensional features using
convolutional layers, followed by several hash layers to fur-
ther learn compact binary representations of sketches. On

792

Input

Sketch Network (SN)

3D shape

2D sketch

â€¦

â€¦

â€¦

Convolutional Layers: 8-(Â·)

12

â€¦
â€¦

Segmented Stochastic-viewing Shape Network (S6N) 

Hash Layers: 72(Â·)

Sketch
code

Shape
code

Batch-Hard 
Binary Coding

Sketch
code

Shape
code

h
a
r
d
e
s
t
 
p
o
s
i
t
i
v
e

B
a
t
c
h
-
w
i
s
e

1

-1

â€¦

-1

1

-1

â€¦

-1

-1

-1

pull

push

-1

1

â€¦

1

1

-1

-1

1

â€¦

-1

1

â€¦

-1

1

1

h
a
r
d
e
s
t
 
n
e
g
a
t
i
v
e

B
a
t
c
h
-
w
i
s
e

Anchor

+/

z

#"

y

â€¦

+-
x

!"

+0

(!", #")~*(+" )
Segmented Stochastic-viewing

r
e
f
l
e
c
t
i
o
n
m
o
d
e
l
:

 

9
(
Â·
)

i

R
e
n
d
n
g
v
i
a

 

â€¦

â€¦

 
t
h
e
 
P
h
o
n
g

â€¦

Weight sharing

â€¦

.
,-

â€¦

.
,/

â€¦

. Â· 5-
,-

â€¦

â€¦ â€¦

View Attention 
Network: $(Â·)
. Â· 5/
,/

â€¦ â€¦ â€¦

View Attention 
Network: $(Â·)

â€¦

.
,0

â€¦

â€¦

. Â· 50
,0

â€¦ â€¦ â€¦

A
v
e
r
a
g
e
 
p
o
o
l
i

n
g

View and shape information

.
,-
3-
4-

.
,/
3/
4/

â€¦

.
,0
30
40

â€¦

1.

N
e
t
w
o
r
k
:

$

(
Â·
)

V
i
e
w
A

 

t
t
e
n
t
i
o
n

 

â€¦

â€¦

!-
#-

!/
#/

â€¦

!0
#0

Sampled rendering views

â€¦

Rendered 

images

Convolutional Layers: 8.(Â·)

View Attention 
Network: $(Â·)

Hash Layers: 7.(Â·)

5- 5/ â€¦ 50
View-specific weights

Figure 2. The overall framework of Deep Sketch-Shape Hashing (DSSH). DSSH consists of two branches, i.e., Sketch Network (SN) and
Segmented Stochastic-viewing Shape Network (S3N). SN encodes sketches into compact representations via convolutional layers and hash
layers. Besides the above layers, S3N projects a 3D shape into a set of rendered 2D images with a Segmented Stochastic-viewing module.
The view-speciï¬c weights are exploited by an intuitive View Attention Network. The weighted features are then aggregated into the ï¬nal
compact representations for the 3D shape via average pooling. To learn semantics-preserving binary codes, we propose a Batch-Hard
Binary Coding scheme, which is jointly trained with SN and S3N for the task of efï¬cient sketch-based 3D shape retrieval.

the other hand, S3N ï¬rst obtains a set of K rendered 2D
images using the stochastic sampling strategy, which are si-
multaneously fed into K weight-sharing convolutional lay-
ers. View attention networks are employed to learn view-
speciï¬c weights. Finally, the weighted features are embed-
ded into a low-dimensional subspace through hash layers
and aggregated to obtain the ï¬nal compact binary codes.

For convenience, we use the superscripts 1 and 2 to in-
dicate the modalities of the 2D sketch and 3D shape, re-
spectively. We denote the functions representing the con-
volutional layers of the 2D sketch and 3D shape as F 1(Â·)
and F 2(Â·), respectively. Similarly, the hash layers for the
2D sketch and 3D shape are denoted by H1(Â·) and H2(Â·),
respectively. As for the Shape Network, the additional 2D
rendering operation on 3D shapes and the view attention
networks are respectively denoted as R(Â·) and A(Â·). In the
following, we will ï¬rst introduce the 3D shape network and
our binary coding scheme in detail. Then, the joint objective
function and the optimization algorithm will be elaborated.

3.1. Segmented Stochastic viewing Shape Network

To represent a 3D shape, we adopt the widely-used
projection-based method, i.e., rendering a 3D model into
a set of 2D images. Speciï¬cally, a virtual camera view-
point (or rendering view) is selected. The pixel value of the
rendered image is determined by interpolating the reï¬‚ected
intensity of the polygon vertices of the 3D shape from the

selected viewpoint, via the Phong reï¬‚ection model [36]. A
rendering view is determined by a 3-d vector (r, Ï†, Î¸). Here,
r is the Euclidean distance between the viewpoint and the
origin. Ï† âˆˆ [0, 2Ï€) is the angle of the azimuth or the hor-
izontal rotation, and Î¸ âˆˆ [0, Ï€) indicates the angle of the
vertical elevation. Usually, r is set to a ï¬xed value, which
means the virtual camera views are actually located on a
sphere S with radius r. Without loss of generality, we omit
the radius r for simplicity, since the rendering view only
depends on (Ï†, Î¸).

Different from existing methods that manually select
multiple views in the horizontal plane, we develop a
stochastic sampling method to obtain arbitrary rendering
views, namely Segmented Stochastic-viewing.
In particu-
lar, we divide S into K segments {S 1, S 2, Â· Â· Â· , S K} with
equal spatial coverage. For instance, if K = 4, we can split
S into the following four segments:

S 1 = {(Ï†, Î¸) | Ï† âˆˆ [0, Ï€); Î¸ âˆˆ [0, Ï€/2)};
S 2 = {(Ï†, Î¸) | Ï† âˆˆ [Ï€, 2Ï€); Î¸ âˆˆ [0, Ï€/2)};
S 3 = {(Ï†, Î¸) | Ï† âˆˆ [0, Ï€); Î¸ âˆˆ [Ï€/2, Ï€)};
S 4 = {(Ï†, Î¸) | Ï† âˆˆ [Ï€, 2Ï€); Î¸ âˆˆ [Ï€/2, Ï€)}.

(1)

After segmenting the sphere, we select one random ren-
dering view (Ï†k, Î¸k) from each segment S k, and ï¬nally ob-
tain K views {(Ï†k, Î¸k)}K
k=1. Thereafter, a given shape M
can be represented by an image set with stochastic render-

ï£±ï£´ï£´ï£²
ï£´ï£´ï£³

793

ing views as follows:

IM = {R(M ; Ï†k, Î¸k)|(Ï†k, Î¸k) âˆ¼ U (S k)}K

k=1 ,

(2)

where R(M ; Ï†k, Î¸k) indicates the rendering operation on
M based on the viewpoint (Ï†k, Î¸k), and U (S k) is the uni-
form distribution on S k.

(cid:8){(Ï†k(t), Î¸k(t))}K

In practice, we employ a batch-wise training pro-
cess so that each 3D shape can be selected as train-
ing data for multiple times. Supposing that M is se-
lected TM times, a sequence of rendering views VM =
will be generated using our sam-
pling strategy, where (Ï†k(t), Î¸k(t)) is the sampled render-
ing view from the k-th spatial segment S k during the t-th
sampling. Correspondingly, M is modeled by a sequence
of image snippets IM = {IM (t)}TM
t=1, where IM (t) is gen-
erated based on the sampled views {(Ï†k(t), Î¸k(t))}K
k=1 dur-
ing the t-th sampling. Thereafter, S3N learns the represen-
tation of a 3D shape M from the sequence {IM (t)}TM
t=1.

k=1(cid:9)TM

t=1

We have the following intuitive observations w.r.t. S3N:
1) When the number of spatial segments is small (e.g.,
K=4), a 3D shape M is modeled by a small image set,
leading to high computational efï¬ciency.

2) We

choose non-overlapping spatial

segments
S 1, Â· Â· Â· , S K that cover the entire sphere S jointly. By sam-
pling in a stochastic way, the selected views {(Ï†k, Î¸k)}K
k=1
can capture non-redundant and complementary characteris-
tics of a 3D shape, making the rendered image set always
informative for sketch-based 3D shape retrieval.

t=1{(Ï†k(t), Î¸k(t))}K

3) When TM â†’ +âˆ, âˆªTM

k=1 â†’ S.
In other words, the sampled rendering views can cover the
whole space S. Therefore, the proposed sampling strategy
has the ability of capturing all 2D viewings of M , which
is beneï¬cial for learning discriminative and comprehensive
representations of 3D shapes.

After obtaining the rendering views via the segmented
stochastic sampling, we model M by S3N as follows:

S3N (M ) = H2(cid:0)A(F 2(IÏ†1,Î¸1 )), Â· Â· Â· , A(F 2(IÏ†K ,Î¸K ))(cid:1) ,

where IÏ†k,Î¸k = R(M ; Ï†k, Î¸k). Here, F 2(Â·) performs con-
volutional operations on the rendered images and gener-
ates a high-dimensional real-valued feature vector f 2
k =
F 2(IÏ†k,Î¸k ) for the k-th view.

k}K

View Attention Network. To fully exploit complemen-
tary information across different views, we propose a view
attention network A(Â·) to capture view-speciï¬c weights of
the features {f 2
k=1 from K rendering views. For compu-
tational convenience, the scalar Ï†k is encoded into a 360-d
one-hot vector Ï†k âˆˆ R360, of which the i-th element is
set to 1 if (i âˆ’ 1) Ã— Ï€/180 â‰¤ Ï†k < i Ã— Ï€/180, and 0
otherwise. In a similar manner, Î¸k is encoded into a 180-
d vector. Considering realistic 3D shapes are usually not
aligned, the weights of rendering views vary for different
3D shapes. To address this problem, A(Â·) also takes the

k = F 2(IÏ†k,Î¸k ) âˆˆ Rd2

feature f 2
as the input to learn shape-
dependant weights. As a result, the concatenated vector
k; Ï†k; Î¸k]T is fed into A(Â·), which then outputs
ak = [f 2
a weight wk = A(ak) âˆˆ (0, 1) for f 2
k.

k}K

By using A(Â·), we can obtain a set of weighted features
{wk Â·f 2
k=1. The hash function H2 w.r.t. 3D shapes further
embeds the weighted features into low-dimensional ones,
which are subsequently aggregated as one feature vector
f2 = H2({wk Â· f 2

k=1) via average pooling.

k}K

Note that, S3N is not sensitive to the input order of
{IÏ†k,Î¸k }K
k=1, since all K convolutional/hash layers (one for
each segment) share weights, and the average pooling used
for feature aggregation is order-invariant.

3.2. Learning Discriminative Binary Codes

As mentioned above, S3N provides a framework to
learn informative and discriminative representations of 3D
shapes. In this subsection, we present the details about how
to obtain the ï¬nal discriminative binary representations.

In practice, mini batches are ï¬rst constructed from the
whole training data by following [7], which can mitigate the
class imbalance problem of existing benchmarks. Specif-
ically, we randomly select C classes and collect K 2D
sketches and K 3D shapes for each class. We denote the
selected N = C Ã— K images of sketches and 3D shapes

respectively. Their corresponding class labels are denoted

by I1 =(cid:8)I 1
as Y1 =(cid:8)y1

1, I 1

2, Â· Â· Â· , I 1

1, y1

2, Â· Â· Â· , y1

N(cid:9) and M = {M1, M2, Â· Â· Â· , MN },
N(cid:9) and Y2 =(cid:8)y2
N(cid:9).

2, Â· Â· Â· , y2

1, y2

1, Â· Â· Â· , f1

i = H1(F 1(I 1

By passing I1 through the Sketch Network, we can ex-
tract the feature vectors for sketches: F1 = [f1
N ]T âˆˆ
RN Ã—L. Here f1
i )), where F 1 and H1 denote
the functions of convolutional and hash layers with regard
to sketches, respectively. Similarly, given a batch M of 3D
shapes, we can extract the features F2 = [f2
N ]T âˆˆ
RN Ã—L, where f2
compute

i = H2(cid:0)A(cid:2)F 2(R(Mi))(cid:3)(cid:1).

the binary representations B1
=
N ]T
âˆˆ
and
sketches
1, Â· Â· Â· , b2
N ]T âˆˆ {âˆ’1, 1}N Ã—L for 3D shapes

{âˆ’1, 1}N Ã—L for

1, Â· Â· Â· , f2

We
[b1
1, Â· Â· Â· , b1
B2 = [b2
as follows:

i = sgn(fm
bm

i ), for m âˆˆ {1, 2} and i = 1, Â· Â· Â· , N.

(3)

Here sgn(Â·) indicates the element-wise sign function, which
outputs 1 for non-negative values, and âˆ’1 otherwise.

3.2.1 Batch-Hard Binary Coding

In order to generate discriminative binary representations
after the quantization step in Eq. (3), we propose Batch-
Hard Binary Coding (BHBC) by incorporating semantic
class-level information. Formally, given a binary code bm
i
of the i-th sample from the modality m, the â€˜hardest posi-
tiveâ€™ code b Ë†m
i,nâˆ— within

i,pâˆ— and the â€˜hardest negativeâ€™ code b Ë†m

794

the batch B Ë†m from the modality Ë†m (m, Ë†m âˆˆ {1, 2}) are
exploited by

pâˆ— =

nâˆ— =

argmax
p=1,Â·Â·Â· ,N ; y Ë†m
argmin
n=1,Â·Â·Â· ,N ; y Ë†m

p =ym

i

p 6=ym

i

dh(bm

i , b Ë†m

p ),

dh(bm

i , b Ë†m

n ),

(4)

where dh(Â·) is the Hamming distance. Eq. (4) implies that
b Ë†m
i,pâˆ— has the maximal intra-class distance to bm
has the minimal inter-class distance to bm
i .

i , and b Ë†m

i , b Ë†m

We aim to learn the binary codes Bm by minimizing
dh(bm
i,nâˆ— ) within a
large margin Î·b. To this end, the loss function LBC w.r.t.
BHBC is deï¬ned as

i,pâˆ— ), whilst maximizing dh(bm

i , b Ë†m

i,pâˆ—

Xm, Ë†m,i(cid:2)Î·b âˆ’ dh(bm

i , b Ë†m

i,nâˆ— ) + dh(bm

i , b Ë†m

,

(5)

i,pâˆ— )(cid:3)+

where Î·b > 0 is the margin, and [Â·]+ = max(Â·, 0).

It can be observed that minimizing Eq. (5) is equivalent

to minimizing the following formula:

Xm, Ë†m,i

Ïƒm, Ë†m

i,Î·b (cid:2)dh(bm

i , b Ë†m

i,nâˆ— ) âˆ’ dh(bm

i , b Ë†m

i,pâˆ— )(cid:3) ,

where Ïƒm, Ë†m
i,Î·b
and 1 otherwise.

= 0 if dh(bm

i , b Ë†m

i,nâˆ— ) âˆ’ dh(bm

i , b Ë†m

i,pâˆ— ) â‰¥ Î·b,

Based on the above equation and the fact

that

dh(bi, bj) = Lâˆ’bT

i Â·bj
2

, we ï¬nally obtain

LBC := Xm, Ë†m,i

Ïƒm, Ë†m
i,Î·b

Â· (bm

i )T (cid:2)b Ë†m

i,pâˆ— âˆ’ b Ë†m

(6)

i,nâˆ—(cid:3) .

3.3. Joint Objective Function

Ideally, the features F1 and F2 learned by DSSH should
be: 1) discriminative, 2) semantically correlative across
modalities, and 3) robust to binary quantization. To this
end, we develop the following joint loss function LDSSH :

min
W,B1,B2

LDSSH := LD + Î»1 Â· LC + Î»2 Â· LQ + LBC, (7)

where W indicates the parameters of DSSH, LD is the loss
for learning discriminative features, LC is the loss for en-
hancing semantic correlations between F1 and F2, LQ is
the binary quantization loss, LBC is the loss function for
BHBC, and Î»1, Î»2 > 0 are trade-off parameters. Since
LBC is introduced in the above subsection, we will present
the remaining three loss functions in detail, respectively.

1) LD. We adopt a similar loss function as BHBC, i.e.,
batch-hard triplet loss [18]. Speciï¬cally, given the i-th sam-
ple fm
from the m-th modality, we ï¬rst explore its â€˜hardest
i
positiveâ€™ samples f Ë†m
i,pâˆ— and â€˜hardest negativeâ€™ samples f Ë†m
i,nâˆ— ,
within the batch F Ë†m, from the Ë†m-th modality, as follows:

pâˆ— = argmax
p=1,Â·Â·Â· ,N
y Ë†m
p =ym

i

d(fm

i , f Ë†m

p ), nâˆ— = argmin
n=1,Â·Â·Â· ,N
p 6=ym
y Ë†m

i

d(fm

i , f Ë†m

n ),

where d(Â·) is the Euclidean distance.

From Eq. (8), we can see that f Ë†m

has the maximal intra-class distance to fm
minimal inter-class distance to fm
as the following triplet hinge loss [51, 18]

i,pâˆ— is the sample that
i,nâˆ— has the
i . LD is then formulated

i , and f Ë†m

Xm, Ë†m=1,2

i=1,Â·Â·Â· ,N

1

4N hÎ· âˆ’ d(fm

i , f Ë†m

i,nâˆ— ) + d(fm

i , f Ë†m

,

(9)

i,pâˆ— )i+

where Î· > 0 is the margin.

By minimizing LD, the maximal intra-class distance will
decrease to a value smaller than the minimal inter-class dis-
tance within a margin Î·. This indicates that DSSH can learn
discriminative features based on LD.

belong to the same class as Ë†pi,j = Ë†p(y1

2) LC . We ï¬rst deï¬ne the likelihood that f1
j |f1

i and f2
j
i , f2
j) =
j are the features of the
i-th sketch and the j-th 3D shape, respectively. We com-
pute LC as the weighted cross-entropy loss between Ë†p(y1
i =
j |f1
y2
j) and the ground-truth probability pi,j as follows:

j(cid:17), where f1

1/(cid:16)1 + eâˆ’f1

i and f2

i = y2

i , f2

T f2

i

âˆ’

N

[

Xi,j=1

1
Np

pi,j log(Ë†pi,j)+

1
Nn

(1âˆ’pi,j) log(1âˆ’Ë†pi,j)], (10)

where pi,j = 1 if y1
i , f2
number of pairs (f1
i , f2
number of pairs (f1

i = y2
j , and 0 otherwise. Np is the
j) from the same class, and Nn is the
j) from different classes.

3) LQ. We simply employ the quantization loss as

LQ =

1

2(cid:0)kF1 âˆ’ B1k2

F + kF2 âˆ’ B2k2

(11)

F(cid:1) ,

where k Â· kF indicates the matrix Frobenius norm.

By training DSSH with LQ, the learned features F1 and
F2 can remain discriminative after the binary quantization
(3), if both B1 and B2 are semantics-
operation in Eq.
preserving, which is guaranteed by the BHBC scheme.

3.3.1 Optimization

In order to solve problem (7), we develop an optimization
method based on alternative iteration. Speciï¬cally, we learn
the binary codes B1 and B2 by ï¬xing the parameters W of
the whole network. Subsequently, we update W by dimin-
ishing LDSSH with ï¬xed B1 and B2. We alternatively opti-
mize (7) based on these two steps until convergence.
1) B-Step. When W is ï¬xed, (7) turns into

min LBC + Î»2
s.t. B1 âˆˆ {âˆ’1, 1}N Ã—L, B2 âˆˆ {âˆ’1, 1}N Ã—L.

2 Â·(cid:0)kF1 âˆ’ B1k2

F + kF2 âˆ’ B2k2

F(cid:1)

(12)

(8)

In general, (12) is an NP-hard problem.

Inspired by
[42], we propose to discretely learn Bm (m âˆˆ {1, 2})

795

For-
by adopting an alternating optimization solution.
mally, we optimize bm
the binary code of the i-
i
th sample from the m-th modality) by ï¬xing the binary

(i.e.,

codes(cid:8)b Ëœm

j |j = 1, Â· Â· Â· , N ; Ëœm âˆˆ {1, 2}; j 6= i if Ëœm = m(cid:9) of

i can then be updated by

the remaining 2N -1 samples. bm
optimizing the following problem:

min

vâˆˆ{âˆ’1,1}L

Ïƒm, Ë†m
i,Î·b

Â· (b Ë†m

i,pâˆ— âˆ’ b Ë†m

vT ï£®
ï£° XË†mâˆˆ{1,2}

+

Î»2
2

Â· kfm

i âˆ’ bm

i k.

i,nâˆ— )ï£¹
ï£»

(13)

(a) SHRECâ€™13

Based on the fact that vT v = L, the loss of (13) is equivalent

It is clear that the closed-form solution to (13) is

Ïƒm, Ë†m
i,Î·b

to vT " PË†mâˆˆ{1,2}
i = sgn( XË†mâˆˆ{1,2}

bm

Â· (b Ë†m

i,pâˆ— âˆ’ b Ë†m

i,nâˆ— ) âˆ’ Î»2 Â· fm

i # + const.

Ïƒm, Ë†m
i,Î·b

Â· (b Ë†m

i,nâˆ— âˆ’ b Ë†m

i,pâˆ— ) + Î»2 Â· fm

i ). (14)

2) W-Step. If we ï¬x the binary codes B1 and B2, the opti-
mization over W is formulated as

min
W

LW := LD + Î»1 Â· LC + Î»2 Â· LQ.

(15)

In practice, we adopt the Adam stochastic gradient descent
algorithm [21] to solve this problem.

4. Experimental Results and Analysis

4.1. Datasets

We evaluate the proposed method on three bench-
marks for sketch-based 3D shape retrieval, i.e., SHRECâ€™13,
SHRECâ€™14, and PART-SHRECâ€™14.

SHRECâ€™13 [25] collects human-drawn sketches and 3D
shapes from the Princeton Shape Benchmark (PSB) [43]
that consists of 7,200 sketches and 1,258 shapes from 90
classes. There are a total of 80 sketches per class, 50 of
which are selected for training and the rest for test. The
numbers of 3D shapes are different for distinct classes,
about 14 on average for each class.

SHRECâ€™14 [28] contains 13,680 sketches and 8,987 3D
shapes from 171 classes. There are 80 sketches, and around
53 3D shapes on average for each class. The sketches are
split into 8,550 training data and 5,130 test data.

PART-SHRECâ€™14 [37] is collected from SHRECâ€™14 to
overcome the shortcomings that all 3D shapes are used for
both training and testing. By using this dataset, we can eval-
uate the performance of current models on retrieving unseen
3D shapes. Concretely, it selects 48 classes that have more
than 50 shapes in SHRECâ€™14, which thereafter result in
7,238 3D shapes and 3,840 sketches. The sketches are split
into 50/30 training/test data, whilst 3D shapes are randomly
split into a training set of 5,812 and a test set of 1,426.

(b) SHRECâ€™14

Figure 3. Precision-Recall curves.

4.2. Implementation Details

For the convolutional layers F 1 and F 2, we adopt the
Inception-ResNet-v2 [46] pretrained on ImageNet as the
base network, by removing the last fully-connected layer.
Both the hash layers H1 and H2 consist of three fully-
connected layers as [1536, 1024, 512, L]. We utilize the
â€˜ReLUâ€™ activation functions for all layers, except that the
last layer uses the â€˜Tanhâ€™ activation function. The view at-
tention network A contains two fully-connected layers as
[2076, 300, 1], where the last layer uses the â€˜Sigmoidâ€™ acti-
vation function. The trade-off parameters Î»1 and Î»2 are se-
lected by cross-validation on the training set, and are set to 1
and 0.001, respectively, for all datasets. Regarding the num-
ber of spatial segments, we empirically set K = 4 consid-
ering both computational efï¬ciency and convergent speed2.

4.3. Evaluation Metrics

We utilize the following widely-adopted metrics [26, 12,
53] for sketch-based 3D shape retrieval: nearest neighbor
(NN), ï¬rst tier (FT), second tier (ST), E-measure (E), dis-
counted cumulated gain (DCG), and mean average preci-
sion (mAP). We also draw precision-recall curves for vi-
sually evaluating the retrieval performance.

2Sample code is available at https://sites.google.com/

site/chenjiaxinx/

796

Table 1. Performance comparisons with the state-of-the-art methods on SHRECâ€™13 and SHRECâ€™14.

SHRECâ€™13

SHRECâ€™14

Method
CDMR [15]
SBR-VC [25]
SP [44]
FDC [25]
DB-VLAT [49]
CAT-DTW [55]
Siamese [50]
KECNN [47]
DCML [12]
DCHML [10]
LWBR [53]
DCML (ResNet) [12]
LWBR (ResNet) [53]
Shape2Vec [48]
DCA [7]
Semantic [37]

DSSH-16 (ResNet)
DSSH-64 (ResNet)
DSSH-256 (ResNet)
DSSH-512 (ResNet)
DSSH-16
DSSH-64
DSSH-256
DSSH-512

NN
0.279
0.164
0.017
0.110

-

0.235
0.405
0.320
0.650
0.730
0.712
0.740
0.735

-

0.783
0.823

0.768
0.798
0.804
0.799
0.809
0.823
0.829
0.831

FT

0.203
0.097
0.016
0.069

-

0.135
0.403
0.319
0.634
0.715
0.725
0.752
0.745

-

0.796
0.828

0.777
0.808
0.817
0.814
0.813
0.835
0.842
0.844

ST

0.296
0.149
0.031
0.107

-

0.198
0.548
0.397
0.719
0.773
0.785
0.797
0.784

-

0.829
0.860

0.797
0.844
0.863
0.860
0.828
0.867
0.879
0.886

E

0.166
0.085
0.018
0.061

-

0.109
0.287
0.236
0.348
0.368
0.369
0.365
0.359

-

0.376
0.403

0.371
0.391
0.402
0.404
0.383
0.403
0.409
0.411

-

0.392
0.607
0.489
0.766
0.816
0.814
0.829
0.825

-

0.856
0.884

0.841
0.869
0.876
0.873
0.865
0.885
0.891
0.893

-

0.141
0.469

-

0.674
0.744
0.752
0.774
0.767

-

0.813
0.843

0.793
0.826
0.834
0.831
0.825
0.849
0.855
0.858

DCG mAP Query Time (sec.) Memory (KB)
0.458
0.348
0.240
0.307

0.250
0.114
0.026
0.086

-
-
-
-
-
-

-
-
-
-
-
-

NN
0.109
0.095

FT

0.057
0.050

ST

0.089
0.081

E

0.041
0.037

DCG mAP Query Time (sec.) Memory (KB)
0.328
0.319

0.054
0.050

-
-

0.160
0.137
0.239

-

0.272
0.403
0.403
0.578
0.621
0.714
0.770
0.804

0.735
0.758
0.771
0.775
0.767
0.788
0.796
0.796

-
-

0.115
0.068
0.212

-

0.275
0.329
0.378
0.591
0.641
0.697
0.789
0.749

0.726
0.758
0.777
0.788
0.762
0.799
0.811
0.813

-
-

0.170
0.102
0.316

-

0.345
0.394
0.455
0.647
0.691
0.748
0.823
0.813

0.771
0.792
0.822
0.831
0.794
0.839
0.851
0.851

-
-

0.079
0.050
0.140

-

0.171
0.201
0.236
0.723
0.760
0.360
0.398
0.395

0.377
0.385
0.400
0.404
0.385
0.407
0.411
0.412

-
-

0.376
0.338
0.496

-

0.498
0.544
0.581
0.351
0.361
0.811
0.859
0.870

0.830
0.841
0.862
0.870
0.844
0.875
0.881
0.881

-
-

0.131
0.060
0.228

-

0.286
0.336
0.401
0.615
0.665
0.720
0.803
0.780

0.742
0.769
0.792
0.806
0.773
0.815
0.826
0.826

-
-
-
-
-
-

-
-
-
-
-
-

1.50Ã—10âˆ’3

-

3.95Ã—10âˆ’1
3.95Ã—10âˆ’1
3.95Ã—10âˆ’1
1.96Ã—10âˆ’1
1.96Ã—10âˆ’1
3.01Ã—10âˆ’2
1.96Ã—10âˆ’1

-

9.21Ã—10âˆ’6
3.76Ã—10âˆ’5
1.45Ã—10âˆ’4
2.61Ã—10âˆ’4
9.21Ã—10âˆ’6
3.76Ã—10âˆ’5
1.45Ã—10âˆ’4
2.61Ã—10âˆ’4

2.2

-

1.4Ã—102
1.4Ã—102
1.4Ã—102
7.0Ã—101
7.0Ã—101
1.7Ã—101

4.4

-

1.7Ã—10âˆ’2
6.9Ã—10âˆ’2
2.7Ã—10âˆ’1
5.5Ã—10âˆ’1
1.7Ã—10âˆ’2
6.9Ã—10âˆ’2
2.7Ã—10âˆ’1
5.5Ã—10âˆ’1

3.13Ã—10âˆ’4

3.1Ã—10âˆ’1

-

4.67Ã—10âˆ’2
4.67Ã—10âˆ’2
4.67Ã—10âˆ’2
2.52Ã—10âˆ’2
2.52Ã—10âˆ’2

-

2.0Ã—101
2.0Ã—101
2.0Ã—101

9.8
9.8

-

2.52Ã—10âˆ’2

6.1Ã—10âˆ’1

-

1.84Ã—10âˆ’6
7.10Ã—10âˆ’6
2.83Ã—10âˆ’5
5.00Ã—10âˆ’5
1.84Ã—10âˆ’6
7.10Ã—10âˆ’6
2.83Ã—10âˆ’5
5.00Ã—10âˆ’5

-

2.4Ã—10âˆ’3
9.6Ã—10âˆ’3
3.8Ã—10âˆ’2
7.7Ã—10âˆ’2
2.4Ã—10âˆ’3
9.6Ã—10âˆ’3
3.8Ã—10âˆ’2
7.7Ã—10âˆ’2

(â€˜-â€™ indicates that the results are not reported, or the source codes as well as implementation details are not available.)

4.4. Comparisons with the State of the Art Meth 

ods for Sketch Based 3D Shape Retrieval

We compare our DSSH with the state-of-the-art meth-
ods for sketch-based 3D shape retrieval, including the hand-
crafted methods [15, 25, 44, 25, 49, 55] and deep learning
based ones [50, 47, 12, 10, 53, 12, 53, 48, 7, 37]. For fair
comparisons with deep learning based methods, we also re-
port our performance by using ResNet-50 as the base net-
work, denote by DSSH (ResNet). Since the bit length L
affects both the retrieval efï¬ciency and accuracy, we pro-
vide the results of DSSH using various bit lengths, denoted
by DSSH-L, where L=16, 64, 256, and 512.

The comparison results are summarized in Tables 1 and
2. Generally, the performance of deep learning based meth-
ods is superior to hand-crafted ones. Due to the quan-
tization loss, the accuracies of hashing methods are usu-
ally lower than non-hashing based ones. Despite this, our
DSSH with 512 bits achieves higher performance than the
best-performing non-hashing based models. Even with
extremely short bits (e.g., 16 bits), DSSH still performs
competitively to existing works. Note that DSSH with
ResNet-50 performs slightly worse than Inception-ResNet-
v2. However, DSSH (ResNet) outperforms the deep mod-
els based on the same backbone, such as DCML (ResNet),
LWBR (ResNet) and DCA. This is because: 1) DSSH de-
signs an effective deep shape model to learn 3D represen-
tations by efï¬ciently exploring its 2D projections. By seg-
mented stochastic sampling, S3N learns 3D features from a
set of 2D images with more view variations than the com-
pared projection-based methods, making the learned fea-
tures more discriminative; 2) the Batch-Hard Binary Coding
module mines the hardest samples, and learns semantics-
preserving binary codes for both sketches and 3D shapes,
which can signiï¬cantly reduce the binary quantization loss.
We also show the precision-recall curves of DSSH with

Table 2. Performance comparisons with the state-of-the-art meth-
ods on PART-SHRECâ€™14.

Methods

Siamese [50]
Semantic [37]

DSSH-16
DSSH-64
DSSH-256
DSSH-512

NN
0.118
0.840
0.810
0.821
0.835
0.838

FT

0.076
0.634
0.748
0.766
0.778
0.777

ST

0.132
0.745
0.796
0.830
0.846
0.848

E

0.073
0.526
0.594
0.615
0.619
0.624

DCG mAP
0.067
0.400
0.848
0.676
0.759
0.866
0.792
0.880
0.803
0.886
0.888
0.806

16 and 512 bits in Figs. 3(a) and 3(b). As illustrated, the
precision rates of DSSH-512 are higher than the compared
approaches when the recall rate is less than 0.9, by either
using ResNet-50 or Inception-ResNet-v2 as the backbone.
Efï¬ciency Analysis. As previously mentioned, by learn-
ing binary representations, our DSSH is much more efï¬-
cient than existing non-hashing based methods. To ver-
ify this, we report the average query time per sketch, by
computing the similarities between one sketch and all 3D
shapes (1,258 shapes on SHRECâ€™13 and 8,987 shapes on
SHRECâ€™14). Moreover, we compare the memory load for
storing all 3D shape data. All experiments are conducted
on a PC with Intel Core CPU (2.6GHz) and 16GB RAM.
As can be seen from Table 1, DSSH is remarkably more
efï¬cient than the compared methods. On both SHRECâ€™13
and SHRECâ€™14, DSSH is at least 100 times faster, whilst
requiring much less memory load.

4.5. Comparisons with Hashing Methods

We further compare DSSH with the state-of-the-art hash-
ing approaches, including 1) single view/modality hashing:
SDH [42], COSDISH [20], deep model based DHN [59]
and DCTQ [32], and 2) cross view/modality hashing: CVH
[23], SCM [57], SePH [29], and deep cross-modal hashing
DSMH [19]. For fair comparisons, we extract the 1,536-d
vectors after the convolutional layers F of our model as the
features for non-deep models. For deep models, they origi-

797

Table 3. mAPs of hashing methods with various bit lengths.

Sketch Query

Retrieved 3D Shapes

Cross-
View

Single-
View

Methods

DCMH [19]
SePH [30]
SCM [57]
CVH [23]

DCTQ [32]
DHN [59]
COSDISH [20]
SDH [42]

DSSH

64 bits
0.672
0.498
0.364
0.544

0.741
0.719
0.659
0.383

0.849

SHRECâ€™13

128 bits

256 bits

0.715
0.547
0.526
0.351

0.755
0.723
0.682
0.510

0.853

0.728
0.589
0.485
0.150

0.773
0.731
0.735
0.646

0.855

SHRECâ€™14

128 bits

256 bits

0.695
0.524
0.456
0.497

0.737
0.687
0.583
0.568

0.821

0.711
0.541
0.360
0.277

0.742
0.695
0.713
0.615

0.826

64 bits
0.658
0.476
0.292
0.346

0.713
0.669
0.401
0.479

0.815

d
n
a
h

h
c
n
e
b

-
i
l

e
h

r
e
t
p
o
c

e
s
r
o
h

-
r
o
t
o
m

e
k
i
b

Table 4. mAPs by using different view sampling strategies on
PART-SHRECâ€™14.
Methods

256 bits

512 bits

16 bits

64 bits

DSSH (horizontal

w/o alignment: Fig. 1 (c))

0.676

0.734

0.742

DSSH (horizontal: Fig. 1 (b))
DSSH (stochastic: Fig. 1 (d))

0.711
0.759

0.757
0.792

0.776
0.803

0.749

0.777
0.806

Table 5. Effect of the view attention network on PART-SHRECâ€™14.

Methods

DSSH (w/o A)
DSSH (with A)

NN
0.815
0.838

FT

0.771
0.777

ST

0.842
0.848

E

0.615
0.624

DCG mAP
0.884
0.799
0.806
0.888

nally use relatively simple base networks such as AlexNet.
We re-implement these methods and replace their base net-
works by Inception-ResNet-v2.

Table 3 shows the mAPs of various methods with differ-
ent bit lengths. Clearly, DSSH signiï¬cantly outperforms all
non-deep hashing approaches. DSSH also achieves higher
mAPs than other deep models, with at least 8% improve-
ments over the second best ones on both datasets.

4.6. Ablation Study

Effect of the stochastic sampling strategy. To evalu-
ate the effect of the proposed sampling strategy, we adopt
two other sampling strategies for comparison, i.e., C1: 12
rendering views are selected in the horizontal plane for the
original aligned shape data (Fig. 1 (b)); C2: Each 3D shape
is rotated by a random angle before selecting the 12 render-
ing views horizontally, which mimics the realistic scenario
where shapes lack alignment (Fig. 1 (c)). From Table 4,
we can see that the proposed stochastic sampling achieves
the best performance. We also observe that the performance
of DSSH with C1 signiï¬cantly drops when 3D shapes are
rotated randomly, i.e., without alignment.

Effect of the view attention network. As previously
described, the view attention network A is employed to ex-
plore the view-speciï¬c weights. Table 5 demonstrates the
results of DSSH with or without A. It is clear that the at-
tention network improves the performance of DSSH, espe-
cially w.r.t. the Nearest Neighbor (NN) matching accuracy.
Effect of the sampling times during test. As illustrated
in Fig. 2, we generate K (K = 4) rendered images for a
3D shape during one sampling, based on which we learn
one feature vector by DSSH. During test, we can sample t
times and use the averaged feature vector as the ï¬nal repre-
sentation. As illustrated in Fig. 5, DSSH can achieve bet-

Figure 4. Top-ranked 3D shapes of some sketch queries by using
DSSH (with 16 bits) on SHRECâ€™13.

0.86

P
A
m

0.84

0.82

0.8

1

SHREC'13

SHREC'14

PART-SHREC'14

2

3

4

5

6

7

8

9

10

Sampling times

Figure 5. mAPs of DSSH with different sampling times for test.

ter performance with more sampling times, since more ren-
dered images of a 3D shape provide more comprehensive
information. Meanwhile, DSSH can already achieve com-
petitive performance based on a single sampling (i.e., us-
ing 4 rendered images). In all our experiments, we set t to
3 for fair comparisons, considering most projection-based
approaches render a 3D shape to 12 2D images.

Qualitative results. We also visually depict some re-
trieval results by using DSSH on SHRECâ€™13 in Fig. 4. Fail-
ure cases are highlighted in the red rectangles.

5. Conclusion

In this paper, we have proposed a hashing based frame-
work, namely Deep Sketch-Shape Hashing (DSSH), for ef-
ï¬cient and accurate sketch-based 3D shape retrieval. A
novel shape network with Segmented Stochastic-viewing
was specially developed to learn discriminative represen-
tations of 3D shapes. Moreover, a Batch-Hard Binary Cod-
ing scheme was presented to learn semantics-preserving bi-
nary codes across modalities, which can diminish the bi-
nary quantization loss. Experimental results demonstrated
that DSSH remarkably improved the retrieval accuracies of
existing works, whilst signiï¬cantly reducing the time costs
and memory load for retrieval.

Acknowledgement

This work was
supported in part by the National
Natural Science Foundation of China under Project
61502081, Sichuan Science and Technology Program
(No.2019YFG0003, 2018GZDZX0032), and Open Fund
Project of Fujian Provincial Key Laboratory of Informa-
tion Processing and Intelligent Control (Minjiang Univer-
sity) (No. MJUKF201708).

798

References

[1] S. Bai, X. Bai, Q. Tian, and L. J. Latecki. Regularized dif-
fusion process on bidirectional context for object retrieval.
TPAMI, 2019.

[2] S. Bai, X. Bai, Z. Zhou, Z. Zhang, and L. Jan Latecki. Gift:
A real-time and scalable 3d shape search engine. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5023â€“5032, 2016.

[3] S. Bai, Z. Zhou, J. Wang, X. Bai, L. Jan Latecki, and Q. Tian.
Ensemble diffusion for retrieval. In Proceedings of the IEEE
International Conference on Computer Vision, pages 774â€“
783, 2017.

[4] X. Bai, S. Bai, Z. Zhu, and L. J. Latecki. 3d shape matching
via two layer coding. IEEE transactions on pattern analysis
and machine intelligence, 37(12):2361â€“2373, 2015.

[5] M. M. Bronstein, A. M. Bronstein, F. Michel, and N. Para-
gios. Data fusion through cross-modality metric learning us-
ing similarity-sensitive hashing. In CVPR, 2010.

[6] Y. Cao, M. Long, B. Liu, J. Wang, and M. KLiss. Deep
cauchy hashing for hamming space retrieval.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1229â€“1237, 2018.

[7] J. Chen and Y. Fang. Deep cross-modality adaptation via
semantics preserving adversarial learning for sketch-based
3d shape retrieval.
In European Conference on Computer
Vision, pages 624â€“640, 2018.

[8] J. Chen, Y. Wang, J. Qin, L. Liu, and L. Shao. Fast per-
son re-identiï¬cation via cross-camera semantic binary trans-
formation. In IEEE Conf. on Computer Vision and Pattern
Recognition, 2017.

[9] J. Chen, Y. Wang, and R. Wu. Person re-identiï¬cation by
distance metric learning to discrete hashing. In 2016 IEEE
International Conference on Image Processing (ICIP), pages
789â€“793. IEEE, 2016.

[10] G. Dai, J. Xie, and Y. Fang. Deep correlated holistic metric
learning for sketch-based 3d shape retrieval. IEEE Transac-
tions on Image Processing, 2018.

[11] G. Dai, J. Xie, and Y. Fang. Siamese cnn-bilstm architecture
for 3d shape representation learning. In IJCAI, pages 670â€“
676, 2018.

[12] G. Dai, J. Xie, F. Zhu, and Y. Fang. Deep correlated metric
learning for sketch-based 3d shape retrieval. In AAAI, pages
4002â€“4008, 2017.

[13] G. Ding, Y. Guo, and J. Zhou. Collective matrix factorization

hashing for multimodal data. In CVPR, 2014.

[14] M. Eitz, R. Richter, T. Boubekeur, K. Hildebrand, and
M. Alexa. Sketch-based shape retrieval. ACM Trans. Graph.,
31(4):31â€“1, 2012.

[15] T. Furuya and R. Ohbuchi. Ranking on cross-domain man-
ifold for sketch-based 3d model retrieval.
In Cyberworlds
(CW), 2013 International Conference on, pages 274â€“281.
IEEE, 2013.

[16] T. Furuya and R. Ohbuchi. Hashing cross-modal manifold
for scalable sketch-based 3d model retrieval. In 3D Vision
(3DV), 2014 2nd International Conference on, volume 1,
pages 543â€“550. IEEE, 2014.

[17] Y. Gong and S. Lazebnik.

Iterative quantization: A pro-
crustean approach to learning binary codes. In CVPR, 2011.
In defense of the
arXiv preprint

loss for person re-identiï¬cation.

[18] A. Hermans, L. Beyer, and B. Leibe.

triplet
arXiv:1703.07737, 2017.

[19] Q.-Y. Jiang and W.-J. Li. Deep cross-modal hashing. In Com-
puter Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on, pages 3270â€“3278. IEEE, 2017.

[20] W. C. Kang, W. J. Li, and Z. H. Zhou. Column sampling

based discrete supervised hashing. In AAAI, 2016.

[21] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In International Conference on Learning Rep-
resentations, 2015.

[22] B. Kulis and T. Darrell. Learning to hash with binary re-
constructive embeddings. In Proc. Neural Inf. Process. Syst.,
2009.

[23] S. Kumar and R. Udupa. Learning hash functions for cross-
view similarity search. In IJCAI proceedings-international
joint conference on artiï¬cial intelligence, volume 22, page
1360, 2011.

[24] H. Lai, Y. L. Y. Pan, and S. Yan. Simultaneous feature learn-
In Proc.

ing and hash coding with deep neural networks.
IEEE Conf. Comput. Vis. Pattern Recog., 2015.

[25] B. Li, Y. Lu, A. Godil, T. Schreck, M. Aono, H. Johan,
J. M. Saavedra, and S. Tashiro. SHREC13 track: large scale
sketch-based 3D shape retrieval. 2013.

[26] B. Li, Y. Lu, A. Godil, T. Schreck, B. Bustos, A. Ferreira,
T. Furuya, M. J. Fonseca, H. Johan, T. Matsuda, et al. A com-
parison of methods for sketch-based 3d shape retrieval. Com-
puter Vision and Image Understanding, 119:57â€“80, 2014.

[27] B. Li, Y. Lu, H. Johan, and R. Fares. Sketch-based 3d
model retrieval utilizing adaptive view clustering and se-
mantic information. Multimedia Tools and Applications,
76(24):26603â€“26631, 2017.

[28] B. Li, Y. Lu, C. Li, A. Godil, T. Schreck, M. Aono,
M. Burtscher, H. Fu, T. Furuya, H. Johan, et al. Shrec14
track: Extended large scale sketch-based 3d shape retrieval.
In Eurographics workshop on 3D object retrieval, volume
2014, 2014.

[29] G. Lin, C. Shen, Q. Shi, A. van den Hengel, and D. Suter.
Fast supervised hashing with decision trees for high-
dimensional data. In CVPR, 2014.

[30] Z. Lin, G. Ding, M. Hu, and J. Wang. Semantics-preserving

hashing for cross-view retrievalg. In CVPR, 2015.

[31] R. Litman, A. Bronstein, M. Bronstein, and U. Castellani.
Supervised learning of bag-of-features shape descriptors us-
ing sparse coding. In Computer Graphics Forum, volume 33,
pages 127â€“136. Wiley Online Library, 2014.

[32] B. Liu, Y. Cao, M. Long, J. Wang, and J. Wang. Deep triplet

quantization. MM, ACM, 2018.

[33] L. Liu, F. Shen, Y. Shen, X. Liu, and L. Shao. Deep sketch
In

hashing: Fast free-hand sketch-based image retrieval.
Proc. CVPR, pages 2862â€“2871, 2017.

[34] W. Liu, J. Wang, R. Ji, Y. G. Jiang, and S. F. Chang. Super-

vised hashing with kernels. In CVPR, 2012.

[35] M. Norouzi and D. M. Blei. Minimal loss hashing for com-

pact binary codes. In ICML, 2011.

799

[53] J. Xie, G. Dai, F. Zhu, and Y. Fang. Learning barycentric rep-
resentations of 3d shapes for sketch-based 3d shape retrieval.
In Computer Vision and Pattern Recognition (CVPR), 2017
IEEE Conference on, pages 3615â€“3623. IEEE, 2017.

[54] J. Xie, G. Dai, F. Zhu, E. K. Wong, and Y. Fang. Deepshape:
deep-learned shape descriptor for 3d shape retrieval. IEEE
transactions on pattern analysis and machine intelligence,
39(7):1335â€“1345, 2017.

[55] Z. Yasseen, A. Verroust-Blondet, and A. Nasri. View se-
lection for sketch-based 3d model retrieval using visual part
shape description. The Visual Computer, 33(5):565â€“583,
2017.

[56] G.-J. Yoon and S. M. Yoon. Sketch-based 3d object recog-
nition from locally optimized sparse features. Neurocomput-
ing, 267:556â€“563, 2017.

[57] D. Zhang and W. J. Li. Large-scale supervised multimodal
hashing with semantic correlation maximization. In AAAI,
2014.

[58] F. Zhu, J. Xie, and Y. Fang. Learning cross-domain neural
networks for sketch-based 3d shape retrieval. In AAAI, pages
3683â€“3689, 2016.

[59] H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing net-
work for efï¬cient similarity retrieval. In AAAI, pages 2415â€“
2421, 2016.

[36] B. T. Phong. Illumination for computer generated pictures.

Communications of the ACM, 18(6):311â€“317, 1975.

[37] A. Qi, Y. Song, and T. Xiang. Semantic embedding for
sketch-based 3d shape retrieval. In British Machine Vision
Conference, 2018.

[38] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiï¬cation and segmentation.
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE, 1(2):4, 2017.

[39] C. R. Qi, H. Su, M. NieÃŸner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view cnns for object classiï¬-
cation on 3d data. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 5648â€“5656,
2016.

[40] J. Qin, L. Liu, L. Shao, F. Shen, B. Ni, J. Chen, and Y. Wang.
Zero-shot action recognition with error-correcting output
codes. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2833â€“2842, 2017.

[41] J. M. Saavedra, B. Bustos, M. Scherer, and T. Schreck. Stela:
sketch-based 3d model retrieval using a structure-based local
approach. In Proceedings of the 1st ACM International Con-
ference on Multimedia Retrieval, page 26. ACM, 2011.

[42] F. Shen, C. Shen, W. Liu, and H. T. Tao. Supervised discrete

hashing. In CVPR, 2015.

[43] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser. The
princeton shape benchmark. In Shape modeling applications,
2004. Proceedings, pages 167â€“178. IEEE, 2004.

[44] P. Sousa and M. J. Fonseca. Sketch-based retrieval of draw-
ings using spatial proximity. Journal of Visual Languages &
Computing, 21(2):69â€“80, 2010.

[45] H. Su, S. Mashaji, E. Kalogerakis, and E. Learned-Miller.
Multi-view convolutional neural networks for 3d shape
recognition. In Proceedings of the IEEE international con-
ference on computer vision, pages 945â€“953, 2015.

[46] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, volume 4, page 12, 2017.
[47] H. Tabia and H. Laga. Learning shape retrieval from different

modalities. Neurocomputing, 253:24â€“33, 2017.

[48] F. P. Tasse and N. Dodgson. Shape2vec: semantic-based de-
scriptors for 3d shapes, sketches and images. ACM Transac-
tions on Graphics (TOG), 35(6):208, 2016.

[49] A. Tatsuma, H. Koyanagi, and M. Aono. A large-scale
shape benchmark for 3d object retrieval: Toyohashi shape
benchmark. In Signal & Information Processing Association
Annual Summit and Conference (APSIPA ASC), 2012 Asia-
Paciï¬c, pages 1â€“10. IEEE, 2012.

[50] F. Wang, L. Kang, and Y. Li. Sketch-based 3d shape retrieval
using convolutional neural networks.
In Computer Vision
and Pattern Recognition (CVPR), 2015 IEEE Conference on,
pages 1875â€“1883. IEEE, 2015.

[51] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiï¬cation. Journal of
Machine Learning Research, 10(Feb):207â€“244, 2009.

[52] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1912â€“1920, 2015.

800

