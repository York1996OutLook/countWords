DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion

Chen Wang2

Danfei Xu1

Yuke Zhu1

Roberto Mart¬¥ƒ±n-Mart¬¥ƒ±n1

Cewu Lu2

Li Fei-Fei1

Silvio Savarese1

1

Department of Computer Science, Stanford University

2

Department of Computer Science, Shanghai Jiao Tong University

Abstract

A key technical challenge in performing 6D object pose
estimation from RGB-D image is to fully leverage the two
complementary data sources. Prior works either extract in-
formation from the RGB image and depth separately or use
costly post-processing steps, limiting their performances in
highly cluttered scenes and real-time applications. In this
work, we present DenseFusion, a generic framework for
estimating 6D pose of a set of known objects from RGB-
D images. DenseFusion is a heterogeneous architecture
that processes the two data sources individually and uses a
novel dense fusion network to extract pixel-wise dense fea-
ture embedding, from which the pose is estimated. Further-
more, we integrate an end-to-end iterative pose reÔ¨Ånement
procedure that further improves the pose estimation while
achieving near real-time inference. Our experiments show
that our method outperforms state-of-the-art approaches in
two datasets, YCB-Video and LineMOD. We also deploy our
proposed method to a real robot to grasp and manipulate
objects based on the estimated pose. Our code and video
are available at https://sites.google.com/view/densefusion/.

1. Introduction

6D object pose estimation is the crux to many important
real-world applications, such as robotic grasping and ma-
nipulation [7, 35, 44], autonomous navigation [6, 11, 42],
and augmented reality [19, 20]. Ideally, a solution should
deal with objects of varying shape and texture, show robust-
ness towards heavy occlusion, sensor noise, and changing
lighting conditions, while achieving the speed requirement
of real-time tasks. The advent of cheap RGB-D sensors
has enabled methods that infer poses of low-textured ob-
jects even in poorly-lighted environments more accurately
than RGB-only methods. Nonetheless, it is difÔ¨Åcult for ex-
isting methods to satisfy the requirements of accurate pose
estimation and fast inference simultaneously.

Classical approaches Ô¨Årst extract features from RGB-D
data and perform correspondence grouping and hypothesis
veriÔ¨Åcation [3, 12, 13, 15, 26, 33, 38]. However, the re-

RGB-D

DenseFusion

Figure 1. We develop an end-to-end deep network model for 6D
pose estimation from RGB-D data, which performs fast and accu-
rate predictions for real-time applications such as robot grasping
and manipulation.

liance on handcrafted features and Ô¨Åxed matching proce-
dures have limited their empirical performances in presence
of heavy occlusion and lighting variation. Recent success
in visual recognition has inspired a family of data-driven
methods that use deep networks for pose estimation from
RGB-D inputs, such as PoseCNN [41] and MCN [16].

However, these methods require elaborate post-hoc re-
Ô¨Ånement steps to fully utilize the 3D information, such
as a highly customized Iterative Closest Point (ICP) [2]
procedure in PoseCNN and a multi-view hypothesis ver-
iÔ¨Åcation scheme in MCN. These reÔ¨Ånement steps cannot
be optimized jointly with the Ô¨Ånal objective and are pro-
hibitively slow for real-time applications. In the context of
autonomous driving, a third family of solutions has been
proposed to better exploit the complementary nature of
color and depth information from RGB-D data with end-
to-end deep models, such as Frustrum PointNet [23] and
PointFusion [42]. These models have achieved good per-
formances in driving scenes and the capacity of real-time
inference. However, as we demonstrate empirically, these
methods fall short under heavy occlusion, which is common

3343

in manipulation domains.

poses in 3D in an end-to-end architecture.

In this work, we propose an end-to-end deep learning ap-
proach for estimating 6-DoF poses of known objects from
RGB-D inputs. The core of our approach is to embed and
fuse RGB values and point clouds at a per-pixel level, as
opposed to prior work which uses image crops to compute
global features [42] or 2D bounding boxes [23]. This per-
pixel fusion scheme enables our model to explicitly rea-
son about the local appearance and geometry information,
which is essential to handle heavy occlusion. Furthermore,
we propose an iterative method which performs pose re-
Ô¨Ånement within the end-to-end learning framework. This
greatly enhances model performance while keeping infer-
ence speed real-time.

We evaluate our method in two popular benchmarks for
6D pose estimation, YCB-Video [41] and LineMOD [12].
We show that our method outperforms the state-of-the-art
PoseCNN after ICP reÔ¨Ånement [41] by 3.5% in pose ac-
curacy while being 200x faster in inference time. In par-
ticular, we demonstrate its robustness in highly cluttered
scenes thanks to our novel dense fusion method. Last, we
also showcase its utility in a real robot task, where the robot
estimates the poses of objects and grasp them to clear up a
table.

In summary, the contributions of this work are two-fold:
First, we present a principled way to combine color and
depth information from the RGB-D input. We augment
the information of each 3D point with 2D information from
an embedding space learned for the task and use this new
color-depth space to estimate the 6D pose. Second, we in-
tegrate an iterative reÔ¨Ånement procedure within the neural
network architecture, removing the dependency of previous
methods of a post-processing ICP step.

2. Related Work

Pose from RGB images. Classical methods rely on detect-
ing and matching keypoints with known object models [1, 7,
9, 27, 44]. Newer methods address the challenge by learn-
ing to predict the 2D keypoints [3, 22, 32, 34, 35] and solve
the poses by PnP [10]. Though prevail in speed-demanding
tasks, these methods become unreliable given low-texture
or low-resolution inputs. Other methods propose to directly
estimate objects pose from images using CNN-based archi-
tectures [28, 36]. Many such methods focus on orientation
estimation: Xiang et al. [39, 40] learns a viewpoint-aware
pose estimator by clustering 3D features from object mod-
els. Mousavian et al. [21] predicts 3D object parameters and
recovers poses by single-view geometry constraints. Sun-
dermeyer et al. [31] implicitly encode orientation in a latent
space and in test time Ô¨Ånd the best match in a codebook as
the orientation prediction. However, pose estimation in 3D
remains a challenge for the lack of depth information. Our
method leverages both image and 3D data to estimate object

Pose from depth / point cloud. Recent studies have pro-
posed to directly tackle the 3D object detection problem in
discretized 3D voxel spaces. For example, Song et al. [29,
30] generate 3D bounding box proposals and estimate the
poses by featuring the voxelized input with 3D ConvNets.
Although the voxel representation effectively encodes ge-
ometric information, these methods are often prohibitively
expensive: [30] takes nearly 20 seconds for each frame.

More recent 3D deep learning architectures have en-
abled methods that directly performs 6D pose estimation
on 3D point cloud data. As an example, both Frustrum
PointNets [23] and VoxelNet [43] use a PointNet-like [24]
structure and achieved state-of-the-art performances on the
KITTI benchmark [11]. Our method also makes use of sim-
ilar architecture. However, unlike urban driving applica-
tions for which point cloud alone provides enough informa-
tion, generic object pose estimation tasks such as the YCB-
Video dataset [41] demands reasoning over both geometric
and appearance information. We address such a challenge
by proposing a novel 2D-3D sensor fusion architecture.

Pose from RGB-D data. Classical approaches extract 3D
features from the input RGB-D data and perform corre-
spondence grouping and hypothesis veriÔ¨Åcation [3, 12, 13,
15, 26, 33, 38]. However, these features are either hard-
coded [12, 13, 26] or learned by optimizing surrogate ob-
jectives [3, 33, 38] such as reconstruction [15] instead of
the true objective of 6D pose estimation. Newer methods
such as PoseCNN [41] directly estimates 6D poses from im-
age data. Li et al. [16] further fuses the depth input as an
additional channel to a CNN-based architecture. However,
these approaches rely on expensive post-processing steps to
make full use of 3D input. In comparison, our method fuses
3D data to 2D appearance feature while retaining the geo-
metric structure of the input space, and we show that it out-
performs [41] on the YCB-Video dataset [41] without the
post-processing step.

Our method is most related to PointFusion [42], in which
geometric and appearance information are fused in a het-
erogeneous architecture. We show that our novel local fea-
ture fusion scheme signiÔ¨Åcantly outperforms PointFusion‚Äôs
naive fusion-by-concatenation method. In addition, we use
a novel iterative reÔ¨Ånement method to further improve the
pose estimation.

3. Model

Our goal is to estimate the 6D pose of a set of known
objects present in an RGB-D image of a cluttered scene.
Without loss of generality, we represent 6D poses as a ho-
mogeneous transformation matrix, p ‚àà SE(3).
In other
words, a 6D pose is composed by a rotation R ‚àà SO(3)
and a translation t ‚àà R3, p = [R|t]. Since we estimate the

3344

object

segmentation

6D pose estimation

CNN

PointNet

masked 
point cloud

image
crop

color

embeddings

pixel-wise dense fusion

(x1,y1)

(xN,yN)

matching

point

...

geometry
embeddings

per-pixel 
feature

MLP

prediction per pixel

pixel (xi,yi)  i = 1...N

rotation

translation
confidence

Ri
 ti
ci

argmax(c)

pixel-wise feature

pose

predictor

...

(x1,y1)
(x2,y2)

(xN,yN)

average
pooling

global
feature

Figure 2. Overview of our 6D pose estimation model. Our model generates object segmentation masks and bounding boxes from RGB
images. The RGB colors and point cloud from the depth map are encoded into embeddings and fused at each corresponding pixel. The
pose predictor produces a pose estimate for each pixel and the predictions are voted to generate the Ô¨Ånal 6D pose prediction of the object.
(The iterative procedure of our approach is not depicted here for simplicity)

6D pose of the objects from camera images, the poses are
deÔ¨Åned with respect to the camera coordinate frame.

Estimating the pose of a known object in adversarial
conditions (e.g. heavy occlusion, poor lighting, . . . )
is
only possible by combining the information contained in
the color and depth image channels. However, the two data
sources reside in different spaces. Extracting features from
heterogeneous data sources and fusing them appropriately
is the key technical challenge in this domain.

We address this challenge with (1) a heterogeneous ar-
chitecture that processes color and depth information dif-
ferently, retaining the native structure of each data source
(Sec. 3.3), and (2) a dense pixel-wise fusion network that
performs color-depth fusion by exploiting the intrinsic map-
ping between the data sources (Sec. 3.4). Finally, the pose
estimation is further reÔ¨Åned with a differentiable iterative
reÔ¨Ånement module (Sec. 3.6). In contrast to the expensive
post-hoc reÔ¨Ånement steps used in [16, 41], our reÔ¨Ånement
module can be trained jointly with the main architecture and
only takes a small fraction of the total inference time.

3.1. Architecture Overview

Fig. 2 illustrates the overall proposed architecture. The
architecture contains two main stages. The Ô¨Årst stage takes
color image as input and performs semantic segmentation
for each known object category. Then, for each segmented
object, we feed the masked depth pixels (converted to 3D
point cloud) as well as an image patch cropped by the
bounding box of the mask to the second stage.

The second stage processes the results of the segmenta-
tion and estimates the object‚Äôs 6D pose. It comprises four
components: a) a fully convolutional network that processes

the color information and maps each pixel in the image crop
to a color feature embedding, b) a PointNet-based [24] net-
work that processes each point in the masked 3D point cloud
to a geometric feature embedding, c) a pixel-wise fusion
network that combines both embeddings and outputs the es-
timation of the 6D pose of the object based on an unsuper-
vised conÔ¨Ådence scoring, and d) an iterative self-reÔ¨Ånement
methodology to train the network in a curriculum learning
manner and reÔ¨Åne the estimation result iteratively. Fig. 2
depicts a), b) and c) and Fig. 3 illustrates d). The details our
architecture are described below.

3.2. Semantic Segmentation

The Ô¨Årst step is to segment the objects of interest in the
image. Our semantic segmentation network is an encoder-
decoder architecture that takes an image as input and gener-
ates an N +1-channelled semantic segmentation map. Each
channel is a binary mask where active pixels depict objects
of each of the N possible known classes. The focus of this
work is to develop a pose estimation algorithm. Thus we
use an existing segmentation architecture proposed by [41].

3.3. Dense Feature Extraction

The key technical challenge in this domain is the correct
extraction of information from the color and depth channels
and their synergistic fusion. Even though color and depth
present a similar format in the RGB-D frame, their infor-
mation resides in different spaces. Therefore, we process
them separately to generate color and geometric features
from embedding spaces that retain the intrinsic structure of
the data sources.
Dense 3D point cloud feature embedding: Previous ap-

3345

proaches have used CNN to process the depth image as an
additional image channel [16]. However, such method ne-
glects the intrinsic 3D structure of the depth channel. In-
stead, we Ô¨Årst convert the segmented depth pixels into a 3D
point cloud using the known camera intrinsics, and then use
a PointNet-like architecture to extract geometric features.

PointNet by Qi et al. [24] pioneered the use of a symmet-
ric function (max-pooling) to achieve permutation invari-
ance in processing unordered point sets. The original archi-
tecture takes as input a raw point cloud and learns to encode
the information about the vicinity of each point and of the
point cloud as a whole. The features are shown to be effec-
tive in shape classiÔ¨Åcation and segmentation [24] and pose
estimation [23, 42]. We propose a geometric embedding
network that generates a dense per-point feature by map-
ping each of the P segmented points to a dgeo-dimensional
feature space. We implement a variant of PointNet architec-
ture that uses average-pooling as opposed to the commonly
used max-pooling as the symmetric reduction function.
Dense color image feature embedding: The goal of the
color embedding network is to extract per-pixel features
such that we can form dense correspondences between 3D
point features and image features. The reason for form-
ing these dense correspondences will be clear in the next
section. The image embedding network is a CNN-based
encoder-decoder architecture that maps an image of size
H √ó W √ó 3 into a H √ó W √ó drgb embedding space. Each
pixel of the embedding is a drgb-dimensional vector repre-
senting the appearance information of the input image at the
corresponding location.

3.4. Pixel wise Dense Fusion

So far we have obtained dense features from both the
image and the 3D point cloud inputs; now we need to fuse
the information. A naive approach would be to generate
a global feature from the dense color and depth features
from the segmented area. However, due to heavy occlu-
sion and segmentation errors, the set of features from the
previous step may contain features of points/pixels on other
objects or parts of the background. Therefore, blindly fus-
ing color and geometric features globally would degrade the
performance of the estimation. In the following we describe
a novel pixel-wise1 dense fusion network that effectively
combines the extracted features, especially for pose estima-
tion under heavy occlusion and imperfect segmentation.
Pixel-wise dense fusion: The key idea of our dense fu-
sion network is to perform local per-pixel fusion instead
of global fusion so that we can make predictions based on
each fused feature. In this way, we can potentially select
the predictions based on the visible part of the object and
minimize the effects of occlusion and segmentation noise.

1Since the mapping between pixels and 3D points is unique, we will

use interchangeably pixel-fusion and point-fusion.

Concretely, our dense fusion procedure Ô¨Årst associates the
geometric feature of each point to its corresponding image
feature pixel based on a projection onto the image plane us-
ing the known camera intrinsic parameters. The obtained
pairs of features are then concatenated and fed to another
network to generate a Ô¨Åxed-size global feature vector using
a symmetric reduction function. While we refrained from
using a single global feature for the estimation, here we en-
rich each dense pixel-feature with the global densely-fused
feature to provide a global context.

We feed each of the resulting per-pixel features into a
Ô¨Ånal network that predicts the object‚Äôs 6D pose. In other
words, we will train this network to predict one pose from
each densely-fused feature. The result is a set of P pre-
dicted poses, one per feature. This deÔ¨Ånes our Ô¨Årst learning
objective, as we will see in Sec. 3.5. We will now explain
our approach to learning to choose the best prediction in a
self-supervised manner, inspired by Xu et al. [42].

Per-pixel self-supervised conÔ¨Ådence: We would like to
train our pose estimation network to decide which pose es-
timation is likely to be the best hypothesis based on the spe-
ciÔ¨Åc context. To do so, we modify the network to output
a conÔ¨Ådence score ci for each prediction in addition to the
pose estimation predictions. We will have to reÔ¨Çect this sec-
ond learning objective in the overall learning objective, as
we will see at the end of the next section.

3.5. 6D Object Pose Estimation

Having deÔ¨Åned the overall network structure, we now
take a closer look at the learning objective. We deÔ¨Åne the
pose estimation loss as the distance between the points sam-
pled on the objects model in ground truth pose and cor-
responding points on the same model transformed by the
predicted pose. SpeciÔ¨Åcally, the loss to minimize for the
prediction per dense-pixel is deÔ¨Åned as

Lp

i =

1
M X

j

||(Rxj + t) ‚àí ( ÀÜRixj + ÀÜti)||

(1)

where xj denotes the jth point of the M randomly selected
3D points from the object‚Äôs 3D model, p = [R|t] is the
ground truth pose, and ÀÜpi = [ ÀÜRi|ÀÜti] is the predicted pose
generated from the fused embedding of the ith dense-pixel.

The above loss function is only well-deÔ¨Åned for asym-
metric objects, where the object shape and/or texture deter-
mines a unique canonical frame. Symmetric objects have
more than one and possibly an inÔ¨Ånite number of canoni-
cal frames, which leads to ambiguous learning objectives.
Therefore, for symmetric objects, we instead minimize the
distance between each point on the estimated model orien-
tation and the closest point on the ground truth model. The

3346

color

embeddings

global 
feature

Dense
Fusion

pose

residual
estimator

rotation residual ùö´R
translation residual
ùö´t

PointNet

next iteration

geometry
embeddings

transformed
point cloud

current input
point cloud

Figure 3. Iterative Pose ReÔ¨Ånement. We introduce an network
module that reÔ¨Ånes the pose estimation in an iterative procedure.

loss function becomes:

Lp

i =

1
M X

j

min

0<k<M

||(Rxj + t) ‚àí ( ÀÜRixk + ÀÜti)||

(2)

N Pi Lp

Optimizing over all predicted per dense-pixel poses
would be to minimize the mean of the per dense-pixel
losses: L = 1
i . However, as explained before,
we would like our network to learn to balance the conÔ¨Å-
dence among the per dense-pixel predictions. To do that we
weight the per dense-pixel loss with the dense-pixel conÔ¨Å-
dence, and add a second conÔ¨Ådence regularization term:

L =

1
N X

i

(Lp

i ci ‚àí w log(ci)),

(3)

where N is the number of randomly sampled dense-pixel
features from the P elements of the segment and w is a bal-
ancing hyperparameter. Intuitively, low conÔ¨Ådence will re-
sult in low pose estimation loss but would incur high penalty
from the second term, and vice versa. We use the pose esti-
mation that has the highest conÔ¨Ådence as the Ô¨Ånal output.

3.6. Iterative ReÔ¨Ånement

The iterative closest point algorithm (ICP) [2] is a pow-
erful reÔ¨Ånement approach used by many 6D pose estima-
tion methods [14, 31, 41]. However, the best-performing
ICP implementations are often not efÔ¨Åcient enough for real-
time applications. Other reÔ¨Ånement methods including[17,
18] assume the access to a real-time rendering engine and
object models with high-quality textures. Here we propose
a neural network-based iterative reÔ¨Ånement module based
on our densely-fused embedding that can improve the Ô¨Ånal
pose estimation result in a fast and robust manner without
additional rendering techniques.

The goal is to enable the network to correct its own pose
estimation error in an iterative manner. The challenge here
is training the network to reÔ¨Åne the previous prediction as
opposed to making new predictions. To do so, we must in-
clude the prediction made in the previous iteration as part of
the input to the next iteration. Our key idea is to consider the

previously predicted pose as an estimate of canonical frame
of the target object and transform the input point cloud into
this estimated canonical frame. This way, the transformed
point cloud implicitly encodes the estimated pose. We then
feed the transformed point cloud back into the network and
predict a residual pose based on the previously estimated
pose. This procedure can be applied iteratively and gener-
ate potentially Ô¨Åner pose estimation each iteration.

The procedure is illustrated in Fig. 3. Concretely, we
train a dedicated pose residual estimator network to per-
form the reÔ¨Ånement given the initial pose estimation from
the main network. At each iteration, we reuse the image fea-
ture embedding from the main network and perform dense
fusion with the geometric features computed for the new
transformed point cloud. The pose residual estimator uses
as input a global feature from the set of fused pixel features.
After K iterations, we obtain the Ô¨Ånal pose estimation as
the concatenation of the per-iteration estimations:

ÀÜp = [RK|tK] ¬∑ [RK‚àí1|tK‚àí1] ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ [R0|t0]

(4)

In our experiments, only after K = 2 iterations, the re-
Ô¨Åned pose is already able to surpass the ICP-reÔ¨Åned results
of prior works. The pose residual estimator can be trained
jointly with the main network. However, the pose estima-
tion at the beginning of the training is too noisy for it to
learn anything meaningful. Thus in practice, the joint train-
ing starts after the main network has converged.

4. Experiments

In the experimental section, we would like to answer the
following questions: (1) How does the dense fusion net-
work compare to naive global fusion-by-concatenation? (2)
Is the dense fusion and prediction scheme robust to heavy
occlusion and segmentation errors? (3) Does the iterative
reÔ¨Ånement module improve the Ô¨Ånal pose estimation? (4)
Is our method robust and efÔ¨Åcient enough for downstream
tasks such as robotic grasping?

To answer the Ô¨Årst three questions, we evaluate our
method on two challenging 6D object pose estimation
datasets: YCB-Video Dataset [41] and LineMOD [12]. The
YCB-Video Dataset features objects of varying shapes and
texture levels under different occlusion conditions. Hence
it‚Äôs an ideal testbed for our occlusion-resilient multi-modal
fusion method. The LineMOD dataset is a widely-used
dataset that allows us to compare with a broader range of ex-
isting methods. We compare our method with state-of-the-
art methods [14, 31, 42] as well as model variants. To an-
swer the last question, we deploy our model to a real robot
platform and evaluate the performance of a robot grasping
task that uses the predictions from our model.
4.1. Datasets

YCB-Video Dataset. The YCB-Video Dataset Xiang et
al. [41] features 21 YCB objects Calli et al. [5] of varying

3347

Figure 4. Qualitative results on the YCB-Video Dataset. All three methods shown here are tested with the same segmentation masks as
in PoseCNN. Each object point cloud in a different color is transformed with the predicted pose and then projected to the 2D image frame.
The Ô¨Årst two rows are former RGB-D methods and the last row is our approach with dense fusion and iterative reÔ¨Ånement (2 iterations).

shape and texture. The dataset contains 92 RGB-D videos,
where each video shows a subset of the 21 objects in differ-
ent indoor scenes. The videos are annotated with 6D poses
and segmentation masks. We follow prior work [41] and
split the dataset into 80 videos for training and 2,949 key
frames chosen from the rest 12 videos for testing and in-
clude the same 80,000 synthetic images released by [41]
in our training set. In our experiments, we compare with
the result of [41] after depth reÔ¨Ånement(ICP) and learning-
based depth method [42].
LineMOD Dataset. The LineMOD dataset Hinterstoisser
et al. [12] consists of 13 low-textured objects in 13 videos.
It is widely adopted by both classical methods [4, 8, 37]
and recent learning-based approaches [17, 31, 34]. We use
the same training and testing set as prior learning-based
works [17, 25, 34] without additional synthetic data and
compare with the best ICP-reÔ¨Åned results of the state-of-
the-art algorithms.

4.2. Metrics

We use two metrics to report on the YCB-Video Dataset.
The average closest point distance (ADD-S) [41] is an
ambiguity-invariant pose error metric which takes care of
both symmetric and non-symmetric objects into an over-
all evaluation. Given the estimated pose [ ÀÜR|ÀÜt] and ground
truth pose [R|t], ADD-S calculates the mean distance from
each 3D model point transformed by [ ÀÜR|ÀÜt] to its closest
neighbour on the target model transformed by [R|t]. We
report the area under the ADD-S curve (AUC) following
PoseCNN [41]. We follow prior work and set the maximum
threshold of AUC to be 0.1m. We also report the percent-
age of ADD-S smaller than 2cm (<2cm), which measures
the predictions under the minimum tolerance for robot ma-

nipulation (2cm for most of the robot grippers).

For the LineMOD dataset, we use the Average Distance
of Model Points (ADD) [13] for non-symmetric objects and
ADD-S for the two symmetric objects (eggbox and glue)
following prior works [13, 31, 34].

4.3. Implementation Details

The image embedding network consists of a Resnet-
18 encoder followed by 4 up-sampling layers as the de-
coder. The PointNet architecture is an MLP followed by
an average-pooling reduction function. Both color and geo-
metric dense feature embedding are of dimension 128. We
choose w = 0.01 for Eq. 3 by empirical evaluation. The
iterative pose reÔ¨Ånement module consists of a 4 fully con-
nected layers that directly output the pose residual from the
global dense feature. We use the 2 reÔ¨Ånement iterations for
all experiments.

4.4. Architectures

We compare four model variants that showcase the ef-

fectiveness of our design choices.
‚Ä¢ PointFusion [42] uses a CNN to extract a Ô¨Åxed-size fea-
ture vector and fuse by directly concatenating the image fea-
ture with the geometry feature. The rest of the network is
similar to our architecture. The comparison to this baseline
demonstrates the effectiveness of our dense fusion network.
‚Ä¢ Ours (single) uses our dense fusion network, but instead
of performing per-point prediction, it only outputs a single
prediction using the global feature vector.
‚Ä¢ Ours (per-pixel) performs per-pixel prediction based on
each densely fused feature.
‚Ä¢ Ours (iterative) is our complete model that uses the iter-
ative reÔ¨Ånement (Sec. 3.6) on top of Ours (per-pixel).

3348

Table 1. Quantitative evaluation of 6D pose (ADD-S[41]) on YCB-Video Dataset. Objects with bold name are symmetric.

002 master chef can
003 cracker box
004 sugar box
005 tomato soup can
006 mustard bottle
007 tuna Ô¨Åsh can
008 pudding box
009 gelatin box
010 potted meat can
011 banana
019 pitcher base
021 bleach cleanser
024 bowl
025 mug
035 power drill
036 wood block
037 scissors
040 large marker
051 large clamp
052 extra large clamp
061 foam brick
MEAN

PointFusion [42] PoseCNN+ICP [41] Ours (single)
AUC
90.9
80.5
90.4
91.9
88.5
93.8
87.5
95.0
86.4
84.7
85.5
81.0
75.7
94.2
71.5
68.1
76.7
87.9
65.9
60.4
91.8
83.9

AUC <2cm AUC
95.2
93.9
92.5
90.8
95.1
94.4
92.9
93.7
95.9
91.2
94.9
94.9
94.7
88.3
95.8
95.4
87.3
90.1
91.5
84.6
94.6
86.9
94.3
91.6
86.6
83.4
90.3
95.5
92.4
83.1
85.5
81.7
96.4
83.6
94.7
91.2
71.6
70.5
66.4
69.0
92.4
92.1
88.2
91.2

<2cm
100.0
91.6
100.0
96.9
100.0
100.0
100.0
100.0
93.6
99.7
100.0
99.4
54.9
99.8
99.6
80.2
95.6
99.7
74.9
48.8
100.0
93.2

<2cm
99.8
62.6
95.4
96.9
84.0
99.8
96.7
100.0
88.5
70.5
79.8
65.0
24.1
99.8
22.8
18.2
35.9
80.4
50.0
20.1
100.0
74.1

Ours (per-pixel)
<2cm
100.0
99.3
100.0
96.9
100.0
100.0
100.0
100.0
93.1
93.9
100.0
99.8
69.5
100.0
97.1
93.4
100.0
99.2
78.5
69.5
100.0
95.3

100.0
98.4
99.2
96.7
97.8
100.0
97.2
100.0
91.4
62.0
80.9
98.2
55.4
94.7
64.2
76.0
75.1
88.6
77.1
50.2
100.0
87.9

AUC
95.8
92.7
98.2
94.5
98.6
97.1
97.9
98.8
92.7
97.1
97.8
96.9
81.0
95.0
98.2
87.6
91.7
97.2
75.2
64.4
97.2
93.0

Ours (iterative)
AUC <2cm
96.4
100.0
99.5
95.5
100.0
97.5
94.6
96.9
100.0
97.2
100.0
96.6
100.0
96.5
100.0
98.1
91.3
93.1
100.0
96.6
100.0
97.1
100.0
95.8
98.8
88.2
97.1
100.0
98.7
96.0
94.6
89.7
100.0
95.2
100.0
97.5
79.2
72.9
69.8
76.3
100.0
92.5
93.1
96.8

frame on YCB-
Table 2. Runtime breakdown (second per
Video Dataset). Our method is approximately 200x faster than
PoseCNN+ICP. Seg means Segmentation, and PE means Pose Es-
timation.

PoseCNN+ICP [41]

Ours

Seg
0.03

PE
0.17

ICP
10.4

ALL
10.6

Seg
0.03

PE
0.02

ReÔ¨Åne
0.01

ALL
0.06

Figure 5. Model performance under increasing levels of occlu-
sion. Here the levels of occlusion is estimated by calculating the
invisible surface percentage of each object in the image frame. Our
methods work more robustly under heavy occlusion compared to
baseline methods.

4.5. Evaluation on YCB Video Dataset

Table 1 shows the evaluation results for all

the 21
objects in the YCB-Video Dataset. We report
the
ADD-S AUC(<0.1m) and the ADD-S<2cm metrics on
PoseCNN [41] and our four model variants. To ensure a fair
comparison, all methods use the same segmentation masks
as in PoseCNN [41]. Among our model variants, Ours
(Iterative) achieves the best performance. Our method is
able to outperform PoseCNN + ICP[41] even without itera-

tive reÔ¨Ånement. In particular, Ours (Iterative) outperforms
PoseCNN + ICP by 3.5% on the ADD-S<2cm metric.
Effect of dense fusion Both of our dense fusion baselines
(Ours (single) and Ours (per-pixel)) outperform PointFu-
sion by a large margin, which shows that dense fusion has
a clear advantage over the global fusion-by-concatenation
method used in PointFusion.
Effect of iterative reÔ¨Ånement Table 1 shows that our iter-
ative reÔ¨Ånement improves the overall pose estimation per-
formance. In particular, it signiÔ¨Åcantly improves the per-
formances for texture-less symmetric object, e.g., bowl
(29%), banana (6%), and extra large clamp (6%)
which suffer from orientation ambiguity.
Robustness towards occlusion The main advantage of our
dense fusion method is its robustness towards occlusions.
To quantify the effect of occlusion on the Ô¨Ånal performance,
we calculate the visible surface ratio of each object instance.
Then we calculate how the accuracy (ADD-S<2cm percent-
age) changes with the extent of occlusion. As shown in
Fig. 5, the performances of PointFusion and PoseCNN+ICP
degrade signiÔ¨Åcantly as the occlusion increases. In contrast,
none of our methods experiences notable performance drop.
In particular, the performance of both Ours (per-pixel) and
Ours (iterative) only decrease by 2% overall.
Time efÔ¨Åciency We compare the time efÔ¨Åciency of our
model with PoseCNN+ICP in Table 2. We can see
that our method is two order of magnitude faster than
PoseCNN+ICP. In particular, PoseCNN+ICP spends most
of the time on the post processing ICP. In contrast, all of
our computation component, namely segmentation (Seg),
pose estimation (PE), and iterative reÔ¨Ånement (ReÔ¨Åne), are
equally efÔ¨Åcient, and the overall runtime is fast enough

3349

Table 3. Quantitative evaluation of 6D pose (ADD[13]) on the LineMOD dataset. Objects with bold name are symmetric.

RGB

RGB-D

BB8 w ref. [25]
DeepIM [17, 41]
Imp. [31]+ICP
SSD6D [14]+ICP
PointFusion [42]
Ours (per-pixel)
Ours (iterative)

ape
40
77
21
65
70
80
92

ben.
92
98
64
80
81
84
93

cam can
64
56
97
94
76
63
86
78
61
61
87
77
94
93

cat
63
82
72
70
79
89
97

drill.

duck

74
95
42
73
47
78
87

44
78
32
66
63
76
92

egg.
58
97
99
100
100
100
100

glue
41
99
96
100
99
99
100

hole.

67
53
50
49
72
79
92

iron
84
98
63
78
83
92
97

lamp

phone MEAN

77
98
92
73
62
92
95

54
88
71
79
79
88
93

63
89
65
79
74
86
94

input crop

initial

iteration 1

iteration 2

iteration 3

ADD (m):

0.029

0.022

0.018

0.018

ADD (m):

0.015

0.010

0.008

0.007

Figure 6. Iterative reÔ¨Ånement performance on LineMOD
dataset We visualize how our iterative reÔ¨Ånement procedure cor-
rects initially sub-optimal pose estimation.

for real-time application (16 FPS, about 5 objects in each
frame).
Qualitative evaluation Fig. 4 visualizes some sample pre-
dictions made by PoseCNN+ICP, PointFusion, and our iter-
ative reÔ¨Ånement model. As we can see, PoseCNN+ICP and
PointFusion fail to estimate the correct pose of the bowl in
the leftmost column and the cracker box in the middle col-
umn due to heavy occlusion, whereas our method remains
robust. Another challenging case is the clamp in the middle
row due to poor segmentation (not shown in the Ô¨Ågure). Our
approach localizes the clamp from only the visible part of
the object and effectively reduces the dependency on accu-
rate segmentation result.

4.6. Evaluation on LineMOD Dataset

Table 3 compares our method with previous RGB meth-
ods with depth reÔ¨Ånement(ICP) (results from [31, 34]) on
the ADD metric [13]. Even without the iterative reÔ¨Ånement
step, our method can outperform 7% over the state-of-the-
art depth reÔ¨Ånement method. After processing the itera-
tive reÔ¨Ånement approach, the Ô¨Ånal result has another 8%
improvement, which proves that our learning-based depth
method is superior to the sophisticated application of ICP
in both accuracy and efÔ¨Åciency. We visualize the estimated
6D pose after each reÔ¨Ånement iteration in Fig. 6, where our
pose estimation improves by an average of 0.8 cm (ADD)
after 2 reÔ¨Ånement iterations. The results of some other
color-only methods are also listed in Table 3 for reference.

4.7. Robotic Grasping Experiment

In our last experiment, we evaluate whether the poses
estimated by our approach are accurate enough to enable

robot grasping and manipulation. As shown in Fig. 1, we
place 5 YCB objects on a table and command the robot to
grasp them using the estimated pose. We follow a similar
procedure to Tremblay et al. [35]: we place the Ô¨Åve ob-
jects in four different random locations on the table, at three
random orientations, including conÔ¨Ågurations with partial
occlusions. Since the order of picking the objects is not op-
timized, we do not allow conÔ¨Ågurations where objects lay
on top of each other. The robot attempts 12 grasps on each
object, 60 attempts in total. The robot uses the estimated
object orientation to compute an alignment of the gripper‚Äôs
Ô¨Ångers to the object narrower dimension.

The robot succeeds on 73% of the grasps using our pro-
posed approach to estimate the pose of the objects. The
most difÔ¨Åcult object to grasp is the banana (7 out of 12 suc-
cessful attempts). One possible reason is that our banana
model is not exactly the same as in the dataset ‚Äì ours is
plain yellow. This characteristic hinders the estimation, es-
pecially of the orientation, and leads to some failed grasp
attempts along the longer axis of the object. In spite of this
less accurate case, our results indicate that our approach is
robust enough to be deployed in real-world robotic tasks
without explicit domain adaptation, even with a different
RGB-D sensor and in a different background than the ones
in the training data.

5. Conclusion

We presented a novel approach to estimating 6D poses of
known objects from RGB-D images. Our approach fuses a
dense representation of features that include color and depth
information based on the conÔ¨Ådence of their predictions.
With this dense fusion approach, our method outperforms
previous approaches in several datasets, and is signiÔ¨Åcantly
more robust against occlusions. Additionally, we demon-
strated that a robot can use our proposed approach to grasp
and manipulate objects.

Acknowledgement

This work has been partially supported by JD.com
American Technologies Corporation (‚ÄúJD‚Äù) under
the
SAIL-JD AI Research Initiative and by an ONR MURI
award (1186514-1-TBCJE). This article solely reÔ¨Çects the
opinions and conclusions of its authors and not JD or any
entity associated with JD.com. We also want to thank Toy-
ota Research Institute for the Human Support Robot which
we used to perform our real robot experiments.

3350

References

[1] M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and J.
Sivic, ‚ÄúSeeing 3d chairs: Exemplar part-based 2d-3d align-
ment using a large dataset of cad models,‚Äù in Proceedings of
the IEEE Computer Vision and Pattern Recognition (cvpr),
2014, pp. 3762‚Äì3769.

[2] P. J. Besl and N. D. McKay, ‚ÄúA method for registration of
3-d shapes,‚Äù Ieee trans. pattern anal. mach. intell., vol. 14,
pp. 239‚Äì256, 1992.

[3] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shot-
ton, and C. Rother, ‚ÄúLearning 6d object pose estimation
using 3d object coordinates,‚Äù in European conference on
computer vision, Springer, 2014, pp. 536‚Äì551.

[4] A. G. Buch, L. Kiforenko, and D. Kraft, ‚ÄúRotational sub-
group voting and pose clustering for robust 3d object recog-
nition,‚Äù in Computer vision (iccv), 2017 ieee international
conference on, IEEE, 2017, pp. 4137‚Äì4145.

[5] B. Calli, A. Singh, A. Walsman, S. S. Srinivasa, P. Abbeel,
and A. M. Dollar, ‚ÄúThe ycb object and model set: To-
wards common benchmarks for manipulation research,‚Äù
2015 international conference on advanced robotics (icar),
pp. 510‚Äì517, 2015.

[6] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, ‚ÄúMulti-view 3d
object detection network for autonomous driving,‚Äù in Pro-
ceedings of the IEEE Computer Vision and Pattern Recog-
nition (cvpr), 2017.

[7] A. Collet, M. Martinez, and S. S. Srinivasa, ‚ÄúThe moped
framework: Object recognition and pose estimation for ma-
nipulation,‚Äù The international journal of robotics research,
vol. 30, no. 10, pp. 1284‚Äì1306, 2011.

[8] B. Drost, M. Ulrich, N. Navab, and S. Ilic, ‚ÄúModel globally,
match locally: EfÔ¨Åcient and robust 3d object recognition,‚Äù
in Computer vision and pattern recognition (cvpr), 2010
ieee conference on, Ieee, 2010, pp. 998‚Äì1005.

[9] V. Ferrari, T. Tuytelaars, and L. Van Gool, ‚ÄúSimultaneous
object recognition and segmentation from single or multi-
ple model views,‚Äù International journal of computer vision,
vol. 67, no. 2, pp. 159‚Äì188, 2006.

[10] M. A. Fischler and R. C. Bolles, ‚ÄúRandom sample consen-
sus: A paradigm for model Ô¨Åtting with applications to im-
age analysis and automated cartography,‚Äù Communications
of the acm, vol. 24, no. 6, pp. 381‚Äì395, 1981.

[11] A. Geiger, P. Lenz, and R. Urtasun, ‚ÄúAre we ready for
autonomous driving? the kitti vision benchmark suite,‚Äù in
Proceedings of the IEEE Computer Vision and Pattern
Recognition (cvpr), IEEE, 2012, pp. 3354‚Äì3361.

[12] S. Hinterstoisser, S. Holzer, C. Cagniart, S. Ilic, K. Kono-
lige, N. Navab, and V. Lepetit, ‚ÄúMultimodal templates for
real-time detection of texture-less objects in heavily clut-
tered scenes,‚Äù Proceedings of the IEEE International Con-
ference on Computer Vision (ICCV), pp. 858‚Äì865, 2011.

[13] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski,
K. Konolige, and N. Navab, ‚ÄúModel based training, detec-
tion and pose estimation of texture-less 3d objects in heav-
ily cluttered scenes,‚Äù in Asian conference on computer vi-
sion, Springer, 2012, pp. 548‚Äì562.

[14] W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab,
‚ÄúSsd-6d: Making rgb-based 3d detection and 6d pose es-
timation great again,‚Äù in Proceedings of the IEEE Inter-
national Conference on Computer Vision (ICCV), 2017,
pp. 22‚Äì29.

[15] W. Kehl, F. Milletari, F. Tombari, S. Ilic, and N. Navab,
‚ÄúDeep learning of local rgb-d patches for 3d object detec-
tion and 6d pose estimation,‚Äù in European conference on
computer vision, Springer, 2016, pp. 205‚Äì220.

[16] C. Li, J. Bai, and G. D. Hager, ‚ÄúA uniÔ¨Åed framework
for multi-view multi-class object pose estimation,‚Äù Arxiv
preprint arxiv:1803.08103, 2018.

[17] Y. Li, G. Wang, X.

Ji, Y. Xiang, and D. Fox,
‚ÄúDeepim: Deep iterative matching for 6d pose estimation,‚Äù
Arxiv:1804.00175, 2018.

[18] F. Manhardt, W. Kehl, N. Navab, and F. Tombari, ‚ÄúDeep

model-based 6d pose reÔ¨Ånement in rgb,‚Äù in Eccv, 2018.

[19] E. Marchand, H. Uchiyama, and F. Spindler, ‚ÄúPose estima-
tion for augmented reality: A hands-on survey,‚Äù Ieee trans-
actions on visualization and computer graphics, vol. 22, no.
12, pp. 2633‚Äì2651, 2016.

[20] E. Marder-Eppstein, ‚ÄúProject tango,‚Äù in Acm siggraph 2016
real-time live!, ser. SIGGRAPH ‚Äô16, Anaheim, California:
ACM, 2016, 40:25‚Äì40:25.

[21] A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka, ‚Äú3d
bounding box estimation using deep learning and geome-
try,‚Äù in Proceedings of the IEEE Computer Vision and Pat-
tern Recognition (cvpr), 2017.

[22] G. Pavlakos, X. Zhou, A. Chan, K. G. Derpanis, and K.
Daniilidis, ‚Äú6-dof object pose from semantic keypoints,‚Äù
Arxiv preprint arxiv:1703.04670, 2017.

[23] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, ‚ÄúFrustum
pointnets for 3d object detection from rgb-d data,‚Äù Arxiv
preprint arxiv:1711.08488, 2017.

[24] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, ‚ÄúPointnet: Deep
learning on point sets for 3d classiÔ¨Åcation and segmenta-
tion,‚Äù Arxiv preprint arxiv:1612.00593, 2016.

[25] M. Rad and V. Lepetit, ‚ÄúBb8: A scalable, accurate, robust
to partial occlusion method for predicting the 3d poses of
challenging objects without using depth.‚Äù

[26] R. Rios-Cabrera and T. Tuytelaars, ‚ÄúDiscriminatively
trained templates for 3d object detection: A real time scal-
able approach,‚Äù in Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2013, pp. 2048‚Äì
2055.

3351

[41] Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox, ‚ÄúPosecnn:
A convolutional neural network for 6d object pose estima-
tion in cluttered scenes,‚Äù Arxiv preprint arxiv:1711.00199,
2017.

[42] D. Xu, D. Anguelov, and A. Jain, ‚ÄúPointfusion: Deep sen-
sor fusion for 3d bounding box estimation,‚Äù Arxiv preprint
arxiv:1711.10871, 2017.

[43] Y. Zhou and O. Tuzel, ‚ÄúVoxelnet: End-to-end learning
for point cloud based 3d object detection,‚Äù Arxiv preprint
arxiv:1711.06396, 2017.

[44] M. Zhu, K. G. Derpanis, Y. Yang, S. Brahmbhatt, M.
Zhang, C. Phillips, M. Lecce, and K. Daniilidis, ‚ÄúSingle im-
age 3d object detection and pose estimation for grasping,‚Äù
in Robotics and automation (icra), 2014 ieee international
conference on, IEEE, 2014, pp. 3936‚Äì3943.

[27] F. Rothganger, S. Lazebnik, C. Schmid, and J. Ponce, ‚Äú3d
object modeling and recognition using local afÔ¨Åne-invariant
image descriptors and multi-view spatial constraints,‚Äù In-
ternational journal of computer vision, vol. 66, no. 3,
pp. 231‚Äì259, 2006.

[28] M. Schwarz, H. Schulz, and S. Behnke, ‚ÄúRgb-d object
recognition and pose estimation based on pre-trained con-
volutional neural network features,‚Äù in Robotics and au-
tomation (icra), 2015 ieee international conference on,
IEEE, 2015, pp. 1329‚Äì1335.

[29] S. Song and J. Xiao, ‚ÄúSliding shapes for 3d object detection
in depth images,‚Äù in European conference on computer vi-
sion, Springer, 2014, pp. 634‚Äì651.

[30] ‚Äî‚Äî, ‚ÄúDeep sliding shapes for amodal 3d object detection
in rgb-d images,‚Äù in Proceedings of the IEEE Computer Vi-
sion and Pattern Recognition (cvpr), 2016, pp. 808‚Äì816.

[31] M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker,
and R. Triebel, ‚ÄúImplicit 3d orientation learning for 6d ob-
ject detection from rgb images,‚Äù in European conference on
computer vision, Springer, 2018, pp. 712‚Äì729.

[32] S. Suwajanakorn, N. Snavely, J. Tompson, and M. Norouzi,
‚ÄúDiscovery of latent 3d keypoints via end-to-end geometric
reasoning,‚Äù Arxiv preprint arxiv:1807.03146, 2018.

[33] A. Tejani, D. Tang, R. Kouskouridas, and T.-K. Kim,
‚ÄúLatent-class hough forests for 3d object detection and pose
estimation,‚Äù in Proceedings of the European Conference on
Computer Vision, Springer, 2014, pp. 462‚Äì477.

[34] B. Tekin, S. N. Sinha, and P. Fua, ‚ÄúReal-Time Seamless
Single Shot 6D Object Pose Prediction,‚Äù in Proceedings of
the IEEE Computer Vision and Pattern Recognition (cvpr),
2018.

[35]

J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox,
and S. BirchÔ¨Åeld, ‚ÄúDeep object pose estimation for seman-
tic robotic grasping of household objects,‚Äù Arxiv preprint
arxiv:1809.10790, 2018.

[36] S. Tulsiani and J. Malik, ‚ÄúViewpoints and keypoints,‚Äù in
Proceedings of the IEEE Computer Vision and Pattern
Recognition (cvpr), 2015, pp. 1510‚Äì1519.

[37]

J. Vidal, C.-Y. Lin, and R. Mart¬¥ƒ±, ‚Äú6d pose estimation us-
ing an improved method based on point pair features,‚Äù in
2018 4th international conference on control, automation
and robotics (iccar), IEEE, 2018, pp. 405‚Äì409.

[38] P. Wohlhart and V. Lepetit, ‚ÄúLearning descriptors for ob-
ject recognition and 3d pose estimation,‚Äù in Proceedings of
the IEEE Computer Vision and Pattern Recognition (cvpr),
2015, pp. 3109‚Äì3118.

[39] Y. Xiang, W. Choi, Y. Lin, and S. Savarese, ‚ÄúData-driven 3d
voxel patterns for object category recognition,‚Äù in Proceed-
ings of the IEEE Computer Vision and Pattern Recognition
(cvpr), 2015, pp. 1903‚Äì1911.

[40] ‚Äî‚Äî, ‚ÄúSubcategory-aware convolutional neural networks
for object proposals and detection,‚Äù in Applications of com-
puter vision (wacv), 2017 ieee winter conference on, IEEE,
2017, pp. 924‚Äì933.

3352

