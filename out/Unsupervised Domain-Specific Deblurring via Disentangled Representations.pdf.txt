Unsupervised Domain-SpeciÔ¨Åc Deblurring via Disentangled Representations

Boyu Lu

Jun-Cheng Chen

Rama Chellappa

UMIACS, University of Maryland, College Park

bylu@umiacs.umd.edu

pullpull@cs.umd.edu

rama@umiacs.umd.edu

Abstract

Image deblurring aims to restore the latent sharp im-
ages from the corresponding blurred ones. In this paper,
we present an unsupervised method for domain-speciÔ¨Åc,
single-image deblurring based on disentangled represen-
tations. The disentanglement is achieved by splitting the
content and blur features in a blurred image using content
encoders and blur encoders. We enforce a KL divergence
loss to regularize the distribution range of extracted blur
attributes such that little content information is contained.
Meanwhile, to handle the unpaired training data, a blurring
branch and the cycle-consistency loss are added to guaran-
tee that the content structures of the deblurred results match
the original images. We also add an adversarial loss on de-
blurred results to generate visually realistic images and a
perceptual loss to further mitigate the artifacts. We per-
form extensive experiments on the tasks of face and text de-
blurring using both synthetic datasets and real images, and
achieve improved results compared to recent state-of-the-
art deblurring methods.

1. Introduction

Image blur is an important factor that adversely affects
the quality of images and thus signiÔ¨Åcantly degrades the
performances of many computer vision applications, such
as object detection [15] and face recognition [22, 21]. To
address this problem, blind image deblurring methods aim
to restore the latent sharp image from a blurred image. Con-
ventional approaches often formulate the image deblurring
task as a blur kernel estimation problem. Since this prob-
lem is highly ill-posed, many priors have been proposed
to model images and kernels [29, 41, 13]. However, most
of these priors only perform well on generic natural im-
ages, but cannot generalize to speciÔ¨Åc image domains, like
face [36], text [9] and low-illumination images [10]. There-
fore, some priors (e.g. L0-regularized intensity and gradient
prior [28], face exemplars [27]) have been developed to han-
dle these domain-speciÔ¨Åc image deblurring problems. But
still these methods can only handle certain types of blur and

(a) Blurred

(b) Madam et al. [25]

(c) Ours

(d) Blurred

(e) CycleGAN [44]

(f) Ours

Figure 1. Real-world blurred face and text results of the proposed
method compared with other state-of-the-art unpaired deblurring
methods. The deblurred image of (b) is from [25]. For (e), we
apply our trained model using the publicly available code of [44].

often require longer inference time.

Recently, some learning-based approaches have been
proposed for blind image deblurring [15, 26, 36]. CNN-
based models can tackle more complex blur types and
the inference is fast due to GPU acceleration. Mean-
while, the Generative Adversarial Networks (GAN) have
been found to be effective in generating more realistic im-
ages. Nonetheless, most of these methods need paired train-
ing data, which is expensive to collect in practice. Al-
though numerous blur generation methods have been devel-
oped [15, 38, 4], they are not capable of learning all types
of blur variants in the wild. Moreover, strong supervision
may cause algorithms to overÔ¨Åt training data and thus can-
not generalize well to real images.

More recently, Nimisha et al. [25] proposed an unsu-
pervised image deblurring method based on GANs where
they add reblur loss and multi-scale gradient loss on the
model. Although they achieved good performance on the
synthetic datasets, their results on some real blurred images
are not satisfactory (Fig. 1(b)). Another solution might be

10225

Figure 2. Overview of the deblurring framework.The data Ô¨Çow of the top blurring branch (bottom deblurring branch) is represented by
blue (orange) arrows. E c
S are content encoders for blurred and sharp images; E b is the blur encoder; GB and GS are blurred image
and sharp image generators. Two GAN losses are added to distinguish bs from blur images, and to distinguish sb from sharp images. The
KL divergence loss is added to the output of E b. Cycle-consistency loss is added to s and ÀÜs, b and ÀÜb. Perceptual loss is added to b and sb.

B and E c

directly using some existing unsupervised methods (Cycle-
GAN [44], DualGAN [42]) to learn the mappings between
sharp and blurred image domains. However, these generic
approaches often encode other factors (e.g., color, texture)
rather than blur information into the generators, and thus do
not produce good deblurred images (Fig. 1(e)).

blurring and achieve competitive performance compared
with other state-of-the-art deblurring methods. We also
evaluate the proposed method on face veriÔ¨Åcation and op-
tical character recognition (OCR) tasks to demonstrate the
effectiveness of our algorithm on recovering semantic infor-
mation.

In this paper, we propose an unsupervised domain-
speciÔ¨Åc image deblurring method based on disentangled
representations. More speciÔ¨Åcally, we disentangle the con-
tent and blur features from blurred images to accurately en-
code blur information into the deblurring framework. As
shown in Fig. 2, the content encoders extract content fea-
tures from unpaired sharp and blurred images, and the blur
encoder captures blur information. We share the weights
of the last layer of these two content encoders so that the
content encoders can project the content features of both
domains onto a common space. However, this structure by
itself does not guarantee that the blur encoder captures blur
features ‚Äî it may encode content or other features as well.
Inspired by [2], we add a KL divergence loss to regularize
the distribution of blur features to suppress the contained
content information. Then, the deblurring generator GS and
the blurring generator GB take corresponding content fea-
tures conditioned on blur attributes to generate deblurred
and blurred images. Similar to CycleGAN [44], we also
use the adversarial loss and the cycle-consistency loss as
regularizers to assist the generator networks to yield more
realistic images, and also preserve the content of the origi-
nal image. To further remove the unpleasant artifacts intro-
duced by the deblurring generator GS, we add the percep-
tual loss to the proposed approach. Some sample deblurred
images are shown in Fig. 1.

We conduct extensive experiments on face and text de-

2. Related Works

2.1. Single Image Blind Deblurring

Generic methods Single image blind deblurring is a
highly ill-posed problem. Over the past decade, various nat-
ural image and kernel priors have been developed to regu-
larize the solution space of the latent sharp images, includ-
ing heavy-tailed gradient prior [35], sparse kernel prior [7],
l0 gradient prior [41], normalized sparsity prior [14], and
dark channels [29]. However, these priors are estimated
from limited observations, and are not accurate enough.
As a result, the deblurred images are often under-deblurred
(images are still blurred) or over-deblurred (images contain
many artifacts).

On the other hand, due to the recent immense success
of deep networks and GANs, several CNN-based methods
have been proposed for image deblurring. Sun et al. [38]
and Schuler et al. [33] use CNN to predict the motion blur
kernels. Chakrabarti et al. [4] predict the Fourier coefÔ¨Å-
cients of the deconvolution Ô¨Ålters by a neural network and
perform deblurring in frequency domains. These methods
combine the advantage of CNN and conventional maximum
a posteriori probability (MAP)-based algorithms. Differ-
ently, Nah et al. [26] train a multi-scale CNN in an end-
to-end manner to directly deblur images without explicitly
estimating the blur kernel. Similarly, Kupyn et al. [15] use

10226

ùë¨ùë∫ùíÑùë¨ùë©ùíÑùë¨ùíÉùëÆùë©ùëÆùë∫ùë¨ùíÉùë¨ùë©ùíÑùë¨ùë∫ùíÑùëÆùë∫ùëÆùë©ùíî‡∑úùíîb‡∑°ùíÉùíÉùíîùíîùíÉWGAN and perceptual loss and achieve state-of-the-art per-
formance on natural image deblurring.

Domain speciÔ¨Åc methods Although the above men-
tioned methods perform well for natural image deblurring,
it is difÔ¨Åcult to generalize them to some speciÔ¨Åc image do-
mains, such as face and text images. Pan et al. [28] propose
the L0-regularized prior on image intensity and gradients
for text image deblurring. Hradis et al. [9] train an end-to-
end CNN speciÔ¨Åc for text image deblurring. Pan et al. [27]
utilize exemplar faces in a reference set to guide the blur
kernel estimation. Shen et al. [36] use the face parsing la-
bels as global semantic priors and local structure regulariza-
tion to improve face deblurring performance.

2.2. Disentangled Representation

There has been many recent efforts on learning disen-
tangled representations. Tran et al. [40] propose DR-GAN
to disentangle the pose and identity components for pose-
invariant face recognition. Bao et al. [2] explicitly dis-
entangle identity features and attributes to learn an open-
set face synthesizing model. Liu et al. [19] construct an
identity distill and dispelling auto encoder to disentangle
identity with other attributes. BicycleGAN [45] combines
cVAE-GAN and cLR-GAN to model the distribution of
possible outputs in image-to-image translation. Recently,
some unsupervised methods decouple images into domain-
invariant content features and domain-speciÔ¨Åc attribute vec-
tors, which produce diverse image-to-image translation out-
puts [17, 1, 11].

3. Proposed Method

B and Ec

The proposed approach consists of four parts: 1) content
encoders Ec
S for blurred and sharp image domains;
2) blur encoder Eb; 3) blurred and sharp image generators
GB and GS; 4) blurred and sharp image discriminators DB
and DS. Given a training sample b ‚àà B in the blurred im-
age domain and s ‚àà S in the sharp image domain, the con-
tent encoders Ec
S extract content information from
corresponding samples and Eb estimates the blur informa-
tion from b. GS then takes Ec
B(b) and Eb(b) to generate
a sharp image sb while GB takes Ec
S(s) and Eb(b) to gen-
erate a blurred image bs. The discriminators DB and DS
distinguish between the real and generated examples. The
end-to-end architecture is illustrated in Fig. 2.

B and Ec

In the following subsections, we Ô¨Årst

introduce the
method to disentangle content and blur components in Sec-
tion 3.1. Then, we discuss the loss functions used in our
approach. In Section 3.5, we describe the testing procedure
of the proposed framework. Finally, the implementation de-
tails are discussed in Section 3.6.

3.1. Disentanglement of Content and Blur

Since the ground truth sharp images are not available
in the unpaired setting, it is not trivial to disentangle the
content information from a blurred image. However, since
sharp images only contain content components without any
blur information, the content encoder Ec
S should be a good
content extractor. We enforce the last layer of Ec
S to
share weights so as to guide Ec
B to learn how to effectively
extract content information from blurred images.

B and Ec

On the other hand, the blur encoder Eb should only en-
code blur information. To achieve this goal, we propose two
methods to help Eb suppress as much content information
as possible. First, we feed Eb(b) together with Ec
S(s) into
GB to generate bs. Since bs is a blurred version of s and it
will not contain content information of b, this structure dis-
courages Eb(b) to encode content information of b. Second,
we add a KL divergence loss to regularize the distribution
of the blur features zb = Eb(b) to be close to the normal
distribution p(z) ‚àº N (0, 1). As shown in [2], this will fur-
ther suppress the content information contained in zb. The
KL divergence loss is deÔ¨Åned as follows:

KL(q(zb)||p(z)) = ‚àíZ q(zb) log

p(z)
q(zb)

dz

(1)

As proved in [13], minimizing the KL divergence is equiv-
alent to minimizing the following loss:

LKL =

1
2

N

Xi=1

(¬µ2

i + œÉ2

i ‚àí log(œÉ2

i ) ‚àí 1)

(2)

where ¬µ and œÉ are the mean and standard deviation of zb
and N is the dimension of zb. Similar to [13], zb is sampled
as zb = ¬µ + z ‚ó¶ œÉ, where p(z) ‚àº N (0, 1) and ‚ó¶ represents
element-wise multiplication.

3.2. Adversarial Loss

In order to make the generated images look more realis-
tic, we apply the adversarial loss on both domains. For the
sharp image domain, we deÔ¨Åne the adversarial loss as:

LDS = E
+ E

s‚àºp(s)[log DS(s)]
b‚àºp(b)[log(1 ‚àí DS(GS(Ec

B(b), zb)))]

(3)

where DS tries to maximize the objective function to distin-
guish between deblurred images and real sharp images. In
contrast, GS aims to minimize the loss to make deblurred
images look similar to real samples in domain S. Similarly,
we deÔ¨Åne the adversarial loss in blurred image domain as
LDB :

LDB = E
+ E

b‚àºp(b)[log DB(b)]
s‚àºp(s)[log(1 ‚àí DB(GB(Ec

S(s), zb)))]

(4)

10227

3.3. Cycle Consistency Loss

After competing with discriminator DS in the minimax
game, GS should be able to generate visually realistic sharp
images. However, since no pairwise supervision is pro-
vided, the deblurred image may not retain the content in-
formation in the original blurred image.
Inspired by Cy-
cleGAN [44], we introduce the cycle-consistency loss to
guarantee that the deblurred image sb can be reblurred to
reconstruct the original blurred sample, and bs can be trans-
lated back to the original sharp image domain. The cycle-
consistency loss further limits the space of the generated
samples and preserve the content of original images. More
speciÔ¨Åcally, we perform the forward translation as:

sb = GS(Ec

B(b), Eb(b)), bs = GB(Ec

S(s), Eb(b))

and the backward translation as:

ÀÜb = GB(Ec

S(sb), Eb(bs)), ÀÜs = GS(Ec

B(bs), Eb(bs))

(5)

(6)

We deÔ¨Åne the cycle-consistency loss on both domains as:

Lcc = E

s‚àºp(s)[ks ‚àí ÀÜsk1] + E

b‚àºp(b)[||b ‚àí ÀÜb||1]

(7)

help but sometimes hurt the performance, we do not include
it for this task. One possible reason is that the pixel inten-
sity distribution of text images is very different from natural
images, which leads to the model pre-trained on ImageNet
to be ineffective for text images.

The full objective function is a weighted sum of all the

losses from (2) to (8):

L = ŒªadvLadv + ŒªKLLKL + ŒªccLcc + ŒªpLp

(9)

where Ladv = LDS + LDB . We empirically set the weights
of each loss to balance their importance.

3.5. Testing

At test time, the blurring branch is removed. Given a
test blurred image bt, Ec
B and Eb extract the content and
blur features. Then GS takes the outputs and generates the
deblurred image sbt :

sbt = GS(Ec

B(bt), Eb(bt))

(10)

3.4. Perceptual Loss

3.6. Implementation Details

From our preliminary experiments, we Ô¨Ånd that the gen-
erated deblurred samples often contain many unpleasant ar-
tifacts. Motivated by the observations from [39, 5] that fea-
tures extracted from pre-trained deep networks contain rich
semantic information, and their distances can act as percep-
tual similarity judgments, we add a perceptual loss between
the deblurred images and the corresponding original blurred
images:

Lp = kœÜl(sb) ‚àí œÜl(b)k2
2

(8)

where œÜl(x) is the features of the l-th layer of the pre-
trained CNN. In our experiments, we use the conv3,3 layer
of VGG-19 network [37] pre-trained on ImageNet [6].

There are two main reasons why we use the blurred im-
age b instead of the sharp one s as the reference image in
the perceptual loss. First, we make an assumption that the
content information of b can be extracted by the pre-trained
CNN. As shown in Section 4.2, the experimental results
conÔ¨Årm this point. Second, since s and b are unpaired, ap-
plying the perceptual loss between s and sb will force sb
to encode irrelevant content information from s. However,
we also notice that the perceptual loss is sensitive to blur
as shown in [43]. We thus carefully balance the weight of
the perceptual loss and other losses to prevent sb from stay-
ing too close to b. The sensitivity evaluation of varying this
weight is shown in the supplementary materials.

It is worth mentioning that the perceptual loss is not
added to bs and s. This is because we do not Ô¨Ånd obvious
artifacts in bs during training. Moreover, for text image de-
blurring, since we observe that the perceptual loss does not

Architecture and training details. For the network ar-
chitectures, we follow the similar structures as the one used
in [17]. The content encoder is composed of three strided
convolution layers and four residual blocks. The blur en-
coder contains four strided convolution layers and a fully
connected layer. For the generator, the architecture is sym-
metric to the content encoder with four residual blocks fol-
lowed by three transposed convolution layers. The discrim-
inator applies a multi-scale structure where feature maps at
each scale go through Ô¨Åve convolution layers and then are
fed into sigmoid outputs. The end-to-end design is imple-
mented in PyTorch [31]. During training, we use Adam
solver [12] to perform two steps of update on discrimina-
tors, and then one step on encoders and generators. The
learning rate is initially set to 0.0002 for the Ô¨Årst 40 epochs,
then we use exponential decay over the next 40 epochs. In
all the experiments, we randomly crop 128 √ó 128 patches
with batch size of 16 for training. For hyper-parameters, we
experimentally set: Œªadv = 1, ŒªKL = 0.01, Œªcc = 10 and
Œªp = 0.1.

Motion blur generation. We follow the procedure in
DeblurGAN [15] to generate motion blur kernels to blur
face images. A random trajectory is generated as described
in [3]. Then the kernels are generated by applying sub-pixel
interpolation to the trajectory vector. For parameters, we
use the same values as in [15] except that we set the proba-
bility of impulsive shake as 0.005, the probability of Gaus-
sian shake uniformly distributed in (0.5, 1.0), and the max
length of the movement as 10.

10228

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 3. Ablation study. (a) shows the blurred image and (g) is the sharp image. (b) only contains deblurring branch (bottom branch of
Fig. 2), (c) adds blurring branch (bottom branch of Fig. 2), (d) adds disentanglement (E b), (e) adds the KL divergence loss, and (f) adds
perceptual loss.

(a) Blurred

(b) [28]

(c) [29]

(d) [36]

(e) [27]

(f) [41]

(g) [14]

(h) [15]

(i) [26]

(j) [44]

(k) Ours

(l) Sharp

Figure 4. Visual performance comparison with state-of-the-art methods on CelebA dataset. Best viewed in color and by zooming in.

Method
Only deblurring branch
Add blurring branch
Add disentanglement
Add KL divergence loss
Add perceptual loss

PSNR SSIM dV GG
82.9
18.83
19.84
65.5
69.8
19.58
60.6
20.29
20.81
57.6

0.56
0.59
0.57
0.61
0.65

Table 1. Ablation study on the effectiveness of different compo-
nents. dV GG represents the distance of feature from VGG-Face,
lower is better.

4. Experimental Results

We evaluate the proposed approach on three datasets:
CelebA dataset [20], BMVC Text dataset [9], and CFP
dataset [34].

4.1. Datasets and Metrics

CelebA dataset: This dataset consists of more than
202,000 face images. Most of the faces are of good qual-
ity and at near-frontal poses. We randomly split the whole
dataset into three mutually exclusive subsets: sharp training
set (100K images), blurred training set (100K images) and
test set (2137 images). For the blurred training set, we use
the method in Section 3.6 to blur the images. The faces are
detected and aligned using the method proposed in [32].

BMVC text dataset: This dataset

is composed of
66,000 text images with size 300 √ó 300 for training and
94 images with size 512 √ó 512 for OCR testing. Similar
to CelebA, we evenly split the training sets as sharp and
blurred set. Since the dataset already contains the blurred

text images, we directly use them instead of generating new
ones.

CFP dataset: This dataset consists of 7,000 still images
from 500 subjects and for each subject, it has ten images in
frontal pose and four images in proÔ¨Åle pose. The datasets
are divided into ten splits and two protocols: frontal-to-
frontal (FF) and frontal-to-proÔ¨Åle (FP). We used the same
method as described above to blur the images. The faces
are detected and aligned similarly as the CelebA dataset.

For CelebA and BMVC Text datasets, we use standard
debluring metrics (PSNR, SSIM) for evaluation. We also
use feature distance (i.e., the L2 distance of the outputs
from some deep networks) between the deblurred image
and the ground truth image as a measure of semantic simi-
larity because we Ô¨Ånd this to be a better perceptual metric
than PSNR and SSIM [43]. For the CelebA dataset, we use
the outputs of pool5 layer from VGG-Face [30] and for
the text dataset, we use the outputs of pool5 layer from
a VGG-19 network. For text deblurring, another meaning-
ful metric is the OCR recognition rate for the deblurred text.
We follow the same protocol as in [9] to report the character
error rate (CER) for OCR evaluation.

To study the inÔ¨Çuence of motion blur on face recognition
and test the performance of different deblurring algorithms,
we perform face veriÔ¨Åcation on the CFP dataset. Both
frontal-to-frontal and frontal-to-proÔ¨Åle protocol are evalu-
ated. The frontal-to-proÔ¨Åle protocol can further be used to
examine the robustness of the deblurring methods on pose.

In order to test the generalization capability of the pro-
posed method, we also try our approach on natural images.

10229

(a) Blurred

(b) [28]

(c) [29]

(d) [36]

(e) [27]

(f) [41]

(g) [14]

(h) [15]

(i) [26]

(j) [44]

(k) Ours

Figure 5. Visual comparisons with state-of-the-art methods on real blurred face images. Best viewed in color and by zooming in.

Method
Pan et al. [28]
Pan et al. [29]
Shen et al. [36]
Pan et al. [27]
Xu et al. [41]
Krishnan et al. [14]
Kupyn et al. [15]
Nah et al. [26]
Zhu et al. [44]
Ours

PSNR SSIM dV GG
17.34
96.6
85.6
17.59
57.9
21.50
166.6
15.16
16.84
102.0
89.4
18.51
116.5
18.86
75.6
18.26
19.40
103.2
57.6
20.81

0.52
0.54
0.69
0.38
0.47
0.56
0.54
0.57
0.56
0.65

Table 2. Quantitative performance comparison with state-of-the-
art methods on CelebA dataset. dV GG represents the distance of
feature from VGG-Face, lower is better.

More details are presented in the supplementary materials.

4.2. Ablation Study

In this section, we perform an ablation study to analyze
the effectiveness of each component or loss in the proposed
framework. Both quantitative and qualitative results on
CelebA dataset are reported for the following Ô¨Åve variants
of our methods where each component is gradually added:
1) only including deblurring branch (i.e., removing the top
cycle in Fig. 2 and the blur encoder Eb); 2) adding blur-
ring branch (adding the top cycle of Fig. 2); 3) adding con-
tent and blur disentanglement; 4) adding the KL divergence
loss; 5) adding the perceptual loss.

We present the PSNR, SSIM and VGG-Face distance
(dV GG) for each variant in Table 1 and the visual com-
parisons are shown in Fig. 3. From Table 1, we can see
that adding the blurring branch signiÔ¨Åcantly improves the
deblurring performance, especially for the perceptual dis-
tance. As shown in Fig. 3 (c) many artifacts are removed
from face and colors are preserved well compared to (b).
This conÔ¨Årms the Ô¨Åndings in CycleGAN [44] that only one

Methods
Blurred
Sharp
Pan et al. [28]
Pan et al. [29]
Shen et al. [36]
Pan et al. [27]
Xu et al. [41]
Krishnan et al. [14]
Kupyn et al. [15]
Nah et al. [26]
Zhu et al. [44]
Ours

F2F Accuracy
0.920¬±0.014
0.988¬±0.005
0.930¬±0.013
0.935¬±0.015
0.959¬±0.008
0.916¬±0.011
0.944¬±0.012
0.941¬±0.012
0.948¬±0.012
0.960¬±0.007
0.941¬±0.012
0.948¬±0.006

F2P Accuracy
0.848¬±0.013
0.949¬±0.014
0.853¬±0.010
0.872¬±0.015
0.821¬±0.022
0.825¬±0.016
0.865¬±0.013
0.857¬±0.014
0.872¬±0.007
0.885¬±0.016
0.864¬±0.015
0.872¬±0.015

Table 3. Face veriÔ¨Åcation results on the CFP dataset. F2F, F2P
represent frontal-to-frontal and frontal-to-proÔ¨Åle protocols.

direction cycle-consistency loss is not enough to recover
good images. However, we Ô¨Ånd that adding a disentangle-
ment component does not help but rather hurt the perfor-
mance ( Fig. 3 (d)). This demonstrates that the blurring en-
coder Eb will induce some noise and confuse the generator
GS if the KL divergence loss is not enforced. In contrast,
when the KL divergence loss is added to Eb (Fig. 3 (e)),
content and blur information can be better disentangled and
we observe some improvements on both PSNR and percep-
tual similarities. Finally, the perceptual loss can improve the
perceptual reality of the face notably. By comparing Fig. 3
(e) and (f), we Ô¨Ånd that the artifacts on cheek and forehead
are further removed. Furthermore, the mouth region of (f)
is more realistic than (e).

4.3. Face Results

Compared methods: We compare the proposed method
with some state-of-the-art deblurring methods [28, 29, 36,
27, 41, 14, 26, 44, 15]. We directly use the pre-trained mod-
els provided by authors except for CycleGAN [44], where

10230

(a) Blurred

(b) [28]

(c) [29]

(d) [26]

(e) [44]

(f) [9]

(g) Ours

(h) Sharp

Figure 6. Visual results compared with state-of-the-art methods on BMVC Text dataset. Best viewed by zooming in.

(a) Blurred

(b) [28]

(c) [29]

(d) [26]

(e) [44]

(f) [9]

(g) Ours

Figure 7. Visual results compared with state-of-the-art methods on real blurred text images. Best viewed by zooming in.

we retrain the model by using the same training set as our
method. Both CNN-based models
[36, 26, 44, 15] and
conventional MAP-based methods are included [28, 29, 27,
41, 14]. Among these approaches, two are speciÔ¨Åc for face
deblurring [27, 36] while others are generic deblurring al-
gorithm. The kernel size for [28, 29] is set to 9. We
found that the face deblurring method [36] is very sensi-
tive to face alignment, we follow the sample image pro-
vided by the author to align the faces before running their
algorithm. Meanwhile, CycleGAN is the only unsupervised
CNN-based method we compare with.

CelebA dataset results. The quantitative results for
CelebA dataset are shown in Table 2 and the visual compar-
isons are illustrated in Fig. 4. Our approach shows superior
performance to other unsupervised algorithms on both con-
ventional metrics and VGG-Face distance. Furthermore, we
achieve comparable results with state-of-the-art supervised
face deblurring method [36]. From Fig. 4 we can see that
conventional methods often over-deblur or under-deblur the
blurred images. Among them, Krishnan et al. [14] perform
the best in PSNR and SSIM and Pan et al. [29] perform the
best in perceptual distance. For CNN-based methods, Shen
et al. [36] include a face parsing branch and achieve the best

performance among the compared methods. The results for
DeblurGAN [15] contain some ringing artifacts and Cycle-
GAN [44] cannot recover the mouth part of both images that
well. Nah et al. [26] shows better visual results than other
CNN-based generic methods but still contains some blur in
local structures.

Face veriÔ¨Åcation results. The face veriÔ¨Åcation re-
sults for the CFP dataset are reported in Table 3. We
train a 27-layer ResNet [22] on the curated MS-Celeb1M
dataset [8, 18] with 3.7 millions face images and extract fea-
tures of the deblurred faces for each method. Cosine simi-
larities of test pairs are used as similarity scores for face ver-
iÔ¨Åcation. We follow the protocols used in [23, 24] and the
veriÔ¨Åcation accuracy for both frontal-to-frontal and frontal-
to-proÔ¨Åle protocols are reported. As shown in Table 3, the
proposed method improves the baseline results of blurred
images and outperforms CycleGAN [44] on both proto-
cols. Moreover, we achieve comparable performance com-
pared to other state-of-the-art supervised deblurring meth-
ods. Shen et al. [36] perform very well for frontal-to-frontal
protocol, yet provide the worst performance on frontal-to-
proÔ¨Åle protocol, which shows that the face parsing network
in their method is sensitive to poses. In contrast, the pro-

10231

Method
Pan et al. [28]
Pan et al. [29]
Nah et al. [26]
Hradis et al. [9]
Zhu et al. [44]
Ours

PSNR SSIM dV GG CER
42.3
21.18
35.3
21.84
50.6
22.27
30.6
7.2
53.0
19.57
22.56
10.1

0.92
0.93
0.92
0.98
0.89
0.95

19.7
15.7
31.9
1.6
18.8
2.2

Table 4. Quantitative performance comparison with state-of-the-
art methods on BMVC Text dataset. dV GG represents the distance
of feature from VGG net, lower is better. CER is the OCR charac-
ter error rate, lower is better.

posed method works for both frontal and proÔ¨Åle face im-
ages even though we do not explicitly train on faces with
extreme poses.

Real blurred images results We also evaluate the pro-
posed method on some real-world images from the datasets
of Lai et al. [16], and the results are shown in Fig. 5. Simi-
lar to what we have observed for CelebA, our method shows
competitive performance compared to other state-of-the-art
approaches. Conventional methods [28, 29, 27, 41, 14] still
tend to under-deblur or over-deblur images, especially on
local regions such as eyes and mouths. On the other hand,
the generic CNN-based method [15] does not perform very
well on face deblurring. CycleGAN [44] fails to recover
sharp faces but only changes the background color of im-
ages (e.g., third row of Fig. 5(j)). Nah et al. [26] pro-
duce good results on the Ô¨Årst two faces, but generate some
artifacts in the third image. Deep semantic face deblur-
ring [36] generate better results than other compared meth-
ods. Nonetheless, due to the existence of face parsing, they
tend to sharpen some facial parts (eye, nose and mouth)
but over-smooth the ears and the background. In contrast,
our method can not only recover sharp faces, but also re-
store sharp textures in the background (e.g., third row of
Fig. 5(k)).

4.4. Text results

BMVC Text dataset results. Similar to face experi-
ments, we train a CycleGAN model using the same training
set as our method. The kernel size for [28, 29] is set to 12.
The quantitative results for BMVC Text dataset are shown
in Table 4 and some sample images are presented in Fig. 6.
We can see that conventional methods [28, 29] and generic
deblurring approaches [26] do not perform well on text de-
blurring. The visual quality is poor and the OCR error rate
is very high. The results for CycleGAN [44] contain some
unexplainable blue background. Although it removes the
blur in images, it fails to recover recognizable text. In con-
trast, our method achieves good visual quality and its per-
formance is comparable to the state-of-the-art supervised
text deblurring method [9] on semantic metrics (i.e., per-

ceptual distance and OCR error rate). Interestingly, we Ô¨Ånd
the PNSR performance for our approach is worse than the
method [9] by large margins. We carefully examine our vi-
sual results and Ô¨Ånd that the proposed method sometimes
changes the font of the text while deblurring. For exam-
ple, as shown in the Ô¨Årst row of Fig. 6(g), the font of our
deblurred text becomes lighter and thinner compared to the
original sharp text image (Fig. 6(h)). The main reason for
this phenomenon is that our method does not utilize paired
training data so that the deblurring generator cannot pre-
serve some local details of text images.

Real blurred text images results We also evaluate our
deblurring method on real blurred text images provided
by Hradis et al. [9]. Due to space limitation, 200 √ó 200
patches are randomly cropped, and some visual results are
illustrated in Fig. 7. Similar to the results of BMVC Text
dataset, we Ô¨Ånd that conventional methods [28, 29] fail to
deblur the given text images. Nah et al. [26], in contrast,
generate a reasonable deblurred result for the Ô¨Årst image
but cannot handle the second one. CycleGAN [44] again
produces blue artifacts and cannot recover meaningful text
information. Hradiset al. [9] and our approach both gener-
ate satisfactory results. Although we mis-recognize some
characters (e.g., in the second images, ‚Äùi.e., BING‚Äù is re-
covered as ‚ÄùLe.,BING‚Äù), we still correctly recover most of
the blurred images.

5. Conclusions

In this paper, we propose an unsupervised method for
domain-speciÔ¨Åc single image deblurring. We disentangle
the content and blur features in a blurred image and add the
KL divergence loss to discourage the blur features to en-
code content information. In order to preserve the content
structure of the original images, we add a blurring branch
and cycle-consistency loss to the framework. The percep-
tual loss helps the blurred image remove unrealistic arti-
facts. Ablation study on each component shows the effec-
tiveness of different modules. We conduct extensive experi-
ments on face and text deblurring. Both quantitative and vi-
sual results show promising performance compared to other
state-of-the-art approaches.

Acknowledgment

This research is based upon work supported by the OfÔ¨Åce of the

Director of National Intelligence (ODNI), Intelligence Advanced Re-

search Projects Activity (IARPA), via IARPA R&D Contract No. 2014-

14071600012. The views and conclusions contained herein are those of

the authors and should not be interpreted as necessarily representing the of-

Ô¨Åcial policies or endorsements, either expressed or implied, of the ODNI,

IARPA, or the U.S. Government. The U.S. Government is authorized to

reproduce and distribute reprints for Governmental purposes notwithstand-

ing any copyright annotation thereon.

10232

References

[1] Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip
Bachman, and Aaron Courville. Augmented cyclegan:
Learning many-to-many mappings from unpaired data. arXiv
preprint arXiv:1802.10151, 2018.

[2] Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang
Hua. Towards open-set identity preserving face synthesis.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 6713‚Äì6722, 2018.

[3] G Boracchi and A Foi. Modeling the performance of image
restoration from motion blur. IEEE Transactions on Image
Processing, 21(8):3502‚Äì3517, 2012.

[4] Ayan Chakrabarti. A neural approach to blind motion deblur-
ring. In Proceedings of the European Conference on Com-
puter Vision (ECCV), pages 221‚Äì235. Springer, 2016.

[5] Qifeng Chen and Vladlen Koltun. Photographic image syn-
thesis with cascaded reÔ¨Ånement networks. In Proceedings of
International Conference on Computer Vision (ICCV), pages
1520‚Äì1529. IEEE, 2017.

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. ImageNet: A Large-Scale Hierarchical Image Database.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2009.

[7] Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T Roweis,
and William T Freeman. Removing camera shake from a
single photograph. In ACM transactions on graphics (TOG),
volume 25, pages 787‚Äì794. ACM, 2006.

[8] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and
Jianfeng Gao. MS-Celeb-1M: A dataset and benchmark for
large scale face recognition. In Proceedings of the European
Conference on Computer Vision (ECCV). Springer, 2016.

[9] Michal Hradi, Jan Kotera, Pavel Zemk, and Filip roubek.
Convolutional neural networks for direct text deblurring. In
Mark W. Jones Xianghua Xie and Gary K. L. Tam, edi-
tors, Proceedings of the British Machine Vision Conference
(BMVC), pages 6.1‚Äì6.13. BMVA Press, September 2015.

[10] Zhe Hu, Sunghyun Cho, Jue Wang, and Ming-Hsuan Yang.
Deblurring low-light images with light streaks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3382‚Äì3389, 2014.

[11] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
Multimodal unsupervised image-to-image translation.
In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), 2018.

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[13] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. arXiv preprint arXiv:1312.6114, 2013.

[14] Dilip Krishnan, Terence Tay, and Rob Fergus. Blind decon-
volution using a normalized sparsity measure. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 233‚Äì240. IEEE, 2011.

ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018.

[16] Wei-Sheng Lai, Jia-Bin Huang, Zhe Hu, Narendra Ahuja,
and Ming-Hsuan Yang. A comparative study for single im-
age blind deblurring. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition(CVPR), pages
1701‚Äì1709, 2016.

[17] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh
Singh, and Ming-Hsuan Yang. Diverse image-to-image
translation via disentangled representations. In Proceedings
of European Conference on Computer Vision (ECCV), pages
36‚Äì52. Springer, 2018.

[18] Wei-An Lin, Jun-Cheng Chen, and Rama Chellappa. A
proximity-aware hierarchical clustering of faces.
In Pro-
ceedings of the IEEE International Conference on Automatic
Face and Gesture Recognition (FG), pages 294‚Äì301. IEEE,
2017.

[19] Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan,
and Xiaogang Wang. Exploring disentangled feature rep-
resentation beyond face identiÔ¨Åcation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018.

[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), 2015.
[21] Boyu Lu, Rama Chellappa, and Nasser M. Nasrabadi. Incre-
mental dictionary learning for unsupervised domain adapta-
tion. In Proceedings of the British Machine Vision Confer-
ence (BMVC), 2015.

[22] Boyu Lu, Jun-Cheng Chen, Carlos D Castillo, and Rama
Chellappa. An experimental evaluation of covariates effects
on unconstrained face veriÔ¨Åcation.
IEEE Transactions on
Biometrics, Behavior, and Identity Science, 2019.

[23] Boyu Lu, Jun-Cheng Chen, and Rama Chellappa. Regular-
ized metric adaptation for unconstrained face veriÔ¨Åcation. In
2016 23rd International Conference on Pattern Recognition
(ICPR), pages 4112‚Äì4117. IEEE, 2016.

[24] Boyu Lu, Jingxiao Zheng, Jun-Cheng Chen, and Rama Chel-
lappa. Pose-robust face veriÔ¨Åcation by exploiting competing
tasks. In 2017 IEEE Winter Conference on Applications of
Computer Vision (WACV), pages 1124‚Äì1132. IEEE, 2017.

[25] Thekke Madam Nimisha, Kumar Sunil, and AN Ra-
jagopalan. Unsupervised class-speciÔ¨Åc deblurring. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 353‚Äì369, 2018.

[26] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 3883‚Äì
3891, 2017.

[27] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. De-
blurring face images with exemplars. In Proceedings of the
European Conference on Computer Vision (ECCV), pages
47‚Äì62. Springer, 2014.

[15] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych,
Dmytro Mishkin, and Jiri Matas. Deblurgan: Blind motion
deblurring using conditional adversarial networks.
In Pro-

[28] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. De-
blurring text images via l0-regularized intensity and gradient
prior. In Proceedings of the IEEE Conference on Computer

10233

[43] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018.

[44] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of Interna-
tional Conference on Computer Vision (ICCV), 2017.

[45] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation.
In Proceed-
ings of the Advances in Neural Information Processing Sys-
tems (NIPS), 2017.

Vision and Pattern Recognition (CVPR), pages 2901‚Äì2908,
2014.

[29] Jinshan Pan, Deqing Sun, Hanspeter PÔ¨Åster, and Ming-
Hsuan Yang. Blind image deblurring using dark channel
prior. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1628‚Äì1636,
2016.

[30] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face
In Proceedings of the British Machine Vision

recognition.
Conference (BMVC), 2015.

[31] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[32] Rajeev Ranjan, Swami Sankaranarayanan, Carlos D Castillo,
and Rama Chellappa. An all-in-one convolutional neural net-
work for face analysis. In Proceedings of the IEEE Interna-
tional Conference on Automatic Face and Gesture Recogni-
tion (FG), pages 17‚Äì24. IEEE, 2017.

[33] Christian Schuler, Michael Hirsch, Stefan Harmeling, and
Bernhard Scholkopf. Learning to deblur. IEEE Transactions
on Pattern Analysis & Machine Intelligence, (1):1‚Äì1.

[34] S Sengupta, JunCheng Cheng, C.D Castillo, V.M Patel, R
Chellappa, and D.W Jacobs. Frontal to proÔ¨Åle face veriÔ¨Åca-
tion in the wild. In IEEE Winter Conference on Applications
of Computer Vision, February 2016.

[35] Qi Shan, Jiaya Jia, and Aseem Agarwala. High-quality mo-
tion deblurring from a single image. In Acm transactions on
graphics (tog), volume 27, page 73. ACM, 2008.

[36] Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, and Ming-
Hsuan Yang. Deep semantic face deblurring.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.

[37] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[38] Jian Sun, Wenfei Cao, Zongben Xu, and Jean Ponce. Learn-
ing a convolutional neural network for non-uniform motion
blur removal.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages
769‚Äì777, 2015.

[39] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsu-
arXiv preprint

pervised cross-domain image generation.
arXiv:1611.02200, 2016.

[40] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled rep-
resentation learning gan for pose-invariant face recognition.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), Honolulu, HI, July 2017.

[41] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse
representation for natural image deblurring.
In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition (CVPR), pages 1107‚Äì1114, 2013.

[42] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan:
Unsupervised dual learning for image-to-image translation.
In Proceedings of International Conference on Computer Vi-
sion (ICCV), pages 2868‚Äì2876. IEEE, 2017.

10234

