Deep Metric Learning Beyond Binary Supervision

Sungyeon Kim‚Ä†

Minkyo Seo‚Ä†

Ivan Laptev‚Ä°

Minsu Cho‚Ä†

Suha Kwak‚Ä†

Inria / ¬¥Ecole Normale Sup¬¥erieure, Paris, France‚Ä°
POSTECH, Pohang, Korea‚Ä†
{tjddus9597, mkseo, mscho, suha.kwak}@postech.ac.kr, ivan.laptev@inria.fr

Abstract

Metric Learning for visual similarity has mostly adopted
binary supervision indicating whether a pair of images are
of the same class or not. Such a binary indicator covers only
a limited subset of image relations, and is not sufÔ¨Åcient to
represent semantic similarity between images described by
continuous and/or structured labels such as object poses,
image captions, and scene graphs. Motivated by this, we
present a novel method for deep metric learning using con-
tinuous labels. First, we propose a new triplet loss that al-
lows distance ratios in the label space to be preserved in
the learned metric space. The proposed loss thus enables
our model to learn the degree of similarity rather than just
the order. Furthermore, we design a triplet mining strategy
adapted to metric learning with continuous labels. We ad-
dress three different image retrieval tasks with continuous
labels in terms of human poses, room layouts and image
captions, and demonstrate the superior performance of our
approach compared to previous methods.

1. Introduction

The sense of similarity has been known as the most ba-
sic component of human reasoning [36]. Likewise, un-
derstanding similarity between images has played essen-
tial roles in many areas of computer vision including image
retrieval [19, 43, 44, 50], face identiÔ¨Åcation [12, 39, 46],
place recognition [4], pose estimation [45], person re-
identiÔ¨Åcation [10, 40], video object tracking [42, 47], lo-
cal feature descriptor learning [25, 58], zero-shot learn-
ing [7, 57], and self-supervised representation learning [52].
Also, the perception of similarity has been achieved by
learning similarity metrics from labeled images, which is
called metric learning.

Recent approaches in metric learning have improved
performance dramatically by adopting deep Convolutional
Neural Networks (CNNs) as their embedding functions.
SpeciÔ¨Åcally, such methods train CNNs to project images
onto a manifold where two examples are close to each
other if they are semantically similar and far apart other-

Figure 1. A conceptual illustration for comparing existing meth-
ods [4, 16, 27, 32, 45] and ours. Each image is labeled by human
pose, and colored in red if its pose similarity to the anchor is high.
(a) Existing methods categorize neighbors into positive and neg-
ative classes, and learn a metric space where positive images are
close to the anchor and negative ones far apart. In such a space,
the distance between a pair of images is not necessarily related to
their semantic similarity since the order and degrees of similarities
between them are disregarded. (b) Our approach allows distance
ratios in the label space to be preserved in the learned metric space
so as to overcome the aforementioned limitation.

wise. While in principle such a metric can be learned using
any type of semantic similarity labels, previous approaches
typically rely on binary labels over image pairs indicating
whether the image pairs are similar or not. In this aspect,
only a small subset of real-world image relations has been
addressed by previous approaches. Indeed, binary similar-
ity labels are not sufÔ¨Åcient to represent sophisticated rela-
tions between images with structured and continuous labels,
such as image captions [30, 35, 56], human poses [3, 21],
camera poses [5, 13], and scene graphs [24, 31]. Met-

12288

ric learning with continuous labels has been addressed
in [4, 16, 27, 32, 45]. Such methods, however, reduce the
problem by quantizing continuous similarity into binary la-
bels (i.e., similar or dissimilar) and applying the existing
metric learning techniques. Therefore, they do not fully ex-
ploit rich similarity information in images with continuous
labels as illustrated in Figure 1(a) and require a careful tun-
ing of parameters for the quantization.

In this paper, we propose a novel method for deep metric
learning to overcome the aforementioned limitations. We
Ô¨Årst design a new triplet loss function that takes full advan-
tage of continuous labels in metric learning. Unlike exist-
ing triplet losses [39, 53, 54] that are interested only in the
equality of class labels or the order of label distances, our
loss aims to preserve ratios of label distances in the learned
embedding space. This allows our model to consider de-
grees of similarities as well as their order and to capture
richer similarity information between images as illustrated
in Figure 1(b).

Current methods construct triplets by sampling a posi-
tive (similar) and a negative (dissimilar) examples to obtain
the binary supervision. Here we propose a new strategy for
triplet sampling. Given a minibatch composed of an anchor
and its neighbors, our method samples every triplet includ-
ing the anchor by choosing every pair of neighbors in the
minibatch. Unlike the conventional approaches, our method
does not need to introduce quantization parameters to cate-
gorize neighbors into the two classes and can utilize more
triplets given the same minibatch.

Our approach can be applied to various problems with
continuous and structured labels. We demonstrate the efÔ¨Å-
cacy of the proposed method on three different image re-
trieval tasks using human poses, room layouts, and image
captions, respectively, as continuous and structured labels.
In all the tasks, our method outperforms the state of the art,
and our new loss and the triplet mining strategy both con-
tribute to the performance boost. Moreover, we Ô¨Ånd that our
approach learns a better metric space even with a signif-
icantly lower embedding dimensionality compared to pre-
vious ones. Finally, we show that a CNN trained by our
method with caption similarity can serve as an effective vi-
sual feature for image captioning, and it outperforms an Im-
ageNet pre-trained counterpart in the task.

2. Related Work

In this section, we Ô¨Årst review loss functions and tuple
mining techniques for deep metric learning, then discuss
previous work on metric learning with continuous labels.

2.1. Loss Functions for Deep Metric Learning

Contrastive loss [6, 12, 17] and triplet loss [39, 50, 54]
are standard loss functions for deep metric learning. Given
an image pair, the contrastive loss minimizes their distance

in the embedding space if their classes are the same, and
separates them a Ô¨Åxed margin away otherwise. The triplet
loss takes triplets of anchor, positive, and negative images,
and enforces the distance between the anchor and the posi-
tive to be smaller than that between the anchor and the neg-
ative. One of their extensions is quadruple loss [10, 42],
which considers relations between a quadruple of images
and is formulated as a combination of two triplet losses. A
natural way to generalize the above losses is to use a higher
order relations. For example, n-tuplet loss [41] takes as its
input an anchor, a positive, and n ‚àí 2 negative images, and
jointly optimizes their embedding vectors. Similarly, lifted
structured loss [44] considers all positive and negative pairs
in a minibatch at once by incorporating hard-negative min-
ing functionality within itself. For the same purpose, in [48]
the area of intersection between similarity distributions of
positive and negative pairs are minimized, and in [28, 43]
clustering objectives are adopted for metric learning.

All the aforementioned losses utilize image-level class
labels or their equivalent as supervision. Thus, unlike ours,
it is not straightforward for them to take relations between
continuous and/or structured labels of images into account.

2.2. Techniques for Mining Training Tuples

Since tuples of k images are used in training, the number
of possible tuples increases exponentially with k. The mo-
tivation of mining techniques is that some of such a large
number of tuples do not contribute to training or can even
result in decreased performance. A representative exam-
ple is semi-hard triplet mining [39], which utilizes only
semi-hard triplets for training since easy triplets do not up-
date the network and hardest ones may have been corrupted
due to labeling errors. It also matters how to measure the
hardness. A common strategy [39, 44] is to utilize pair-
wise Euclidean distances in embedding space, e.g., negative
pairs with small Euclidean distances are considered hard.
In [19, 20, 55], an underlying manifold of embedding vec-
tors, which is ignored in Euclidean distances, is taken into
account to improve the effectiveness of mining techniques.
Also, in [57] multiple levels of hardness are captured by a
set of embedding models with different complexities.

Although the above techniques substantially improve the
quality of learned embedding space, they are commonly
based on binary relations between image pairs, thus they are
not directly applicable for metric learning with continuous
labels.

2.3. Metric Learning Using Continuous Labels

There have been several metric learning methods using
data with continuous labels. For example, similarities be-
tween human pose annotations have been used to learn an
image embedding CNN [27, 32, 45]. This pose-aware CNN
then extracts pose information of given image efÔ¨Åciently

2289

Figure 2. The binary quantization strategies and their limitations. The orange circle indicates a rare example dissimilar to most of the
others, and the orange pentagon is a common example similar with a large number of samples. (a) If the quantization is done by a single
distance threshold, populations of positive and negative examples would be signiÔ¨Åcantly imbalanced. (b) In the case of nearest neighbor
search, positive neighbors of a rare example would be dissimilar and negative neighbors of a common example would be too similar.

without explicit pose estimation, which can be transferred to
other tasks relying on pose understanding like action recog-
nition. Also, in [16] caption similarities between image
pairs are used as labels for metric learning, and the learned
embedding space enables image retrieval based on more
comprehensive understanding of image content. Other ex-
amples of continuous labels that have been utilized for met-
ric learning include GPS data for place recognition [4] and
camera frusta for camera relocalization [5].

However, it is hard for the above methods to take full ad-
vantage of continuous labels because they all use conven-
tional metric learning losses based on binary relations. Due
to their loss functions, they quantize continuous similarities
into binary levels through distance thresholding [4, 32, 45]
or nearest neighbor search [16, 27]. Unfortunately, both
strategies are unnatural for continuous metric learning and
have clear limitations as illustrated in Figure 2. Further-
more, it is not straightforward to Ô¨Ånd a proper value for their
quantization parameters since there is no clear boundary be-
tween positive and negative examples whose distances to
the anchors are continuous. To the best of our knowledge,
our work is the Ô¨Årst attempt to directly use continuous labels
for metric learning.

3. Our Framework

To address limitations of existing methods described
above, we propose a new triplet loss called log-ratio loss.
Our loss directly utilizes continuous similarities without
quantization. Moreover, it considers degrees of similari-
ties as well as their rank so that the resulting model can
infer sophisticated similarity relations between continuous
labels. In addition, we present a new, simple yet effective
triplet mining strategy supporting our log-ratio loss since
the existing mining techniques in Section 2.2 cannot be used
together with our loss.

In the following sections, we brieÔ¨Çy review the conven-
tional triplet loss [39] for a clear comparison, then present
details of our log-ratio loss and the new triplet mining tech-
nique.

3.1. Review of Conventional Triplet Loss

The triplet loss takes a triplet of an anchor, a positive,
and a negative image as input. It is designed to penalize
triplets violating the rank constraint, namely, that the dis-
tance between the anchor and the positive must be smaller
than that between the anchor and the negative in the embed-
ding space. The loss is formulated as

‚Ñìtri(a, p, n) = hD(fa, fp) ‚àí D(fa, fn) + Œ¥i+

,

(1)

where f indicates an embedding vector, D(¬∑) means the
squared Euclidean distance, Œ¥ is a margin, and [¬∑]+ denotes
the hinge function. Note that the embedding vectors should
be L2 normalized since, without such a normalization, their
magnitudes tend to diverge and the margin becomes trivial.
For training, gradients of ‚Ñìtri with respect to the embedding
vectors are computed by

‚àÇ‚Ñìtri(a, p, n)

‚àÇfp

‚àÇ‚Ñìtri(a, p, n)

‚àÇfn

‚àÇ‚Ñìtri(a, p, n)

‚àÇfa

= 2(fp ‚àí fa) ¬∑ ‚ú∂(cid:0)‚Ñìtri(a, p, n) > 0(cid:1),
= 2(fa ‚àí fn) ¬∑ ‚ú∂(cid:0)‚Ñìtri(a, p, n) > 0(cid:1),

‚àÇ‚Ñìtri(a, p, n)

‚àÇ‚Ñìtri(a, p, n)

= ‚àí

‚àí

‚àÇfp

‚àÇfn

(2)

(3)

,

(4)

where ‚ú∂ is the indicator function. One may notice that the
gradients only consider the directions between the embed-
ding vectors and the rank constraint violation indicator. If
the rank constraint is satisÔ¨Åed, all the gradients are zero.

3.2. Log ratio Loss

Given a triplet with samples, we propose a log-ratio loss
that aims to approximate the ratio of label distances by the
ratio of distances in the learned embedding space. SpeciÔ¨Å-
cally, we deÔ¨Åne the loss function as

‚Ñìlr(a, i, j) = (cid:26) log

D(fa, fi)
D(fa, fj)

‚àí log

D(ya, yi)

D(ya, yj)(cid:27)2

,

(5)

where f indicates an embedding vector, y is a continuous la-
bel, and D(¬∑) denotes the squared Euclidean distance. Also,

2290

AnchorNeighborsAnchor(a) Thresholding(b) Nearest neighbor searchNeighbors(a, i, j) is a triplet of an anchor a and its two neighbors i
and j without positive-negative separation, unlike p and n
in Eq. (1). By approximating ratios between label distances
instead of the distances themselves, the proposed loss en-
ables to learn a metric space more Ô¨Çexibly regardless of the
scale of the labels.

The main advantage of the log-ratio loss is that it allows
a learned metric space to reÔ¨Çect degrees of label similarities
as well as the rank of them. Ideally, the distance between
two images in the learned metric space will be proportional
to their distance in the label space. Hence, an embedding
network trained with our loss can represent continuous sim-
ilarities between images more thoroughly than those focus-
ing only on the rank of similarities like the triplet loss. This
property of the log-ratio loss can be also explained through
its gradients, which are given by

‚àÇ‚Ñìlr(a, i, j)

‚àÇfi

‚àÇ‚Ñìlr(a, i, j)

‚àÇfj

=

=

(fi ‚àí fa)
D(fa, fi)
(fa ‚àí fj)
D(fa, fj)

¬∑ ‚Ñì‚Ä≤

lr(a, i, j),

¬∑ ‚Ñì‚Ä≤

lr(a, i, j),

(6)

(7)

‚àÇ‚Ñìlr(a, i, j)

‚àÇfa

= ‚àí

‚àÇ‚Ñìlr(a, i, j)

‚àÇfi

‚àí

‚àÇ‚Ñìlr(a, i, j)

‚àÇfj

,

(8)

where ‚Ñì‚Ä≤

lr(a, i, j) is a scalar value computed by

‚Ñì‚Ä≤

lr(a, i, j) = 4(cid:26)log

D(fa, fi)
D(fa, fj)

‚àí log

D(ya, yi)

D(ya, yj)(cid:27) .

(9)

As shown in Eq. (6) and (7), the gradients of the log-ratio
loss are determined not only by the directions between the
embedding vectors but also by ‚Ñì‚Ä≤
lr(a, i, j) that quantiÔ¨Åes the
discrepancy between the distance ratio in the label space
and that in the embedding space. Thus, even when the rank
constraint is satisÔ¨Åed, the magnitudes of the gradients could
be signiÔ¨Åcant if ‚Ñì‚Ä≤
lr(a, i, j) is large. In contrast, the gradients
of the triplet loss in Eq. (2) and (3) become zero under the
same condition.

Another advantage of the log-ratio loss is that it is
parameter-free. Unlike ours, the triplet loss requires the
margin, which is a hyper-parameter tuned manually and
forces embedding vectors to be L2 normalized. Last but
not least, we empirically Ô¨Ånd that the log-ratio loss can out-
perform the triplet loss even with embeddings of a signiÔ¨Å-
cantly lower dimensionality, which enables a more efÔ¨Åcient
and effective image retrieval.

3.3. Dense Triplet Mining

The existing triplet mining methods in Section 2.2 can-
not be used in our framework since they are specialized to
handle images annotated by discrete and categorical labels.
Hence, we design our own triplet mining method that is well
matched with the log-ratio loss.

First of all, we construct a minibatch B of training sam-
ples with an anchor, k nearest neighbors of the anchor in
terms of label distance, and other neighbors randomly sam-
pled from the remaining ones. Note that including near-
est neighbors helps speed up training. Since the label dis-
tance between an anchor and its nearest neighbor is rela-
tively small, triplets with a nearest neighbor sample in gen-
eral induce large log-ratios of label distances in Eq. (9),
which may increase the magnitudes of the associated gra-
dients consequently.

Given a minibatch, we aim to exploit all triplets sharing
the anchor so that our embedding network can observe the
greatest variety of triplets during training. To this end, we
sample triplets by choosing every pair of neighbors (i, j)
in the minibatch and combining them with the anchor a.
Furthermore, since (a, i, j) and (a, j, i) have no difference
in our loss, we choose only (a, i, j) and disregard (a, j, i)
when D(ya, yi) < D(ya, yj) to avoid duplication. We call
the above procedure dense triplet mining. The set of triplets
densely sampled from the minibatch B is then given by

T (B) = (cid:8)(a, i, j) | D(ya, yi) < D(ya, yj),

(10)

i ‚àà B \ {a}, j ‚àà B \ {a}(cid:9).

Note that our dense triplet mining strategy can be com-

bined also with the triplet loss, which is re-formulated as

‚Ñìdense
tri

(a, i, j) = hD(fa, fi) ‚àí D(fa, fj) + Œ¥i+

subject to (a, i, j) ‚àà T (B).

(11)

where the margin Œ¥ is set small compared to that of ‚Ñìtri in
Eq. (1) since the label distance between i and j could be
quite small when they are densely sampled. This dense
triplet loss is a strong baseline of our log-ratio loss. How-
ever, it still requires L2 normalization of embedding vec-
tors and ignores degrees of similarities as the conventional
triplet loss does. Hence, it can be regarded as an interme-
diary between the existing approaches in Section 2.3 and
our whole framework, and will be empirically analyzed for
ablation study in the next section.

4. Experiments

The effectiveness of the proposed framework is validated
on three different image retrieval tasks based on contin-
uous similarities: human pose retrieval on the MPII hu-
man pose dataset [3], room layout retrieval on the LSUN
dataset [59], and caption-aware image retrieval on the MS-
COCO dataset [30]. We also demonstrate that an image
embedding CNN trained with caption similarities through
our framework can be transferred to image captioning as an
effective visual representation.

In the rest of this section, we Ô¨Årst deÔ¨Åne evaluation met-
ric and describe implementation details, then present qual-

2291

itative and quantitative analysis of our approach on the re-
trieval and representation learning tasks.

4.1. Evaluation: Measures and Baselines

Evaluation metrics. Since image labels are continuous
and/or structured in our retrieval tasks, it is not appropri-
ate to evaluate performance based on standard metrics like
Recall@k. Instead, following the protocol in [27], we adopt
two evaluation metrics, mean label distance and a modiÔ¨Åed
version of nDCG [8, 27]. The mean label distance is the
average of distances between queries and retrieved images
in the label space, and a smaller means a better retrieval
quality. The modiÔ¨Åed nDCG considers the rank of retrieved
images as well as their relevance scores, and is deÔ¨Åned as

nDCGK(q) =

1
ZK

K

Xi=1

2ri

log2 (i + 1)

,

(12)

where K is the number of top retrievals of our interest and
ZK is a normalization factor to guarantee that the maximum
value of nDCGK is 1. Also, ri = ‚àí log2 (kyq ‚àí yik2+1)
denotes the relevance between query q and the ith retrieval,
which is discounted by log2 (i + 1) to place a greater em-
phasis on one returned at a higher rank. A higher nDCG
means a better retrieval quality.
Common baselines. In the three retrieval tasks, our method
is compared with its variants for ablation study. These ap-
proaches are denoted by combinations of loss function L
and triplet mining strategy M , where Log-ratio is our log-
ratio loss, Triplet means the triplet loss, Dense denotes the
dense triplet mining, and Binary indicates the triplet min-
ing based on binary quantization. SpeciÔ¨Åcally, M (Binary)
is implemented by nearest neighbor search, where 30 neigh-
bors closest to anchor are regarded as positive. Our model is
then represented as L(Log-ratio)+M (Dense). We also com-
pare our model with the same network trained with the mar-
gin based loss and distance weighted sampling [55], a state-
of-the-art approach in conventional metric learning. Fi-
nally, we present scores of Oracle and ImageNet pretrained
ResNet-34 as upper and lower performance bounds. Note
that nDCG of Oracle is always 1.

4.2. Implementation Details

Datasets. For the human pose retrieval, we directly adopt
the dataset and setting of [27]. Among in total 22,285 full-
body pose images, 12,366 images are used for training and
9,919 for testing, while 1,919 images among the test set are
used as queries for retrieval. For the room layout retrieval,
we adopt the LSUN room layout dataset [59] that contains
4,000 training images and 394 validation images of 11 lay-
out classes. Since we are interested in continuous and Ô¨Åne-
grained labels only, we use only 1,996 images of the 5th
layout class, which is the class with the largest number of

images. Among them 1,808 images are used for training
and 188 for testing, in which 30 images are employed as
queries. Finally, for the caption-aware image retrieval, the
MS-COCO 2014 caption dataset [30] is used. We follow the
Karpathy split [22], where 113,287 images are prepared for
training and 5,000 images for validation and testing, respec-
tively. The retrieval test is conducted only on the testing set,
where 500 images are used as queries.
Preprocessing and data augmentation. For the human
pose retrieval, we directly adopt the data augmentation tech-
niques used in [27]. For the room layout retrieval, the im-
ages are resized to 224 √ó 224 for both training and testing,
and Ô¨Çipped horizontally at random during training. For the
caption-aware retrieval, images are jittered in both scale and
location, cropped to 224 √ó 224, and Ô¨Çipped horizontally at
random during training. Meanwhile, test images are simply
resized to 256 √ó 256 and cropped at center to 224 √ó 224.
Embedding networks and their training. For the human
pose and room layout retrieval, we choose ResNet-34 [18]
as our backbone network and append a 128-D FC layer on
top for embedding. They are optimized by the SGD with
learning rate 10‚àí2 and exponential decay for 15 epochs. For
the caption-aware image retrieval, ResNet-101 [18] with a
1,024 dimensional embedding layer is adopted since cap-
tions usually contain more comprehensive information than
human poses and room layouts. This network is optimized
by the ADAM [23] with learning rate 5 ¬∑ 10‚àí6 for 5 epochs.
All the networks are implemented in PyTorch [34] and pre-
trained on ImageNet [38] before being Ô¨Ånetuned.
Hyper-parameters. The size of minibatch is set to 150 for
the human pose, 100 for the room layout, and 50 for the
caption-aware image retrieval, respectively. On the other
hand, k, the number of nearest neighbors in the minibatch
for the dense triplet mining, is set to 5 for all experiments.
For the common baselines, the margin Œ¥ of the conventional
triplet loss is set to 0.2 and that of the dense triplet loss 0.03.

4.3. Human Pose Retrieval

The goal of human pose retrieval is to search for images
similar with query in terms of human poses they exhibit.
Following [27], the distance between two poses is deÔ¨Åned
as the sum of Euclidean distances between body-joint lo-
cations. Our model is compared with the previous pose re-
trieval model called thin-slicing [27] and a CNN for explicit
pose estimation [11] as well as the common baselines.

Quantitative evaluation results of these approaches are
summarized in Figure 3(a), where our model clearly out-
performs all the others. In addition, through comparisons
between ours and its two variants L(Triplet)+M (Dense) and
L(Triplet)+M (Binary), it is demonstrated that both of our
log-ratio loss and the dense triplet mining contribute to the
improvement. Qualitative examples of human pose retrieval
are presented in Figure 4. Our model and thin-slicing over-

2292

Figure 3. Quantitative evaluation of the three retrieval tasks in terms of mean label distance (top) and mean nDCG (bottom).

Our models

ÔÄÄ(Log-ratio) + ÔÄÄ(Dense)  128-D
ÔÄÄ(Log-ratio) + ÔÄÄ(Dense)  64-D
ÔÄÄ(Log-ratio) + ÔÄÄ(Dense)  32-D
ÔÄÄ(Log-ratio) + ÔÄÄ(Dense)  16-D
BaselinesÔÄÄ(Triplet) + ÔÄÄ(Dense)  128-D
ÔÄÄ(Triplet) + ÔÄÄ(Dense)  64-D
ÔÄÄ(Triplet) + ÔÄÄ(Dense)  32-D
ÔÄÄ(Triplet) + ÔÄÄ(Dense)  16-D

Figure 5. Performance versus embedding dimensionality.

that of L(Triplet)+M (Dense) is reduced signiÔ¨Åcantly. Con-
sequently, the 16-D embedding of our model outperforms
128-D embedding of L(Triplet)+M (Dense). This result
demonstrates the superior quality of the embedding space
learned by our log-ratio loss.

4.4. Room Layout Retrieval

The goal of this task is to retrieve images whose 3-D
room layouts are most similar with that of query image,
with no explicit layout estimation in test time. We de-
Ô¨Åne the distance between two rooms i and j in terms of
their room layouts as 1 ‚àí mIoU(Ri, Rj), where R denotes
the groundtruth room segmentation map and mIoU denotes
mean Intersection-over-Union.

Since this paper is the Ô¨Årst attempt to tackle the room lay-
out retrieval task, we compare our approach only with the
common baselines. As shown quantitatively in Figure 3(b),
the advantage of the dense triplet mining is not signiÔ¨Åcant in

2293

Figure 4. Qualitative results of human pose retrieval.

all successfully retrieve images exhibiting similar human
poses with queries, while ResNet-34 focuses mostly on ob-
ject classes and background components. Moreover, ours
tends to capture subtle characteristics of human poses (e.g.,
bending left-arms in Figure 4(b)) and handle rare queries
(e.g., Figure 4(e)) better than thin-slicing.

Finally, we evaluate the human pose retrieval perfor-
mance by varying embedding dimensionality to show how
much effective our embedding space is. As illustrated in
Figure 5, when decreasing the embedding dimensionality
to 16, the performance of our model drops marginally while

(a) Human pose retrieval(b) Room layout retrieval(c) Caption-aware image retrievalùêø(Triplet) + ùëÄ(Binary)ùêø(Triplet) + ùëÄ(Dense)Thin-slicing + ùëÄ(Dense)Chen& Yuille [11]Baselines for pose retrievalOur modelùêø(Log-ratio) + ùëÄ(Dense)Common baselinesImageNet pretrainedMargin based loss [55] OracleThin-slicing [27]OracleQueryThin-slicingResNet34(a)Ours(b)(c)(d)(e)Figure 6. Qualitative results of room layout retrieval. For an easier evaluation, the retrieved images are blended with their groundtruth
masks, and their mIoU scores are reported together. Binary Tri.: L(Triplet)+M (Binary). ImgNet: ImageNet pretraiend ResNet101.

this task, probably because room layout labels of the train-
ing images are diverse and sparse so that it is not straight-
forward to sample triplets densely. Nevertheless, our model
outperforms all the baselines by a noticeable margin thanks
to the effectiveness of our log-ratio loss.

Qualitative results of the room layout retrieval are illus-
trated in Figure 6. As in the case of the pose retrieval, re-
sults of the ImageNet pretrained model are frequently af-
fected by object classes irrelevant to room layouts (e.g., bed
in Figure 6(b) and sofa in Figure 6(d)), while those of our
approach are accurate and robust against such distractors.

4.5. Caption aware Image Retrieval

An image caption describes image content thoroughly. It
is not a simple combination of object classes, but involves
richer information including their numbers, actions, interac-
tions, relative locations. Thus, using caption similarities as
supervision allows our model to learn image relations based
on comprehensive image understanding.

Motivated by this, we address the caption-aware image
retrieval task, which aims to retrieve images described by
most similar captions with query. To deÔ¨Åne a caption-aware
image distance, we adopt a sentence distance metric called
Word Mover‚Äôs Distance (WMD) [26]. Let W (x, y) be the
WMD between two captions x and y. As each image in our
target dataset [30] has 5 captions, we compute the distance
between two caption sets X and Y through WMD by

W (X, Y ) = Xx‚ààX

min
y‚ààY

W (x, y) + Xy‚ààY

min
x‚ààX

W (x, y). (13)

We train our model and the common baselines with the

WMD labels. As shown in Figure 3(c), our model out-
performs all the baselines, and both of the log-ratio loss
and the dense triplet mining clearly contribute to the per-
formance boost, while the improvement is moderate due to
the difÔ¨Åculty of the task itself. As illustrated in Figure 7, our
model successfully retrieves images that contain high-level
image content described by queries like object-object inter-
actions (e.g., person-umbrella in Figure 7(a)), object actions
(e.g.,holding something in Figure 7(b,d)), and speciÔ¨Åc ob-
jects of interest (e.g., hydrant in Figure 7(c)). In contrast,
the two baselines in Figure 7 often fail to retrieve relevant
images, especially those for actions and interactions.

4.6. Representation Learning for Image Captioning

An ImageNet pretrained CNN has been widely adopted
as an initial or Ô¨Åxed visual feature extractor in many im-
age captioning models [9, 14, 37, 51]. As shown in Fig-
ure 7, however, similarities between image pairs in the Im-
ageNet feature space do not guarantee their caption similar-
ities. One way to further improve image captioning quality
would be exploiting caption labels for learning a visual rep-
resentation specialized to image captioning.

We are motivated by the above observation, and be-
lieve that a CNN learned with caption similarities through
our continuous metric learning framework can be a way to
implement the idea. To this end, we adopt our caption-
aware retrieval model described in Section 4.5 as an initial,
caption-aware visual feature extractor of two image cap-
tioning networks: Att2all2 [37] and Topdown [2]. SpeciÔ¨Å-
cally, our caption-aware feature extractor is compared with
the ImageNet pretrained baseline of ours, and (14 √ó 14 √ó

2294

Query0.9020.9390.7920.8330.823Ours0.9020.7920.8330.8630.737Binary Tri.0.6400.7440.8200.6730.636ImgNet0.8160.8410.8110.7320.801Ours0.6600.5730.7660.8160.615Binary Tri.0.4890.6360.5440.4810,573ImgNetTop-5 Retrievals(a)(c)0.8630.7570.8420.8360.822Ours0.7430.8630.6170.6090.757Binary Tri.0.6140.7270.6440.5370.590ImgNet0.9070.9130.9110.7280.821Ours0.6690.7410.7340.7090.802Binary Tri.0.7280.4560.6040.7820.873ImgNetQueryTop-5 Retrievals(d)(b)Figure 7. Qualitative results of caption-aware image retrieval. Binary Tri.: L(Triplet)+M (Binary). ImgNet: ImageNet pretraiend ResNet101.

Model

Train

ATT

TD

Img

Cap

Img

Cap

XE
RL
XE
RL
XE
RL
XE
RL

B4

0.3302
0.3348
0.3402
0.3465
0.3421
0.3573
0.3479
0.3623

C

1.029
1.131
1.052
1.159
1.087
1.201
1.097
1.213

M

0.2585
0.2630
0.2608
0.2673
0.2691
0.2739
0.2707
0.2758

R

0.5456
0.5565
0.5504
0.5613
0.5543
0.5699
0.5573
0.5718

S

0.192
0.1965
0.1942
0.2010
0.2011
0.2085
0.2012
0.2107

Table 1. Captioning performance on the Karpathy test split [22].
We report scores obtained by a single model with the beam search
algorithm (beam size = 2). ATT: Att2all2 [37]. TD: Topdown [2].
Img: ImageNet pretrained feature. Cap: Caption-aware feature.
XE: Pretrained with cross-entropy. RL: Finetuned by reinforce-
ment learning. B4: BLEU-4 [33]. C: CIDEr-D [49]. M: ME-
TEOR [15]. R: ROGUE-L [29]. S: SPICE [1].

2048) average pooled outputs of their last convolution lay-
ers are utilized as caption-aware and ImageNet pretrained
features. For training the two captioning networks, we di-
rectly follow the training scheme proposed in [37], which
Ô¨Årst pretrains the networks with cross-entropy (XE) loss
then Ô¨Ånetunes them using reinforcement learning (RL) with
the CIDEr-D [49] metric.

Table 1 quantitatively summarizes captioning perfor-
mance of the ImageNet pretrained feature and our caption-
aware feature. The scores of reproduced baseline are similar
or higher than those reported in its original paper. Nonethe-
less, our caption-aware feature consistently outperforms the
baseline in all evaluation metrics and for both of two cap-
tioning models. Also, qualitative examples of captions gen-
erated by the models in Table 1 are presented in Figure 8,
where baselines generate incorrect captions while the mod-

Figure 8. Captions generated by the Topdown attention [2]. GT:
groundtruth caption.
Img: ImageNet pretrained feature. Cap:
Caption-aware feature. XE: Pretrained with cross-entropy. RL:
Finetuned by reinforcement learning.

els based on our caption-aware feature avoid choosing the
wrong word and generate better captions.

5. Conclusion

We have presented a novel loss and tuple mining strat-
egy for deep metric learning using continuous labels. Our
approach has achieved impressive performance on three dif-
ferent image retrieval tasks with continuous labels using hu-
man poses, room layouts and image captions. Moreover, we
have shown that our framework can be used to learn visual
representation with continuous labels. In the future, we will
explore the effect of label distance metrics and a hard tuple
mining technique for continuous metric learning to further
improve the quality of learned metric space.

Acknowledgement: This work was supported by Basic Science
Research Program and R&D program for Advanced Integrated-
intelligence for IDentiÔ¨Åcation through the National Research
Foundation of Korea funded by the Ministry of Science, ICT
(NRF-2018R1C1B6001223, NRF-2018R1A5A1060031, NRF-
2018M3E3A1057306, NRF-2017R1E1A1A01077999), and by
the Louis Vuitton - ENS Chair on ArtiÔ¨Åcial Intelligence.

2295

QueryOursBinary Tri.ImgNetOursBinary Tri.ImgNetTop-5 Retrievals(a)(c)OursBinary Tri.ImgNetOursBinary Tri.ImgNetQueryTop-5 Retrievals(d)(b)GT1: a tennis player swingingthe rackets towards the ballGT2: a man swingshis acket to hita tennis ballImg XE:  a tennis player in a red shirt is playingtennisCap XE:  a tennis player swinging a racket at a ball Img RL: a man holdinga tennis ball on a tennis court Cap RL: a man hittinga tennis ball with a tennis racket References

[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice:
Semantic propositional image caption evaluation. In Proc.
European Conference on Computer Vision (ECCV), pages
382‚Äì398. Springer, 2016. 8

[2] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down attention
for image captioning and visual question answering. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 7, 8

[3] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d
human pose estimation: New benchmark and state of the art
analysis. In Proc. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2014. 1, 4

[4] R. Arandjelovi¬¥c, P. Gronat, A. Torii, T. Pajdla, and J. Sivic.
NetVLAD: CNN architecture for weakly supervised place
recognition. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016. 1, 2, 3

[5] V. Balntas, S. Li, and V. Prisacariu. Relocnet: Continuous
metric learning relocalisation using neural nets. In Proc. Eu-
ropean Conference on Computer Vision (ECCV), 2018. 1,
3

[6] J. Bromley, I. Guyon, Y. Lecun, E. Sckinger, and R. Shah.
Signature veriÔ¨Åcation using a ‚Äùsiamese‚Äù time delay neural
network.
In Proc. Neural Information Processing Systems
(NIPS), 1994. 2

[7] M. Bucher, S. Herbin, and F. Jurie. Improving semantic em-
bedding consistency by metric learning for zero-shot classi-
Ô¨Åcation. In Proc. European Conference on Computer Vision
(ECCV), 2016. 1

[8] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using gra-
dient descent. In Proc. International Conference on Machine
Learning (ICML), 2005. 5

[9] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and
T.-S. Chua. Sca-cnn: Spatial and channel-wise attention
in convolutional networks for image captioning.
In 2017
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 6298‚Äì6306. IEEE, 2017. 7

[10] W. Chen, X. Chen, J. Zhang, and K. Huang. Beyond triplet
loss: A deep quadruplet network for person re-identiÔ¨Åcation.
In Proc. IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 1, 2

[11] X. Chen and A. Yuille. Articulated pose estimation by a
graphical model with image dependent pairwise relations. In
Proc. Neural Information Processing Systems (NIPS), 2014.
5

[12] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriÔ¨Åcation.
In Proc. IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2005. 1, 2

[13] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Niessner. Scannet: Richly-annotated 3d reconstruc-
tions of indoor scenes. In Proc. IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2017. 1

[14] B. Dai and D. Lin. Contrastive learning for image caption-
ing. In Advances in Neural Information Processing Systems,
pages 898‚Äì907, 2017. 7

[15] M. Denkowski and A. Lavie. Meteor universal: Language

In
speciÔ¨Åc translation evaluation for any target language.
Proceedings of the ninth workshop on statistical machine
translation, 2014. 8

[16] A. Gordo and D. Larlus. Beyond instance-level image re-
trieval: Leveraging captions to learn a global visual repre-
sentation for semantic retrieval. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2017.
1, 2, 3

[17] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In Proc. IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2006. 2

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proc. IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2016. 5
[19] C. Huang, C. C. Loy, and X. Tang. Local similarity-aware
deep feature embedding. In Proc. Neural Information Pro-
cessing Systems (NIPS), 2016. 1, 2

[20] A. Iscen, G. Tolias, Y. Avrithis, and O. Chum. Mining on
In Proc. IEEE
manifolds: Metric learning without labels.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 2

[21] S. Johnson and M. Everingham. Learning effective human
pose estimation from inaccurate annotation. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2011. 1

[22] A.

Karpathy.

Neuraltalk2.

https://github.com/karpathy/neuraltalk2. 5, 8

[23] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In Proc. International Conference on Learning
Representations (ICLR), 2015. 5

[24] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bern-
stein, and L. Fei-Fei. Visual genome: Connecting language
and vision using crowdsourced dense image annotations. In-
ternational Journal of Computer Vision (IJCV), 2017. 1

[25] V. Kumar B G, G. Carneiro, and I. Reid. Learning local
image descriptors with deep siamese and triplet convolu-
tional networks by minimising global loss functions. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 1

[26] M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger.
From word embeddings to document distances. In Proc. In-
ternational Conference on Machine Learning (ICML), 2015.
7

[27] S. Kwak, M. Cho, and I. Laptev. Thin-slicing for pose:
Learning to understand pose without explicit pose estima-
tion.
In Proc. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016. 1, 2, 3, 5

[28] M. T. Law, R. Urtasun, and R. S. Zemel. Deep spectral clus-
In Proc. International Conference on Ma-

tering learning.
chine Learning (ICML), 2017. 2

[29] C.-Y. Lin. Rouge: A package for automatic evaluation of

summaries. Text Summarization Branches Out, 2004. 8

[30] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll¬¥ar, and C. L. Zitnick. Microsoft COCO: com-
mon objects in context.
In Proc. European Conference on
Computer Vision (ECCV), 2014. 1, 4, 5, 7

[31] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual rela-

2296

tionship detection with language priors. In Proc. European
Conference on Computer Vision (ECCV), 2016. 1

tion using the multibatch method. In Proc. Neural Informa-
tion Processing Systems (NIPS), 2016. 1

[32] G. Mori, C. Pantofaru, N. Kothari, T. Leung, G. Toderici,
A. Toshev, and W. Yang. Pose embeddings: A deep archi-
tecture for learning to match human poses. arXiv preprint
arXiv:1507.00302, 2015. 1, 2, 3

[33] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a
method for automatic evaluation of machine translation. In
Proceedings of the 40th annual meeting on association for
computational linguistics, 2002. 8

[34] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Au-
tomatic differentiation in pytorch. In AutoDiff, NIPS Work-
shop, 2017. 5

[35] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. In Proc. IEEE International Conference
on Computer Vision (ICCV), 2015. 1

[36] W. V. Quine. Ontological relativity, and other essays.

Columbia University Press, New York, 1969. 1

[37] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.
Self-critical sequence training for image captioning. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 7, 8

[38] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), pages 1‚Äì42, April 2015. 5

[39] F. Schroff, D. Kalenichenko, and J. Philbin. FaceNet: A uni-
Ô¨Åed embedding for face recognition and clustering. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2015. 1, 2, 3

[40] H. Shi, Y. Yang, X. Zhu, S. Liao, Z. Lei, W. Zheng, and S. Z.
Li. Embedding deep metric for person re-identiÔ¨Åcation: A
study against large variations. In Proc. European Conference
on Computer Vision (ECCV), 2016. 1

[41] K. Sohn. Improved deep metric learning with multi-class n-
pair loss objective. In Proc. Neural Information Processing
Systems (NIPS), 2016. 2

[42] J. Son, M. Baek, M. Cho, and B. Han. Multi-object track-
ing with quadruplet convolutional neural networks. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2

[43] H. O. Song, S. Jegelka, V. Rathod, and K. Murphy. Deep
metric learning via facility location. In Proc. IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2017. 1, 2

[44] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep met-
ric learning via lifted structured feature embedding. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 1, 2

[45] O. Sumer, T. Dencker, and B. Ommer. Self-supervised learn-
ing of pose embeddings from spatiotemporal relations in
videos.
In Proc. IEEE International Conference on Com-
puter Vision (ICCV), 2017. 1, 2, 3

[46] O. Tadmor, T. Rosenwein, S. Shalev-Shwartz, Y. Wexler, and
A. Shashua. Learning a metric embedding for face recogni-

[47] R. Tao, E. Gavves, and A. W. M. Smeulders. Siamese in-
stance search for tracking.
In Proc. IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016. 1
[48] E. Ustinova and V. Lempitsky. Learning deep embeddings
with histogram loss. In Proc. Neural Information Processing
Systems (NIPS), 2016. 2

[49] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
Consensus-based image description evaluation.
In Proc.
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2015. 8

[50] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,
J. Philbin, B. Chen, and Y. Wu. Learning Ô¨Åne-grained im-
age similarity with deep ranking. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2014.
1, 2

[51] L. Wang, A. Schwing, and S. Lazebnik. Diverse and accurate
image description using a variational auto-encoder with an
additive gaussian encoding space.
In Advances in Neural
Information Processing Systems, pages 5756‚Äì5766, 2017. 7
[52] X. Wang and A. Gupta. Unsupervised learning of visual rep-
resentations using videos. In Proc. IEEE International Con-
ference on Computer Vision (ICCV), 2015. 1

[53] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric
learning for large margin nearest neighbor classiÔ¨Åcation. In
Proc. Neural Information Processing Systems (NIPS), 2006.
2

[54] P. Wohlhart and V. Lepetit. Learning descriptors for object
recognition and 3d pose estimation. In Proc. IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2015. 2

[55] C.-Y. Wu, R. Manmatha, A. J. Smola, and P. Krahenbuhl.
Sampling matters in deep embedding learning. In Proc. IEEE
International Conference on Computer Vision (ICCV), 2017.
2, 5

[56] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From im-
age descriptions to visual denotations: New similarity met-
rics for semantic inference over event descriptions. Transac-
tions of the Association for Computational Linguistics, 2014.
1

[57] Y. Yuan, K. Yang, and C. Zhang. Hard-aware deeply cas-
caded embedding. In Proc. IEEE International Conference
on Computer Vision (ICCV), 2017. 1, 2

[58] S. Zagoruyko and N. Komodakis. Learning to compare im-
age patches via convolutional neural networks.
In Proc.
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2015. 1

[59] Y. Zhang, F. Yu, S. Song, P. Xu, A. Seff, and J. Xiao. Large-
scale scene understanding challenge: Room layout estima-
tion. 4, 5

2297

