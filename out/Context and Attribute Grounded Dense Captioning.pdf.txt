Context and Attribute Grounded Dense Captioning

Guojun Yin1, Lu Sheng2,4, Bin Liu1‚àó, Nenghai Yu1, Xiaogang Wang2, Jing Shao3

1University of Science and Technology of China, Key Laboratory of Electromagnetic Space Information,
The Chinese Academy of Sciences, 2CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong,

3SenseTime Research, 4College of Software, Beihang University

gjyin@mail.ustc.edu.cn, lsheng@buaa.edu.cn,

{flowice,ynh}@ustc.edu.cn, xgwang@ee.cuhk.edu.hk, shaojing@sensetime.com

Abstract

Dense captioning aims at simultaneously localizing se-
mantic regions and describing these regions-of-interest
(ROIs) with short phrases or sentences in natural language.
Previous studies have shown remarkable progresses, but
they are often vulnerable to the aperture problem that a cap-
tion generated by the features inside one ROI lacks contex-
tual coherence with its surrounding context in the input im-
age. In this work, we investigate contextual reasoning based
on multi-scale message propagations from the neighboring
contents to the target ROIs. To this end, we design a novel
end-to-end context and attribute grounded dense captioning
framework consisting of 1) a contextual visual mining mod-
ule and 2) a multi-level attribute grounded description gen-
eration module. Knowing that captions often co-occur with
the linguistic attributes (such as who, what and where), we
also incorporate an auxiliary supervision from hierarchi-
cal linguistic attributes to augment the distinctiveness of the
learned captions. Extensive experiments and ablation stud-
ies on Visual Genome dataset demonstrate the superiority
of the proposed model in comparison to the state-of-the-art
methods.

1. Introduction

Dense captioning, which was Ô¨Årst introduced by [20],
is to parse semantic contents in an input image and describe
them with captions in natural languages. It can beneÔ¨Åt other
tasks, including image captioning [38], segmentation [28],
visual question answering [14] and etc. In this paper, we
mainly focus on the caption generation and adopt Faster
RCNN [29] for semantic instances localization, following
recent advances [20, 34].

Differing from subjective image descriptions for high-
level abstraction of an entire image, captions of semantic

‚àóBin Liu is the corresponding author.

Figure 1. Dense captioning with different levels of contextual in-
teractions: (i) without any contextual cues (marked by blue) [20],
(ii) with guidance from the global cue (marked by red) [34], and
(iii) with mutual interactions from neighboring (marked by or-
ange) and global visual information. (Best viewed in color.)

instances in compact bounding boxes are far more objective
and less affected by ambiguities raised by subjective anno-
tations. That is, incorrect captions may be generated when
the target regions are visually ambiguous without contex-
tual reasoning. For example, it may falsely caption the tar-
get ROIs marked in blue-box as ‚Äúyellow balloons‚Äù rather
than ‚Äúyellow pants‚Äù in Fig. 1(a-i), if not aware of their con-
textual visual contents [20]. An alternative solution pro-
posed in [34] try to exploit the global feature from the entire
image as the contextual cue to improve the region caption-
ing. However, the descriptions sometimes are corrupted by
global appearance, especially for small and unusual objects
against dominant global contents. The ‚Äúyellow pants‚Äù in
Fig. 1 (a-ii) is mistakenly described as ‚Äúyellow kite in the
sky‚Äù. The similar phenomenon happens in Fig. 1 (a-ii) that
it mistakenly generates ‚Äúa mirror‚Äù rather than ‚Äúa lamp‚Äù.

In contrast to the prior arts, in this study, we show
that the innovative model, named as Context and Attribute
Grounded Network (CAG-Net), designed with contextual
correlated visual cues (i.e., local, neighboring, global) per-
mits multi-scale contextual message passing to reinforce re-
gional description generation. For example, the neighbor-

43216241

a white stonea lamp on the walla mirror of a motorcycleyellow pants of the skiera yellow kite in the sky yellow balloons (ii)(i)(iii)(b)(a)(ii)(i)(iii)‚Ä¶‚Ä¶target [20]target+ global [34]target+ global+ neighboringtarget [20]target+ global [34]target+ global+ neighboringing ROIs marked in warm-box in Fig. 1(a-iii), semantically
connecting to the visual features in the target in blue-box in
Fig. 1(a-i), provide more valuable hints that the target is a
‚Äúyellow pants‚Äù belonging to a skier. Such contextual learn-
ing has shown its remarkable potential in other tasks includ-
ing object detection, segmentation and retrieval. However,
the learning of contextual representation, and how it can
effectively function on dense captioning, remains an open
problem. SpeciÔ¨Åcally, the proposed CAG-Net consists of
two vital modules:

1) Contextual Feature Extractor, establishing a non-local
similarity graph for the feature interaction between the tar-
get ROI and its neighboring ROIs based on their feature
afÔ¨Ånity and spatial nearness, allows adaptive contextual in-
formation sharing from multiple adjacent ROIs (i.e., global
and neighbors) to interact with the target ROI.

2) Attribute-Grounded Caption Generator adopts LSTM
as the fundamental unit and fuses contextual features to gen-
erate the description for the target ROI. To reinforce the
coarse-to-Ô¨Åne structure of description generation, we adopt
coarse-level and Ô¨Åned-level linguistic attribute losses as the
additional supervision respectively at the sequential LSTM
cells. Without sequential restrictions from the ground-truth
captions, such keywords or attributes are more recognizable
by the content in the target ROI, and thus own a more stable
discriminative power for the extraction of visual patterns.
To some extent, it is similar with the visual attributes of ob-
jects in multi-label classiÔ¨Åcation.

Our contributions are listed as follows:

1) We design a context and attribute grounded dense cap-
tioning model that permits multi-scale (i.e., local, neigh-
boring, global) contextual information sharing and message
passing, where the knowledge integration is built on a non-
local similarity graph among instances in the input image.

2) A coarse-to-Ô¨Åne linguistic attribute supervision is pro-
posed to enhance the discriminativeness of the generated
captions, in which the ground-truth hierarchical linguistic
attributes are matched to the predicted keywords through a
novel coarse-to-Ô¨Åne manner.

3) Extensive experiments demonstrate the effectiveness of
the proposed CAG-Net model on the challenging large-
scale VG dataset.

2. Related Work

Image captioning to describe a general image with nat-
ural language was explored in recent years [5, 26, 12, 30,
2, 27, 35, 25]. The works [5, 36, 1, 6, 19, 7] focused on
improve iÔ¨Åguremage captioning by the attention-embedded
features generated by an additional attention model. Based
on the attention model, Gu et al. [15] adopted a coarse-
to-Ô¨Åne framework which increased the model complexity
gradually with increasingly reÔ¨Åned attention weights for

image captioning. In our work, dense image captioning ren-
ders individual captions for different ROIs in the image. As
for dense captioning, we Ô¨Årstly adopt the multi-scale fea-
ture interaction and attribute grounded generation for ac-
curate region descriptions. Our coarse-to-Ô¨Åne strategy is
based on the hierarchical attribute supervisions rather than
the different attention inputs of the description generation
modules [15]. The previous works [38, 36] adopted the at-
tributes (the words in the vocabulary) to train an additional
model for another input of the LSTM cells for the descrip-
tion generation. Differing from that, our work adopts the
linguistic attributes as the auxiliary supervision for coarse-
to-Ô¨Åne generation without any external branches or input.

Dense Image Captioning. Dense image captioning is sup-
posed to not only localize the regions of interest in the im-
age but also generate descriptions with natural language,
which was Ô¨Årst proposed in [20]. Johnson et al. [20] in-
troduced a new dense localization layer, which was fully
differentiable and used bilinear interpolation to smoothly
extract the activations inside each region. Yang et al. [34]
exploited more accurate localization for regions by joint in-
ference of localization and description for a given region
proposal, while the global feature of the image was used as
the contextual cues to improve region captioning. However,
these previous works did not capture the relative features of
different regions and valid message passing between con-
textual regions for accurate region captioning.

Contextual Learning. Contextual learning was employed
in various topics in recent years [32, 24, 22, 39, 33, 10,
9], e.g., object detection, segmentation, and retrieval. For
both detection and segmentation, learning feature represen-
tations from a global view rather than the located object
itself has been proven effective by [37, 28]. Gkioxari et
al. [13] used more than one region proposals for action
recognition while Hu et al. [17] processed a set of objects si-
multaneously through interaction between their appearance
feature and geometry, thus allowing modeling of their re-
lations for object detection. As for contextual learning for
image captioning, Yao et al. [35] computed the probabil-
ity distribution on all the semantic relation classes for each
object pair with the learnt visual relation classiÔ¨Åer to estab-
lish the semantic graph for image captioning. Contextual
feature learning among the located regions for dense cap-
tioning has never been explored in the previous works. In
our work, we establish contextual message passing module
without additional branches or any auxiliary relation labels.

3. Context and Attribute Grounded Dense

Captioning (CAG-Net)

In this paper, we propose a novel end-to-end dense im-
age captioning framework, named as Context and Attribute
Grounded Dense Captioning (CAG-Net). As shown in

43226242

Figure 2. The architecture of CAG-Net. The multi-scale features are generated by the proposed Contextual Feature Extractor after region
proposals. Then the local (in blue) feature of the target region and multi-scale context cues, i.e., global (in red) and neighboring (in orange),
broadcast into the Attribute Grounded Caption Generator for region captioning in parallel. The Ô¨Ånal descriptions of the target region are
generated jointly by the hierarchical structures trained with the auxiliary attribute losses.

Attributes, hierarchically upon the outputs of the sequen-
tial LSTM cells, as in Fig. 2. The proposed model is trained
to both minimize the sentence loss and the binary cross-
entropy losses (attribute losses) for caption generation.

3.1. Contextual Feature Extractor

Figure 3. An example of Contextual Feature Extractor for the
target proposal.
(left) The similarity graph between target (in
blue) proposal and contextual neighboring (in orange) proposals
are generated considering both spatial conÔ¨Åguration and appear-
ance similarity. (right) The neighboring feature are obtained by
fusing the contextual neighboring proposals with the similarity
graph. Best viewed in color.

Fig. 2, we Ô¨Årst learn visual features of the input image by
a CNN model as the way adopted by Faster RCNN [29],
and obtain the semantic features. Such semantic features
are used to generate a set of candidate regions (ROIs) by
a Region Proposal Network (RPN) [29]. Based on these
ROI features, we introduce a Contextual Feature Extrac-
(CFE) which generates the global, neighboring and
tor
local (i.e., target itself) cues (Sec. 3.1). The neighboring
cues are collected by establishing a similarity graph be-
tween the target ROI and the neighboring ROIs, shown in
Fig. 3. The multi-scale contextual cues, broadcast in par-
allel, are fused by an Attribute Grounded Caption Gen-
erator (AGCG) which employs multiple LSTM [16] cells
(Sec. 3.2). To generate rich and Ô¨Åne-grained descriptions
and reinforce the coarse-to-Ô¨Åne procedure of description
generation, we adopt an auxiliary supervision, Linguistic

i, Fn

i , and Fg

i , respectively, where Fg

Denote the regions-of-interest (ROIs) in an image as
R = {Ri|i = 1, 2, ..., N } and the entire image as R‚àó.
The contextual features for the local region Ri are from the
multi-scale contextual cues of local region Ri, neighboring
i = R/Ri, and the global region R‚àó. For the
region Rn
target region Ri, denote the local, neighboring and global
features as Fl
i refers
to the features extracted from the entire input image and Fl
i
is the feature of the target instance. The Contextual Fea-
ture Extractor (CFE) focuses on exploring the neighboring
features Fn
i ).
We design a region-level similarity graph (i.e., ROI-
level) for neighboring ROIs aggregation, inspired by pixel-
level non-local operations. Non-local means [4] has been
often used as a Ô¨Ålter by computing a weighted mean of all
pixels in an image, which allows pixels to contribute to the
Ô¨Åltered response based on the patch appearance similarity.
Similarly, neighboring ROIs with similar semantic appear-
ance are supposed to contribute more on the feature extrac-
tion for the target local instance. Following the operation
in [4], we rewrite the formulation of f (Ri, Rn

i which can be formulated as Fn

i = f (Ri, Rn

i ) as

f (Ri, Rn

i ) = X

‚àÄj,j6=i

G(Fl

i, Fl

j)Fl
j,

(1)

where G(Fl
gion Ri and Rj , and Fl

j) is the appearance similarity between re-
i is the Ô¨Åxed-length local feature of

i, Fl

43236243

A young surfer is standing on a surfboard.Input ImageneighborglobaltargetSemantic Feature CuboidContextual Feature ExtractorAttribute Grounded Caption GeneratorResultsCoarse-level Attribute LossesFined-level Attribute Lossespersonstandon‚óè‚óè‚óèsurferstandingon‚óè‚óè‚óèLSTM cellFusionTargetNeighborGlobalSimilarity Graph‚Ä¶Œ±1Œ±2Œ±K‚Ä¶Neighboring ROIsTarget ROIFigure 4. Comparisons between different network structures. (a) L generates the descriptions separately after region proposals; (b) L +
G generates descriptions with not only the local feature but also the global feature of the image; (c) L + G + N (CCI) integrates global,
neighboring and local information for the target to generate descriptions; (d) CAG-Net by multiple LSTM cells is a stacked version of (c)
CCI but supervised with hierarchical linguistic attribute losses.

such that the closer ROIs are more relative to the target ROI.
We sort ROIs in Rn
i based on the IoU (intersection over
union) metric with the target region Ri. By sampling the
top-k proposals as ÀÜRn
i , the calculation of the neighboring
features can be accelerated as Fn

i = f (Ri, ÀÜRn
i ).

3.2. Attribute Grounded Caption Generator

We present a novel caption generator with two parts: (1)
a Contextual Cue Integrator to fuse contextual features pro-
duced by the CFE in Sec. 3.1, and (2) an Attribute Grounded
Coarse-to-Fine Generator with coarse-level and Ô¨Åned-level
linguistic attribute losses as the additional supervision to en-
hance the discriminativeness of the generated captions.

Contextual Cue Integrator (CCI) - The contextual cue in-
tegrator adopts multiple LSTM cells to hierarchically inte-
grate the multi-scale contextual features into the localized
features. The local, neighboring and global features are
spread through the LSTM cells so as to generate context-
aware descriptions for the target ROI. These descriptions
are fused together for the Ô¨Ånal captioning of the target re-
gion at each time step of LSTM. The unrolled contextual
cue integration module is shown in Fig. 5(a). The local
branch is regarded as the backbone for the target and the
global and neighboring branches are grouped as multi-scale
contextual cues to provide complementary guidances. Thus,
the contextual cues are adaptively combined at Ô¨Årst, and
they are then added to the local branch via a second adap-
tive fusion, as shown in Fig. 4(c). The importance of differ-
ent levels‚Äô features is regularized by the adaptive weights,
which are optimized during training the framework.

Attribute Grounded Coarse-to-Fine Generator - It is
challenging in generating rich and accurate descriptions just
by the sequential LSTMs. To this end, we increase its repre-
sentative power by introducing a coarse-to-Ô¨Åne caption gen-
eration procedure with sequential LSTM cells, i.e., coarse
stage and reÔ¨Åned stage supervised with the auxiliary hierar-
chical linguistic attribute losses.

The linguistic attribute losses serve as the intermediate
and auxiliary supervisions from coarse to Ô¨Åne in addition
to the general sentence loss of captioning, implemented at

43246244

Figure 5. The unrolled structure of Contextual Cue Integrator
(CCI). (a) Unrolled structure integrates the local (in blue) informa-
tion and multi-scale context cues, i.e., global (in red) and neigh-
boring (in orange). The hollow circle stands for the LSTM cell
while the plus sign for the feature fusion brieÔ¨Çy. (b) The caption-
ing loss consists of a sentence loss and an attribute loss.

region Ri. The similarity G is the normalized cross correla-
tion based on Gaussian function, formulated as,

G(Fl

i, Fl

j) =

‚ä§

exp(Fl
i

Fl
j)
‚ä§
P‚àÄj,j6=i exp(Fl

i

,

Fl
j)

(2)

‚ä§

Fl

where Fl
j is dot-product similarity of cross correlation.
i
Therefore, we can obtain the similarity graph for each target
ROI with its neighboring ROIs in the image.

General object detection algorithm usually generates re-
dundant region candidates (ROIs) to ensure the accuracy
and robustness in region localization and detection. How-
ever, in this case, the integrated neighboring feature Fn
i will
be contaminated by distant and independent proposals, and
the amount of ROIs in Rn
i also tremendously increase the
computation cost and noises in the environment. Therefore,
we sample a subset of Rn
i based on their spatial nearness

(b) L + G(d) CAG-Net(c)  L + G + N ( CCI )(a) LA‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óèyoungEOSsurfer(a)  Unrolled Structure(b) Captioning LossA young surfer is standing on the surfboard.Attribute LossA young surfer stands on the surfboard.Sentence Losssurfersurfboardstandingonyoung‚óè‚óè‚óèsurfersurfboardstandsonyoung‚óè‚óè‚óèGround TruthPredictionsFigure 6. Illustration of sentence itemization. Fined-level attributes A1: the original sentences of training annotations (bottom) are item-
ized to individual words and divided into four groups: object/scene (noun), attribute/status (adjective), interaction/action (verb) and re-
lation/spatially (preposition). Coarse-level attributes A2: the individual words are normalized and clustered by semantical similarity for
high-level words, e.g., the girl and man in A1 belong to person in A2.

each stage as shown in Fig. 2. The attribute losses are for-
matted as binary classiÔ¨Åcation (i.e., exist or not) losses for
each attribute separately during the training procedure. As
shown in Fig. 5 (b), the attributes, e.g., surfer, standing,
young and on, will be measured individually regardless of
the speech order, similar as the multi-label classiÔ¨Åcation for
attribute recognition of objects.

The subsequent LSTM layer (refined stage) is sup-
posed to serve as the Ô¨Åne-grained decoders for the coarse
regional descriptions generated by the preceding one
(coarse stage). The hidden vectors of LSTM cells pro-
duced by the coarse stage are taken as the disambiguating
cues to the reÔ¨Åned stage. The outputs of global and neigh-
boring branches at the coarse stage are used as the inputs
of the respective branches directly at the reÔ¨Åned stage. The
adaptive fusion of these three branches at the coarse stage
is fed as the input at the reÔ¨Åned stage. Meanwhile, these
vectors are used for coarse-level attribute prediction. The
connection of the branches at the multiple stages is shown
in the Fig. 4(d). The Ô¨Ånal outputs of the word decoder at the
reÔ¨Åned stage are the generated descriptions for the target re-
gion. Meanwhile, these outputs are used for the Ô¨Åned-level
attribute prediction as well.

These linguistic attributes are predicted from the out-
puts of the LSTMs during the training procedure and the
unsolved problem here is how to get the ground-truth lin-
guistic attributes. In our work, the hierarchical linguistic at-
tributes are obtained by itemizing the sentences in the train-
ing split with natural language processing toolkit (NLTK).

1) Fined-level attributes A1 for refined stage. We distill
the linguistic knowledge from the training annotations (sen-
tences or phrases) to individual keywords/attributes, by the
speech toolkit from NLTK , as shown in Fig. 6. The refer-
ence sentences are parsed into four groups by the part-of-
speech, i.e., nouns, adjectives, verbs and prepositions from
the following aspects respectively: (1) The noun words are
usually the labels of objects or scenes, e.g., person, bus,
sidewalk and etc.; (2) adjectives represent attributes or sta-

tus, e.g., young, black; (3) verbs are meanings of actions
or interactions, e.g., standing, talks; (4) prepositions for re-
lations or spatiality, e.g., behind. The Ô¨Åned-level attributes
like surfer and standing are used at the latter stage for the
exact information extraction.
2) Coarse-level attributes A2 for coarse stage. We use
the high-level semantically clustered attributes, e.g., per-
son, stand to stand for the major information. We observe
that labels with the same concept may have different sin-
gular and plural forms or different participles, e.g., persons
versus person, talks versus talking. These words are nor-
malized to a uniÔ¨Åed format by NLTK Lemmatizer, e.g., talk
from talks and talking. Furthermore, labels having closer
semantic correlation (e.g., girl and man are hyponyms of
person) need to be distinguished from other semantic con-
cepts like cloth, as shown in the top panel of Fig. 6. There-
fore, we cluster the labels with their semantical similarities
computed by Leacock-Chodorow distance [31]. We Ô¨Ånd a
threshold of 0.85 is well-suited for splitting semantic con-
cepts. The coarse-level items like person and stand are used
at the preceding stage for the key information extraction.

4. Experiments

4.1. Experiment Settings

Visual Genome (VG)

Dataset.
region captioning
dataset [21] is used as the evaluation benchmark in our ex-
periments. For fair comparisons, we use the dataset of ver-
sion 1.0 and the same train/validation/test splits as in [20],
i.e., 77398 images for training and 5000 images each as-
signed for validation and test.
Evaluation Metric. Following [20], the mean Average Pre-
cision (mAP) are measured across a range of thresholds
for both accurate localization and language description, in-
spired by the evaluation metrics in object detection [11, 23]
and image captioning [3]. For localization, intersection
over union (IoU) thresholds .3, .4, .5, .6, .7 are used while
METEOR [3] score thresholds 0, .05, .1, .15, .2, .25 used

43256245

The person wears white shirt.The younger girl has  short hair.Persons are talking on the sidewalk.. . .   The young man is standing behind bus.A man talks to the girl.personObject/ Scene girlpersonsman. . .   . . .   shirt. . .   personclothyoungerAttributes/ Statusyoung. . .   . . .   white. . .   short. . .   youngwhiteshorttalksInteraction/ Action talking. . .   . . .   has. . .   standing. . .   talkhavestandRelation/ Spatialityon. . .   behind. . .   to. . .   onbehindtoCoarse-levelAttributesùíú2. . .   . . .   . . .   Training Annotations(Sentences)Fined-levelAttributesùíú1for language similarity. The average precision is measured
across all pairwise settings, i.e., (IoU, METEOR), of these
methods and report the mean AP (mAP), which means the
mAP is computed for different IoU thresholds for localiza-
tion accuracy, and different METEOR score thresholds for
language similarity, then averaged as the Ô¨Ånal score.

To isolate the accuracy of language in the predicted cap-
tions without localization, the predicted captions are eval-
uated neglecting their spatial positions. Following [20],
the references of each prediction are generated by merg-
ing ground truth across each image into a bag of reference
sentences. Apart from the mAP score introduced above, the
METEOR score will be reported as the auxiliary evalua-
tion metric, denoted as METEOR. Note that the references
from all regions in an image only offer the global and coarse
ground truth descriptions.

Implementation Details. We use VGG-16 [21] pretrained
on ImageNet [8] as the network backbone.
In Fig.2, we
use 6 LSTM cells in total, i.e., one LSTM for local, neigh-
boring, global features respectively at each stage. The
newly-introduced layers and LSTM cells are randomly ini-
tialized and our proposed CAG-Net is end-to-end trained.
The implementations are based on Faster RCNN [29] using
Caffe [18], and the networks are optimized via stochastic
gradient descent (SGD) with base learning rate as 0.001.
The input image is re-sized to have a longer side of 720 pix-
els and 256 proposals are sampled per image in each for-
ward pass of training. The LSTM cell for sequential model-
ing has 512 hidden nodes. The most 10, 000 frequent words
in the training annotations are remained as the vocabulary
and other words are collapsed into a special <UNK> token
under the same conditions as in [34]. Following [20], we
discard all sentences with more than 10 words (7% of anno-
tations), that is the time length of the LSTMs is 10.

The losses of our framework are from two aspects: 1)
Location: Smooth ‚Ñì1 loss for bounding box regression
(Lbbox) and softmax loss for binary foreground/background
classiÔ¨Åer (Lcls), 2) Caption: Cross entropy loss of sentences
for description generation (Lsent), following [34] and bi-
nary cross entropy loss for linguistic attribute recognition
(Lattr) . The total loss function is L = Lsent + Œ±Lbbox +
Œ≤Lcls + Œ≥Lattr, where Œ± = 0.1, Œ≤ = 0.1 and Œ≥ = 0.01 in
our experiments with the empirical values.

In evaluation, we follow the settings of [20] for fair com-
parisons. 300 proposals with the highest predicted conÔ¨Å-
dence are remained after non-maximum suppression (NMS)
with IoU threshold 0.7. We use efÔ¨Åcient beam-1 search to
produce region descriptions, where the word with the high-
est conÔ¨Ådence is selected at each time step. With another
round NMS with IoU threshold 0.3, the remaining regions
and their generated descriptions are used for the Ô¨Ånal eval-
uation. To establish an upper bound regardless of region
proposals, we evaluate the models on ground truth bound-

Methods

CAG-Net

T-LSTM [34]

FCLN [20]

RPN

METEOR

0.279
0.275
0.273

mAP
10.51
9.31
5.39

GT

METEOR

0.316
0.307
0.305

mAP
36.29
33.58
27.03

Table 1. Quantitative results on Visual Genome comparing with
state-of-the-art methods, T-LSTM [34] and FLCN [20]. Results in
bold indicate the best performance. The metrics on T-LSTM, i.e.,
METEOR, are not provided in the paper and we measure these
metrics using the model provided by the authors.

Methods

CAG-Net

L+G+N

mAP

RPN
GT

10.51
36.29

9.55
33.50

L+G
7.97
31.77

L

6.31
26.70

Table 2. Ablation study on CAG-Net compared the variants of the
contextual cue integration module, i.e., 1) L, local cue without
neighboring nor global features, 2) L+G, local and global cue in-
tegration and 3) L+G+N, local, global and neighboring integration
without stacking contextual cue integration modules. Results in
bold indicate the best performance.

ing boxes as well, marked as GT in the tables.

4.2. Comparison with State of the Art Methods

We quantitatively compare the performances of the pro-
posed Context and Attribute Grounding Dense Captioning
(CAG-Net) model with the previous state-of-the-arts, i.e.,
FCLN[20] and T-LSTM [34]. FCLN [20] introduces a fully
differentiable layer for dense localization. The captioning
per region is generated solitarily without any message pass-
ing from the contextual features. T-LSTM [34] designs net-
work structures that incorporate two novel parts: joint in-
ference for accurate localization and context fusion with the
global scene for accurate description regardless of the inter-
actions among the relative regions.

The comparison experiments use the same settings as the
prior arts, shown in Tab. 1. The CAG-Net signiÔ¨Åcantly out-
performs these methods by achieving a gain on mAP score
from 9.31% to 10.51% using RPN and from 33.58% to
36.29% using the ground truth bounding boxes compared
to the previous state-of-the-art, T-LSTM [34]. The per-
formance gains are mainly from the beneÔ¨Åts of attribute
grounded coarse-to-Ô¨Åne description generation using the
contextual feature extractor and message integration among
the regions. The proposed CAG-Net presents a strong ca-
pability in capturing the correlation among relative regions
and generating more accurate descriptions.

It is observed that the METEOR scores of different meth-
ods are approximate while the mAP scores have a large
margin. That is because that the METEOR score for the
predicted caption is calculated by using all ground truth de-
scriptions of all the regions in the image as the references.
These references are coarse and may not be accurate for a
certain region. In the following ablation study (Sec. 5), we
mainly focus on the comparison of mAP scores.

43266246

Figure 7. Qualitative results of CAG-Net compared with variants of different module conÔ¨Ågurations on VG dataset, i.e., (a) L(Local Cue),
(b) L+G (Local and Global Integration), (c) L+G+N (CCI) (Local, Global and Neighboring Integration).

5. Ablation Study

5.1. CAG Net

Attribute Grounded Caption Generator with Contex-
tual Cues. To demonstrate the beneÔ¨Åts of multi-scale con-
texts and attribute grounded captioning module, we com-
pare the results of CAG-Net in Fig. 4 (d) with the variants by
removing individual cue step by step, i.e., 1) L, local cue as
the baseline without either contextual neighboring or global
features as shown in Fig. 4 (a), 2) L+G, local and global cue
integration without contextual neighboring cues in Fig. 4
(b) and 3) L+G+N, local, global and neighboring integra-
tion without stacking contextual cue integration modules in
Fig. 4 (c), deÔ¨Åned as CCI in Sec 3.2. The quantitative results
are reported in Tab. 2.

Compare with basic L, the mAP of L+G+N can be im-
proved from 6.31% to 9.55% using RPN and from 26.70%
to 33.50% using ground truth boxes by involving contextual
feature extractor and message integration while the mAP of
L+G which only includes the global cues achieves 7.97%
using RPN and 31.77% using ground truth bounding boxes.
The signiÔ¨Åcant improvement demonstrates the importances
of contextual cue integration between multi-scale contexts
and individual regions for region generation and the con-
textual cues, i.e., global and neighboring make a certain
contribution to improving the Ô¨Ånal performances. Further-
more, with the assistance of the linguistic attribute losses,
the mAP of CAG-Net achieves 10.51% in mAP using RPN
by a gain of 0.96% compared to L+G+N (CCI) while a gain
of 1.79% using the ground truth bounding boxes. No doubt
that the generated descriptions are more accurate and rich
for the regions when adopting attribute grounded coarse-to-
Ô¨Åne captioning module.

The qualitative results are shown in Fig. 7. The descrip-
tions directly generated by the target regions are fallible due
to lack of enough visual information, e.g., mistaking the
baseball glove for a brown bag, the apple pieces for a white

CAG-Net

mAP

Methods

9.59
33.78

9.95
35.02

9.99
35.17

9.93
34.98

RPN 10.51
36.29
GT

(A2, A1)(A1, A1) (A2, A2) (1k, 1k) (‚àí, ‚àí) CCI
9.55
33.50
Table 3. Ablation study on CAG-Net compared the variants of
linguistic attribute losses, i.e., 1) (A2, A1), with the proposed
coarse-to-Ô¨Åne attributes, 2) (A1, A1), only with the Ô¨Åned-level
attributes A1, 3) (A2, A2), only with the coarse-level attributes
A2, 4) (1k, 1k), replacing the proposed attributes with the top 1k
attributes, 5) (‚àí, ‚àí), stacked structure without any attributes, 6)
CCI, just one stage without attributes. Results in bold are the best.

cake and the steel basket for black bench. The involved
global cues of the image also lead to deviation, e.g., the
tv in the room is mistakenly predicted as a wooden door,
although positive effect sometimes, e.g., the glove is ac-
curately predicted with the assistance of the global image
feature. Furthermore, the coarse-to-Ô¨Åne generation module
will reinforce more rich descriptions a black tv on the table
compared with individual module a black tv shown in the
Ô¨Ågure. The results shows the excellent performance of the
proposed context and attribute grounded generation struc-
ture for dense captioning.
Linguistic Attribute Losses. To demonstrate the beneÔ¨Åts
of the proposed linguistic attribute losses, we compare the
performances of CAG-Net with the variants of linguistic at-
tributes by 1) ‚Äú(‚àí, ‚àí)‚Äù, removing all the auxiliary linguistic
attribute losses in the framework, 2) ‚Äú(A1, A1)‚Äù, only with
the Ô¨Åned-level attributes A1 at two stages, 3) ‚Äú(A2, A2)‚Äù,
only with the coarse-level attributes A2 at two stages, 4)
‚Äú(1k,1k)‚Äù, replacing the proposed linguistic attributes with
the top 1k attributes (the top 1k most frequent words in the
vocabulary) at two stages.

The results are shown in Tab. 3 and CAG-Net with the
proposed coarse-to-Ô¨Åne linguistic attributes is denoted as
‚Äú(A2, A1)‚Äù. Compared with ‚ÄúCCI‚Äù, CAG-Net without any
attributes (denoted as ‚Äú(‚àí, ‚àí)‚Äù in Tab.3) gets the approx-
imate results because the navie stacking description gen-

43276247

person are in a row.people are in the fieldtwo people are riding horsespeople riding horses in the fieldthe riders are riding on the horsesLL + GL + G +N (CCI)CAG-NetGround Trutha black bencha metal fencea metal racka metal basket a steel basketa white cakea white frostinga donut with pink frostingpieces of applesliced up apple piecesa brown baga baseball glovea brown baseball glovea brown glove of the playera brown glove of the pitchera black boxa wooden  doora black tva black tv on the tablea black tv on the deskmAP

Methods

FC
8.132

9.144
33.411

Random Nearest

RPN 8.626
GT
32.274

AVE MAX
8.024
7.981

SG
9.315
33.412 30.272 29.937 30.121
Table 4. Results of Contextual Feature Extractor with different
settings. ‚ÄúRandom‚Äù means selecting the contextual neighboring
regions randomly from all the regions in the image. ‚ÄúNearest‚Äù
means selecting the relative regions from the nearest ones sorted
by the IoU scores. ‚ÄúSG‚Äù means fusing these neighboring regions
with similarity graph. ‚ÄúFC‚Äù means fusing k-sorted neighboring
regions with fully connected layer. ‚ÄúAVE‚Äù means average-pooling
of k-sorted neighboring regions. ‚ÄúMAX‚Äù means max-pooling of
k-sorted neighboring regions.

eration modules cannot signiÔ¨Åcantly improve the perfor-
mance although with more parameters. In contrast, the at-
tribute grounded structure with the proposed coarse-to-Ô¨Åne
attributes can achieve a gain from 9.59% to 10.51%(using
RPN) and from 33.78% to 36.29% (using ground truth
boxes) because of the auxiliary hierarchical supervision of
the proposed linguistic attribute losses. Furthermore, to
evaluate the effectiveness of the coarse-to-Ô¨Åne structure, we
compare CAG-Net, i.e., ‚Äú(A2, A1)‚Äù, with the variants of
different linguistic attributes, i.e., ‚Äò(A1, A1)‚Äù, ‚Äú(A2, A2)‚Äù
and ‚Äú(1k, 1k)‚Äù. Without the coarse-to-Ô¨Åne strategy at two
stages, the stacked structures with different attributes can-
not achieve as good performance as CAG-Net both using
RPN and using ground truth bounding boxes. It is signif-
icant that the proposed linguistic attribute losses from the
coarse to Ô¨Åne stage can improve the description generation
of target regions.

5.2. Contextual Feature Extractor

In this section, we compare the performances of Contex-
tual Feature Extractor (CFE) with variants by changing one
of the hyper-parameters or settings step by step to explore
the best practice of the proposed contextual feature extrac-
tor. As for the generation structure, we use CCI instead of
CAG-Net due to the faster speed and less computation cost.
Contextual Feature Extractor of k-nearest neighboring
regions performs best. To explore the beneÔ¨Åts of similarity
graph in Contextual Feature Extractor in our framework, we
replace the similarity graph in the CCI shown in Fig. 4 (c)
with 1) ‚ÄúFC‚Äù, the fully-connected layer, 2) ‚ÄúMAX‚Äù, max-
pooling layer, 3) ‚ÄúAVE‚Äù, average-pooling layer after con-
catenating all the feature vectors of k neighboring regions.
The results are shown in Tab. 4. The similarity graph oper-
ation can improve all the evaluation metrics compared with
the simple fully-connected/ max-pooling/ average-pooling
operation after concatenating all the feature vectors of k
neighboring regions. That‚Äôs because the similarity graph
not only includes the visual features of k neighboring re-
gions but also utilizes the relation between the target re-
gion and the neighboring region. Furthermore, Tab. 4 shows

k

mAP

20

10

100
8.749
33.260 33.412 33.411 33.389 33.089

8.804

9.144

9.109

30

50

RPN 8.915
GT

Table 5. Results with different numbers of k-nearest regions
for neighboring features in the Contextual Feature Extractor.
The results are reported when hyper-parameter k is set as
10, 20, 30, 50, 100 respectively.

that the nearest-neighbor regions (‚ÄúNearest‚Äù) perform better
than the regions randomly-selected from all the regions in
the image (‚ÄúRandom‚Äù) due to more correlated regions in-
volved in the description generation.
Contextual Feature Extractor with hyper-parameter
k = 20 outperforms others. The number of neighbor-
ing regions is worth investigating because it can be used
to Ô¨Ånd a trade-off between the effective message passing
and the noises from non-correlated proposals in the im-
age. We validate the number of neighboring regions among
10, 20, 30, 50 and 100 of CCI. The results are reported in
Tab. 5. We adopt k as 20 for further experiments for the
best performance (9.144%) on mAP considering region lo-
calization and description jointly.

6. Conclusion

In this paper, we propose a novel end-to-end frame-
work for dense captioning, named as Context and Attribute
Grounded Dense Captioning (CAG-Net) by utilizing the vi-
sual information of both the target region and multi-scale
contextual cues, i.e., global and neighboring. The proposed
contextual feature extractor exploits the message passing
between the target region and k-nearest neighboring regions
in the image while the attribute grounded contextual cue
integration modules reinforce rich and accurate description
generation. To enhance the description generation for the
regions, we extract linguistic attributes from the reference
sentences as the auxiliary supervision at each stage during
the training process. Extensive experiments demonstrate
the respective effectiveness and signiÔ¨Åcance of the proposed
CAG-Net on the challenging large-scale VG dataset.

Acknowledgment This work is supported in part by the
National Natural Science Foundation of China (Grant
No. 61371192), the Key Laboratory Foundation of the
Chinese Academy of Sciences (CXJJ-17S044) and the
Fundamental Research Funds for the Central Universities
(WK2100330002, WK3480000005), in part by SenseTime
Group Limited,
the General Research Fund sponsored
by the Research Grants Council of Hong Kong (Nos.
CUHK14213616, CUHK14206114, CUHK14205615,
CUHK14203015,
CUHK419412,
CUHK14207-814, CUHK14208417, CUHK14202217),
the Hong Kong Innovation and Technology Support
Program (No.ITS/121/15FX).

CUHK14239816,

43286248

References

[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In CVPR, 2018.

[2] Jyoti Aneja, Aditya Deshpande, and Alexander G. Schwing.

Convolutional image captioning. In CVPR, June 2018.

[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R.
Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. In ACM MM,
2014.

[19] Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, and Tong
Zhang. Recurrent fusion network for image captioning.
In The European Conference on Computer Vision (ECCV),
September 2018.

[3] S. Banerjee and A. Lavie. Meteor: An automatic metric for
mt evaluation with improved correlation with human judg-
ments. In ACL workshop, 2005.

[20] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning. In
CVPR, 2016.

[4] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm

for image denoising. In CVPR, 2005.

[5] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian
Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and
channel-wise attention in convolutional networks for image
captioning. In CVPR, July 2017.

[6] Shi Chen and Qi Zhao. Boosted attention: Leveraging hu-
man attention for image captioning. In The European Con-
ference on Computer Vision (ECCV), September 2018.

[7] Tianlang Chen, Zhongping Zhang, Quanzeng You, Chen
Fang, Zhaowen Wang, Hailin Jin, and Jiebo Luo. ‚Äúfactual‚Äù or
‚Äúemotional‚Äù: Stylized image captioning with adaptive learn-
ing and attention. In The European Conference on Computer
Vision (ECCV), September 2018.

[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009.

[9] C. Desai, D. Ramanan, and C. C. Fowlkes. Discriminative
In IJCV, volume 95,

models for multi-class object layout.
pages 1‚Äì12, 2011.

[21] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L. J. Li, D. A. Shamma, M. S. Bern-
stein, and L. Fei-Fei. Visual genome: Connecting language
and vision using crowdsourced dense image annotations. In
IJCV, volume 123, pages 32‚Äì73, 2017.

[22] Y. Li, C. Huang, C. C. Loy, and X. Tang. Human attribute

recognition by deep hierarchical contexts. In ECCV, 2016.

[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014.

[24] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet:
Learning pixel-wise contextual attention for saliency detec-
tion. In CVPR, 2018.

[25] Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, and Xi-
aogang Wang. Show, tell and discriminate: Image captioning
by self-retrieval with partially labeled data. In The European
Conference on Computer Vision (ECCV), September 2018.

[26] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.
Knowing when to look: Adaptive attention via a visual sen-
tinel for image captioning. In CVPR, July 2017.

[10] Nikita Dvornik, Julien Mairal, and Cordelia Schmid. Mod-
eling visual context is key to augmenting object detection
datasets. In ECCV, 2018.

[27] Ruotian Luo, Brian Price, Scott Cohen, and Gregory
Shakhnarovich. Discriminability objective for training de-
scriptive captions. In CVPR, 2018.

[11] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. In IJCV, volume 88, pages 303‚Äì338.
Springer, 2010.

[12] Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth
Tran, Jianfeng Gao, Lawrence Carin, and Li Deng. Semantic
compositional networks for visual captioning. In CVPR, July
2017.

[13] G. Gkioxari, R. Girshick, and J. Malik. Contextual action

recognition with R* CNN. In ICCV, 2015.

[14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answer-
ing. In CVPR, 2017.

[15] Jiuxiang Gu, Jianfei Cai, Gang Wang, and Tsuhan Chen.
Stack-captioning: Coarse-to-Ô¨Åne learning for image caption-
ing. In AAAI, 2018.

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory.
In Neural computation, volume 9, pages 1735‚Äì1780. MIT
Press, 1997.

[28] R. Mottaghi, X. Chen, X. Liu, N. Cho, S. W. Lee, S. Fidler,
R. Urtasun, and A. Yuille. The role of context for object
detection and semantic segmentation in the wild. In CVPR,
2014.

[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015.

[30] Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and Li-
Jia Li. Deep reinforcement learning-based image captioning
with embedding reward. In CVPR, July 2017.

[31] N. Seco, T. Veale, and J. Hayes. An intrinsic information
content metric for semantic similarity in wordnet.
In Pro-
ceedings of the 16th European conference on artiÔ¨Åcial intel-
ligence, 2004.

[32] Luming Tang, Yexiang Xue, Di Chen, and Carla P Gomes.
Multi-entity dependence learning with rich context via con-
ditional variational auto-encoder. In AAAI, 2018.

[33] Yunchao Wei, Zhiqiang Shen, Bowen Cheng, Honghui Shi,
Jinjun Xiong, Jiashi Feng, and Thomas Huang.
Ts2c:
Tight box mining with surrounding segmentation context for
weakly supervised object detection. In ECCV, 2018.

[17] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks

[34] L. Yang, K. Tang, J. Yang, and L.-J. Li. Dense captioning

for object detection. In CVPR, 2018.

with joint inference and visual context. In CVPR, 2017.

43296249

[35] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring

visual relationship for image captioning. In ECCV, 2018.

[36] T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei. Boosting image

captioning with attributes. In ICCV, 2017.

[37] Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang
Wang, Jing Shao, and Chen Change Loy. Zoom-net: Mining
deep feature interactions for visual relationship recognition.
In ECCV, 2018.

[38] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image cap-
tioning with semantic attention. In CVPR, pages 4651‚Äì4659,
2016.

[39] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang,
Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Con-
text encoding for semantic segmentation.
In CVPR, June
2018.

43306250

