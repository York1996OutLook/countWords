Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking

Department of Computer and Information Sciences, Temple University, Philadelphia, PA USA

Heng Fan

Haibin Ling‚àó

{hengfan,hbling}temple.edu

Abstract

Recently, the region proposal networks (RPN) have been
combined with the Siamese network for tracking, and shown
excellent accuracy with high efÔ¨Åciency. Nevertheless, previ-
ously proposed one-stage Siamese-RPN trackers degenerate
in presence of similar distractors and large scale variation.
Addressing these issues, we propose a multi-stage tracking
framework, Siamese Cascaded RPN (C-RPN), which con-
sists of a sequence of RPNs cascaded from deep high-level
to shallow low-level layers in a Siamese network. Com-
pared to previous solutions, C-RPN has several advantages:
(1) Each RPN is trained using the outputs of RPN in the
previous stage. Such process stimulates hard negative sam-
pling, resulting in more balanced training samples. Con-
sequently, the RPNs are sequentially more discriminative
in distinguishing difÔ¨Åcult background (i.e., similar distrac-
tors). (2) Multi-level features are fully leveraged through a
novel feature transfer block (FTB) for each RPN, further im-
proving the discriminability of C-RPN using both high-level
semantic and low-level spatial information. (3) With mul-
tiple steps of regressions, C-RPN progressively reÔ¨Ånes the
location and shape of the target in each RPN with adjusted
anchor boxes in the previous stage, which makes localiza-
tion more accurate. C-RPN is trained end-to-end with the
multi-task loss function. In inference, C-RPN is deployed as
it is, without any temporal adaption, for real-time tracking.
In extensive experiments on OTB-2013, OTB-2015, VOT-
2016, VOT-2017, LaSOT and TrackingNet, C-RPN consis-
tently achieves state-of-the-art results and runs in real-time.

1. Introduction

Visual tracking is one of the most fundamental problems
in computer vision, and has a long list of applications such
as robotics, human-machine interaction, intelligent vehicle,
surveillance and so forth. Despite great advances in recent
years, visual tracking remains challenging due to many fac-

‚àóCorresponding author.

GT

C-RPN

Siamese-RPN

Figure 1. Comparisons between one-stage Siamese-RPN [23] and
C-RPN on two challenging sequences: Bolt2 (the top row) with
similar distractors and CarScale (the bottom row) with large scale
changes. We observe that C-RPN can distinguish the target from
distractors, while Siamese-RPN drifts to the background in Bolt2.
In addition, compared to using a single regressor in Siamese-RPN,
multi-regression in C-RPN can better localize the target in pres-
ence of large scale changes in CarScale. Best viewed in color.

tors including occlusion, scale variation, etc.

Recently, Siamese network has drawn great attention in
the tracking community owing to its balanced accuracy and
speed. By formulating object tracking as a matching prob-
lem, Siamese trackers [2, 17, 19, 23, 45, 46, 51, 59] aim to
learn ofÔ¨Çine a generic similarity function from a large set of
videos. Among these methods, the work of [23] proposes a
one-stage Siamese-RPN for tracking by introducing the re-
gional proposal networks (RPN), originally used for object
detection [38], into Siamese network. With the proposal
extraction by RPN, this approach simultaneously performs
classiÔ¨Åcation and localization from multiple scales, achiev-
ing excellent performance. Besides, the use of RPN avoids
applying the time-consuming pyramid for target scale esti-
mation [2], leading to a super real-time solution.

1.1. Problem and Motivation

Despite having achieved promising result, Siamese-RPN
may drift to the background especially in presence of sim-
ilar semantic distractors (see Fig. 1). We identify two rea-
sons accounting for this.

First, the distribution of training samples is imbalanced:

7952

(1) positive samples are far less than negative samples, lead-
ing to ineffective training of the Siamese network; and (2)
most negative samples are easy negatives (non-similar non-
semantic background) that contribute little useful informa-
tion in learning a discriminative classiÔ¨Åer [29]. As a con-
sequence, the classiÔ¨Åer is dominated by the easily classiÔ¨Åed
background samples, and degrades when encountering dif-
Ô¨Åcult similar semantic distractors.

Second, low-level spatial features are not fully explored.
In Siamese-RPN (and other Siamese trackers), only features
of the last layer, which contain more semantic information,
are explored to distinguish target/background. In tracking,
nevertheless, background distractors and the target may be-
long to the same category, and/or have similar semantic fea-
tures [49]. In such case, the high-level semantic features are
less discriminative in distinguishing target/background.

In addition to the issues above, the one-stage Siamese-
RPN applies a single regressor for target localization using
pre-deÔ¨Åned anchor boxes. These anchors are expected to
work well when having a high overlap with the target. How-
ever, for model-free visual tracking, no prior information re-
garding the target object is known, and it is hard to estimate
how the scale of target changes. Using pre-deÔ¨Åned coarse
anchor boxes in a single step regression is insufÔ¨Åcient for
accurate localization [3, 14] (see again Fig. 1).

The class imbalance issue is addressed in two-stage ob-
ject detectors (e.g., Faster R-CNN [38]). The Ô¨Årst proposal
stage rapidly Ô¨Ålters out most background samples, and then
the second classiÔ¨Åcation stage adopts sampling heuristics
such as a Ô¨Åxed foreground-to-background ratio to maintain
a manageable balance between foreground and background.
In addition, two steps of regressions achieve accurate local-
ization even for objects with extreme shapes.

Inspired by the two-stage detectors, we propose a multi-
stage tracking framework by cascading a sequence of RPNs
to solve the class imbalance problem, and meanwhile fully
explore features across layers for robust visual tracking.

1.2. Contribution

As the Ô¨Årst contribution, we introduce a novel multi-
stage tracking framework, the Siamese Cascaded RPN (C-
RPN), to solve the problem of class imbalance by perform-
ing hard negative sampling [40, 48]. C-RPN consists of a
sequence of RPNs cascaded from the high-level to the low-
level layers in the Siamese network. In each stage (level),
an RPN performs classiÔ¨Åcation and localization, and out-
puts the classiÔ¨Åcation scores and the regression offsets for
the anchor boxes in this stage. The easy negative anchors
are then Ô¨Åltered out, and the rest, treated as hard examples,
are used as training samples for the RPN of the next stage.
Through such process, C-RPN performs stage by stage hard
negative sampling. Consequently, the distributions of train-
ing samples are sequentially more balanced, and the clas-

siÔ¨Åers of RPNs are sequentially more discriminative in dis-
tinguishing more difÔ¨Åcult distractors (see Fig. 1).

Another beneÔ¨Åt of C-RPN is more accurate target local-
ization compared to the one-stage SiamRPN [23]. Instead
of using the pre-deÔ¨Åned coarse anchor boxes in a single re-
gression step, C-RPN consists of multiple steps of regres-
sions due to multiple RPNs. In each stage, the anchor boxes
(including locations and sizes) are adjusted by the regressor,
which provides better initialization for the regressor of next
stage. As a result, C-RPN can progressively reÔ¨Åne the target
bounding box for better localization as shown in Fig. 1.

Leveraging features from different layers in the networks
has been proven to be beneÔ¨Åcial for improving model dis-
criminability [27, 28, 30]. To fully explore both the high-
level semantic and the low-level spatial features for visual
tracking, we make the second contribution by designating
a novel feature transfer block (FTB). Instead of separately
using features from a single layer in one RPN, FTB enables
us to fuse the high-level features into low-level RPN, which
further improves its discriminative power to deal with com-
plex background, resulting in better performance of C-RPN.
Fig. 2 illustrates the framework of C-RPN.

In extensive experiments on six benchmarks, including
OTB-2013 [52], OTB-2015 [53], VOT-2016 [20], VOT-
2017 [21], LaSOT [10] and TrackingNet [34], our C-RPN
achieves the state-of-the-art results and runs in real-time1.

2. Related Work

Visual tracking has been extensively researched in recent
decades. In the following we discuss the most related work,
and refer readers to [25, 41, 54] for recent surveys.

Deep tracking. Inspired by the successes in image classi-
Ô¨Åcation [18, 22], deep convolutional neural network (CNN)
has been introduced into visual tracking and demonstrated
excellent performances [7, 8, 12, 32, 35, 43, 49, 50]. Wang
et al. [50] propose a stacked denoising autoencoder to learn
generic feature representation for object appearance mod-
eling in tracking. Wang et al. [49] present a fully convo-
lutional network tracking approach by transferring the pre-
trained deep features to improve tracking. Ma et al. [32]
apply deep feature for correlation Ô¨Ålter tracking, achieving
remarkable gains. Nam and Han [35] propose a light archi-
tecture of CNN with online update to learn generic feature
for tracking target. Fan and Ling [12] extend this approach
by introducing a recurrent neural network (RNN) to capture
object structure. Song et al. [43] apply adversary learning in
CNN to learn richer representation for tracking. Danelljan
et al. [8] propose continuous convolution Ô¨Ålters for correla-
tion Ô¨Ålter tracking, and later optimize this method in [7].

Siamese tracking. Siamese network has attracted increas-

1The project is at http://www.dabi.temple.edu/Àúhbling/

code/CRPN/crpn.htm

7953

Figure 2. Illustration of the architecture of C-RPN, including a Siamese network for feature extraction and cascaded regional proposal
networks for sequential classiÔ¨Åcations and regressions. The FTB transfers the high-level semantic features for the low-level RPN, and ‚ÄúA‚Äù
represents the set of anchor boxes, which are gradually reÔ¨Åned stage by stage. Best viewed in color.

ing interest for tracking because of its balanced accuracy
and efÔ¨Åciency. Tao et al. [45] use Siamese network to learn
a matching function from videos, then use the Ô¨Åxed match-
ing function to search for the target. Bertinetto et al. [2]
present a fully convolutional Siamese network (SiamFC) for
tracking by measuring the region-wise feature similarity be-
tween the target and the candidate. Owing to its light struc-
ture and without model update, SiamFC runs efÔ¨Åciently at
80 fps. Held et al. [19] propose the GOTURN approach by
learning a motion prediction model with the Siamese net-
work. Valmadre et al. [46] use a Siamese network to learn
the feature representation for correlation Ô¨Ålter tracking. He
et al. [17] introduce a two-fold Siamese network for track-
ing. Later in [16], they improve this two-fold Siamese track-
ing by incorporating angle estimation and spatial mask-
ing. Wang et al. [51] introduce an attention mechanism
into Siamese network to learn a more discriminative met-
ric for tracking. Notably, Li et al. [23] combine Siamese
network with RPN and propose a one-stage Siamese-RPN
tracker, achieving excellent performance. Zhu et al. [59]
utilize more negative samples to improve the Siamese-RPN
tracker. Despite improvement, this approach requires large
extra training data from other domains.
Multi-level features. The features from different layers in
the neural network contain different information. The high-
level feature consists of more abstract semantic cues, while
the low-level layers contains more detailed spatial informa-
tion [30]. It has been proven that tracking can be beneÔ¨Åted
using multi-level features. In [32], Ma et al. separately use
features in three different layers for three correlation mod-
els, and fuse their outputs for the Ô¨Ånal tracking result. Wang
et al. [49] develop two regression models with features from
two layers to distinguish similar semantic distractors.
Cascaded structure. Cascaded structures have been a pop-
ular strategy to improve performance. Viola et al. [48] pro-
pose a boosted cascade of simple feature for efÔ¨Åcient object
detection. Li et al. [24] present a cascaded structure built

on CNN for face detection and achieve powerful discrimi-
native capability with high efÔ¨Åciency. Cai et al. [3] propose
a multi-stage object detection framework, cascade R-CNN,
aiming at high quality detection by sequentially increasing
IoU thresholds. Zhang et al. [55] utilize a cascade to reÔ¨Åne
detection results by adjusting anchors.

Our approach. In this paper, we focus on solving the prob-
lem of class imbalance to improve model discriminability.
Our approach is related but different from the Siamese-RPN
tracker [23], which applies one-stage RPN for classiÔ¨Åcation
and localization and skips the data imbalance problem. In
contrast, our approach cascades a sequence of RPNs to ad-
dress the data imbalance by performing hard negative sam-
pling, and progressively reÔ¨Ånes anchor boxes for better tar-
get localization using multi-regression. Our method is also
related to [32, 49] using multi-level features for tracking.
However, unlike [32, 49] in which multi-level features are
separately used for independent models (i.e., decision-level
fusion), we propose a feature transfer block to fuse the fea-
tures across layers for each RPN (i.e., feature-level fusion),
improving its discriminative power in distinguishing the tar-
get object from complex background.

3. Siamese Cascaded RPN (C-RPN)

In this section, we detail the Siamese Cascaded RPN (re-

ferred to as C-RPN) as shown in Fig. 2.

C-RPN contains two subnetworks: the Siamese network
and the cascaded RPN. The Siamese network is utilized to
extract the features of the target template x and the search
region z. Afterwards, C-RPN receives the features of x and
z for each RPN. Instead of only using the features from one
layer, we apply feature transfer block (FTB) to fuse the fea-
tures from high-level layers for RPN. An RPN simultane-
ously performs classiÔ¨Åcation and localization on the feature
maps of z. Based on the classiÔ¨Åcation scores and regression
offsets, we Ô¨Ålter out easy negative anchors (e.g., an anchor

7954

Sharing WeightsSiamese NetworkRPN(stage 1)Feature Transfer BlockFeature Transfer BlockRPN(stage 3)Feature Transfer BlockFeature Transfer BlockRPN(stage 2)A2A1Cascaded Regional Proposal NetworksA3A4‚Ä¶‚Ä¶‚Ä¶ùúë1(ùê≥)Œ¶1(ùê≥)ùúë2(ùê≥)ùúë3(ùê≥)ùúë3(ùê±)ùúë2(ùê±)ùúë1(ùê±)Œ¶1(ùê≥)Œ¶2(ùê≥)Œ¶2(ùê≥)Œ¶3(ùê≥)Œ¶3(ùê≥)Œ¶1(ùê±)Œ¶1(ùê±)Œ¶2(ùê±)Œ¶2(ùê±)Œ¶3(ùê±)Œ¶3(ùê±)GT

C-RPN

Siamese-RPN

Figure 4. Localization using a single regressor and multiple re-
gressors.The multiple regressors in C-RPN can better handle large
scale changes for more accurate localization. Best viewed in color.

Figure 3. Architecture of RPN. Best viewed in color.

for each anchor can be computed as

with negative conÔ¨Ådence is larger than a preset threshold Œ∏)
and reÔ¨Åne the rest for training RPN in the next stage.

3.1. Siamese Network

As in [2], we adopt the modiÔ¨Åed AlexNet [22] to develop
our Siamese network. The Siamese network comprises two
identical branches, the z-branch and the x-branch, which are
employed to extract features from z and x, respectively (see
Fig. 2). The two branches are designed to share parameters
to ensure the same transformation applied to both z and x,
which is crucial for the similarity metric learning. More
details about the Siamese network can be referred to [2].

Different from [23] that only uses the features from the
last layer of the Siamese network for tracking, we leverage
the features from multiple levels to improve model robust-
ness. For convenience in next, we denote œïi(z) and œïi(x)
as the feature transformations of z and x from the conv-i
layer in the Siamese network with N layers2.

3.2. One Stage RPN in Siamese Network

Before describing C-RPN, we Ô¨Årst review the one-stage
Siamese RPN tracker [23], which consists of two branches
of classiÔ¨Åcation and regression for anchors, as depicted in
Fig. 3. It takes as inputs the feature transformations œï1(z)
and œï1(x) of z and x and outputs classiÔ¨Åcation scores and
regression offsets for anchors. For simplicity, we remove
the subscripts in feature transformations in next.

To ensure classiÔ¨Åcation and regression for each anchor,
two convolution layers are utilized to adjust the channels of
œï(z) into suitable forms, denoted as [œï(z)]cls and [œï(z)]reg,
for classiÔ¨Åcation and regression, respectively. Likewise, we
apply two convolution layers for œï(x) but keep the channels
unchanged, and obtain [œï(x)]cls and [œï(x)]reg. Therefore,
the classiÔ¨Åcation scores {ci} and the regression offsets {ri}

2For notation simplicity, we name each layer in the Siamese network in
an inverse order, i.e., conv-N , conv-(N ‚àí 1), ¬∑ ¬∑ ¬∑ , conv-2, conv-1 for the
low-level to the high-level layers.

{ci} = corr([œï(z)]cls, [œï(x)]cls)
{ri} = corr([œï(z)]reg, [œï(x)]reg)

(1)

where i is the anchor index, and corr(a, b) denotes correla-
tion between a and b where a is served as the kernel. Each
ci is a 2d vector, representing for negative and positive con-
Ô¨Ådences of the ith anchor. Similarly, each ri is a 4d vector
which represents the offsets of center point location and size
of the anchor to groundtruth. Siamese RPN is trained with a
multi-task loss consisting of two parts, i.e., the classiÔ¨Åcation
loss (i.e., softmax loss) and the regression loss (i.e., smooth
L1 loss). We refer readers to [23, 38] for further details.

3.3. Cascaded RPN

As mentioned earlier, previous Siamese trackers mostly
ignore the problem of class imbalance, resulting in degen-
erated performance in presence of similar semantic distrac-
tors. Besides, they only use the high-level semantic features
from the last layer, which does not fully explore multi-level
features. To address these issues, we propose a multi-stage
tracking framework by cascading a set of L (L ‚â§ N ) RPNs.
For RPNl in the lth (1 < l ‚â§ L) stage, it receives fused
features Œ¶l(z) and Œ¶l(x) of conv-l layer and high-level lay-
ers from FTB. The Œ¶l(z) and Œ¶l(x) are obtained as follows,

Œ¶l(z) = FTB(cid:0)Œ¶l‚àí1(z), œïl(z)(cid:1)
Œ¶l(x) = FTB(cid:0)Œ¶l‚àí1(x), œïl(x)(cid:1)

(2)

where FTB(¬∑, ¬∑) denotes FTB as described in Section 3.4.
For RPN1, Œ¶1(z) = œï1(z) and Œ¶1(x) = œï1(x). Therefore,
i} and the regression offsets {rl
the classiÔ¨Åcation scores {cl
i}
for anchors in stage l are calculated as

{cl
{rl

i} = corr([Œ¶l(z)]cls, [Œ¶l(x)]cls)
i} = corr([Œ¶l(z)]reg, [Œ¶l(x)]reg)

(3)

where [Œ¶l(z)]cls, [Œ¶l(x)]cls, [Œ¶l(z)]reg and [Œ¶l(x)]reg are
derived by performing convolutions on Œ¶l(z) and Œ¶l(x).

Let Al denote the anchor set in stage l. With classiÔ¨Åca-
tion scores {cl
i}, we can Ô¨Ålter out anchors in Al whose neg-
ative conÔ¨Ådences are larger than a preset threshold Œ∏, and

7955

Conv.Conv.Conv.Conv.Corr.cls. scoresCorr.reg. scoresclassificationregressionlossùúë(ùê±)ùúë(ùê≥)[ùúëùê≥]ùëêùëôùë†[ùúëùê≥]ùëüùëíùëî[ùúëùê±]ùëüùëíùëî[ùúëùê±]ùëêùëôùë†ùêª‚Ä≤√óùëä‚Ä≤√óùê∂‚Ä≤Œ¶(ùê±)Œ¶(ùê≥)[Œ¶ùê≥]ùëêùëôùë†[Œ¶ùê≥]ùëüùëíùëî[Œ¶ùê±]ùëüùëíùëî[Œ¶ùê±]ùëêùëôùë†‚Ñé√óùë§√ó2ùëò‚Ñé√óùë§√ó4ùëòùúë(ùê±)ùúë(ùê≥)[ùúëùê≥]ùëêùëôùë†[ùúëùê≥]ùëüùëíùëî[ùúëùê±]ùëüùëíùëî[ùúëùê±]ùëêùëôùë†‚Ñé√óùë§√ó2ùëò‚Ñé√óùë§√ó4ùëòloss(a) Region of interest

(b) Response map in stage 1

(c) Response map in stage 2

(d) Response map in stage 3

(a) Region of interest

(b) From left to right: response maps of stage 1, stage 2 and stage 3

Figure 5. Response maps in different stages. Image (a) is the re-
gion of interest, and (b) shows the response maps obtained by RPN
in three stages. We can see that RPN is sequentially more discrim-
inative in distinguishing distractors. Best viewed in color.

the rest are formed into a new set of anchor Al+1, which
is employed for training RPNl+1. For RPN1, A1 is pre-
deÔ¨Åned. Besides, in order to provide a better initialization
for regressor of RPNl+1, we reÔ¨Åne the anchors in Al+1 us-
ing regression results {rl
i} in RPNl, thus generate more ac-
curate localization compared to a single step regression in
Siamese RPN [23], as illustrated in Fig. 4. Fig. 2 shows the
cascade architecture of C-RPN.

The loss function ‚ÑìRPNl for RPNl is composed of classi-
Ô¨Åcation loss function Lcls (softmax loss) and regression loss
function Lloc (smooth L1 loss) as follows,

‚ÑìRPNl ({c

l

i}, {r

l

i}) = X

Lcls(c

l
i, c

l‚àó

i ) + Œª X

l‚àó

i Lloc(r

l
i, r

l‚àó

i )

c

i

i

(4)
where i is the anchor index in Al of stage l, Œª a weight to
balance losses, cl‚àó
the true
i
distance between anchor i and groundtruth. Following [38],
rl‚àó
i = (rl‚àó

the label of anchor i, and rl‚àó
i

i(h)) is a 4d vector, such that

i(w), rl‚àó

i(x), rl‚àó

i(y), rl‚àó

rl‚àó
i(x) = (x‚àó ‚àí xl
a)/wl
rl‚àó
i(w) = log(w‚àó/wl
a)

a

rl‚àó
i(y) = (y‚àó ‚àí yl
rl‚àó
i(h) = log(y‚àó/hl
a)

a)/hl

a

(5)

where x, y, w and h are center coordinates of a box and its
width and height. Variables x‚àó and xl
a are for groundtruth
and anchor of stage l (likewise for y, w and h). It is worth
noting that, different from [23] using Ô¨Åxed anchors, the an-
chors in C-RPN are progressively adjusted by the regressor
in the previous stage, and computed as

a = xl‚àí1
xl
a = wl‚àí1

rl‚àí1
a + wl‚àí1
i(x)
exp(rl‚àí1
i(w))

wl

a

a

a = yl‚àí1
yl
a = hl‚àí1
hl

a + hl‚àí1
exp(rl‚àí1
i(h))

a rl‚àí1
i(y)

a

(6)

For the anchor in A1, x1

a, y1

a, w1

a and h1

a are pre-deÔ¨Åned.

The above procedure forms the proposed cascaded RPN.
Due to the rejection of easy negative anchors, the distribu-
tion of training samples for each RPN is gradually more bal-
anced. As a result, the classiÔ¨Åer of each RPN is sequentially
more discriminative in distinguishing difÔ¨Åcult distractors.
Besides, multi-level feature fusion further improves the dis-
criminability in handing complex background. Fig. 5 shows
the discriminative powers of different RPNs by demonstrat-
ing detection response map in each stage.

Figure 6. Overview of feature transfer block. Best viewed in color.

The loss function ‚ÑìCRPN of C-RPN consists of the loss
functions of all RPNl. For each RPN, loss function is com-
puted using Eq. (4), and ‚ÑìCRPN is expressed as

‚ÑìCRPN =

L

X

l=1

‚ÑìRPNl

(7)

3.4. Feature Transfer Block

To effectively leverage multi-level features, we introduce
FTB to fuse features across layers so that each RPN is able
to share high-level semantic feature to improve the discrim-
inability. In detail, a deconvolution layer is used to match
the feature dimensions of different sources. Then, different
features are fused using element-wise summation, followed
a ReLU layer. In order to ensure the same groundtruth for
anchors in each RPN, we apply the interpolation to rescale
the fused features such that the output classiÔ¨Åcation and re-
gression maps have the same resolution for all RPN. Fig. 6
shows the feature transferring for RPNl (l > 1).

3.5. Training and Tracking

Training. The training of C-RPN is performed on the image
pairs that are sampled from the same sequence as in [23].
The multi-task loss function in Eq. (7) enables us to train
C-RPN in an end-to-end manner. Considering that the scale
of target changes smoothly in two consecutive frames, we
employ one scale with different ratios for each anchor. The
ratios of anchors are set to [0.33, 0.5, 1, 2, 3] as in [23].

For each RPN, we adopt the strategy as in object detec-
tion [38] to determine positive and negative training sam-
ples. We deÔ¨Åne the positive samples as anchors whose In-
tersection over union (IOU) with groundtruth is larger than
a threshold œÑpos, and negative samples as anchors whose
IoU with groundtruth bounding box is less than a threshold
œÑneg. We generate at most 64 samples from one image pair.
Tracking. We formulate tracking as multi-stage detection.
For each video, we pre-compute feature embeddings for the
target template in the Ô¨Årst frame. In a new frame, we extract
a region of interest according to the result in last frame, and

7956

ùêª√óùëä√óùê∂ùêª‚Ä≤√óùëä‚Ä≤√óùê∂‚Ä≤ùëò√óùëò√óùê∂‚Ä≤3√ó3√óùê∂‚Ä≤ReLU3√ó3√óùê∂‚Ä≤3√ó3√óùê∂‚Ä≤ùêª‚Ä≤√óùëä‚Ä≤√óùê∂‚Ä≤ùêª√óùëä√óùê∂‚Ä≤Deconvùëò√óùëò√óùê∂‚Ä≤Conv 3√ó3√óùê∂‚Ä≤Conv 3√ó3√óùê∂‚Ä≤Conv 3√ó3√óùê∂‚Ä≤ùêª√óùëä√óùê∂ùêª‚Ä≤√óùëä‚Ä≤√óùê∂‚Ä≤ùúëùëôŒ¶ùëô‚àí1Eltw sumReLUInterpolationùêª‚Ä≤√óùëä‚Ä≤√óùê∂‚Ä≤ùêª√óùëä√óùê∂‚Ä≤Algorithm 1: Tracking with C-RPN
1 Input: frame sequences {Xt}T

t=1 and groundtruth

bounding box b1 of X1, trained model C-RPN;

t=2;

2 Output: Tracking results {bt}T
3 Extract target template z in X1 using b1 ;
4 Extract features {œïl(z)}L
5 for t = 2 to T do
6

l=1 for z from C-RPN;

Extract the search region x in Xt using bt‚àí1 ;
Extract features {œïl(x)}L
Initialize anchors A1;
for l = 1 to L do

l=1 for x from C-RPN;

if l equals to 1 then

Œ¶l(z) = œïl(z), Œ¶l(x) = œïl(x);

else

Œ¶l(z), Œ¶l(x) ‚Üê Eq. (2) ;

i}, {rl

i} ‚Üê Eq. (3);

end
{cl
Remove any anchor i from Al with negative
conÔ¨Ådence cl
Al+1 ‚Üê ReÔ¨Åne the rest anchors in Al with
{rl

i(neg) > Œ∏ ;

i} using Eq. (6);

end
Target proposals ‚Üê AL+1 ;
Select the best proposal as tracking result bk

using strategies in [23];

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21 end

then perform detection using C-RPN on this region. In each
stage, an RPN outputs classiÔ¨Åcation scores and regression
offsets for anchors. The anchors with negative scores larger
than Œ∏ are discarded, and the rest are reÔ¨Åned and taken over
by RPN in next stage. After the last stage L, the remained
anchors are regarded as target proposals, from which we se-
lect the best one as the Ô¨Ånal tracking result using strategies
in [23]. Alg. 1 summarizes the tracking process by C-RPN.

4. Experiments

Implementation detail. We implement C-RPN in Matlab
using MatConvNet [47] on a single Nvidia GTX 1080 with
8GB memory. The backbone Siamese network adopts the
modiÔ¨Åed AlexNet [22]. Instead of training from scratch, we
borrow the parameters from the pretrained model on Ima-
geNet [9]. During training, the parameters of Ô¨Årst two lay-
ers are frozen. The number L of stages is 3. The thresholds
Œ∏, œÑpos and œÑneg are empirically set to 0.95, 0.6 and 0.3. C-
RPN is trained end-to-end over 50 epochs using SGD, and
the learning rate is annealed geometrically at each epoch
from 10‚àí2 to 10‚àí6. We train our C-RPN using the training
data from [10] for experiments on LaSOT [10], and using
VID [39] and YT-BB [37] for other experiments.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 
s
s
e
c
c
u
S

Success plots of OPE

SA-Siam [0.676]
C-RPN [0.675]
CREST [0.673]
PTAV [0.663]
SiamRPN [0.658]
ACT [0.657]
DaSiamRPN [0.655]
ECO-HC [0.652]
TRACA [0.652]
BACF [0.648]
SINT [0.635]
SiamFC [0.607]
HCFT [0.605]
HDT [0.603]
CFNet [0.603]
Staple [0.600]

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 
s
s
e
c
c
u
S

Success plots of OPE

C-RPN [0.663]
DaSiamRPN [0.658]
SA-Siam [0.656]
ECO-HC [0.643]
SiamRPN [0.637]
PTAV [0.635]
ACT [0.625]
CREST [0.623]
BACF [0.617]
TRACA [0.603]
SiamFC [0.582]
Staple [0.581]
SINT [0.580]
CFNet [0.566]
HDT [0.564]
HCFT [0.562]

0

0

0.1

0.2

0.4

0.3
0.7
Overlap threshold

0.5

0.6

0.8

0.9

1

0

0

0.1

0.2

0.4

0.3
0.7
Overlap threshold

0.5

0.6

0.8

0.9

1

Figure 7. Comparisons with stage-of-the-art tracking approaches
on OTB-2013 [52] and OTB-2015 [53]. C-RPN achieves the best
results on both benchmarks. Best viewed in color.

4.1. Experiments on OTB 2013 and OTB 2015

We conduct experiments on the popular OTB-2013 [52]
and OTB-2015 [53] which consist of 51 and 100 fully an-
notated videos, respectively. C-RPN runs at around 36 fps.
Following [52], we employ the success plot in one-pass
evaluation (OPE) to assess different trackers. The compari-
son with 15 state-of-the-art trackers (SiamRPN [23], DaSi-
amRPN [59], TRACA [6], ACT [4], BACF [13], ECO-
HC [7], CREST [42], SiamFC [2], Staple [1], PTAV [11],
SINT [45], CFNet [46], SA-Siam [17], HDT [36] and
HCFT [32]) is shown in Fig. 7. C-RPN achieves promising
performance on both two benchmarks. In speciÔ¨Åc, we ob-
tain the 0.675 and 0.663 precision scores on OTB-2013 and
OTB-2015, respectively. In comparison with the baseline
SiamRPN with 0.658 and 0.637 precision scores, we obtain
improvements by 1.9% and 2.6%, showing the advantages
of multi-stage RPNs in accurate localization. DaSiamRPN
uses extra negative training data from other domains to im-
prove the ability to handle similar distractors, and obtains
0.655 and 0.658 precision scores. Without using extra train-
ing data, C-RPN outperforms DaSiamRPN by 2.0% and
0.5%. More results and comparisons on OTB-2013 [52] and
OTB-2015 [53] are shown in the supplementary material.

4.2. Experiments on VOT 2016 and VOT 2017

VOT-2016 [20] consists of 60 sequences, aiming at as-
sessing the short-term performance of trackers. The overall
performance of a tracking algorithm is evaluated using Ex-
pected Average Overlap (EAO) which takes both accuracy
and robustness into account. The speed of a tracker is rep-
resented with a normalized speed (EFO).

We evaluate C-RPN on VOT-2016, and compare it with
11 trackers including the baseline SiamRPN [23] and other
top ten approaches in VOT-2016. Fig. 8 shows the EAO
of different trackers. C-RPN achieves the best results, sig-
niÔ¨Åcantly outperforming SiamRPN and other approaches.
Tab. 1 lists the comparisons of different trackers on VOT-
2016, and we can see that C-RPN outperforms other track-

7957

Expected overlap scores for baseline

0.38

0.36

0.34

0.32

0.3

0.28

p
a
l
r
e
v
o

d
e

t
c
e
p
x
e
e
g
a
r
e
v
A

C-RPN
SiamRPN
CCOT
TCNN
SSAT
MLDF
Staple
DDC
EBT
SRBT
STAPLEp
DNT

12

11

10

9

8

7
6
Order

5

4

3

2

1

Figure 8. Comparisons on VOT-2016 [20]. Larger (right) value in-
dicates better performance. Our C-RPN signiÔ¨Åcantly outperforms
the baseline and other approaches. Best viewed in color.

Table 1. Detailed comparisons on VOT-2016 [20]. The best two
results are highlighted in red and blue fonts, respectively.

Tracker

EAO

Accuracy

Failure

EFO

C-RPN 0.363

SiamRPN [23]
C-COT [8]
TCNN [20]
SSAT [20]
MLDF [20]
Staple [1]
DDC [20]
EBT [58]
SRBT [20]
STAPLEp [20]
DNT [5]

0.344
0.331
0.325
0.321
0.311
0.295
0.293
0.291
0.290
0.286
0.278

0.594

0.560
0.539
0.554
0.577
0.490
0.544
0.541
0.465
0.496
0.557
0.515

0.95

1.12
0.85
0.96
1.04
0.83
1.35
1.23
0.90
1.25
1.32
1.18

9.3

23.0
0.5
1.1
0.5
1.2
11.1
0.2
3.0
3.7
44.8
1.1

e

t

a
r
 
s
s
e
c
c
u
S

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Table 2. Comparisons on VOT-2017 [21]. The best two results are
highlighted in red and blue fonts, respectively.

Tracker Baseline EAO Real-time EAO

C-RPN

SiamRPN [23]
LSART [44]
CFWCR [21]
CFCF [15]
ECO [7]
Gnet [21]
MCCT [21]
C-COT [8]
CSRDCF [31]
SiamDCF [21]
MCPF [56]

0.289

0.243
0.323
0.303
0.286
0.280
0.274
0.270
0.267
0.256
0.249
0.248

0.273

0.244
0.055
0.062
0.059
0.078
0.060
0.061
0.058
0.100
0.135
0.060

Success plots of OPE on LaSOT

[0.459] C-RPN
[0.440] SiamRPN
[0.413] MDNet
[0.412] VITAL
[0.358] SiamFC
[0.356] StructSiam
[0.353] DSiam
[0.340] ECO
[0.339] SINT
[0.315] STRCF
[0.311] ECO_HC
[0.296] CFNet
[0.285] TRACA
[0.280] MEEM
[0.277] BACF
[0.272] HCFT
[0.271] SRDCF
[0.269] PTAV
[0.266] Staple
[0.263] CSRDCF
[0.262] Staple_CA
[0.258] SAMF
[0.246] LCT
[0.234] Struck
[0.233] DSST
[0.232] fDSST
[0.228] TLD
[0.214] SCT4
[0.211] ASLA
[0.211] KCF
[0.186] CN
[0.178] CT
[0.172] CSK
[0.168] L1APG
[0.163] MIL
[0.151] STC
[0.136] IVT

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e

t

a
r
 
s
s
e
c
c
u
S

Success plots of OPE on LaSOT Testing Set

[0.455] C-RPN
[0.433] SiamRPN
[0.397] MDNet
[0.390] VITAL
[0.336] SiamFC
[0.335] StructSiam
[0.333] DSiam
[0.324] ECO
[0.314] SINT
[0.308] STRCF
[0.304] ECO_HC
[0.275] CFNet
[0.259] BACF
[0.257] TRACA
[0.257] MEEM
[0.250] HCFT
[0.250] PTAV
[0.245] SRDCF
[0.244] CSRDCF
[0.243] Staple
[0.238] Staple_CA
[0.233] SAMF
[0.221] LCT
[0.212] Struck
[0.210] TLD
[0.207] DSST
[0.203] fDSST
[0.194] ASLA
[0.191] SCT4
[0.178] KCF
[0.170] CN
[0.158] CT
[0.155] L1APG
[0.149] CSK
[0.139] MIL
[0.138] STC
[0.118] IVT

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Overlap threshold

Overlap threshold

ers in both accuracy and robustness, and runs efÔ¨Åciently.

VOT-2017 [21] contains 60 sequences, which are devel-
oped by replacing the 10 least challenging videos in VOT-
2016 [20] with 10 difÔ¨Åcult sequences. Different from VOT-
2016 [20], VOT-2017 [21] introduces a new real-time ex-
periment by taking into both tracking performance and efÔ¨Å-
ciency. We compare C-RPN with SiamRPN [23] and other
top ten approaches in VOT-2017 using the EAO of base-
line and real-time experiments. As shown in Tab. 2, C-RPN
achieves the EAO score of 0.289, which signiÔ¨Åcantly out-
performs SiamRPN [23] with 0.243 EAO score. Besides,
compared to LSART [44] and CFWCR [21], C-RPN shows
competitive performance. In real-time experiment, C-RPN
obtains the best performance with EAO score of 0.273, out-
performing all other trackers.

4.3. Experiment on LaSOT

LaSOT [10] is a recent large-scale dataset aiming at both
training and evaluating trackers. There are 1,400 videos in
LaSOT. We compare the proposed C-RPN to 35 approaches
in LaSOT, including ECO [7], MDNet [35], SiamFC [2],
VITAL [43], StructSiam [57], TRACA [6], BACF [13] and
so on. We refer readers to [10] for more details about these

Figure 9. Comparisons with state-of-the-art tracking methods on
LaSOT [10]. C-RPN outperforms existing approaches on success
by large margins under all two protocols. Best viewed in color.

trackers. In addition, we also compare C-RPN with the re-
cent SiamRPN [23] tracker as it is an important baseline.

Following [10], we report the results of success (SUC)
for different trackers as shown in Fig. 9. It shows that our C-
RPN outperforms all other trackers under two protocols. We
achieve SUC scores of 0.459 and 0.455 under protocol I and
II, outperforming the second best tracker SiamRPN with
SUC scores 0.44 and 0.433 by 1.9% and 2.2%, respectively.
Compared to SiamFC with 0.358 and 0.336 SUC scores,
C-RPN gains the improvements by 11.1% and 11.9%. C-
RPN runs at around 23 fps on LaSOT. We refer readers to
supplementary material for more details about results and
comparisons on LaSOT.

4.4. Experiment on TrackingNet

TrackingNet [34] is proposed to assess the performance
of a tracker in the wild. We evaluate C-RPN on its testing
set with 511 videos. Following [34], we utilize three metrics
precision (PRE), normalized precision (NPRE) and success
(SUC) for evaluation. Tab. 3 demonstrates the comparison

7958

Table 3. Comparisons on TrackingNet [34] with the best two re-
sults highlighted in red and blue fonts, respectively.

PRE

NPRE

SUC

C-RPN 0.619

MDNet [35]
CFNet [46]
SiamFC [2]
ECO [7]
CSRDCF [31]
SAMF [26]
ECO-HC [7]
Staple [1]
Staple CA [33]
BACF [13]

0.565
0.533
0.533
0.492
0.48
0.477
0.476
0.470
0.468
0.461

0.746

0.705
0.654
0.663
0.618
0.622
0.598
0.608
0.603
0.605
0.580

0.669

0.606
0.578
0.571
0.554
0.534
0.504
0.541
0.528
0.529
0.523

Table 4. Effect on the number of stages in C-RPN.

# Stages One stage

Two stages

Three stages

SUC on LaSOT
Speed on LaSOT

0.417
48 fps

EAO on VOT-2017

0.248

0.446
37 fps

0.278

0.455
23 fps

0.289

Table 5. Effect on negative anchor Ô¨Åltering (NAF) in C-RPN.

C-RPN w/o NAF C-RPN w/ NAF

SUC on LaSOT

EAO on VOT-2017

0.439

0.282

0.455

0.289

Table 6. Effect on feature transfer block in C-RPN. ‚ÄúS‚Äù and ‚ÄúM‚Äù
indicate using single and multiple layers, respectively.

C-RPN w/o

C-RPN w/o

FTB (S)

FTB (M)

C-RPN w/
FTB (M)

SUC on LaSOT

EAO on VOT-2017

0.442

0.278

0.449

0.282

0.455

0.289

results to trackers with top PRE scores3, showing that C-
RPN achieves the best results on all three metrics. In spe-
ciÔ¨Åc, C-RPN obtains the PRE score of 0.619, NPRE score
of 0.746 and SUC score of 0.669, outperforming the second
best tracker MDNet with PRE score of 0.565, NPRE score
of 0.705 and SUC score of 0.606 by 5.4%, 4.1% and 6.3%,
respectively. Besides, C-RPN runs efÔ¨Åciently at a speed of
around 32 fps.

4.5. Ablation Experiment

To validate the impact of different components, we con-
duct ablation experiments on LaSOT (Protocol II) [10] and
VOT-2017 [21].
Number of stages? As shown in Tab. 4, adding the second
stage signiÔ¨Åcantly improves one-stage baseline. The SUC
on LaSOT is improved by 2.9% from 0.417 to 0.446, and

3The result of C-RPN on TrackingNet [34] is evaluated by the server
provided by the organizer at http://eval.tracking-net.org/
web/challenges/challenge-page/39/leaderboard/42.
The results of compared trackers are reported from [34]. Full comparison
is shown in the supplementary material.

the EAO on VOT-2017 is increased by 3.5% from 0.248 to
0.283. The third stage produces 0.9% and 0.6% improve-
ments on LaSOT and VOT-2017, respectively. We observe
that the improvement by the second stage is higher than that
by the third stage. This suggests that most difÔ¨Åcult back-
ground is handled in the second stage. Adding more stages
may lead to further improvements, but also the computation.
Negative anchor Ô¨Åltering? Filtering out the easy negatives
aims to provide more balanced training samples for RPN in
next stage. To show its effectiveness, we set threshold Œ∏ to
1 such that all reÔ¨Åned anchors will be send to the next stage.
Tab. 5 shows that removing negative anchors in C-RPN can
improve the SUC on LaSOT by 1.6% from 0.439 to 0.455,
and the EAO on VOT-2017 by 0.7% from 0.282 to 0.289,
respectively, which evidences balanced training samples are
crucial for training more discriminative RPN.
Feature transfer block? We conduct experiments on two
baselines to show the effect of FTB on performance of C-
RPN: (a) we only employ the features from one single con-
volution layer (the last layer) in C-RPN; (b) we utilize mul-
tiple layers (the last three layers) in C-RPN but Ô¨Årstly per-
form correlation for each layer and then fuse the results of
all layers (i.e., decision-level fusion). Note that, for both
baselines (a) and (b), we adopt the cascading strategy. The
comparisons of these two baselines with the proposed ap-
proach is demonstrated in Tab. 6. We observe that the use
of features from multiple layers helps improve the SUC on
LaSOT by 0.7% from 0.442 to 0.449, and the EAO on VOT-
2017 by 0.4% from 0.278 to 0.282. Further, the combina-
tion of FTB and multi-layer features boosts the performance
to 0.455 and 0.289 on LaSOT and VOT-2017, respectively,
validating the effectiveness of multi-level feature fusion in
improving performance.

These studies show that each ingredient brings individual
improvement, and all of them work together to produce the
excellent tracking performance.

5. Conclusion

In this paper, we propose a novel multi-stage framework
C-RPN for tracking. Compared with previous state-of-the-
arts, C-RPN demonstrates more robust performance in han-
dling complex background such as similar semantic distrac-
tors by performing hard negative sampling within a cascade
architecture. In addition, we present a novel FTB module
that enables effective feature usage across layers for more
discriminative representation. Moreover, C-RPN progres-
sively reÔ¨Ånes the target bounding box using multiple steps
of regressions, leading to more accurate localization. In our
extensive experiments on six popular benchmarks, C-RPN
consistently achieves the state-of-the-art results and runs in
real-time.

Acknowledgement. This work was supported in part by
US NSF grants 1814745, 1618398, 1407156, and 1350521.

7959

References

[1] Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej
Miksik, and Philip HS Torr. Staple: Complementary learners
for real-time tracking. In CVPR, 2016. 6, 7, 8

[2] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea
Vedaldi, and Philip HS Torr. Fully-convolutional siamese
networks for object tracking. In ECCVW, 2016. 1, 3, 4, 6, 7,
8

[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving

into high quality object detection. In CVPR, 2018. 2, 3

[4] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and
Huchuan Lu. Real-time actor-critictracking. In ECCV, 2018.
6

[5] Zhizhen Chi, Hongyang Li, Huchuan Lu, and Ming-Hsuan
TIP,

Dual deep network for visual

tracking.

Yang.
26(4):2005‚Äì2015, 2017. 7

[6] Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo
Yun, Kyuewang Lee, Jiyeoup Jeong, Yiannis Demiris, and
Jin Young Choi. Context-aware deep feature compression
for high-speed visual tracking. In CVPR, 2018. 6, 7

[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan,
Michael Felsberg, et al. Eco: EfÔ¨Åcient convolution opera-
tors for tracking. In CVPR, 2017. 2, 6, 7, 8

[8] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,
and Michael Felsberg. Beyond correlation Ô¨Ålters: Learn-
ing continuous convolution operators for visual tracking. In
ECCV, 2016. 2, 7

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 6

[10] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia
Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
Lasot: A high-quality benchmark for large-scale single ob-
ject tracking. arXiv, 2018. 2, 6, 7, 8

[11] Heng Fan and Haibin Ling. Parallel tracking and verifying:
A framework for real-time and high accuracy visual tracking.
In ICCV, 2017. 6

[12] Heng Fan and Haibin Ling. Sanet: Structure-aware network

for visual tracking. In CVPRW, 2017. 2

[19] David Held, Sebastian Thrun, and Silvio Savarese. Learning
to track at 100 fps with deep regression networks. In ECCV,
2016. 1, 3

[20] Matej Kristan et al. The visual object tracking vot2016 chal-

lenge results. In ECCVW, 2016. 2, 6, 7

[21] Matej Kristan et al. The visual object tracking vot2017 chal-

lenge results. In ICCVW, 2017. 2, 7, 8

[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiÔ¨Åcation with deep convolutional neural net-
works. In NIPS, 2012. 2, 4, 6

[23] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
High performance visual tracking with siamese region pro-
posal network. In CVPR, 2018. 1, 2, 3, 4, 5, 6, 7

[24] Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and
Gang Hua. A convolutional neural network cascade for face
detection. In CVPR, 2015. 3

[25] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep
visual tracking: Review and experimental comparison. PR,
76:323‚Äì338, 2018. 2

[26] Yang Li and Jianke Zhu. A scale adaptive kernel correlation

Ô¨Ålter tracker with feature integration. In ECCVW, 2014. 8

[27] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D
Reid. ReÔ¨Ånenet: Multi-path reÔ¨Ånement networks for high-
resolution semantic segmentation. In CVPR, 2017. 2

[28] Tsung-Yi Lin, Piotr Doll¬¥ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, 2017. 2

[29] Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming He, and
Piotr Doll¬¥ar. Focal loss for dense object detection. In ICCV,
2017. 2

[30] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015. 2, 3

[31] Alan Lukezic, Tomas Vojir, Luka Cehovin Zajc, Jiri Matas,
and Matej Kristan. Discriminative correlation Ô¨Ålter with
channel and spatial reliability. In CVPR, 2017. 7, 8

[32] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan
Yang. Hierarchical convolutional features for visual tracking.
In ICCV, 2015. 2, 3, 6

[13] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.
Learning background-aware correlation Ô¨Ålters for visual
tracking. In ICCV, 2017. 6, 7, 8

[33] Matthias Mueller, Neil Smith, and Bernard Ghanem.
In CVPR, 2017.

Context-aware correlation Ô¨Ålter tracking.
8

[14] Spyros Gidaris and Nikos Komodakis. Object detection via
a multi-region and semantic segmentation-aware cnn model.
In ICCV, 2015. 2

[15] Erhan Gundogdu and A Aydƒ±n Alatan. Good features to cor-

relate for visual tracking. TIP, 27(5):2526‚Äì2540, 2018. 7

[16] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. To-
wards a better match in siamese network based visual object
tracker. In ECCVW, 2018. 3

[17] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. A
In

twofold siamese network for real-time object tracking.
CVPR, 2018. 1, 3, 6

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 2

[34] Matthias M¬®uller, Adel Bibi, Silvio Giancola, Salman Al-
Subaihi, and Bernard Ghanem. Trackingnet: A large-scale
dataset and benchmark for object tracking in the wild.
In
ECCV, 2018. 2, 7, 8

[35] Hyeonseob Nam and Bohyung Han. Learning multi-domain
convolutional neural networks for visual tracking. In CVPR,
2016. 2, 7, 8

[36] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao,
Qingming Huang, Jongwoo Lim, and Ming-Hsuan Yang.
Hedged deep tracking. In CVPR, 2016. 6

[37] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan,
and Vincent Vanhoucke. Youtube-boundingboxes: A large
high-precision human-annotated data set for object detection
in video. In CVPR, 2017. 6

7960

[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015. 1, 2, 4, 5

[57] Yunhua Zhang, Lijun Wang, Jinqing Qi, Dong Wang,
Mengyang Feng, and Huchuan Lu. Structured siamese net-
work for real-time visual tracking. In ECCV, 2018. 7

[58] Gao Zhu, Fatih Porikli, and Hongdong Li. Beyond local
search: Tracking objects everywhere with instance-speciÔ¨Åc
proposals. In CVPR, 2016. 7

[59] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and
Weiming Hu. Distractor-aware siamese networks for visual
object tracking. In ECCV, 2018. 1, 3, 6

[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge.
IJCV, 115(3):211‚Äì252,
2015. 6

[40] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
Training region-based object detectors with online hard ex-
ample mining. In CVPR, 2016. 2

[41] Arnold WM Smeulders, Dung M Chu, Rita Cucchiara, Si-
mone Calderara, Afshin Dehghan, and Mubarak Shah. Vi-
sual tracking: An experimental survey. TPAMI, 36(7):1442‚Äì
1468, 2014. 2

[42] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Ryn-
son WH Lau, and Ming-Hsuan Yang. Crest: Convolutional
residual learning for visual tracking. In ICCV, 2017. 6

[43] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao
Bao, Wangmeng Zuo, Chunhua Shen, Rynson Lau, and
Ming-Hsuan Yang. Vital: Visual tracking via adversarial
learning. In CVPR, 2018. 2, 7

[44] Chong Sun, Huchuan Lu, and Ming-Hsuan Yang. Learning
spatial-aware regressions for visual tracking. In CVPR, 2018.
7

[45] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders.
In CVPR, 2016. 1,

Siamese instance search for tracking.
3, 6

[46] Jack Valmadre, Luca Bertinetto, JoÀúao Henriques, Andrea
Vedaldi, and Philip HS Torr. End-to-end representation
learning for correlation Ô¨Ålter based tracking. In CVPR, 2017.
1, 3, 6, 8

[47] Andrea Vedaldi and Karel Lenc. Matconvnet: Convolutional

neural networks for matlab. In ACM MM, 2015. 6

[48] Paul Viola, Michael Jones, et al. Rapid object detection using

a boosted cascade of simple features. In CVPR, 2001. 2, 3

[49] Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan
In

Lu. Visual tracking with fully convolutional networks.
ICCV, 2015. 2, 3

[50] Naiyan Wang and Dit-Yan Yeung. Learning a deep compact

image representation for visual tracking. In NIPS, 2013. 2

[51] Qiang Wang, Zhu Teng, Junliang Xing, Jin Gao, Weiming
Hu, and Stephen Maybank. Learning attentions: residual
attentional siamese network for high performance online vi-
sual tracking. In CVPR, 2018. 1, 3

[52] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object

tracking: A benchmark. In CVPR, 2013. 2, 6

[53] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-

ing benchmark. TPAMI, 37(9):1834‚Äì1848, 2015. 2, 6

[54] Alper Yilmaz, Omar Javed, and Mubarak Shah. Object track-

ing: A survey. ACM CSUR, 38(4):13, 2006. 2

[55] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and
Stan Z Li. Single-shot reÔ¨Ånement neural network for object
detection. In CVPR, 2018. 3

[56] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.
Multi-task correlation particle Ô¨Ålter for robust object track-
ing. In CVPR, 2017. 7

7961

