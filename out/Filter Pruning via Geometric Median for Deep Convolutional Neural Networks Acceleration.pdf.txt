Filter Pruning via Geometric Median

for Deep Convolutional Neural Networks Acceleration

Yang He1

Ping Liu1,2

Ziwei Wang3

Zhilan Hu4

Yi Yang1,5 âˆ—

1CAI, University of Technology Sydney

2JD.com

3Information Science Academy, CETC

4Huawei

5Baidu Research

{yang.he-1}@student.uts.edu.au {pino.pingliu,wangziwei26,yee.i.yang}@gmail.com

Abstract

Previous works utilized â€œsmaller-norm-less-importantâ€
criterion to prune ï¬lters with smaller norm values in a con-
volutional neural network. In this paper, we analyze this
norm-based criterion and point out that its effectiveness de-
pends on two requirements that are not always met: (1)
the norm deviation of the ï¬lters should be large; (2) the
minimum norm of the ï¬lters should be small. To solve
this problem, we propose a novel ï¬lter pruning method,
namely Filter Pruning via Geometric Median (FPGM), to
compress the model regardless of those two requirements.
Unlike previous methods, FPGM compresses CNN models
by pruning ï¬lters with redundancy, rather than those with
â€œrelatively lessâ€ importance. When applied to two image
classiï¬cation benchmarks, our method validates its useful-
ness and strengths. Notably, on CIFAR-10, FPGM reduces
more than 52% FLOPs on ResNet-110 with even 2.69%
relative accuracy improvement. Moreover, on ILSVRC-
2012, FPGM reduces more than 42% FLOPs on ResNet-
101 without top-5 accuracy drop, which has advanced the
state-of-the-art. Code is publicly available on GitHub:
https://github.com/he-y/ï¬lter-pruning-geometric-median

1. Introduction

The deeper and wider architectures of deep CNNs bring
about the superior performance of computer vision tasks [6,
26, 45]. However, they also cause the prohibitively ex-
pensive computational cost and make the model deploy-
ment on mobile devices hard if not impossible. Even the
latest architecture with high efï¬ciencies, such as residual
connection [12] or inception module [34], has millions
of parameters requiring billions of ï¬‚oat point operations
(FLOPs) [15]. Therefore, it is necessary to attain the deep
CNN models which have relatively low computational cost

â‡¤Corrsponding Author. Part of this work was done when Yi Yang was

visiting Baidu Research during his Professional Experience Program.

Figure 1. An illustration of (a) the pruning criterion for norm-
based approach and the proposed method; (b) requirements for
norm-based ï¬lter pruning criterion. In (a), the green boxes denote
the ï¬lters of the network, where deeper color denotes larger norm
of the ï¬lter. For the norm-based criterion, only the ï¬lters with
the largest norm are kept based on the assumption that smaller-
norm ï¬lters are less important. In contrast, the proposed method
prunes the ï¬lters with redundant information in the network. In
this way, ï¬lters with different norms indicated by different inten-
sities of green may be retained. In (b), the blue curve represents
the ideal norm distribution of the network, and the v1 and v2 is the
minimal and maximum value of norm distribution, respectively.
To choose the appropriate threshold T (the red shadow), two re-
quirements should be achieved, that is, the norm deviation should
be large, and the minimum of the norm should be arbitrarily small.

but high accuracy.

Recent developments on pruning can be divided into
two categories, i.e., weight pruning [11, 1] and ï¬lter prun-
ing [21, 39]. Weight pruning directly deletes weight values

14340

Previous method(a) Criterion for filter pruningFilters beforepruningNumber of filtersValue of norm0Filters to be prunedğ“¥1ğ“¥2ğ’¯(b) Requirements for norm-based criterionIdeal distribution:ğ•Requirement 1: Ïƒ(ğ•)>> 0Requirement 2: ğ’—1â†’0ğ•Large normMedium normSmall normFilter SpacePruningOur methodin a ï¬lter which may cause unstructured sparsities. This
irregular structure makes it difï¬cult to leverage the high-
efï¬ciency Basic Linear Algebra Subprograms (BLAS) li-
braries [25]. In contrast, ï¬lter pruning directly discards the
whole selected ï¬lters and leaves a model with regular struc-
tures. Therefore, ï¬lter pruning is more preferred for accel-
erating the networks and decreasing the model size.

Current practice [21, 38, 15] performs ï¬lter pruning
by following the â€œsmaller-norm-less-importantâ€ criterion,
which believes that ï¬lters with smaller norms can be pruned
safely due to their less importance. As shown in the top
right of Figure 1(a), after calculating norms of ï¬lters in a
model, a pre-speciï¬ed threshold T is utilized to select ï¬l-
ters whose norms are smaller than it.

However, as illustrated in Figure 1(b), there are two pre-
requisites to utilize this â€œsmaller-norm-less-importantâ€ cri-
terion. First, the deviation of ï¬lter norms should be sig-
niï¬cant. This requirement makes the searching space for
threshold T wide enough so that separating those ï¬lters
needed to be pruned would be an easy task. Second, the
norms of those ï¬lters which can be pruned should be arbi-
trarily small, i.e., close to zero; in other words, the ï¬lters
with smaller norms are expected to make absolutely small
contributions, rather than relatively less but positively large
contributions, to the network. An ideal norm distribution
when satisfactorily meeting those two requirements is illus-
trated as the blue curve in Figure 1. Unfortunately, based
on our analysis and experimental observations, this is not
always true.

To address the problems mentioned above, we propose
a novel ï¬lter pruning approach, named Filter Pruning via
Geometric Median (FPGM). Different from the previous
methods which prune ï¬lters with relatively less contribu-
tion, FPGM chooses the ï¬lters with the most replaceable
contribution. Speciï¬cally, we calculate the Geometric Me-
dian (GM) [8] of the ï¬lters within the same layer. Accord-
ing to the characteristics of GM, the ï¬lter(s) F near it can
be represented by the remaining ones. Therefore, pruning
those ï¬lters will not have substantial negative inï¬‚uences on
model performance. Note that FPGM does not utilize norm
based criterion to select ï¬lters to prune, which means its
performance will not deteriorate even when failing to meet
requirements for norm-based criterion.

Contributions. We have three contributions:
(1) We analyze the norm-based criterion utilized in pre-
vious works, which prunes the relatively less important
ï¬lters. We elaborate on its two underlying requirements
which lead to its limitations;

(2) We propose FPGM to prune the most replace-
able ï¬lters containing redundant information, which can
still achieve good performances when norm-based criterion
fails;

(3) The extensive experiment on two benchmarks

demonstrates the effectiveness and efï¬ciency of FPGM.

2. Related Works

Most previous works on accelerating CNNs can be
roughly divided into four categories, namely, matrix de-
composition [42, 35], low-precision weights [44, 43, 32],
knowledge distilling [17, 19] and pruning. Pruning-based
approaches aim to remove the unnecessary connections of
the neural network [11, 21, 24]. Essentially, weight prun-
ing always results in unstructured models, which makes it
hard to deploy the efï¬cient BLAS library, while ï¬lter prun-
ing not only reduces the storage usage on devices but also
decreases computation cost to accelerate the inference. We
could roughly divide the ï¬lter pruning methods into two cat-
egories by whether the training data is utilized to determine
the pruned ï¬lters, that is, data dependent and data indepen-
dent ï¬lter pruning. Data independent method is more efï¬-
cient than data dependent method as the utilizing of training
data is computation consuming.

Weight Pruning. Many recent works [11, 10, 9, 36, 1,
15, 41, 4] focus on pruning ï¬ne-grained weight of ï¬lters.
For example, [11] proposes an iterative method to discard
the small weights whose values are below the predeï¬ned
threshold. [1] formulates pruning as an optimization prob-
lem of ï¬nding the weights that minimize the loss while sat-
isfying a pruning cost condition.

Data Dependent Filter Pruning. Some ï¬lter pruning
approaches [23, 25, 16, 27, 7, 33, 39, 37, 46, 14, 18, 22]
need to utilize training data to determine the pruned ï¬lters.
[25] adopts the statistics information from the next layer to
guide the ï¬lter selections. [7] aims to obtain a decomposi-
tion by minimizing the reconstruction error of training set
sample activation. [33] proposes an inherently data-driven
method which use Principal Component Analysis (PCA)
to specify the proportion of the energy that should be pre-
served. [37] applies subspace clustering to feature maps to
eliminate the redundancy in convolutional ï¬lters.

Data Independent Filter Pruning. Concurrently with
our work, some data independent ï¬lter pruning strate-
gies [21, 15, 38, 47] have been explored. [21] utilizes an `1-
norm criterion to prune unimportant ï¬lters. [15] proposes
to select ï¬lters with a `2-norm criterion and prune those se-
lected ï¬lters in a soft manner. [38] proposes to prune mod-
els by enforcing sparsity on the scaling parameter of batch
normalization layers. [47] uses spectral clustering on ï¬lters
to select unimportant ones.

Discussion. To the best of our knowledge, only one
previous work reconsiders the smaller-norm-less-important
criterion [38]. We would like to highlight our advantages
compared to this approach as below: (1) [38] pays more
attention to enforcing sparsity on the scaling parameter in
the batch normalization operator, which is not friendly to
the structure without batch normalization. On the contrary,

4341

our approach is not limited by this constraint.
(2) After
pruning channels selected, [38] need ï¬ne-tuning to reduce
performance degradation. However, our method combines
the pruning operation with normal training procedure. Thus
extra ï¬ne-tuning is not necessary.
(3) Calculation of the
gradient of scaling factor is needed for [38]; therefore lots
of computation cost are inevitable, whereas our approach
could accelerate the neural network without calculating the
gradient of scaling factor.

3. Methodology

3.1. Preliminaries

We formally introduce symbols and notations in this sub-
section. We assume that a neural network has L layers. We
use Ni and Ni+1, to represent the number of input chan-
nels and the output channels for the ith convolution layer,
respectively. Fi,j represents the jth ï¬lter of the ith layer,
then the dimension of ï¬lter Fi,j is RNiâ‡¥Kâ‡¥K , where K is
the kernel size of the network1. The ith layer of the net-
work W(i) could be represented by {Fi,j, 1 â‰¤ j â‰¤ Ni+1}.
The tensor of connection of the deep CNN network could be
parameterized by {W(i) âˆˆ RNi+1â‡¥Niâ‡¥Kâ‡¥K, 1 â‰¤ i â‰¤ L}.

3.2. Analysis of Norm-based Criterion

Figure 1 gives an illustration for the two requirements
for successful utilization of the norm-based criterion. How-
ever, these requirements may not always hold, and it might
lead to unexpected results. The details are illustrated in Fig-
ure 2, in which the blue dashed curve and the green solid
curve indicates the norm distribution in ideal and real cases,
respectively.

Figure 2. Ideal and Reality of the norm-based criterion: (a) Small
Norm Deviation and (b) Large Minimum Norm. The blue dashed
curve indicates the ideal norm distribution, and the green solid
curve denotes the norm distribution might occur in real cases.

(1) Small Norm Deviation. The deviation of ï¬lter norm
distributions might be too small, which means the norm val-
ues are concentrated to a small interval, as shown in Fig-
ure 2(a). A small norm deviation leads to a small search
space, which makes it difï¬cult to ï¬nd an appropriate thresh-
old to select ï¬lters to prune.

(2) Large Minimum Norm. The ï¬lters with the mini-
mum norm may not be arbitrarily small, as shown in the

1Fully-connected layers equal to convolutional layers with k = 1

Figure 2(b), v00
1 >> v1 â†’ 0. Under this condition, those
ï¬lters considered as the least important still contribute sig-
niï¬cantly to the network, which means every ï¬lter is highly
informative. Therefore, pruning those ï¬lters with minimum
norm values will cast a negative effect on the network.

3.3. Norm Statistics in Real Scenarios

In Figure 3, statistical information collected from pre-
trained ResNet-110 on CIFAR-10 and pre-trained ResNet-
18 on ILSVRC-2012 demonstrates previous analysis. The
small green vertical lines show each observation in this
norm distribution, and the blue curves denote the Ker-
nel Distribution Estimate (KDE) [30], which is a non-
parametric way to estimate the probability density function
of a random variable. The norm distribution of ï¬rst layer
and last layer in both structures are drawn. In addition, to
clearly illustrate the relation between norm points, two dif-
ferent x-scale, i.e., linear x-scale and log x-scale, are pre-
sented.

(1) Small Norm Deviation in Network. For the ï¬rst con-
volutional layer of ResNet-110, as shown in Figure 3(b),
there is a large quantity of ï¬lters whose norms are concen-
trated around the magnitude of 10 6. For the last convo-
lutional layer of ResNet-110, as shown in Figure 3(c), the
interval span of the value of norm is roughly 0.3, which is
much smaller than the interval span of the norm of the ï¬rst
layer (1.7). For the last convolutional layer of ResNet-18, as
shown in Figure 3(g), most ï¬lter norms are between the in-
terval [0.8, 1.0]. In all these cases, ï¬lters are distributed too
densely, which makes it difï¬cult to select a proper threshold
to distinguish the important ï¬lters from the others.

(2) Large Minimum Norm in Network. For the last con-
volutional layer of ResNet-18, as shown in Figure 3(g), the
minimum norm of these ï¬lters is around 0.8, which is large
comparing to ï¬lters in the ï¬rst convolutional layer (Fig-
ure 3(e)). For the last convolutional layer of ResNet-110,
as shown in Figure 3(c), only one ï¬lter is arbitrarily small,
while the others are not. Under those circumstances, the ï¬l-
ters with minimum norms, although they are relatively less
important according to the norm-based criterion, still make
signiï¬cant contributions in the network.

3.4. Filter Pruning via Geometric Median

To get rid of the constraints in the norm-based criterion,
we propose a new ï¬lter pruning method inspired from geo-
metric median. The central idea of geometric median [8] is
as follows: given a set of n points a(1), . . . , a(n) with each
a(i) âˆˆ Rd, ï¬nd a point xâ‡¤ âˆˆ Rd that minimizes the sum of
Euclidean distances to them:

xâ‡¤ 2 arg min

x2Rd

f (x) where f (x) def= X

kx   a(i)k2

(1)

i2[1,n]

4342

Number of filtersValue ofnormNumber of filtersValue of norm00ğ“¥ğ“¥2ğ“¥ğ“¥1ğ“¥ğ“¥2ğ‘£ğ‘£2â€²ğ‘£ğ‘£1â€²â€²ğ‘£ğ‘£1â€²ğ‘£ğ‘£2â€²â€²ğ“¥ğ“¥1Problem 1:Ïƒ(ğ•ğ•â€²)<< Ïƒ(ğ•ğ•)(a) Small Norm DeviationProblem 2: ğ‘£ğ‘£1â€²â€²â‰«ğ‘£ğ‘£1â†’0(b) Large Minimum Normğ•ğ•ğ•ğ•â€²ğ•ğ•ğ•ğ•â€²â€²(a) ResNet-110 (linear x-scale)

(b) ResNet-110 (log x-scale)

(c) ResNet-110 (linear x-scale)

(d) ResNet-110 (log x-scale)

(e) ResNet-18 (linear x-scale)

(f) ResNet-18 (log x-scale)

(g) ResNet-18 (linear x-scale)

(h) ResNet-18 (log x-scale)

Figure 3. Norm distribution of ï¬lters from different layers of ResNet-110 on CIFAR-10 and ResNet-18 on ILSVRC-2012. The small
green vertical lines and blue curves denote each norm and Kernel Distribution Estimate (KDE) of the norm distribution, respectively.

As the geometric median is a classic robust estimator of
centrality for data in Euclidean spaces [8], we use the geo-
metric median F GM
to get the common information of all
the ï¬lters within the single ith layer:

i

F GM

i

2 arg min

g(x),

x2RNiâ‡¥Kâ‡¥K

where

g(x) def= X

kx   Fi,j

0 k2.

0

j

2[1,Ni+1]

(2)

(3)

In the ith layer, if some ï¬lters have the same, or similar

values as the geometric median in that layer, which is:

Fi,jâ‡¤ 2 arg min

0

j

2[1,Ni+1]

kFi,j

0   F GM

i

k2,

(4)

then those ï¬lters, Fi,j â‡¤ , can be represented by the other ï¬l-
ters in the same layer, and therefore, pruning them has little
negative impacts on the network performance.

As geometric median is a non-trivial problem in com-
putational geometry, the previous fastest running times for
computing a (1 + âœ)-approximate geometric median were
eO(dn4/3 Â· âœ 8/3) by [2], O(nd log3(n/âœ)) by [3]. In our
case, as the ï¬nal result Fi,j â‡¤ are a list of know points, that
is, the candidate ï¬lters in the layer, we could relax the above
problem.

We assume that

kFi,jâ‡¤   F GM

i

k2 = 0,

(5)

so the Equation.4 is achieved. Then the above Equation.2
becomes to

Fi,jâ‡¤ 2 arg min

jâ‡¤2[1,Ni+1]

X

kx   Fi,j

0 k2

0

j

2[1,Ni+1]

(6)

= arg min

g(x)

jâ‡¤2[1,Ni+1]

Note that even if the ï¬lter need to be pruned, Fi,j â‡¤ , is
not included in the calculation of the geometric median in
Equation.62, we could also achieve the same result. In this
setting, we want to ï¬nd the ï¬lter

Fi,jâ‡¤ 0 2 arg min

g0(x),

jâ‡¤2[1,Ni+1]

where

g0(x) = X

kx   Fi,j

0 k2.

0

j

2[1,Ni+1],j

0

6=jâ‡¤

(7)

(8)

With the above Equation.6 and Equation.8, we could get

that:

g0(x) = g(x)   X
0 =jâ‡¤

j

kx   Fi,j

0 k2

= g(x)   kx   Fi,jâ‡¤ k2.

(9)

2To select multiple ï¬lters, we choose several j that makes g(x) to the

smallest extent.

4343

Algorithm 1 Algorithm Description of FPGM
Input: training data: X.

1: Given: pruning rate Pi
2: Initialize: model parameter W = {W(i), 0 â‰¤ i â‰¤ L}
3: for epoch = 1; epoch â‰¤ epochmax; epoch + + do
4:

Update the model parameter W based on X
for i = 1; i â‰¤ L; i + + do

Find Ni+1Pi ï¬lters that satisfy Equation 6
Zeroize selected ï¬lters

5:

6:

7:

end for

8:
9: end for
10: Obtain the compact model Wâ‡¤ from W
Output: The compact model and its parameters Wâ‡¤

then we could get

min g0(x) = min{g(x)   kx   Fi,jâ‡¤ k2}

= min g(x)   min kx   Fi,jâ‡¤ k2

(10)

= g(Fi,jâ‡¤ )   min kx   Fi,jâ‡¤ k2.

For the second component of the right side for Equa-

tion.10, when x = Fi,j â‡¤ , we can get:

3.5.2 Realistic Acceleration

In the above analysis, only the FLOPs of convolution op-
erations for computation complexity comparison is consid-
ered, which is common in previous works [21, 15]. This is
because other operations such as batch normalization (BN)
and pooling are insigniï¬cant comparing to convolution op-
erations.

However, non-tensor layers (e.g., BN and pooling layers)
also need the inference time on GPU [25], and inï¬‚uence the
realistic acceleration. Besides, the wide gap between the
theoretical and realistic acceleration could also be restricted
by the IO delay, buffer switch, and efï¬ciency of BLAS li-
braries. We compare the theoretical and practical accelera-
tion in Table 5.

4. Experiments

We evaluate FPGM for single-branch network (VGGNet
[31]), and multiple-branch network (ResNet) on two bench-
marks: CIFAR-10 [20] and ILSVRC-2012 [29]3. The
CIFAR-10 [20] dataset contains 60, 000 32 Ã— 32 color im-
ages in 10 different classes, in which 50, 000 training im-
ages and 10, 000 testing images are included.
ILSVRC-
2012 [29] is a large-scale dataset containing 1.28 million
training images and 50k validation images of 1,000 classes.

Fi,jâ‡¤ 0 = Fi,jâ‡¤

(11)

4.1. Experimental Settings

since kx   Fi,j

0 k2 = 0

Since the geometric median is a classic robust estimator
of centrality for data in Euclidean spaces [8], the selected
ï¬lter(s), Fi,j â‡¤ , and left ones share the most common infor-
mation. This indicates the information of the ï¬lter(s) Fi,j â‡¤
could be replaced by others. After ï¬ne-tuning, the network
could easily recover its original performance since the infor-
mation of pruned ï¬lters can be represented by the remain-
ing ones. Therefore, the ï¬lter(s) Fi,j â‡¤ could be pruned with
negligible effect on the ï¬nal result of the neural network.
The FPGM is summarized in Algorithm 1.

3.5. Theoretical and Realistic Acceleration

3.5.1 Theoretical Acceleration

Suppose the shapes of input tensor I âˆˆ Ni Ã— Hi Ã— Wi and
output tensor O âˆˆ Ni+1 Ã— Hi+1 Ã— Wi+1. Set the ï¬lter
pruning rate of the ith layer to Pi, then Ni+1 Ã— Pi ï¬lters
should be pruned. After ï¬lter pruning, the dimension of
input and output feature map of the ith layer change to I0 âˆˆ
[Ni Ã— (1 âˆ’ Pi)] Ã— Hi Ã— Wi and O0 âˆˆ [Ni+1 Ã— (1 âˆ’ Pi)] Ã—
Hi+1 Ã— Wi+1, respectively.

If setting pruning rate for the (i + 1)th layer to Pi+1,
then only (1 âˆ’ Pi+1) Ã— (1 âˆ’ Pi) of the original com-
putation is needed. Finally, a compact model {Wâ‡¤(i) âˆˆ
RNi+1(1 Pi)â‡¥Ni(1 Pi 1)â‡¥Kâ‡¥K} is obtained.

Training setting. On CIFAR-10, the parameter setting
is the same as [13] and the training schedule is the same
as [40]. In the ILSVRC-2012 experiments, we use the de-
fault parameter settings which is same as [12, 13]. Data ar-
gumentation strategies for ILSVRC-2012 is the same as Py-
Torch [28] ofï¬cial examples. We analyze the difference be-
tween starting from scratch and the pre-trained model. For
pruning the model from scratch, We use the normal training
schedule without additional ï¬ne-tuning process. For prun-
ing the pre-trained model, we reduce the learning rate to
one-tenth of the original learning rate. To conduct a fair
comparison of pruning scratch and pre-trained models, we
use the same training epochs to train/ï¬ne-tune the network.
The previous work [21] might use fewer epochs to ï¬netune
the pruned model, but it converges too early, and its accu-
racy can not improve even with more epochs, which can be
shown in section 4.2.

Pruning setting. In the ï¬lter pruning step, we simply
prune all the weighted layers with the same pruning rate at
the same time, which is the same as [15]. Therefore, only
one hyper-parameter Pi = P is needed to balance the accel-
eration and accuracy. The pruning operation is conducted at

3As stated in [21], â€œcomparing with AlexNet or VGG (on ILSVRC-
2012), both VGG (on CIFAR-10) and Residual networks have fewer pa-
rameters in the fully connected layersâ€, which makes pruning ï¬lters in
those networks challenging.

4344

Depth

Method

SFP [15]

20

32

Ours (FPGM-only 30%)
Ours (FPGM-only 40%)
Ours (FPGM-mix 40%)

MIL [5]
SFP [15]

Ours (FPGM-only 30%)
Ours (FPGM-only 40%)
Ours (FPGM-mix 40%)

PFEC [21]

CP [16]
SFP [15]

Ours (FPGM-only 40%)
Ours (FPGM-mix 40%)

56

PFEC [21]

CP [16]

Ours (FPGM-only 40%)
Ours (FPGM-mix 40%)

MIL [5]

PFEC [21]
SFP [15]

110

Ours (FPGM-only 40%)
Ours (FPGM-mix 40%)

PFEC [21]
NISP [39]

Ours (FPGM-only 40%)

Fine-tune? Baseline acc. (%) Accelerated acc. (%) Acc. # (%)

FLOPs

FLOPs #(%)

7

7

7

7

7

7

7

7

7

7

7

7

7

7

3

3

3

3

7

7

7

7

7

3

3

3

92.20 (Â±0.18)
92.20 (Â±0.18)
92.20 (Â±0.18)
92.20 (Â±0.18)

92.33

92.63 (Â±0.70)
92.63 (Â±0.70)
92.63 (Â±0.70)
92.63 (Â±0.70)

93.04
92.80

93.59 (Â±0.58)
93.59 (Â±0.58)
93.59 (Â±0.58)

93.04
92.80

90.83 (Â±0.31)
91.09 (Â±0.10)
90.44 (Â±0.20)
91.99 (Â±0.15)

90.74

92.08 (Â±0.08)
92.31 (Â±0.30)
91.93 (Â±0.03)
92.82 (Â±0.03)

91.31
90.90

92.26 (Â±0.31)
92.93 (Â±0.49)
92.89 (Â±0.32)

93.06
91.80

93.59 (Â±0.58)
93.59 (Â±0.58)

93.49 (Â±0.13)
93.26 (Â±0.03)

93.63
93.53

93.68 (Â±0.32)
93.68 (Â±0.32)
93.68 (Â±0.32)

93.53

â€“

93.44
92.94

93.38 (Â±0.30)
93.73 (Â±0.23)
93.85 (Â±0.11)

93.30

â€“

93.68 (Â±0.32)

93.74 (Â±0.10)

1.37
1.11
1.76
0.21

1.59
0.55
0.32
0.70
-0.19

1.75
1.90
1.33
0.66
0.70
-0.02
1.00
0.10
0.33

0.19
0.61
0.30
-0.05
-0.17
0.20
0.18
-0.16

2.43E7
2.43E7
1.87E7
1.87E7

4.70E7
4.03E7
4.03E7
3.23E7
3.23E7

9.09E7

â€“

5.94E7
5.94E7
5.94E7
9.09E7

â€“

5.94E7
5.94E7

-

1.55E8
1.50E8
1.21E8
1.21E8
1.55E8

â€“

1.21E8

42.2
42.2
54.0
54.0

31.2
41.5
41.5
53.2
53.2

27.6
50.0
52.6
52.6
52.6
27.6
50.0
52.6
52.6

34.2
38.6
40.8
52.3
52.3
38.6
43.8
52.3

Table 1. Comparison of pruned ResNet on CIFAR-10. In â€œFine-tune?â€ column, â€œ3â€ and â€œ7â€ indicates whether to use the pre-trained model
as initialization or not, respectively. The â€œAcc. #â€ is the accuracy drop between pruned model and the baseline model, the smaller, the
better.

the end of every training epoch. Unlike previous work [21],
sensitivity analysis is not essential in FPGM to achieve good
performances, which will be demonstrated in later sections.
Apart from FPGM only criterion, we also use a mix-
ture of FPGM and previous norm-based method [15] to
show that FPGM could serve as a supplement to previ-
ous methods. FPGM only criterion is denoted as â€œFPGM-
onlyâ€, the criterion combining the FPGM and norm-based
criterion is indicated as â€œFPGM-mixâ€. â€œFPGM-only 40%â€
means 40% ï¬lters of the layer are selected with FPGM only,
while â€œFPGM-mix 40%â€ means 30% ï¬lters of the layer
are selected with FPGM, and the remaining 10% ï¬lters
are selected with norm-based criterion [15]. We compare
FPGM with previous acceleration algorithms, e.g., MIL [5],
PFEC [21], CP [16], ThiNet [25], SFP [15], NISP [39], Re-
thinking [38]. Not surprisingly, our FPGM method achieves
the state-of-the-art result.

4.2. Single-Branch Network Pruning

VGGNet on CIFAR-10. As the training setup is not
publicly available for [21], we re-implement the pruning
procedure and achieve similar results to the original pa-
per. The result of pruning pre-trained and scratch model

is shown in Table 3 and Table 4, respectively. Not surpris-
ingly, FPGM achieves better performance than [21] in both
settings.

4.3. Multiple-Branch Network Pruning

ResNet on CIFAR-10. For the CIFAR-10 dataset, we
test our FPGM on ResNet-20, 32, 56 and 110 with two dif-
ferent pruning rates: 30% and 40%.

As shown in Table 1, our FPGM achieves the state-
of-the-art performance. For example, MIL [5] without
ï¬ne-tuning accelerates ResNet-32 by 31.2% speedup ratio
with 1.59% accuracy drop, but our FPGM without ï¬ne-
tuning achieves 53.2% speedup ratio with even 0.19% accu-
racy improvement. Comparing to SFP [15], when pruning
52.6% FLOPs of ResNet-56, our FPGM has only 0.66% ac-
curacy drop, which is much less than SFP [15] (1.33%). For
pruning the pre-trained ResNet-110, our method achieves
a much higher (52.3% v.s. 38.6%) acceleration ratio with
0.16% performance increase, while PFEC [21] harms the
performance with lower acceleration ratio. These results
demonstrate that FPGM can produce a more compressed
model with comparable or even better performances.

ResNet on ILSVRC-2012.

For the ILSVRC-2012

4345

Depth

Method

Fine-
tune?

Baseline

top-1

acc.(%)

Accelerated

Baseline

Accelerated

top-1

top-5

top-5

acc.(%)

acc.(%)

acc.(%)

Top-1

Top-5

acc. #(%)

acc. #(%)

FLOPs#(%)

18

34

50

MIL [5]
SFP [15]

7

7

7

Ours (FPGM-only 30%)
7
Ours (FPGM-mix 30%)
Ours (FPGM-only 30%) 3
Ours (FPGM-mix 30%) 3

SFP [15]

Ours (FPGM-only 30%)
Ours (FPGM-mix 30%)

7

7

7

PFEC [21]

3
Ours (FPGM-only 30%) 3
Ours (FPGM-mix 30%) 3

SFP [15]

Ours (FPGM-only 30%)
Ours (FPGM-mix 30%)
Ours (FPGM-only 40%)

ThiNet [25]

SFP [15]
NISP [39]

7

7

7

7

3

3

3

CP [16]

3
Ours (FPGM-only 30%) 3
Ours (FPGM-mix 30%) 3
Ours (FPGM-only 40%) 3

101

Rethinking [38]

3
Ours (FPGM-only 30%) 3

69.98
70.28
70.28
70.28
70.28
70.28

73.92
73.92
73.92
73.23
73.92
73.92

76.15
76.15
76.15
76.15
72.88
76.15

â€“
â€“

76.15
76.15
76.15

77.37
77.37

66.33
67.10
67.78
67.81
68.34
68.41

71.83
71.79
72.11
72.17
72.54
72.63

74.61
75.03
74.94
74.13
72.04
62.14

â€“
â€“

75.59
75.50
74.83

75.27
77.32

89.24
89.63
89.63
89.63
89.63
89.63

91.62
91.62
91.62

â€“

91.62
91.62

92.87
92.87
92.87
92.87
91.14
92.87

â€“

92.20
92.87
92.87
92.87

â€“

93.56

86.94
87.78
88.01
88.11
88.53
88.48

90.33
90.70
90.69

â€“

91.13
91.08

92.06
92.40
92.39
91.94
90.67
84.60

â€“

90.80
92.63
92.63
92.32

â€“

93.56

3.65
3.18
2.50
2.47
1.94
1.87

2.09
2.13
1.81
1.06
1.38
1.29

1.54
1.12
1.21
2.02
0.84
14.01
0.89

â€“

0.56
0.65
1.32

2.10
0.05

2.30
1.85
1.62
1.52
1.10
1.15

1.29
0.92
0.93

â€“

0.49
0.54

0.81
0.47
0.48
0.93
0.47
8.27

â€“

1.40
0.24
0.21
0.55

â€“

0.00

34.6
41.8
41.8
41.8
41.8
41.8

41.1
41.1
41.1
24.2
41.1
41.1

41.8
42.2
42.2
53.5
36.7
41.8
44.0
50.0
42.2
42.2
53.5

47.0
42.2

Table 2. Comparison of pruned ResNet on ILSVRC-2012. â€œFine-tune?â€ and â€acc. #â€ have the same meaning with Table 1.

Model \ Acc (%) Baseline

PFEC [21]

Ours

93.58
(Â±0.03)

93.58
(Â±0.03)

Pruned
w.o. FT

77.45
(Â±0.03)

80.38
(Â±0.03)

FT

FT

40 epochs

160 epochs

93.22

(Â±0.03 )

93.24
(Â±0.01)

93.28
(Â±0.07)

94.00
(Â±0.13)

Model

Baseline
time (ms)

Pruned

time (ms)

Realistic
Acc.(%)

Theoretical

Acc.(%)

ResNet-18
ResNet-34
ResNet-50
ResNet-101

37.05
63.89
134.57
219.70

26.77
45.24
83.22
147.45

27.7
29.2
38.2
32.9

41.8
41.1
53.5
42.2

Table 3. Pruning pre-trained VGGNet on CIFAR-10. â€œw.o.â€ means
â€œwithoutâ€ and â€œFTâ€ means â€œï¬ne-tuningâ€ the pruned model.

Table 5. Comparison on the theoretical and realistic acceleration.
Only the time consumption of the forward procedure is considered.

Model

SA Baseline

Pruned From Scratch FLOPs#(%)

PFEC [21] Y 93.58 (Â±0.03)
Y 93.58 (Â±0.03)
N 93.58 (Â±0.03)

Ours
Ours

93.31 (Â±0.02)
93.54 (Â±0.08)
93.23 (Â±0.13)

34.2
34.2
35.9

Table 4. Pruning scratch VGGNet on CIFAR-10. â€œSAâ€ means
â€œsensitivity analysisâ€. Without sensitivity analysis, FPGM can still
achieve comparable performances comparing to [21]; after intro-
ducing sensitivity analysis, FPGM can surpass [21].

dataset, we test our FPGM on ResNet-18, 34, 50 and 101
with pruning rates 30% and 40%. Same with [15], we do
not prune the projection shortcuts for simpliï¬cation.

Table 2 shows that FPGM outperforms previous meth-
ods on ILSVRC-2012 dataset, again. For ResNet-18, pure

FPGM without ï¬ne-tuning achieves the same inference
speedup with [15], but its accuracy exceeds by 0.68%.
FPGM-only with ï¬ne-tuning could even gain 0.60% im-
provement over FPGM-only without ï¬ne-tuning, thus ex-
ceeds [15] by 1.28%. For ResNet-50, FPGM with ï¬ne-
tuning achieves more inference speedup than CP [16], but
our pruned model exceeds their model by 0.85% on the ac-
curacy. Moreover, for pruning a pre-trained ResNet-101,
FPGM reduces more than 40% FLOPs of the model without
top-5 accuracy loss and only negligible (0.05%) top-1 accu-
racy loss. In contrast, the performance degradation is 2.10%
for Rethinking [38]. Compared to the norm-based criterion,
Geometric Median (GM) explicitly utilizes the relationship
between ï¬lters, which is the main cause of its superior per-

4346

(a) Different pruning intervals

(b) Different pruned FLOPs

Figure 4. Accuracy of ResNet-110 on CIFAR-10 regarding dif-
ferent hyper-parameters. Solid line and shadow denotes the mean
values and standard deviation of three experiments, respectively.

formance.

To compare the theoretical and realistic acceleration, we
measure the forward time of the pruned models on one
GTX1080 GPU with a batch size of 64. The results 4 are
shown in Table 5. As discussed in the above section, the
gap between the theoretical and realistic model may come
from the limitation of IO delay, buffer switch, and efï¬ciency
of BLAS libraries.

4.4. Ablation Study

Inï¬‚uence of Pruning Interval In our experiment set-
ting, the interval of pruning equals to one, i.e., we conduct
our pruning operation at the end of every training epoch.
To explore the inï¬‚uence of pruning interval, we change the
pruning interval from one epoch to ten epochs. We use
the ResNet-110 under pruning rate 40% as the baseline, as
shown in Fig. 4(a). The accuracy ï¬‚uctuation along with the
different pruning intervals is less than 0.3%, which means
the performance of pruning is not sensitive to this parame-
ter. Note that ï¬ne-tuning this parameter could even achieve
better performance.

Varying Pruned FLOPs We change the ratio of Pruned
FLOPs for ResNet-110 to comprehensively understand
FPGM, as shown in Fig. 4(b). When the pruned FLOPs
is 18% and 40%, the performance of the pruned model even
exceeds the baseline model without pruning, which shows
FPGM may have a regularization effect on the neural net-
work.

Inï¬‚uence of Distance Type We use `1-norm and cosine
distance to replace the distance function in Equation 3. We
use the ResNet-110 under pruning rate 40% as the baseline,
the accuracy of the pruned model is 93.73 Â± 0.23 %. The
accuracy based on `1-norm and cosine distance is 93.87 Â±
0.22 % and 93.56 Â± 0.13, respectively. Using `1-norm as
the distance of ï¬lter would bring a slightly better result, but
cosine distance as distance would slightly harm the perfor-
mance of the network.

4Optimization of the addition of ResNet shortcuts and convolutional

outputs would also affect the results.

Combining FPGM with Norm-based Criterion We
analyze the effect of combining FPGM and previous norm-
based criterion. For ResNet-110 on CIFAR-10, FPGM-
mix is slightly better than FPGM-only. For ResNet-18
on ILSVRC-2012, the performances of FPGM-only and
FPGM-mix are almost the same. It seems that the norm-
based criterion and FPGM together can boost the perfor-
mance on CIFAR-10, but not on ILSVRC-2012. We believe
that this is because the two requirements for the norm-based
criterion are met on some layers of CIFAR-10 pre-trained
network, but not on that of ILSVRC-2012 pre-trained net-
work, which is shown in Figure 3.

4.5. Feature Map Visualization

We visualize the feature maps of the ï¬rst layer of the
ï¬rst block of ResNet-50. The feature maps with red titles
(7,23,27,46,56,58) correspond to the selected ï¬lter activa-
tion when setting the pruning rate to 10%. These selected
feature maps contain outlines of the bamboo and the pandaâ€™s
head and body, which can be replaced by remaining fea-
ture maps: (5,12,16,18,22, et al.) containing outlines of the
bamboo, and (0,4,33,34,47, et al.) containing the outline of
panda.

0

8

1

9

2

3

4

5

6

7

1 0

1 1

1 2

1 3

1 4

1 5

1 6

1 7

1 8

1 9

2 0

2 1

2 2

2 3

2 4

2 5

2 6

2 7

2 8

2 9

3 0

3 1

3 2

3 3

3 4

3 5

3 6

3 7

3 8

3 9

4 0

4 1

4 2

4 3

4 4

4 5

4 6

4 7

4 8

4 9

5 0

5 1

5 2

5 3

5 4

5 5

5 6

5 7

5 8

5 9

6 0

6 1

6 2

6 3

Figure 5.
Input image (left) and visualization of feature maps
(right) of ResNet-50-conv1. Feature maps with red bounding
boxes are the channels to be pruned.

5. Conclusion and Future Work

In this paper, we elaborate on the underlying require-
ments for norm-based ï¬lter pruning criterion and point out
their limitations. To solve this, we propose a new ï¬lter prun-
ing strategy based on the geometric median, named FPGM,
to accelerate the deep CNNs. Unlike the previous norm-
based criterion, FPGM explicitly considers the mutual re-
lations between ï¬lters. Thanks to this, FPGM achieves the
state-of-the-art performance in several benchmarks. In the
future, we plan to work on how to combine FPGM with
other acceleration algorithms, e.g., matrix decomposition
and low-precision weights, to push the performance to a
higher stage.

4347

References

[1] M. A. Carreira-PerpinÂ´an and Y. Idelbayev.

compression algorithms for neural net pruning.
2018. 1, 2

learning-
In CVPR,

[2] H. H. Chin, A. Madry, G. L. Miller, and R. Peng. Runtime
guarantees for regression problems.
In Proceedings of the
4th conference on Innovations in Theoretical Computer Sci-
ence, pages 269â€“282. ACM, 2013. 4

[3] M. B. Cohen, Y. T. Lee, G. Miller, J. Pachocki, and A. Sid-
ford. Geometric median in nearly linear time. In Proceed-
ings of the forty-eighth annual ACM symposium on Theory
of Computing, pages 9â€“21. ACM, 2016. 4

[4] X. Dong, S. Chen, and S. Pan. Learning to prune deep
neural networks via layer-wise optimal brain surgeon.
In
Advances in Neural Information Processing Systems, pages
4857â€“4867, 2017. 2

[5] X. Dong, J. Huang, Y. Yang, and S. Yan. More is less: A
more complicated network with less inference complexity.
In CVPR, 2017. 6, 7

[6] X. Dong and Y. Yang. Searching for a robust neural architec-
ture in four gpu hours. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2019. 1

[7] A. Dubey, M. Chatterjee, and N. Ahuja. Coreset-based neu-

ral network compression. In ECCV, 2018. 2

[8] P. T. Fletcher, S. Venkatasubramanian, and S. Joshi. Robust
statistics on riemannian manifolds via the geometric median.
In CVPR, 2008. 2, 3, 4, 5

[9] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for

efï¬cient DNNs. In NIPS, 2016. 2

[10] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In ICLR, 2015. 2

[11] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
and connections for efï¬cient neural network. In NIPS, 2015.
1, 2

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 5

[13] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in

deep residual networks. In ECCV, 2016. 5

[14] Y. He and S. Han. ADC: Automated deep compression
and acceleration with reinforcement learning. arXiv preprint
arXiv:1802.03494, 2018. 2

[15] Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang. Soft ï¬lter
pruning for accelerating deep convolutional neural networks.
In IJCAI, 2018. 1, 2, 5, 6, 7

[16] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerat-

ing very deep neural networks. In ICCV, 2017. 2, 6, 7

[17] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge

in a neural network. In NIPS, 2015. 2

[18] Q. Huang, K. Zhou, S. You, and U. Neumann. Learning
to prune ï¬lters in convolutional neural networks. In WACV,
2018. 2

[19] J. Kim, S. Park, and N. Kwak. Paraphrasing complex net-
In NIPS,

work: Network compression via factor transfer.
2018. 2

[20] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 5

[21] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf.
Pruning ï¬lters for efï¬cient ConvNets. In ICLR, 2017. 1, 2,
5, 6, 7

[22] S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, and
D. Doermann. Towards optimal structured cnn pruning via
generative adversarial learning. In CVPR, 2019. 2

[23] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efï¬cient convolutional networks through network
slimming. In ICCV, 2017. 2

[24] Z. Liu, J. Xu, X. Peng, and R. Xiong. Frequency-domain dy-
namic pruning for convolutional neural networks. In NIPS,
2018. 2

[25] J.-H. Luo, J. Wu, and W. Lin. ThiNet: A ï¬lter level prun-
ing method for deep neural network compression. In ICCV,
2017. 2, 5, 6, 7

[26] Y. Luo, L. Zheng, T. Guan, J. Yu, and Y. Yang. Taking a
closer look at domain shift: Category-level adversaries for
semantics consistent domain adaptation. In CVPR, 2019. 1

[27] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz.
Pruning convolutional neural networks for resource efï¬cient
transfer learning. In ICLR, 2017. 2

[28] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. In NIPS-W, 2017. 5

[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
ImageNet large scale visual recognition challenge.
IJCV, 2015. 5

[30] B. W. Silverman. Density estimation for statistics and data

analysis. Routledge, 2018. 3

[31] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
5

[32] S. Son, S. Nah, and K. Mu Lee. Clustering convolutional
kernels to compress deep neural networks. In The European
Conference on Computer Vision (ECCV), 2018. 2

[33] X. Suau, L. Zappella, V. Palakkode, and N. Apostoloff. Prin-
cipal ï¬lter analysis for guided network compression. arXiv
preprint arXiv:1807.10585, 2018. 2

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1

[35] C. Tai, T. Xiao, Y. Zhang, X. Wang, et al. Convolutional neu-
ral networks with low-rank regularization. In ICLR, 2016. 2
[36] F. Tung and G. Mori. Clip-q: Deep network compression
learning by in-parallel pruning-quantization. In CVPR, 2018.
2

[37] D. Wang, L. Zhou, X. Zhang, X. Bai, and J. Zhou. Explor-
ing linear relationship in feature map subspace for convnets
compression. arXiv preprint arXiv:1803.05729, 2018. 2

[38] J. Ye, X. Lu, Z. Lin, and J. Z. Wang. Rethinking the
smaller-norm-less-informative assumption in channel prun-
ing of convolution layers. In ICLR, 2018. 2, 3, 6, 7

[39] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han,
M. Gao, C.-Y. Lin, and L. S. Davis. NISP: Pruning networks

4348

using neuron importance score propagation. In CVPR, 2018.
1, 2, 6, 7

[40] S. Zagoruyko and N. Komodakis. Wide residual networks.

In BMVC, 2016. 5

[41] T. Zhang, S. Ye, K. Zhang, J. Tang, W. Wen, M. Fardad, and
Y. Wang. A systematic dnn weight pruning framework using
alternating direction method of multipliers. arXiv preprint
arXiv:1804.03294, 2018. 2

[42] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
deep convolutional networks for classiï¬cation and detection.
IEEE T-PAMI, 2016. 2

[43] A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen. Incremen-
tal network quantization: Towards lossless cnns with low-
precision weights. In ICLR, 2017. 2

[44] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary

quantization. In ICLR, 2017. 2

[45] F. Zhu, L. Zhu, and Y. Yang. Sim-real joint reinforcement
transfer for 3d indoor navigation.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2019. 1

[46] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y. Guo, Q. Wu,
J. Huang, and J. Zhu. Discrimination-aware channel prun-
ing for deep neural networks. In NIPS, 2018. 2

[47] H. Zhuo, X. Qian, Y. Fu, H. Yang, and X. Xue. Scsp: Spec-
tral clustering ï¬lter pruning with soft self-adaption manners.
arXiv preprint arXiv:1806.05320, 2018. 2

4349

