Adaptive Weighting Multi-Field-of-View CNN

for Semantic Segmentation in Pathology

Hiroki Tokunaga1 Yuki Teramoto2 Akihiko Yoshizawa2 Ryoma Bise1,3
1Kyushu University, Fukuoka, Japan 2Kyoto University Hospital, Kyoto, Japan

3Research Center for Medical Bigdata, National Institute of Informatics, Tokyo, Japan

Abstract

Automated digital histopathology image segmentation
is an important task to help pathologists diagnose tumors
and cancer subtypes. For pathological diagnosis of can-
cer subtypes, pathologists usually change the magniÔ¨Åca-
tion of whole-slide images (WSI) viewers. A key assump-
tion is that the importance of the magniÔ¨Åcations depends
on the characteristics of the input image, such as cancer
subtypes. In this paper, we propose a novel semantic seg-
mentation method, called Adaptive-Weighting-Multi-Field-
of-View-CNN (AWMF-CNN), that can adaptively use image
features from images with different magniÔ¨Åcations to seg-
ment multiple cancer subtype regions in the input image.
The proposed method aggregates several expert CNNs for
images of different magniÔ¨Åcations by adaptively changing
the weight of each expert depending on the input image. It
leverages information in the images with different magni-
Ô¨Åcations that might be useful for identifying the subtypes.
It outperformed other state-of-the-art methods in experi-
ments.

1. Introduction

Automated digital pathology image analysis is an im-
portant task to help pathologists diagnose tumors and can-
cer subtypes.
In particular, many segmentation methods,
such as for segmenting tumor regions, have been proposed.
The state-of-the-art methods accurately distinguish regions
(normal and tumor) in digital pathology images [7][42][43].
Recent progress in medicine has emphasized, the impor-
tance of cancer subtype analysis in histology. For example,
Yoshizawa et al. [45] showed that knowledge of lung adeno-
carcinoma subtypes could be used to predict the prognosis
of a patient who underwent surgical resection statistically.
The development of multiple-subtype segmentation will be
important for pathological image analysis.

Convolutional neural networks (CNN) [28] have been
used for classiÔ¨Åcation and segmentation tasks and they have
been shown to outperform traditional computer vision tech-

Figure 1. Examples of pathology images at difference magniÔ¨Åca-
tions; Top: 5x, Middle: 10x, Bottom: 20x. The columns indicate
the subtypes: from left to right, Normal, Lepidic, Acinar/Papillary,
Micropapillary, and Solid.

niques in various applications. Whole-slide images (WSI),
which are often used in digital pathology, cannot be inputted
to a CNN because they are so large (e.g., 100,000 √ó 50,000
pixels) compared with a natural image (i.e., WSI is over
10000 times the size of a natural image). Therefore, most
methods take a patch-based classiÔ¨Åcation approach that Ô¨Årst
segments a large image into small patches and then classi-
Ô¨Åes each patch [40]. However, a small patch image has less
context of a wide range of texture patterns that might be
useful for classiÔ¨Åcation. In order to extend the Ô¨Åeld of view
under the size limit, the input image is usually downsam-
pled. As a result, the spatial resolution is reduced (Figure 1).
This trade-off between the size of the Ô¨Åeld of view and the
resolution of the input image makes it difÔ¨Åcult to segment
cancer subtypes from an input patch image.

On the other hand,

to make pathological diagnoses,
pathologists usually check images by changing their mag-
niÔ¨Åcation in WSI viewers (i.e., they use several different
scaled images). They can check a wide range of texture pat-

112597

Field of viewResolutionNarrowWideHighLowTrade-off20x10x5x20x10x5xLepidicAcinar/Papillary MicropapillarySolidNormalterns in a low-magniÔ¨Åcation image, whereas they use high-
magniÔ¨Åcation images to check details, such as the shapes of
individual cells that are too small to be clearly seen in the
low-resolution images (Figure 1). It means that it is impor-
tant for diagnosis to use both of high-resolution images with
the narrow Ô¨Åeld of view and low-resolution images with the
wide Ô¨Åeld of view. A key assumption is that the impor-
tance of the magniÔ¨Åcations depends on the characteristic of
the input image. For example in Figure 1, images with a
wide Ô¨Åeld of view (top) have more discriminative features
than the narrow Ô¨Åeld of view with high-resolution (bottom)
for distinguishing acinar/papillary and lepidic subtypes. On
the other hand, high-resolution images (bottom) have more
discriminative features than the wide Ô¨Åeld of view (top) for
distinguishing normal and lepidic subtypes. This indicates
that it is important to adaptively use the images with differ-
ent magniÔ¨Åcation depending on the input image.

In summary, the main contributions of our study are:

‚Ä¢ We propose a semantic segmentation method that can
aggregate contextual information from multiple mag-
niÔ¨Åcation images by adaptively weighting several seg-
mentation networks (expert CNNs) that are trained us-
ing different-magniÔ¨Åcation images. Our method lever-
ages the contexts from both wide Ô¨Åeld-of-view and
high-resolution images that might be useful for iden-
tifying the subtypes.

‚Ä¢ Our end-to-end learning re-trains the expert CNNs so
that all experts work complementarily to increase the
cases that either expert can predict a correct answer,
and trains an aggregating CNN to be able to adaptively
aggregate the predicted results. This contributes to im-
proving the segmentation performance. We also an-
alyzed the effectiveness of the learning by comparing
the prediction results of experts between the before and
after the end-to-end learning.

‚Ä¢ We demonstrate the effectiveness of our method on a
challenging task; segmentation of subtype regions of
lung adenocarcinomas. Our method outperformed the
state-of-art methods, in particular, in a multi-class seg-
mentation task. We also show that our method can be
applied to any type of network.

2. Related works

Many methods have been proposed for pattern recogni-
tion tasks in pathology: Patient-level and WSI-level pN-
stage estimation (CAMELYON 2016 and 2017) [4][5], and
segmentation of tumors. Segmentation methods distinguish
the tumor regions from normal regions in a pathological im-
age (WSI), and they can be roughly classiÔ¨Åed into patch-
wise classiÔ¨Åcations and pixel-wise semantic segmentations.
Patch-based approach: The patch-based methods segment
the large WSI into small image patches and then classify

each patch image [2][6][13][33][41][49]. Wang et al. [40]
used a CNN to extract features from each patch and assign
a prediction score. They performed breast metastasis can-
cer detection based on the predicted score map in WSIs of
sentinel lymph node biopsies. Hou et al.[19] proposed an
EM-based classiÔ¨Åcation method with a CNN that automat-
ically identiÔ¨Åes discriminative patches by utilizing the spa-
tial relationships of patches. It was used to classify glioma
and non-small-cell lung carcinoma cases into subtypes. As
discussed in the introduction, contextual information from
image features of a Ô¨Åxed patch size is not enough to identify
the cancer type. To address this shortcoming, several meth-
ods that incorporate multi-scale contextual information into
a patch-wise classiÔ¨Åcation have been proposed [1][27][38].
These methods are efÔ¨Åcient for rough segmentation (i.e.,
patch-level segmentation). To obtain the pixel-level seg-
mentation, the sliding window strategy is required. One
of the drawbacks is its slowness; the classiÔ¨Åcation process
must be run separately on each pixel, and there is a lot of
redundancy due to overlapping patches.

Semantic Segmentation: To overcome the above short-
coming, FCN produces a segmentation mask image with
high resolution; its architecture consists of downsampling
layers for extracting image features and upsampling lay-
ers for obtaining a segmentation mask [32]. U-net [36] is
widely used for segmentation problems; it introduces skip
connections from downsampling layers to upsampling lay-
ers to preserve the information in high-resolution images.
This network won the ISBI challenge 2015 for segmentation
of neuronal structures in electron microscopy. Many se-
mantic segmentation methods have been proposed for nat-
ural image analysis [18][26][44].
In particular, graphical
model [8][22][48][39], spatial pyramid pooling [8][9][47],
dilated convolution [46] and multi-scale inputs (i.e., image
pyramid) [8][10][17][31][34] exploit contextual informa-
tion for segmentation. These models have shown promis-
ing results on several segmentation benchmarks by aggre-
gating multi-scale information. They assume that an entire
image can be inputted to a single network; the entire im-
age is scaled to change the range of feature extraction. In
this scaling, where the Ô¨Åeld-of-view of the scaled images
are same. However, as we discussed in the introduction, the
trade-off between spatial resolution and the size of the Ô¨Åeld
of view remains a problem for semantic segmentation in
pathology because WSIs are huge and the input to a single
network is limited due to the size of the GPU memory size.
In this study, we incorporate multi-Ô¨Åeld-of-view and multi-
resolution contextual information into a pixel-wise seman-
tic segmentation scheme.

Weighting (Gating): We propose an aggregation method
that can adaptively weight multiple CNNs trained with dif-
ferent Ô¨Åeld-of-view images. There are several methods that
can adaptively weight the effective channels [20], pixels (lo-

12598

cation) [30], and scales [15][35] in a single network. Hu
et al. [20] proposed SENet which adaptively weights the
channel-wise feature responses by explicitly modeling in-
terdependencies between channels. Their network improves
state-of-the-art deep learning architectures. Sam et al. [37]
proposed a hard-switch-CNN network that chooses a single
optimal regressor from among several independent regres-
sors. It was used for counting the number of people in patch
images. Kumagai et al. [29] proposed a mixture of counting
CNNs for the regression problem of estimating the number
of people in an image.

The studies most related to ours are follows. Alsubaie
et al. [1] proposed a multi-resolution method that simply
inputs multi-Ô¨Åeld-of-view images as multi-channels into a
single network. The method has slightly higher accuracy
compared with that of a single scale network. Sirinukun-
wattana et al. [38] systematically compared different archi-
tectures. They trained each CNN on images with different
magniÔ¨Åcations, and then fused the results from the CNNs
in several ways, such as CNN, LSTM, to obtain the Ô¨Ånal re-
sults. These methods are for classifying patch images; they
cannot be directly applied to semantic segmentation CNNs.
In addition, they do not take into account that the impor-
tance of the magniÔ¨Åcations depends on an input image.

Unlike these current methods, our novel neural network
architecture and learning algorithm can adaptively use the
features from the multi-Ô¨Åeld-of-view images depending on
the characteristic of the input image for semantic segmenta-
tion in pathology.

3. Effect of Ô¨Åeld of view

In this section, we will investigate how the contextual
information (resolution and Ô¨Åeld-of-view) is related to the
discriminative features for segmenting cancer subtypes. We
compared the outputs from CNNs that were trained individ-
ually using images with different magniÔ¨Åcations (20x, 10x,
5x) that have a trade-off between their resolution and the
Ô¨Åeld-of-view size.

In the experiment, we used lung adenocarcinoma images
annotated by pathologists. We trained expert networks 1, 2
and 3 by using images with the different magniÔ¨Åcations as
shown in the top, middle and bottom row in Figure 1 respec-
tively. The details of the dataset and setup are described in
Section 5.

In Figure 2, a circle indicates a set of pixels that an ex-
pert correctly predicted, and the value indicates the ratio of
pixels correctly predicted to all pixels. The overlap region
of the three experts indicates the set of pixels that all ex-
perts correctly predicted. The non-overlap region indicates
the pixels that only one expert correctly predicted. Union
indicates the set of pixels that either expert predicted a cor-
rect class. This Venn diagram shows the similarity of the
predicted results of the expert networks.

Union: 0.951

Union: 0.725

2-class

Multi-class

Figure 2. Venn diagrams of correct answer rates for individually
pre-trained expert CNNs on two-class (left) and multi-class (right)
segmentation tasks.

The left Ô¨Ågure showing the results of the two-class seg-
mentation task indicates that the experts gave very similar
results. The right Ô¨Ågure showing the results of the multi-
class segmentation task indicates that the non-overlap ar-
eas were large, and thus, the union of the experts was much
larger than a single expert (by over 15%). This indicates that
the contextual information from different-magniÔ¨Åcation im-
ages is effective for multiple subtype semantic segmentation
(i.e., our assumption that the importance of the magniÔ¨Åca-
tions depends on an input image is reasonable.).

These results indicate that if a method can adaptively ag-
gregate experts depending on the characteristic of the in-
put image, the method can outperform the single experts, in
which the accuracy is expected to close to the accuracy of
the union of the experts. In addition, if a method can re-
train the experts so that each expert is specialized to make
the difference large and the size of the union increases, the
method will be able to outperform than the union of the
original individual experts. To achieve this goal, we select
the mixture of experts approach that aggregates the expert
CNNs for different-magniÔ¨Åcation images while adaptively
changing the weight of each CNN depending on the input
image. The details of this method are presented in the next
section.

4. Proposed method

i for 10x, X 3

Figure 3 shows an overview of our Adaptive-Weighting-
Multi-Field-of-View-CNN (AWMF-CNN). To address the
trade-off between the resolution and the size of the Ô¨Åeld of
view, we use three different-magniÔ¨Åcation images Xi (X 1
i
for 20x, X 2
i for 5x) as inputs, where each input
image has different spatial resolutions and Ô¨Åelds of view. i
indicates an index of an image data set. Here, X 1
is the
i
target image patch that will be segmented, which has the
highest spatial resolution with the narrowest Ô¨Åeld of view.
X 2
i , where
the image center area (red dotted boxes) are the same as
shown in Figure 3. In addition to the target area, the im-

i are low magniÔ¨Åcation images of X 1

i and X 3

12599

output of fE1 . Here, the cropped heat map is estimated also
using the peripheral context i.e., image features from the
outer regions of the target area (outside of the red box). The
outputs of these experts are aggregated on the aggregating
CNN to produce the Ô¨Ånal results.

Under the assumption that the importance of magniÔ¨Åca-
tion images differ depending on the input image, we devel-
oped a weighting CNN that adaptively estimates the weights
of the expert CNNs by using the input images. We modi-
Ô¨Åed Xception [11] (developed by Google) for classiÔ¨Åcation
by replacing the fully connected (FC) layers with global av-
erage pooling, a 3-class FC layer, and a sigmoid activation
function to output three weights and Ô¨Åne-tuned the network
by using the trained parameters for 1000-class classiÔ¨Åcation
as the initial values. The range of each weight is 0 to 1.

The aggregating CNN concatenates the outputs of expert
CNNs with the estimated weights. This network outputs the
Ô¨Ånal segmentation result of the target region. It has a sim-
ple architecture consisting of Ô¨Åve convolutional layers and a
softmax function as shown in Figure 3. Each convolutional
layer except for the last is followed by a batch normaliza-
tion [21] and an exponential linear unit function [12].

4.2. Training algorithm

i , X 3
i,c, T 3

i , X 2
i,c, T 2
k=1, fA), and NX ‚Ä≤

Algorithm 1 is an overview of the training proce-
dure. We use two training data sets for the weighting
CNN fW and other CNNs (fE1 , fE2 , fE3 , fA). NX
i }NX
training image patches {X 1
i=1 with a ground-
i,c|c = 1, ..., M }NX
truth segmentation map {T 1
i=1
are used to train ({fEk }3
training
images {X ‚Ä≤1
i }NX ‚Ä≤
i,c|c =
1, ..., M }NX ‚Ä≤
i=1 are used to train the weighting CNN fW
where M is the number of classes. The image patches
{X 1
i } are different-magniÔ¨Åcation images that con-
tain the same target regions, as explained above.
(0) Initialization (Pre-train)

i=1 with {T ‚Ä≤1

i,c, T ‚Ä≤3

i,c, T ‚Ä≤2

i , X ‚Ä≤2

i , X ‚Ä≤3

i , X 2

i , X 3

The three expert CNNs {fEk }3

k=1 are pre-trained inde-
pendently to estimate the heat maps for each magniÔ¨Åcation
using different training sets. For the loss function of the ex-
perts, we used the sum of weighted cross-entropy terms for
each spatial position (pixel) in the CNN output map. The
loss functions are deÔ¨Åned as follows:

LossEk = ‚àí

NX

X

i=1

X
j‚ààX k
i

M

X

c=1

Œ±cT k

i,c(j) log Y k

i,c(j),

(1)

Œ±c =

N umber of all pixels

M √ó N umber of pixels of class c

,

(2)

where c is the class index, X k
i
for the k-th expert, j is the j-th pixel of X k
ground-truth of the j-th pixel of T k
annotations, Y k

is the input image patch
i,c(j) is th
i,c from manually labelled
i,c(j) is the prediction of the network, and Œ±c

i , T k

12600

Figure 3. Overview of Adaptive-Weighting-Multi-Field-of-View-
CNN architecture. Red dotted boxes on the input images are the
target regions of semantic segmentation. After each expert CNN
makes a prediction, the cropped target area is upsampled to the
same size of X 1
i .

i , X 3

age features from the peripheral regions, which is the outer
regions of the red box in X 2
i , are used to segment the
multiple-subtype cancer regions in X 1
i . Our network can
adaptively estimate the weights of three different magniÔ¨Å-
cations depending on an input image and aggregate these
image features by using the estimated weights to segment
cancer-subtypes.

4.1. Network architecture

Our AWMF-CNN consists of three types of network:
expert CNNs (fE1 , fE2 , fE3 ), a weighting CNN fW , and
an aggregating CNN fA as shown in Figure 3. We use
the U-net architecture [25][36] for each expert CNN. The
CNNs are trained such that each one becomes specialized
for segmenting images of a particular magniÔ¨Åcation in pre-
training. Each channel image in the output layer of each
network is a heat map of the likelihood of each subtype in
the target region, where the number of the output channels
equals the number of subtypes to be segmented. Since the
Ô¨Åeld of views in X i
3 are different from that of the
target region X i
1, the target region (red box) of each output
heat map is cropped and upsampled to the same size as the

2 and X i

ExpertCNN1ExpertCNN2ExpertCNN3(ùë§ùë§1,ùë§ùë§2,ùë§ùë§3)WeightingCNNConcatenateùë§ùë§1ùë§ùë§2ùë§ùë§3Output of  M-classSemantic Segmentationùëìùëìùê∏ùê∏1ùëìùëìùê∏ùê∏2ùëìùëìùê∏ùê∏3ùëãùëãùëñùëñ1ùëãùëãùëñùëñ2ùëãùëãùëñùëñ3ùëìùëìAùëìùëìùëäùëäC 3√ó3 | 64C 3√ó3 | 64C 3√ó3 | 32C 3√ó3 | 32C 1√ó1 | MSoftmaxWeightingCroppingUpsamplingAlgorithm 1 AWMF-CNN training algorithm
1: Input: NX training image patches {X 1

i }NX
i=1
i,c}i,c,k, and NX ‚Ä≤ training images

i , X 3

i , X 2

with ground truth {T k
{X ‚Ä≤1

i , X ‚Ä≤3

i , X ‚Ä≤2

i }NX ‚Ä≤

i=1 with {T ‚Ä≤k

i,c}i,c,k

2: % Initialization: Pre-training for fE1 , fE2 , fE3
3: Backpropagating to train {Œò(0)

k=1 using {T k

k }3

i,c}3

k=1

respectively

4: % Training for L epochs
5: for l = 1 to L do
6: % Generate training data for fW
7:

for i = 1 to NX ‚Ä≤ do

8:

9:

10:

11:

17:

18:

19:

20:

% Output fEk with input X ‚Ä≤
Y ‚Ä≤k

i

k

i ; Œò(l‚àí1)
i,c|
, w
i,c|

)
(l)
i = [w1

i , w2

i , w3
i ]

wk

i,c = fEk (X ‚Ä≤k
i,c‚à©T ‚Ä≤ k
i =
i,c|+|T ‚Ä≤ k
end for
Strain = {X ‚Ä≤

2|Y ‚Ä≤ k
|Y ‚Ä≤ k

12:
13: % Training fW for 1 epoch

i, wi}NX ‚Ä≤
i=1

Train fW with Strain and update Œò(l)

W

14:
15: % Train ({fEk }3
16:

for i = 1 to NX do

k=1, fA) with fW

(l)

i = fW (Xi; Œò(l)
W )

% Estimate weights wi by fW with current Œò(l)
w
(l)
% Train {Œòk}3
k=1 and ŒòA with w
i
Backpropagating to train ({fEk }3
k=1, fA) with
k=1 and Œò(l)
w

and update {Œò(l)

k }3

(l)
i

W

A

end for

21:
22: end for
23: Output: trained parameters {Œòk}3

fA and ŒòW for fW

k=1 for fEk , ŒòA for

is the weight for eliminating bias due to the imbalance in
the number of images in different classes. Y k
i,c for k = 2, 3
is the output of the CNN before cropping. The loss is opti-
mized by back-propagating the CNN via the optimizer for
each network and the network parameters {Œòk}3
k=1 are up-
dated. In the initialization training, each expert CNN is spe-
cialized for images of a speciÔ¨Åc magniÔ¨Åcation.

Two types of networks: the integrated network consist-
ing of four networks ({fEk }3
k=1, fA) (black dotted box in
Figure 3), and weighting CNN fW , are alternately opti-
mized by iteratively processing the following step. The ini-
tialized parameters are used in the Ô¨Årst iteration.
(1-1) Generate Training Data for Weighting CNN

i }3

First,

k=1, {T ‚Ä≤k

i,c}c,k}NX ‚Ä≤

to train the weighting CNN fW ,

the training
data for weight set is generated by using the training data
{{X ‚Ä≤k
i=1 . The key idea in training the
weighting CNN is that when a magniÔ¨Åcation of a test image
has more discriminative features than the other magniÔ¨Åca-
tion, the corresponding expert should produce a good esti-
mate for the test image. To estimate the weights, we use

the Dice coefÔ¨Åcient between the estimation image Y ‚Ä≤k
the ground-truth T ‚Ä≤k
experts,

i,c and
i,c for each class c as the weights of the

wk

i =

2|Y ‚Ä≤k
|Y ‚Ä≤k

i,c ‚à© T ‚Ä≤k
i,c| + |T ‚Ä≤k

i,c|
i,c|

, w

(l)
i = [w1

i , w2

i , w3
i ],

(3)

where ‚à© means the element-wise product and | ¬∑ | means the
sum of elements. l is an iteration index.
(1-2) Train Weighting CNN

The weighting CNN fW is trained using a set of training
images and weights Strain = {X ‚Ä≤
i=1 , and the net-
work parameters ŒòW are updated by backpropagating the
CNN via optimizer: the loss function is the mean squared
error (MSE) deÔ¨Åned as follows:

i , wi}NX ‚Ä≤

LossW =

NX ‚Ä≤
X

3

X

i=1

k=1

(cid:0)wk

i ‚àí yk

i (cid:1)2

,

(4)

where k is the index of the expert CNNs, and yk
i
weight predicted by the weighting CNN.
(2) End-to-End Learning of the integrated network

is the

i }3

i,c}3

k=1, {T k

The integrated network consisting of {fEk }3

k=1, fA are
trained with weights estimated from the weighting CNN
with the training data {{X k
i=1 in end-to-
end learning. The weights of each expert w
are Ô¨Årst esti-
mated for a training image Xi by weighting CNN fW with
the current Œò(l)
W . Using the estimated weights, the integrated
network is trained in an end-to-end manner by backpropa-
gating the CNN via the optimizer using the weighted cross-
entropy loss,

k=1}NX

(l)
i

LossA = ‚àí

NX

X

i=1

X
j‚ààX k
i

M

X

c=1

Loss = LossA +

Œ±cTi,c(j) log Yi,c(j),

(5)

3

X

k=1

LossEk ,

(6)

where Yi,c(j) is the estimated score of the j-th pixel in the
output image Yi,c of the aggregating layer. This training
process is iterated for every training data Xi and {Œò(l)
k=1,
and Œò(l)

k }3

A are updated.

This training algorithm is run until the maximum epoch
or convergence. Through it, each expert CNN becomes spe-
cialized for images in which the magniÔ¨Åcation is useful for
segmentation. The weighting CNN is trained to estimate
the weights of the specialized experts depending on an input
image. The aggregating CNN is trained to estimate the Ô¨Ånal
segmentation results of the target area, which aggregates the
experts using the estimated weights. Given a test image, the
trained weighting CNN Ô¨Årst estimates the weights, and then
the trained integrated network predicts the Ô¨Ånal segmenta-
tion result.

12601

Figure 4. Example of WSI. Left: original WSI (50, 000√ó75, 000);
Middle: annotated image provided by pathologists; Right: anno-
tation label and corresponding colors.

5. Experiments

We evaluated our method on two segmentation problems
from whole slide images (WSI), including two-class seg-
mentation into tumor and normal, and multi-subtype seg-
mentation in lung adenocarcinoma. For these experiments,
we compared the segmentation accuracy with the following
state-of-the-art methods: U-net [25][36], SegNet [3][24],
Dilated-net [46], DeepLabv3+ [9][23], and Hard-Switch-
CNN (HS) [37] that adaptively selects an expert net-
work (does not aggregate multiple images). For the pro-
posed method, we evaluated two versions: Ours (Adaptive)
is Adaptive-Weighting-Multi-Field-of-View CNN (AWMF-
CNN), and Ours (Fixed) is the Multi-Field-of-View CNN
that uses the Ô¨Åxed weight 1.0 for aggregation, where the
other setup was same as AWMF-CNN.

i,c|k = 1, ..., 3, c = 1, ..., M }Nx

stride size was 256 pixels, and the magniÔ¨Åcations were
(20x, 10x, 5x)2. The corresponding scaled annotation mask
images {T k
i=1 were used for
the label data. The image patches were randomly Ô¨Çipped
along the horizontal axis and vertical axis for data augmen-
tation. We experimented using Ô¨Åve-fold cross-validation;
the 29 WSIs were divided into Ô¨Åve sets. After that, each
WSI image was split into image patches. The image patches
of one set were used in the test, and the other patches were
used for training. We used 167,766 image patches for train-
ing the two-class segmentation and 20,848 image patches
for training four-class segmentation.The class ratios of the
training images were [Normal, Tumor] = [0.67, 0.33] and
[Lepidic, Acinar/Papillary, Micro Papillary, Solid] = [0.25,
0.29, 0.23, 0.23] at 20x magniÔ¨Åcation. Twenty percent of
the randomly selected training data was used as validation
data to prevent overÔ¨Åtting. This validation data was also
used as the training data X ‚Ä≤ for the weighting CNN. In
the weighting CNN, we used the second magniÔ¨Åcation im-
ages as the inputs in all experiments because this setup was
slightly better than the case when all magniÔ¨Åcation images
were used. The Ô¨Åve-fold data set was used to evaluate all the
compared methods. We used Nadam optimizer [16] with a
learning rate of 10‚àí4. The optimization was performed un-
til 50 epochs or convergence.

5.1. Dataset

5.3. Experimental results

Images of sliced lung adenocarcinoma stained by hema-
toxylin and eosin (H&E) were captured using a virtual slide
scanner with a maximum magniÔ¨Åcation of 40x, and 29
WSIs were used in the experiments. All images were taken
from different patients and the sizes of the images were up
to 54, 000 √ó 108, 000. To generate the training and test data,
pathologists manually annotated the regions of Ô¨Åve cancer
subtypes: 1. Normal, 2. Lepidic, 3. Acinar/Papillary1, 4.
Micropapillary, and 5. Solid, where ‚ÄòNormal‚Äô indicates the
region of outside the tumors, and the other four classes are
tumor subtypes. To segment the images into multi-subtype
regions, they were Ô¨Årst segmented using the two-class seg-
mentation and the tumor regions were then segmented into
subtype regions. Figure 4 shows a typical WSI and the cor-
responding annotated mask image. Some of the tumor re-
gions cannot be identiÔ¨Åed with any subtype. These regions
are shown in black, so in total there are six classes (four
subtypes, normal and unclear labeled tumors).

5.2. Training

To train our AWMF-CNN model, we extracted a set
of three different-magniÔ¨Åcation patches {X 1
i=1,
corresponding to the same regions, from the WSIs (Fig-
ure 1), where the window size was 256 √ó 256 pixels, the

i , X 3

i , X 2

i }N

Figure 5 shows examples of the results in comparison.
These were generated by overlaying multi-subtype segmen-
tation results in the tumor region of the two-class seg-
mentation. The results of U-net, SegNet, Dilated-net, and
Hard-Switch-CNN contain many fragmented regions (Fig-
ure 5 (c), (d), (e), and (g)). Of the compared methods,
DeepLabv3+ (f) produced best results. Both of our meth-
ods gave better results than DeepLabv3+. Their results in
the two-class image segmentation task (Figure 5 (h) and (i))
were qualitatively similar, but the adaptive weight version
was better than the Ô¨Åxed weight version in the semantic seg-
mentation task, as shown in the enlarged images of Figure 5.
We also evaluated three metrics; the overall pixel (OP)
accuracy, the mean of the per-class (PC) accuracy, and the
mean of the intersection over the union (mIoU) for two-
class and four-class segmentation tasks. These metrics [14]
are deÔ¨Åned as:

OP =

Pc T Pc

Pc (T Pc + F Pc)

, PC =

1
M X

c

T Pc

T Pc + F Pc

,

mIoU =

1
M X

c

T Pc

T Pc + F Pc + F Nc

,

(7)

1Since it is quite difÔ¨Åcult for even pathologists to identify Acinar and

210x is the magniÔ¨Åcation with which pathologists usually check images

Papillary [45], we put these two classes into one class.

for diagnosis.

12602

WSIAnnotated Image75,00050,000NormalTumorLepidicAcinar/PapillaryMicropapillarySolidBackgroundFigure 5. Examples of segmentation results. (a) original images, segmentation images from (b) manual annotation, (c) U-net [25][36], (d)
SegNet [3][24], (e) Dilated-net [46], (f) DeepLabv3+ [9][23], (g) Hard-Switch-CNN [37], (h) Ours (Fixed), and (i) Ours (Adaptive). The
color of the region indicates the subtype class (see Figure 4).

where M is the number of classes, and T Pc, F Pc, and F Nc
are the numbers of true positives, false positives, and false
negatives for class c, respectively.

image features from the different-magniÔ¨Åcation images de-
pending on the input image. This made it more accurate
than the other methods.

The performance metrics of each method are shown in
Table 1 and 2.
In the two-class segmentation task, both
of our CNNs had better metrics in all case in comparison
with U-net, SegNet, Dilated-net, and Hard-Switch-CNN.
Their performance and that of the best Ô¨Åeld of view (5x)
of DeepLabv3+ were not signiÔ¨Åcantly different. As shown
in the left image in Figure 2, each individual expert had high
accuracy by itself, and the results of these experts were very
similar. In this case, we consider that the adaptive weight-
ing strategy was not so effective, but the multi-Ô¨Åeld-of-view
strategy improved the performance compared with those of
the individual experts. Consequently, we consider that our
methods had similar performance and were better than the
others.

In the multiple-subtypes

segmentation tasks, our
AWMF-CNN achieved the best performance and the Ô¨Åxed
version was second best. The improvement was larger than
in the two-class segmentation task. As shown in the right
image in Figure 2, the results of these experts had the dif-
ferent regions and the union of the experts was much larger
than the region identiÔ¨Åed by a single expert. In this case,
we consider that our AWMF-CNN can adaptively use the

5.4. Changing expert CNNs in end to end training

Figure 6 shows the change in the correct answer rate
for individually pre-trained expert CNNs (top) and after the
end-to-end learning (bottom). Through end-to-end learn-
ing, the union of the prediction results of the experts be-
came large. In particular, the end-to-end learning more im-
proved the union of micro-papillary that had the smallest
union than the others. Although the circle of each expert
became small on average after the end-to-end learning, it is
considered that performance improved overall because all
experts work complementarily. Overall, the union of the
correct answer rates was 7% higher than that of the union
of the individual experts. We consider that this specializa-
tion of each expert by our model contributed to improving
the performance.

5.5. Varying expert networks

To demonstrate that our AWMF-CNN can be adapted to
any network, we trained it using U-net, SegNet, Dilated-
net, and DeepLabv3+ as the expert networks for the subtype
segmentation task; the training and test data sets were the

12603

(a)(b)(c)(d)(e)(f)(g)(h)(i)(a)(b)(c)(h)(i)Overall pixel

Per class

Union: 0.725

Union: 0.916

Union: 0.617

Union: 0.476

Union: 0.691

L

A/P

M

S

Union: 0.791

Union: 0.826

Union: 0.726

Union: 0.614

Union: 0.715

y
l
t
n
e
d
n
e
p
e
d
n
I

d
e
n
i
a
r
T

d
n
e
-
o
t
-
d
n
E

L

A/P

M

S

Figure 6. Venn diagrams of the correct answer rates of individually pre-trained expert CNNs (Top) and after end-to-end learning (Bottom).
Each column indicates the accuracy for each subtype; from left to right, average, Lepidic, Acinar/Papillary, Micropapillary, and Solid.

Table 1. Comparison of two-class normal or tumor segmentations.

Table 2. Comparison of four-class subtypes segmentations.

Network

MagniÔ¨Åcation

OP

PC

mIoU

Network

MagniÔ¨Åcation

OP

PC

mIoU

U-net [25][36]
U-net [25][36]
U-net [25][36]
SegNet [3][24]
SegNet [3][24]
SegNet [3][24]
Dilated-net [46]
Dilated-net [46]
Dilated-net [46]

DeepLabv3+ [9][23]
DeepLabv3+ [9][23]
DeepLabv3+ [9][23]

Hard-Switch-CNN [37]

Ours (Fixed)

Ours (Adaptive)

20x
10x
5x
20x
10x
5x
20x
10x
5x
20x
10x
5x

(20x,10x,5x)
(20x,10x,5x)
(20x,10x,5x)

0.890
0.913
0.910
0.911
0.909
0.907
0.908
0.900
0.905
0.911
0.912
0.917
0.902
0.921
0.916

0.876
0.895
0.899
0.898
0.902
0.893
0.888
0.889
0.898
0.894
0.895
0.915
0.890
0.907
0.904

0.774
0.813
0.810
0.811
0.810
0.804
0.804
0.793
0.802
0.811
0.812
0.825
0.795
0.831
0.821

same as in the above experiments. Table 3 shows that ev-
ery AWMF-CNN trained by every type of expert was 3% to
16% more accurate than the corresponding individual net-
work with the best magniÔ¨Åcation that produced the highest
performance (Table 2).

6. Conclusion

We proposed a novel Adaptive-Weighting-Multi-Field-
of-View CNN that can adaptively use image features from
different-magniÔ¨Åcation images depending on the input im-
age to segment multiple cancer subtype regions in pathol-
ogy. Our method mimics that pathologists check images
by changing their magniÔ¨Åcation adaptively depending on
the characteristic of the target image to make pathologi-
cal diagnoses. In experiments, we analyzed how each ex-
pert was specialized after the end-to-end learning; experts
were re-trained to work complementarily, as a result, it in-
creased the cases that either expert can predict a correct an-
swer. Our method outperformed state-of-the-art segmenta-

U-net [25][36]
U-net [25][36]
U-net [25][36]
SegNet [3][24]
SegNet [3][24]
SegNet [3][24]
Dilated-net [46]
Dilated-net [46]
Dilated-net [46]

DeepLabv3+ [9][23]
DeepLabv3+ [9][23]
DeepLabv3+ [9][23]

Hard-Switch-CNN [37]

Ours (Fixed)

Ours (Adaptive)

20x
10x
5x
20x
10x
5x
20x
10x
5x
20x
10x
5x

(20x,10x,5x)
(20x,10x,5x)
(20x,10x,5x)

0.446
0.484
0.524
0.477
0.547
0.492
0.433
0.445
0.515
0.585
0.625
0.588
0.486
0.641
0.672

0.446
0.481
0.537
0.477
0.544
0.525
0.422
0.456
0.528
0.580
0.624
0.583
0.484
0.642
0.676

0.300
0.331
0.379
0.320
0.398
0.326
0.274
0.314
0.378
0.438
0.474
0.433
0.347
0.505
0.536

Table 3. Segmentation accuracy of AWMF-CNN using other ex-
pert networks and improvement over the best individual expert.
MagniÔ¨Åcation mIoU improvement

Expert network

U-net [25][36]
SegNet [3][24]
Dilated-net [46]

DeepLabv3+ [9][23]

(20x,10x,5x)
(20x,10x,5x)
(20x,10x,5x)
(20x,10x,5x)

0.536
0.459
0.537
0.510

0.157
0.061
0.159
0.036

tion networks on multiple-subtypes segmentation tasks. We
also showed that it can be applied to any type of network.
In addition to magniÔ¨Åcations (experts), the importance of
subtypes (channels) and locations (pixels) may depend on
the characteristics of the input image. In the future work,
we will develop a method that can weight the combination
of magniÔ¨Åcations, subtypes, and locations.

Acknowledgments

This work was partially supported by Technology and In-
novation (Cabinet OfÔ¨Åce, Government of Japan) and JSPS
KAKENHI Grant Number JP18H05104 and JP17K08740.

12604

References

[1] N. Alsubaie, M. Shaban, D. Snead, A. Khurram, and N. Ra-
jpootation. A multi-resolution deep learning framework for
lung adenocarcinoma growth pattern classiÔ¨Åc.
In MIUA,
pages 3‚Äì11, 2018. 2, 3

[2] D. Altunbay, C. Cigir, C. Sokmensuer, and C. GunduzDemi.
Color graphs for automated cancer diagnosis and grading.
IEEE Trans. Biomedical Engineering, 57(3):665‚Äì674, 2010.
2

[3] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. TPAMI, 39(12):2481‚Äì2495, 2017. 6, 7, 8

[4] P. Bandi, O. Geessink, Q. Manson, and M. van Dijket.al.
From detection of individual metastases to classiÔ¨Åcation of
lymph node status at the patient level: the camelyon17 chal-
lenge. TMI, 2018. 2

[5] B. E. Bejnordi, M. Veta, P. J. van Diest, and B. van Gin-
neken et.al. Diagnostic assessment of deep learning algo-
rithms for detection of lymph node metastases in women
with breast cancer. JAMA, 318(22):2199‚Äì2210, 2017. 2

[6] H. Chang, Y. Zhou, A. Borowsky, and K. B. et.al. Stacked
predictive sparse decomposition for classiÔ¨Åcation of histol-
ogy sections. IJCV, 113(1):3‚Äì18, 2014. 2

[7] H. Chen, X. Qi, L. Yu, and P.-A. Heng. Dcan: deep contour-
aware networks for accurate gland segmentation. In CVPR,
pages 2487‚Äì2496, 2016. 1

[8] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. Yuille. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected
crfs. TPAMI, 40(4):834‚Äì848, 2017. 2

[9] L. Chen, G. Papandreou, and F. Schroff. Rethinking atrous
In arXiv,

convolution for semantic image segmentation.
2017. 2, 6, 7, 8

[10] L. Chen, Y. Yang, J. Wang, W. Xu, and A. Yuille. Atten-
tion to scale: Scaleaware semantic image segmentation. In
CVPR, 2016. 2

[11] F. Chollet. Xception: Deep learning with depthwise separa-

ble convolutions. In CVPR, pages 1800‚Äì1807, 2017. 4

[12] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and
accurate deep network learning by exponential linear units
(elus). In ICLR, 2015. 4

[13] A. Cruz-Roa, A. Basavanhally, F. Gonzalez, and H. G. et.al.
Automatic detection of invasive ductal carcinoma in whole
slide images with convolutional neural networks.
In SPIE
Medical Imaging, 2014. 2

[14] G. Csurka, D. Larlus, and F. Perronnin. What is a good eval-
uation measure for semantic segmentation? In CVPR, 2013.
6

[15] H. Ding, X. Jiang, B. Shuai, L. Qun, and G. Wang. Con-
text contrasted feature and gated multi-scale aggregation for
scene segmentation. In CVPR, 2018. 3

[18] S. Gould, R. Fulton, and D. Koller. Decomposing a scene
into geometric and semantically consistent regions. In ICCV,
2009. 2

[19] L. Hou, D. Samaras, T. M. Kurc, and Y. et.al.. Gao. Patch-
based convolutional neural network for whole slide tissue
image classiÔ¨Åcation. In CVPR, pages 2424‚Äì2433, 2016. 2

[20] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-

works. In CVPR, 2018. 2, 3

[21] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
arXiv preprint arXiv:1502.03167, 2015. 4

[22] V. Jampani, M. Kiefel, and P. Gehler. Learning sparse high
dimensional Ô¨Ålters: Image Ô¨Åltering, dense crfs and bilateral
neural networks. In CVPR, 2016. 2

[23] bonlime/keras-deeplab-v3-plus: Keras implementation of
deeplab v3+ with pretrained weights. https://github.
com/bonlime/keras-deeplab-v3-plus.
(Ac-
cessed on 11/16/2018). 6, 7, 8

[24] Segnet model implemented using keras framework. https:
//github.com/imlab-uiip/keras-segnet. (Ac-
cessed on 11/16/2018). 6, 7, 8

[25] Keras implementation of u-net. https://github.com/

aymanshams07/Ultra/blob/master/unet.py.
(Accessed on 11/16/2018). 4, 6, 7, 8

[26] P. Kohli and et al.. P.H. Torr. Robust higher order potentials
for enforcing label consistency. IJCV, 82:302‚Äì324, 2009. 2
[27] B. Kong, X. Wang, Z. Li, Q. Song, and S. Zhang. Cancer
metastasis detection via spatially structured deep network.
In IPMI, pages 236‚Äì248, 2017. 2

[28] A. Krizhevsky, Sutskever, Ilya, Hinton, and G. E. Imagenet
classiÔ¨Åcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097‚Äì1105, 2012. 1

[29] S. Kumagai, K. Hotta, and T. Kurita. Mixture of count-
ing cnns: Adaptive integration of cnns specialized to spe-
ciÔ¨Åc appearance for crowd counting.
In arXiv, volume
abs/1703.09393, 2017. 3

[30] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang. Learning con-
volutional networks for content-weighted image compres-
sion. In CVPR, 2018. 3

[31] G. Lin, C. Shen, A. Hengel, and I. Reid. EfÔ¨Åcient piecewise
training of deep structured models for semantic segmenta-
tion. In CVPR, 2016. 2

[32] J. Long, Shelhamer, Evan, and T. Darrell. Fully convolu-
tional networks for semantic segmentation. In CVPR, pages
3431‚Äì3440, 2015. 2

[33] H. Mousavi, V. Monga, G. Rao, and A. U. Rao. Automated
discrimination of lower and higher grade gliomas based on
histopathological image analysis. J. Pathology Informatics,
2015. 2

[34] P. Pinheiro and R. Collobert. Recurrent convolutional neu-
ral networks for scene labeling. In ICML, pages I‚Äì82‚ÄìI‚Äì90,
2014. 2

[16] T. Dozat. Incorporating nesterov momentum into adam. In

[35] Y. Qin, K. Kamnitsas, S. Ancha, and J. N. et.al. Autofocus

ICLR Workshop, 2016. 6

layer for semantic segmentation. In MICCAI, 2018. 3

[17] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In ICCV, 2015. 2

[36] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, pages 234‚Äì241. Springer, 2015. 2, 4, 6, 7, 8

12605

[37] D. B. Sam, S. Surya, and R. V. Babu. Switching convolu-
tional neural network for crowd counting. In CVPR, 2017.
3, 6, 7, 8

[38] K. Sirinukunwattana, N. Alham, C. Verrill, and J. Rittscher.
Improving whole slide segmentation through visual context
- a systematic study. In MICCAI, pages 192‚Äì200, 2018. 2, 3
[39] R. Vemulapalli, O. Tuzel, M. Liu, and R. Chellappa. Gaus-
sian conditional random Ô¨Åeld network for semantic segmen-
tation. In CVPR, 2016. 2

[40] D. Wang, A. Khosla, R. Gargeya, H. Irshad, and A. Beck.
In

Deep learning for identifying metastatic breast cancer.
arXiv, 2016. 1, 2

[41] Y. Xu, Z. Jia, Y. Ai, and F. Z. et.al. Deep convolutional
activation features for large scale brain tumor histopathology
image classiÔ¨Åcation and segmentation. In ICASP, 2015. 2

[42] Y. Xu, Z. Jia, L.-B. Wang, and Y. et.al.. Ai. Large scale tissue
histopathology image classiÔ¨Åcation, segmentation, and visu-
alization via deep convolutional activation features. BMC
bioinformatics, 18(1):281, 2017. 1

[43] Y. Xu, Y. Li, M. Liu, and Y. et.al.. Wang. Gland instance seg-
mentation by deep multichannel side supervision. In MIC-
CAI, pages 496‚Äì504. Springer, 2016. 1

[44] J. Yao, S. Fidler, and R. Urtasun. Describing the scene as
a whole: Joint object detection, scene classiÔ¨Åcation and se-
mantic segmentation. In CVPR, 2012. 2

[45] A. Yoshizawa, N. Motoi, G. J. Riely, and C. et.al.. Sima. Im-
pact of proposed iaslc/ats/ers classiÔ¨Åcation of lung adenocar-
cinoma: prognostic subgroups and implications for further
revision of staging based on analysis of 514 stage i cases.
Modern pathology, 24(5):653, 2011. 1, 6

[46] F. Yu and V. Koltun. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016. 2, 6, 7, 8

[47] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene

parsing network. In CVPR, 2017. 2

[48] S. Zheng, S. Jayasumana, B. Romera-Paredes, and V. V.
et.al. Conditional random Ô¨Åelds as recurrent neural net-
works. In ICCV, 2015. 2

[49] Y. Zhou, H. Chang, K. Barner, P. Spellman, and B. Parvin.
ClassiÔ¨Åcation of histology sections via multispectral convo-
lutional sparse coding.
In CVPR Workshop, pages 3081‚Äì
3088, 2014. 2

12606

