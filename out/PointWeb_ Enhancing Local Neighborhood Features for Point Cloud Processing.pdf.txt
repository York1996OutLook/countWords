PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing

Hengshuang Zhao1 ‚àó Li Jiang1‚àó Chi-Wing Fu1
1The Chinese University of Hong Kong

Jiaya Jia1
2Tencent Youtu Lab

,

2

{hszhao, lijiang, cwfu, leojia}@cse.cuhk.edu.hk

Abstract

This paper presents PointWeb, a new approach to ex-
tract contextual features from local neighborhood in a point
cloud. Unlike previous work, we densely connect each point
with every other in a local neighborhood, aiming to spec-
ify feature of each point based on the local region charac-
teristics for better representing the region. A novel mod-
ule, namely Adaptive Feature Adjustment (AFA) module, is
presented to Ô¨Ånd the interaction between points. For each
local region, an impact map carrying element-wise impact
between point pairs is applied to the feature difference map.
Each feature is then pulled or pushed by other features in
the same region according to the adaptively learned impact
indicators. The adjusted features are well encoded with re-
gion information, and thus beneÔ¨Åt the point cloud recogni-
tion tasks, such as point cloud segmentation and classiÔ¨Å-
cation. Experimental results show that our model outper-
forms the state-of-the-arts on both semantic segmentation
and shape classiÔ¨Åcation datasets.

1. Introduction

We have witnessed great progress in image recognition
tasks, such as image classiÔ¨Åcation [11, 22, 26, 7, 9, 8]
and semantic segmentation [14, 3, 35], which are mainly
advanced by the development of deep learning techniques
with massive model capacity. Beyond 2D image recog-
nition,
there is increasingly growing interest in 3D vi-
sion [18, 36, 6, 34, 4] for applications of autonomous driv-
ing, augmented reality, robotics, etc. Advent of large-scale
high-res 3D datasets [1, 5] also gives rise to the environment
of reasoning 3D data with deep neural networks.

Directly extending 2D image deep learning methods to
3D recognition tasks is not always feasible, since 3D scenes
are usually described by a set of points that are unordered
and scattered in 3D. It is also not reasonable to simply apply
2D feature aggregating operations like convolution to irreg-
ular point clouds, since these operations generally work on

‚àóEqual contribution.

(a)

(b)

(c)

shared
MLP

shared
MLP

shared
MLP

Max
Pool

Max
Pool

Max
Pool

Figure 1. 3D segmentation illustration. (a) PointNet++ [20]. (b)
DGCNN [32]. (c) Our approach with the Adaptive Feature Ad-
justment (AFA) module. Blue points represent the features that in-
tegrate pair information. Red points are the features that integrate
region information. Compared with aggregating pair features from
a center point to others by concatenation, our approach adaptively
learns the impact indicator between each point pair, and aggregates
features in the whole region.

regular grids. Approaches [16, 23, 21] addressed this prob-
lem by voxelizing point clouds and applying 3D CNN for
feature learning, which is a natural thought. These meth-
ods run slowly and may suffer from information loss dur-
ing the voxelization. Alternatively, PointNet [18] architec-
ture directly processes raw point clouds with shared Multi-
Layer Perception (MLP). The following PointNet++ [20]
further improved the performance by introducing a hierar-
chical structure to extract global and local features. Unlike
2D convolutions that integrate features for a pixel with its
local neighborhood, feature aggregation of a local region in
PointNet++ is implemented by max pooling as in Fig. 1 (a).

For the Dynamic Graph CNN (DGCNN) [32], it aggre-
gates information in each local region by concatenating fea-
tures of a center point with the feature difference between

5565

the center point and its k nearest neighbors, followed by an
MLP and max pooling (Fig. 1 (b)). Here, the pair relation-
ship is only considered for the center point and the consoli-
dation of region information is still limited, since the actual
operation for region aggregation is also achieved by a sim-
ple max pooling. PointCNN [13] addressed this problem by
sorting the points into a potentially canonical order and ap-
plying convolution on the points. Whether points permuted
and weighted by the X -transformation [13] are in a canoni-
cal order or not yet requires further investigation.

Different from the above methods, we exhaust the con-
textual information in a local region by connecting and ex-
ploring all pairs of points in the region. To this end, we for-
mulate a novel module, namely the Adaptive Feature Ad-
justment (AFA), to interconnect all pairs of points in the
local region and eventually form a locally fully-linked web.
We then learn the adjustment on the point feature from the
point-pair difference as shown in Fig. 1 (c). This strategy
enriches point feature in local region, and forms aggregated
feature to better describe local region for 3D recognition.

Our network learns the impact indicators from the point-
pair difference to determine feature adjustment, thus allow-
ing an appropriate adaptation for better generality. Fig. 2
illustrates the process of feature adjustment. Further, we
propose the PointWeb framework with our novel AFA mod-
ule for 3D scene recognition, as shown in Fig. 4, which
achieves state-of-the-art results on point cloud scene under-
standing tasks, including point cloud semantic segmenta-
tion and shape classiÔ¨Åcation. The top ranking experimental
results are achieved on the three most competitive datasets,
i.e., Stanford Large-Scale 3D Indoor Space (S3DIS) [1] for
semantic segmentation, ScanNet [5] for semantic voxel la-
beling, and ModelNet40 [33] for shape classiÔ¨Åcation. We
believe this effective adaptive feature adjustment module
can beneÔ¨Åt other point cloud understanding tasks. We
give all the implementation details, and make our code and
trained models publicly available1. Our main contribution
is twofold.

‚Ä¢ We enable information interchange between each
paired 3D points through adaptive feature adjustment
(AFA). This module largely enhances the representa-
tion ability of the learned point-wise features.

‚Ä¢ We propose the PointWeb framework with the key
module AFA. It achieves top performance on various
competitive point cloud datasets, thus demonstrating
its effectiveness and generality.

2. Related Work

3D Data Representation Real scanned data has a collec-
tion of 3D point coordinates. To adapt the data for con-

1https://github.com/hszhao/PointWeb

volution, one straightforward approach is to voxelize it in
a 3D grid structure [16, 23]. However, the representation
is clearly inefÔ¨Åcient, since most voxels are usually unoccu-
pied. Later, OctNet [21] explored the sparsity of voxel data
and alleviated this problem. However, the memory occu-
pancy is still high when it comes to deeper neural networks.
Moreover, since voxels are discrete representation of space,
this method still requires high resolution grids with large
memory consumption as a trade-off to keep a level of rep-
resentation quality.

Another common 3D representation is in multi-view [19,
24, 25], where the point data is projected to various speciÔ¨Åc
image planes in the 3D space to form 2D images. By this
means, point data can be processed using conventional con-
volution on 2D images. This approach, however, ignores the
intrinsic geometric relationship of 3D points, and the choice
of image planes could heavily affect results. Occluded parts
in the 3D data due to projection are not handled.

Deep Learning on Point Cloud PointNet [18] Ô¨Årst dis-
cusses the irregular format and permutation invariance of
point sets, and presents a network that directly consumes
point clouds. PointNet++ [20] extends PointNet by fur-
ther considering not only the global information but also
the local details with a farthest sampling layer and a group-
ing layer. Although the local context is exploited in Point-
Net++, information in local regions may not be well aggre-
gated using only max pooling. Hence, DGCNN [32] aggre-
gates the local context information by linking each center
point with its k nearest neighbors. The paired features are
then independently encoded by an MLP. The aggregation
operation on the local regions is still a simple max pooling.
Recent methods improve context integration by extend-
ing the convolution operator to regular grid structure to han-
dle unordered points. PCCN [31] has parametric continuous
convolution operations that deÔ¨Åne a kernel function over
the continuous support domain. PointCNN [13] exploits
the canonical order of points by permuting and weighting
input points and features with a X -Conv operator, the re-
organized points are then processed by a conventional con-
volution. Besides, Superpoint Graph (SPG) [12], on the
other hand, focuses on dealing with large point clouds. The
points are adaptively partitioned into geometrically homo-
geneous elements to build a superpoint graph, which is then
fed into a graph neural network for producing the semantic
labels.

Our work also centers on the aggregation of local fea-
tures. Unlike previous methods that adapt convolution in
point clouds, we put our attention onto the interaction be-
tween points in each local neighborhood region. By ex-
hausting the context information between all point pairs,
our network module reÔ¨Ånes the features to make them more
descriptive regarding the local neighborhood region.

5566

XYZ Space 

Feature Space

Local 
Patch 4

!#

!$

‚Ä¶ !"

Local
Patch 4

!$

‚ãØ

‚ãØ

!$

!"

(/0

‚ãØ

‚ãØ

!"

sum(-1)

‚®Ä

‚®Å

5
!#

5
‚Ä¶!"

5
!$

Feature Map  !%

: &√ó(

Difference Map  )*%++

(%)

: &√ó(√ó(

Impact Map  1%

: &√ó(√ó(

(a)

(b)

(c)

Adjusted Feature Map 

5
!%

: &√ó(

Figure 2. Our Adaptive Feature Adjustment (AFA) module.

3. Our Method

Exploring the relationship among points in a local re-
gion is the focus of this paper. Particularly, after we extract
pointwise features (or point features, for short) using a neu-
ral network, further aggregating these local features helps
improve the point cloud recognition quality for the tasks of
semantic segmentation and classiÔ¨Åcation.

Given a 3D point cloud, PointNet++ [20] uses the far-
thest point sampling to choose points as centroids, and then
applies kNN to Ô¨Ånd the neighboring points around each
centroid, which well deÔ¨Ånes the local patches in the point
cloud. For a local patch (or local neighborhood) R of M
points, we denote by F the set of point features in R, such
that F = {F1, F2, ..., FM }, where Fi ‚àà RC . C denotes the
number of channels in each point feature.

Here, the ultimate goal is to obtain a representative fea-
ture Fout ‚àà RCout of region R, where Cout is the number
of channels in the output feature. PointNet++ obtains the
representative feature using MLP followed by max pooling.
However, the procedure does not involve regional informa-
tion exchange among the points in local neighborhood.

In our approach, we densely connect points in R as a
local web of points, and formulate an Adaptive Feature Ad-
justment (AFA) module to learn the impact of each point on
other points for adjusting their features. By this means, we
take the neighborhood context into the point features and
enhance the capability of the features to describe the local
neighborhood. Fig. 2 gives an overview of the AFA module.
We call our overall network PointWeb, since our approach
effectively extracts local neighborhood context through a
web of densely-connected points.

3.1. Adaptive Feature Adjustment (AFA) Module

Given region R and its feature set F = {F1, F2, ..., FM },
we Ô¨Årst formulate the adaptive feature adjustment (AFA)
module to enhance the point features in F by learning the
contextual information in local neighborhood as

F ‚Ä≤

i = Fi + ‚àÜFi and ‚àÜFi = fmod(Fi, F), ‚àÄ Fi ‚àà F,

(1)

where F ‚Ä≤
through the feature modulator denoted as fmod.

i is the enhanced Fi, and ‚àÜFi is learned from F

The next challenge is to formulate the feature modulator
to efÔ¨Åciently exchange and further aggregate information in
F. Intuitively, different features in the local region impose
varied impact to enhance every Fi. Our feature modulator
addresses this problem by adaptively learning the amount of
impact given by each feature in F on each Fi. It is expressed
as

fmod(Fi, F) = XM

j=1

fimp(Fi, Fj) ¬∑ frel(Fi, Fj),

(2)

where fimp is a function that is learned for calculating the
amount of impact of Fj on Fi, while frel represents how
Fj relates to Fi. It is worth noting that we also include the
self-impact of Fi in the modulator.

3.1.1

Impact Function fimp

The Multi-Layer Perception (MLP) network was presented
in [18, 20] that approximates a general function over a point
set. We use MLP to calculate the impact function fimp, as
illustrated in Fig. 3. It is formulated as

wij = fimp(Fi, Fj) = MLP(g(Fi, Fj)),

(3)

where g is a function to combine features Fi and Fj , and
wij is the resulting impact indicator of Fj on Fi.

One simple approach to model g is to just concatenate
the two features. This solution has an obvious limitation
that g completely contains Fi and Fj , where half of the
feature channels remain unchanged even if Fj varies. This
makes Fi dominate when calculating the impact. Another
choice is to take the feature sum (Fi + Fj) as g. We note
this strategy is also problematic since the impact of Fj on
Fi becomes the same as that of Fi on Fj . This type of sym-
metric impact yields an undesirable property, which will be
demonstrated experimentally later.

With these considerations, we thus model g(Fi, Fj) =
Fi ‚àí Fj , making the impact calculated as the difference be-
tween the two feature vectors. We will show later in Table 3

5567

Impact  Function ùëì"%&

3.1.3 Element-wise Impact Map

ùêπ"
ùêπ#

		ùëêùëúùëöùëèùëñùëõùëí

	ùëÄùêøùëÉ

ùíòùíäùíã

Figure 3. Illustration of the impact function fimp for Fj on Fi.

the statistics from experiments of using the three different
forms of g. Note here i = j is a special case, for which we
set g(Fi, Fi) as Fi. Therefore, the impact of Fi on itself is
estimated by its own feature Fi.

3.1.2 Relation Function frel

On the other hand, the relation function frel aims to decide
how the impact indicator wij acts on Fi. A naive method is
to directly multiply wij with Fj as

frel(Fi, Fj) = Fj.

(4)

Then, the overall fmod in Eq. (2) becomes

fmod(Fi, F) =

M

Xj=1

fimp(Fi, Fj) ¬∑ Fj.

(5)

Though the result quality of point cloud recognition tasks
using this naive relation function already improves com-
pared to the baseline, the performance of our framework
can be further boosted using a different vector form for frel.
Mathematically, we model the relation function as

frel(Fi, Fj) = (cid:26) Fi ‚àí Fj

Fi

if i 6= j
if i = j

.

(6)

Please refer to the ablation study in Section 4 to compare
the performance using different forms of relation functions.
Now, for each feature Fi in local region R, the overall

output of the feature adjustment is

‚Ä≤

F

i = Œ±

(i)
i

¬∑ Fi +

M

Xj=1,j6=i

Œ±

(i)
j

¬∑ (Fj ‚àí Fi),

(7)

where

Œ±

(i)

j = (cid:26) ‚àífimp(Fi, Fj)

1 + fimp(Fi, Fi)

if i 6= j
if i = j

.

(8)

In other words, the formulation works like a force Ô¨Åeld in
the local region R (see Fig. 2 (c)), where every other feature
in R acts on Fi with a force (in the feature space), trying to
push Fi towards or away from itself. The intensity and di-
(i)
j ,
rection of the force are determined by the coefÔ¨Åcient Œ±
which is adaptively learned according to the difference be-
tween the two feature vectors. Hence, the output F
i incor-
porates the context information of the whole region, thus
better describing the characteristics of the region.

‚Ä≤

Besides the two key functions fimp and frel, it is noted that
the impact factors wij = fimp(Fi, Fj) (j = 1, .., M ) oper-
ate on the feature difference map in an element-wise fash-
ion. The length of each factor equals the number of chan-
nels in the feature. Considering that the interaction between
two point features may differ throughout the channels, in-
stead of computing point-wise impact factors, an element-
wise impact map covering the whole region and all channels
are obtained for each feature in the local region. The impact
map for Fi is formulated as

Wi = [wi1, wi2, ..., wiM ].

(9)

Denote the number of channels as C and the size of the
impact map as C √ó M . The feature modulator is then rep-
resented in matrix form as

fmod(Fi, F) = (Wi ‚äô F

(i)
dif f )e,

(10)

where ‚äô denotes element-wise multiplication, e is an all-
(i)
one vector, and F
dif f is the feature difference map of size
C √ó M . SpeciÔ¨Åcally,

F

(i)
dif f = [F1 ‚àí Fi, ..., Fi, ..., FM ‚àí Fi].

(11)

3.2. PointWeb with Local Feature Modulation

Our framework is built upon the PointNet++ architec-
ture, which is a hierarchical network composed of several
set-abstraction levels.
Inside each local region in the ab-
straction module, both the global and local features are in-
corporated for point set recognition. However, in Point-
Net++, the only operation to aggregate information in each
local region is max pooling, while our PointWeb framework
builds a fully-linked web of points among the point features
in each local region and then integrates the information of
the region by learning to determine their mutual inÔ¨Çuence.
The overall framework of PointWeb for semantic segmenta-
tion is shown in Fig. 4. The major ingredient for improving
the performance is the Adaptive Feature Adjustment (AFA)
module, which is the highlighted box in the Ô¨Ågure.

In detail, each grouping layer is followed by an AFA
module introduced in Section 3.1. Please refer to Fig. 2
for the illustration of the AFA module for a local region.
SpeciÔ¨Åcally, every two features in the same local region are
linked to form a web (Fig. 2 (b)). Then, the features are
updated by considering the interactions among them (Fig. 2
(c)). This framework makes the same feature grouped in
different local regions speciÔ¨Åed on the basis of the region
characteristics.

Without changing the number and size of features, our
AFA module can be seen as a feature transformation mod-
ule, which exchanges and aggregates context information

5568

Set 
Abstraction
Levels 

! √ó (3 + *)

Set Abstraction Level

! √ó *./00!12

Feature 
Propagation
Levels 

Cross 
Entropy
Loss

GT

!"#$ √ó 3

!"#$ √ó ' √ó (3 + *+,)

!"#$ √ó ' √ó (3 + *+,)

!"#$ √ó ' √ó *"#$

!"#$ √ó *"#$

Input

!+, √ó (3 + *+,)

Farthest 

Point 

Sampling

K-NN

AFA 
Module

Shared
MLP

Max

Pooling

Output

!"#$ √ó (3 + *"#$)

Figure 4. Architecture of our PointWeb for point cloud semantic segmentation. Adaptive Feature Adjustment (AFA) module follows the
kNN grouping layer for constructing a fully-connected web in each local region and transferring information among points.

throughout the space and channel in each local region. The
adjusted features are then given to MLP and max pooling
for further information integration in different channels.

4. Experimental Evaluation

Our proposed PointWeb framework is effective for point
cloud scene understanding. To demonstrate the effective-
ness, we conduct experiments on both point cloud seman-
tic segmentation and classiÔ¨Åcation tasks. Two large-scale
3D point cloud segmentation datasets, including Stanford
Large-Scale 3D Indoor Space (S3DIS) [1] and ScanNet [5],
are adopted. Another shape classiÔ¨Åcation dataset Model-
Net [33] is used for classiÔ¨Åcation evaluation.

4.1. Implementation Details

We conduct our experiments based on the PyTorch [17]
platform. During the training, we use the SGD solver with
base learning rate of 0.05 and a mini-batch size of 16. Mo-
mentum and weight decay are set to 0.9 and 0.0001, respec-
tively. For the S3DIS dataset, we train for 100 epochs and
decay the learning rate by 0.1 for every 25 epochs. For the
ScanNet and ModelNet40 datasets, we train for 200 epochs
and decay the learning rate by 0.1 for every 50 epochs.

4.2. S3DIS Semantic Segmentation

Data and Metric The S3DIS [1] dataset contains 3D
scans in six areas including 271 rooms. Each point in the
scan is annotated with one of the semantic labels from 13

categories (chair, table, ceiling, Ô¨Çoor, clutter etc.). To pre-
pare the training data, we follow [20], where the points are
uniformly sampled into blocks of area size 1m√ó1m. Each
point is represented with a 9D vector (XY Z, RGB and a
normalized location in the room).

During the training, we randomly sample 4,096 points
from each block on-the-Ô¨Çy. During the testing, we adopt
all the points for evaluation. Following [20, 28, 12, 13],
we report the results on two settings, i.e., testing on Area
5 (rooms are not present in other folds) and 6-fold cross
validation (calculating the metrics with results from dif-
ferent folds merged). For the evaluation metrics, we use
mean of class-wise intersection over union (mIoU), mean
of class-wise accuracy (mAcc), and overall point-wise ac-
curacy (OA).

Performance Comparison Tables 1 and 2 show the
quantitative results of different methods under the two set-
tings mentioned above. In this highly competitive dataset,
our PointWeb achieves the highest performance in terms of
mIoU and OA on Area 5 evaluation and yields the high-
est mIoU and mAcc on the 6-fold setting. The mIoU of
PointWeb reaches 60.28% on Area 5, 2.01% higher than
the current state-of-the-art, PCCN. Meanwhile, the mIoU
of PointWeb on the 6-fold cross evaluation reaches 66.73%,
outperforming the previous best method by 1.34 points.

Visual demonstration is given in Fig. 5. PointWeb well
captures certain detailed structures in the point clouds. As
shown in the Ô¨Ågure, inconspicuous object parts, like legs
of chair and table, can be distinguished and recognized cor-

5569

Method

OA mAcc mIoU ceiling Ô¨Çoor wall beam column window door

table chair

sofa bookcase board clutter

-
-

48.98 41.09 88.80 97.33 69.80 0.05
PointNet [18]
SegCloud [28]
57.35 48.92 90.06 96.05 69.86 0.00
PointCNN [13] 85.91 63.86 57.26 92.31 98.24 79.41 0.00
86.38 66.50 58.04 89.35 96.87 78.12 0.00
SPGraph [12]
67.01 58.27 92.26 96.20 75.89 0.27
-
PCCN [31]
86.97 66.64 60.28 91.95 98.48 79.39 0.00
PointWeb

3.92
18.37
17.60
42.81
5.98
21.11

46.26
38.35
22.77
48.93
69.49
59.72

10.76 58.93 52.61 5.85
23.12 70.40 75.89 40.88
62.09 74.39 80.59 31.67
61.58 84.66 75.41 69.84
63.45 66.87 65.63 47.28
34.81 76.33 88.27 46.89

40.28
58.42
66.67
52.60
68.91
69.30

26.38 33.22
12.96 41.60
62.05 56.74
2.10 52.22
59.10 46.22
64.91 52.46

Table 1. Semantic segmentation results on S3DIS dataset evaluated on Area 5.

Method

OA mAcc mIoU ceiling Ô¨Çoor wall beam column window door

table chair

sofa bookcase board clutter

47.6

78.5
-
85.5

PointNet [18]
RSNet [10]
SPGraph [12]
89.9
PointCNN [13] 88.14 75.61 65.39 94.78
PointWeb

66.2
23.1
66.45 56.47 92.48 92.83 78.56 32.75 34.37
73.0
47.1
95.1
97.3 75.82 63.25 51.71
87.31 76.19 66.73 93.54 94.21 80.84 52.44 41.33

42.4

62.8

88.0

69.3

62.1

76.4

88.7

47.5
51.62
55.3
58.38
64.89

9.6

54.1

42.0

51.6
68.11 60.13 59.72 50.22
68.4
63.2
57.18 71.63 69.12 39.08
68.13 71.35 67.05 50.34

69.2

73.5

38.2
16.42
45.9
61.15
62.68

35.2
29.4
44.85 52.03
52.9
52.19 58.59
62.20 58.49

8.7

Table 2. Semantic segmentation results on S3DIS dataset with 6-folds cross validation.

Method

mIoU

mAcc

OA

Baseline
No-Link
Summation
Subtraction
Concatenation

55.88
55.12
53.07
60.28
58.65

62.30
61.55
59.87
66.64
65.41

85.29
85.89
84.25
86.97
86.22

Table 3. Ablation results on the S3DIS dataset Area 5.

Method

mIoU

mAcc

OA

GANN
PointWeb
with Softmax
with Channel Share

58.23
60.28
59.11
58.27

65.15
66.64
65.79
64.99

86.10
86.97
86.44
86.22

Table 4. Compared with GANN [29] on the S3DIS dataset Area 5.

rectly. The last row shows two failure cases. In the left case,
part of clutter on the wall is misclassiÔ¨Åed as board. While
in the second one, clutter-liked part on the wall is recog-
nized in our algorithm but not in the ground truth. Part of
the clutter in the left is mixed with the bookcase.

Ablation Study To better understand the inÔ¨Çuence of our
design logic, we conduct ablation experiments on the S3DIS
dataset adopting Area 5 for evaluation. We Ô¨Årst investigate
the combination function g in fimp inside each local group
with four different styles, i.e., no combination between each
paired points, combination through feature summation, fea-
ture subtraction, and feature concatenation. The quantita-
tive results are shown in Table 3. Our choice of subtraction
operation on local feature for adjustment is more discrimi-
native and representative than the other alternatives.

Learning adjustment on individual feature without inter-
action with other features fails to harvest the local region
context. Summation operation of paired features results in

a decrease of discriminative ability of features in local re-
gions. It thus harms the recognition ability. Concatenation
of paired features makes part of the feature representation
almost the same, which is not as good as our choice of sub-
traction. With our subtraction operation, the combination
between each feature pair Fi and Fj is unique and thus en-
hances the classiÔ¨Åcation ability.

Further, we compare with the Graph Attention Network
(GANN) [29] that is designed for graph problems. GANN
builds connections with concatenation. Softmax operation
is applied on the attention coefÔ¨Åcients whereas AFA does
not. Hence, the impact of AFA can be positive or neg-
ative, acting as push or pull. Also, our impact indicators
vary over channels as represented in Eq. (9), giving higher
Ô¨Çexibility than the scale coefÔ¨Åcients in GANN that share
weights across channels. The comparisons are listed in Ta-
ble 4 where our PointWeb gets the highest scores.

Last but not least, for subtraction operation, we investi-
gate the self-adaptive learned impact indicators operated on
the feature itself directly other than working on the feature
difference map alternatively. With impact factors imposed
directly on the features (frel(Fi, Fj) = Fj ), the testing re-
sult is 59.79%/66.63%/86.51% (for mIoU/mAcc/OA). It is
not as good as the impact feature that operates on the dif-
ference maps (frel(Fi, Fj) = Fi ‚àí Fj ), which yields the
highest performance as 60.28/66.64/86.97 (%).

Feature Visualization For full understanding of the pro-
posed AFA module, we produce the T-SNE [15] visualiza-
tion on the input and output features that are with or without
the adjustment module processing. Features on S3DIS with
13 categories are used. We randomly sample 100 points
from the full area for each category and plot the feature dis-
tribution as illustrated in Fig. 7. From left to right are the
input features, output features generated by the baseline net-

5570

Input

Ground Truth

Our PointWeb

Input

Ground Truth

Our PointWeb

ceiling 

floor

wall

beam

column

window

door

table

chair

sofa

bookcase

board

clutter

Figure 5. Visualization of semantic segmentation results on Stanford 3D Indoor Space dataset.

our densely-connected feature adaption can promote the
network learning to generate more compact, representative
and more distinctive feature representations. Thus points
of different categories are recognized more easily and pre-
cisely. The qualitative visualization indicates that the AFA
module clearly enhances the discriminative ability of point
features.

4.3. ScanNet Semantic Voxel Labeling

The ScanNet [5] dataset contains 1,513 scanned and re-
constructed indoor scenes, split into 1201/312 for training
and testing. For the semantic voxel labeling task, 20 cat-
egories are used for evaluation and 1 class for free space.
We follow previous data processing pipeline [5, 20], where
points are uniformly sampled from scenes and are divided
into blocks, each of size 1.5m√ó1.5m. During the training,
8,192 point samples are chosen, where no less than 2% vox-
els are occupied and at least 70% of the surface voxels have
valid annotation. Points are sampled on-the-Ô¨Çy. All points
in the testing set are used for evaluation and a smaller sam-
pling stride of 0.5 between each pair of adjacent blocks is
adopted during the testing. In the evaluation, overall seman-
tic voxel labeling accuracy is adopted. For fair comparisons
with the previous approaches, we do not use the RGB color

5571

Figure 6. Semantic voxel labeling accuracy on ScanNet dataset.
Compared approaches are 3DCNN [2], PointNet [18], TCDP [27],
PointNet++ [20], PointCNN [13], and our PointWeb.

work without AFA and output feature by our PointWeb ar-
chitecture with the AFA module to enhance the information
exchange and connect paired points inside local regions.

Compared with the baseline network, where the local re-
gion features are processed independently with just MLPs,

(a) Input

(b) Baseline

(c) PointWeb

Figure 7. T-SNE visualization of features on S3DIS datasets with 13 classes: (a) input feature, (b) baseline output feature without AFA
module (c) PointWeb output feature with AFA module incorporated.

Method

input

mAcc

3DShapeNets [33]
VoxNet [16]
Subvolume [19]
MVCNN [24]
PointNet [18]
PointNet++ [20]
SpecGCN [30]
DGCNN [32]
PointCNN [13]
PointWeb

voxel
voxel
voxel
image
point
point
point
point
point
point

77.3
83.0
86.0

-

86.2

-
-

90.2
88.1
89.4

OA

84.7
85.9
89.2
90.1
89.2
91.9
92.1
92.2
92.2
92.3

Table 5. Shape classiÔ¨Åcation results on ModelNet40 dataset.

Two dropout layers with drop rate 0.5 are added into the
last two FC layers to reduce the over-Ô¨Åtting.

Table 5 shows the results of different methods evaluated
regarding the overall accuracy and mean accuracy in each
category. Our method is one of the top ranking solutions
on this dataset. It shows a principled way to improve the
applicability of point-based strategies for point cloud un-
derstanding.

5. Concluding Remarks

We have presented the adaptive feature adjustment
(AFA) module and PointWeb architecture for 3D point
cloud processing and recognition. It sets a dense connection
of point pairs inside local regions, thus enabling every point
to gather features from all others. Compared with the tradi-
tional approaches that are not aware of the local context and
information interchange with other components, our frame-
work enables better learning of feature representations for
point cloud processing. Extensive experiments with our
state-of-the-art results on three competitive datasets demon-
strate the effectiveness and generality of our approach. We
believe the proposed module can, in principle, advance the
research of 3D scene understanding in the community.

5572

(a) Ground Truth

(b) Our PointWeb

Figure 8. ScanNet result visualization.

information for training and testing.

Fig. 6 shows the semantic voxel labeling results. Our
method reaches the Ô¨Årst place on this dataset. Visual results
are shown in Fig. 8, where PointWeb well classiÔ¨Åes the sur-
rounding objects and stuff in various rooms.

4.4. ModelNet40 ClassiÔ¨Åcation

The ModelNet40 [33] dataset contains 12,311 CAD
models from 40 object categories. They are split, so that
9,843 models are used for training and 2,468 for testing.
Following [20], we uniformly sample the points from each
CAD model together with computed normal vectors from
the object meshes. For the classiÔ¨Åcation framework, we re-
place the feature propagation layers with global max pool-
ing and fully connected (FC) layers. Points in each shape
model are randomly shufÔ¨Çed to augment the training set.

References

[1] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioan-
nis Brilakis, Martin Fischer, and Silvio Savarese. 3D seman-
tic parsing of large-scale indoor spaces. In CVPR, 2016.

[2] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Le-
cun. Spectral networks and locally connected networks on
graphs. In ICLR, 2014.

[3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. TPAMI, 2018.

[4] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3d-r2n2: A uniÔ¨Åed approach for
single and multi-view 3d object reconstruction.
In ECCV,
2016.

[5] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nie√üner. ScanNet:
Richly-annotated 3D reconstructions of indoor scenes.
In
CVPR, 2017.

[6] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In CVPR, 2018.

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[8] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. In CVPR, 2018.

[9] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens
van der Maaten. Densely connected convolutional networks.
In CVPR, 2017.

[10] Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Recur-
rent slice networks for 3d segmentation of point clouds. In
CVPR, 2018.

[11] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiÔ¨Åcation with deep convolutional neural net-
works. In NIPS, 2012.

[12] Loic Landrieu and Martin Simonovsky. Large-scale point
In

cloud semantic segmentation with superpoint graphs.
CVPR, 2018.

[13] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan
Pointcnn: Convolution on X -

Di, and Baoquan Chen.
transformed points. In NIPS, 2018.

[14] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015.

[15] Laurens van der Maaten and Geoffrey Hinton. Visualizing

data using t-sne. JMLR, 2008.

[16] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-
volutional neural network for real-time object recognition. In
IROS, 2015.

[17] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS Workshop, 2017.

[18] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and
Leonidas J. Guibas. Pointnet: Deep learning on point sets
for 3d classiÔ¨Åcation and segmentation. In CVPR, 2017.

[19] Charles Ruizhongtai Qi, Hao Su, Matthias Nie√üner, Angela
Dai, Mengyuan Yan, and Leonidas Guibas. Volumetric and
multi-view cnns for object classiÔ¨Åcation on 3d data.
In
CVPR, 2016.

[20] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J.
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In NIPS, 2017.

[21] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.
Octnet: Learning deep 3d representations at high resolutions.
In CVPR, 2017.

[22] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR,
2015.

[23] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene comple-
tion from a single depth image. In CVPR, 2017.

[24] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and
Erik G. Learned-Miller. Multi-view convolutional neural
networks for 3d shape recognition. In ICCV, 2015.

[25] Hao Su, Fan Wang, Eric Yi, and Leonidas J. Guibas. 3d-
assisted feature synthesis for novel views of an object. ICCV,
2015.

[26] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, 2015.

[27] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-
Yi Zhou. Tangent convolutions for dense prediction in 3D.
CVPR, 2018.

[28] Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunY-
oung Gwak, and Silvio Savarese. Segcloud: Semantic seg-
mentation of 3d point clouds. In 3DV, 2017.

[29] Petar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-
tention networks. In ICLR, 2018.

[30] Chu Wang, Babak Samari, and Kaleem Siddiqi. Local spec-
In

tral graph convolution for point set feature learning.
ECCV, 2018.

[31] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei
Pokrovsky, and Raquel Urtasun. Deep parametric continu-
ous convolutional neural networks. In CVPR, 2018.

[32] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma,
Michael M. Bronstein, and Justin M. Solomon. Dynamic
graph cnn for learning on point clouds. arXiv:1801.07829,
2018.

[33] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3d
shapenets: A deep representation for volumetric shapes. In
CVPR, 2015.

[34] Li Xu and Jiaya Jia. Stereo matching: An outlier conÔ¨Ådence

approach. In ECCV, 2008.

[35] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
In

Wang, and Jiaya Jia. Pyramid scene parsing network.
CVPR, 2017.

[36] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning

for point cloud based 3d object detection. In CVPR, 2018.

5573

