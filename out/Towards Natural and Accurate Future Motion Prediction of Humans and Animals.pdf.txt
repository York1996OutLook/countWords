Towards Natural and Accurate Future Motion Prediction of Humans and

Animals

Zhenguang Liuâˆ—1, Shuang Wuâˆ—2,3, Shuyuan Jin4, Qi Liu4, Shijian Lu3, Roger Zimmermann4, Li Cheng2,5

1Zhejiang Gongshang University, 2Bioinformatics Institute, A*STAR,

3Nanyang Technological University, 4National University of Singapore, 5University of Alberta

liuzhenguang2008@gmail.com, wushuang@bii.a-star.edu.sg, shuyuanjin@u.nus.edu,

leuchine@gmail.com, shijian.lu@ntu.edu.sg, rogerz@comp.nus.edu.sg, chengli@bii.a-star.edu.sg

Abstract

Anticipating the future motions of 3D articulate objects
is challenging due to its non-linear and highly stochastic
nature. Current approaches typically represent the skeleton
of an articulate object as a set of 3D joints, which unfor-
tunately ignores the relationship between joints, and fails
to encode ï¬ne-grained anatomical constraints. Moreover,
conventional recurrent neural networks, such as LSTM and
GRU, are employed to model motion contexts, which inher-
ently have difï¬culties in capturing long-term dependencies.
To address these problems, we propose to explicitly encode
anatomical constraints by modeling their skeletons with a
Lie algebra representation. Importantly, a hierarchical re-
current network structure is developed to simultaneously
encodes local contexts of individual frames and global con-
texts of the sequence. We proceed to explore the applica-
tions of our approach to several distinct quantities includ-
ing human, ï¬sh, and mouse. Extensive experiments show
that our approach achieves more natural and accurate pre-
dictions over state-of-the-art methods.

1. Introduction

For a human being, it is usually not very hard to pre-
dict short-term future motions of the moving objects around
them. Without this ability, it would be extremely difï¬cult
for us to walk in a crowded street, get past defenders in
a football game, or avoid imminent dangers in movement.
Similarly, anticipating the movements of articulate objects,
especially humans and animals, is crucial for a machine to
adjust its behavior, plan its action, and properly allocate its
attention when interacting with humans and animals. Natu-
ral and accurate future motion prediction is also highly valu-
able for a wide range of applications including high-ï¬delity

âˆ—Denotes equal contribution

animal simulation in games and movies, human or animal
tracking, and intelligent driving [4, 25, 21, 31].

In this paper, we focus on the problem of predicting fu-
ture 3D poses of an articulate object given its prior skeleton
sequence. The problem is challenging due to the non-linear
dynamics, high dimensionality, and stochastic nature of hu-
man or animal movements. Conventional approaches utilize
latent-variable models, such as hidden Markov models [18],
Gaussian processes [29], and restricted Boltzmann machine
[26] to capture the temporal dynamics of human motions.
Recently, recurrent neural networks (RNNs) based methods
are introduced with improved performance. For example,
[9] uses an Encoder-Recurrent-Decoder network where the
long short-term memory (LSTM) is utilized in the recurrent
layer. [17] divides human body into spine, arms, and legs,
and uses multiple RNNs to model interactions between dif-
ferent body parts. Further, [21] and [25] resort to residual
Gated Recurrent Unit (GRU) and Modiï¬ed Highway Unit
(MHU) to capture motion contexts.

Scrutinizing the released implementations of existing
methods [17, 21, 9], one observes that current methods of-
ten encounter difï¬culties in obtaining natural and accurate
future motion prediction. Speciï¬cally, for relatively long-
term prediction, existing methods tend to degrade into mo-
tionless states or drift away to non-human like motions. For
short-term prediction, there often exists a clear discontinu-
ity between the prior pose sequence and the ï¬rst prediction
[17].
Interestingly, quantitative evaluations revealed that
many existing methods may be outperformed by a trivial
baseline that simply predicts the future as its last observed
pose [21].

We believe these issues are mainly due to the following
reasons. First, current algorithms do not respect the phys-
ical laws of motions based on the skeletal anatomy. This
often leads to strange distortions in the predicted motion.
Second, In modelling temporal motion dynamics, current
approaches rely on conventional recurrent units, such as

110004

LSTM and GRU, where the hidden state sequentially reads
in a frame and updates its value. The hidden state then tends
to be overwhelmed by the inputs in recent time steps and
such recurrent units are known to have difï¬culties in cap-
turing long-term dependencies [2]. Moreover, the sequen-
tial updating characteristic of these architectures may lend
itself to undesirable computational bottlenecks in practice.
To address these issues, we propose a novel architecture,
which consists of a dedicated Hierarchical Motion Recur-
rent (HMR) network incorporated with a Lie algebra rep-
resentation. Speciï¬cally, we characterize the pose of an
articulate object, being e.g. a human, a mouse or a ï¬sh,
as a kinematic tree consisting of one or multiple kinematic
chains based on the mathematical formalism of Lie algebra.
Fine-grained anatomical constraints are explicitly and natu-
rally encoded. The pose sequence is then fed into our pro-
posed HMR network to learn its temporal evolution. Mo-
tion contexts are jointly modeled by a state hierarchy con-
sisting of local states for individual frames and an overall
sequence-level state. At each recurrent step, a frame ex-
changes contextual information with its neighboring frames
in two directions, as well as with the sequence-level state.
Differing from the traditional RNN architecture, the num-
ber of recurrent steps in our network does not scale with the
sequence length. Empirically, our method can effectively
model motion contexts in around 10 recurrent steps.

Moreover, state-of-the-art motion prediction methods
typically focus on humans. The principled approach that
we have developed generalizes well across object categories
and is easily adapted to modeling animal motions. Empir-
ically, our approach achieves state-of-the-art results on the
H3.6m benchmark dataset with a much enhanced long-term
proï¬ciency, capable of predicting natural human-like mo-
tions over 50 seconds, and works well on animal datasets
such as ï¬sh and mouse.

To summarize, our key contributions are: 1) A novel
hierarchical RNN structure is proposed to effectively
model global and local motion contexts.
2) A Lie al-
gebra skeletal representation is formalized following the
kinematic body structure, which explicitly encodes the
anatomical constraints, and is applicable to a range of
articulate objects including but not
limited to human
3) Our method sets the new state-of-the-art on
body.
short-term and long-term motion predictions, and over-
all provides insights into the challenges in motion con-
text modeling using RNNs. Our implementation can be
found on https://github.com/BII-wushuang/
Lie-Group-Motion-Prediction.

2. Related Work

Skeleton-based Human Pose Representation

Hu-
man pose representation is a fundamental problem in com-
puter vision and graphics. Skeleton-based human pose rep-

resentations have attracted intense attention due to their ro-
bustness to viewpoint change, human body scale and mo-
tion speed as well as real-time performance [11, 12]. Many
existing approaches such as [4, 13] directly utilize raw 3D
joint positions to represent human skeleton. [8] follows this
line of work but divides the human skeleton into hierar-
chical body parts, while [5] selects only a subset of most
informative joints.
[17] and [6] characterize the orienta-
tion of a joint by an exponential map introduced in [10].
Displacement based skeletal representation has also been
explored either as displacements between pairwise skeletal
joints such as in [28] or displacements w.r.t. a global ref-
erence joint (hip center) as in [19]. [15] & [27] model the
relative geometry between each pair of joints with the Spe-
cial Euclidean group SE(3).

Motion Prediction

Conventional motion prediction
approaches have typically utilized shallow models includ-
ing hidden Markov models [18], Gaussian processes [29],
and restricted Boltzmann machine [26] to learn temporal
dynamics of human motions. Recently, deep learning based
methods have attracted increasing interests due to their su-
perior performance. For instance, [9] presents an Encoder-
Recurrent-Decoder network where LSTM is utilized for the
recurrent layer and non-linear transformations are incorpo-
rated in the encoder and decoder. [21] employs GRU as the
RNN unit and estimates the joint velocities instead of di-
rectly predicting body pose. [17] represents human body
parts as a structured graph of node RNNs linked by edge
RNNs. [25] introduces a motion context modeling network
using modiï¬ed highway unit, while [3] develops a proba-
bilistic human motion prediction network employing gen-
erative adversarial networks.

Animal Datasets Now, let us consider the other two
articulated objects that we investigate in this work, i.e., ï¬sh
and mouse. They are important model organisms in the life
science community and there are increasing interests and
efforts in development of visual behavioral analysis toolkits
adopting computer vision and machine learning. Existing
literature [7, 23, 22] mostly focus on pose estimation and
tracking. [14, 30] are two recent work analyzing mice social
behaviors, where a mouse is characterized by a straight-line
and an ellipse, respectively.

3. Our Approach

Problem Formulation

Presented with an observed
3D pose sequence hp1, p2, Â· Â· Â· , pti of an articulate object,
we are interested in predicting its future pose sequence
hpt+1, pt+2, Â· Â· Â· , pt+T i. Note pose sequences can now be
conveniently acquired by commodity motion capture sys-
tems or extracted from depth images and videos using pose
estimation algorithms (e.g., [24, 32]).

There have been efforts to cast a pose as 3D coordi-
nates of all skeletal joints [4], which in fact regards joints

10005

as independent entities and fails to capture intrinsic geo-
metric constraints. The prediction results may suffer from
severe body distortion (shown in Subsection 4.2). More
importantly, most existing approaches simply adopt LSTM
or GRU, which cannot model motion contexts, especially
long-term dependencies, effectively [2, 25].

Method Overview To tackle these problems, we pro-
pose an approach that consists of two key components: 1) a
hierarchical motion recurrent network (HMR) and 2) a uni-
ï¬ed Lie algebra representation formalism. Speciï¬cally, we
develop a Lie algebra representation for articulate objects,
which follows the kinematic structure of the body and ex-
plicitly encodes the geometric constraints and actual DoFs
(degrees of freedom) of individual joints. Then the pose
sequences, with each pose represented in the compact Lie
algebra space, are fed into the proposed HMR network to
model the dynamic evolution of poses.

3.1. Lie Algebra Representation

An articulate object can be characterized as a kinematic
tree of rigid bones connected by joints. As depicted in
Fig. 1(a), a human full-body is represented by a kinematic
tree consisting of ï¬ve kinematic chains: the spine and the
four limbs, with a total of 57 DoFs. Similarly, a ï¬sh and
a mouse are both represented as a single kinematic chain
along the spine with 44 and 12 DoFs respectively.

We utilize the theory of Lie groups [32, 27] to character-
ize the relative 3D geometry between two successive bones.
Given two successive bones biâˆ’1 and bi, their relative ge-
ometry is modeled as the 3D rigid transformation (trans-
lation and rotation) required to take bi to the position and
orientation of biâˆ’1. Mathematically, 3D rigid transforma-
tions are elements of the Special Euclidean group SE(3),
which is a Lie group. Therefore, the relative geometry
between biâˆ’1 and bi is represented as a point in SE(3),
while the entire skeletal pose is represented as a point in
SE(3) Ã— SE(3) Ã— Â· Â· Â· Ã— SE(3) [27], which is a Lie group
endowed with a manifold structure. A motion corresponds
to a curve on this manifold, and motion prediction amounts
to regressing the future curves. However, regression in this
curved manifold is non-trivial, hence we map the curves
from SE(3)Ã—SE(3)Ã—Â· Â· Â·Ã—SE(3) to its Lie algebra space.
Below we elaborate the mathematical details.

Take the simpliï¬ed ï¬sh kinematic model depicted in
Fig. 1(b) as an example, a local coordinate system is at-
tached to each of the bones such that the x-axis is aligned
with the bone and the origin is aligned with the start joint of
the bone. Descending along the kinematic chain, a 3D rigid
transformation relates the local coordinate systems between
two successive bones, which is represented as a 4Ã—4 matrix

of the form (cid:18)R t

1(cid:19), with R being a 3 Ã— 3 rotation matrix,

0

and t a 3D translation vector. This 3D rigid transformation

.

ï£¶
ï£·ï£·ï£¸
ï£¶
ï£¸ ,

is an element of SE(3). Speciï¬cally, a joint with coordi-
nates x = (x, y, z)âŠº w.r.t. coordinate system i + 1 will have
coordinates xâ€² = (xâ€², yâ€², z â€²)âŠº w.r.t coordinate system i with

1(cid:19) = (cid:18)Ri
(cid:18)xâ€²

0

ti

1(cid:19)(cid:18)x
1(cid:19).

Therefore, the entire forward chain pose is naturally rep-
resented as the product of rigid transformations. Techni-
cally, a skeletal pose is an element in SE(3) Ã— SE(3) Ã—
Â· Â· Â· Ã— SE(3) which is a Lie group. A pose corresponds to
a point in this Lie group manifold, while a motion amounts
to a curve on the manifold. Being a Lie group, the manifold
also comes with its associated tangent space or Lie alge-
bra se(3) which possesses a vector space structure, so our
familiar linear algebra techniques can work.

Lie algebra se(3)

The tangent space at the identity of
SE(3) is referred to as its Lie algebra se(3). The SE(3) â†’
se(3) association is effectuated by the logarithm map 1 log :

(cid:18)R t

1(cid:19) 7â†’ Î¾Ã— =

0

0
Ï‰3
âˆ’Ï‰2

0

ï£«
ï£¬ï£¬ï£­

âˆ’Ï‰3

0
Ï‰1
0

Ï‰2
âˆ’Ï‰1

0
0

Î½1
Î½2
Î½3
0

This has the closed-form solution [20]

Î¸

Ï‰1
Ï‰2
Ï‰3

ï£¶
ï£¸ =

Ï‰ = ï£«
ï£­

ï£«
ï£­
(cid:17) ,
where Î¸ = arccos(cid:16) Tr(R)âˆ’1

2 sin Î¸

2

R(3, 2) âˆ’ R(2, 3)
R(1, 3) âˆ’ R(3, 1)
R(2, 1) âˆ’ R(1, 2)

(1)

Î½ = ï£«
ï£­

Î½1
Î½2
Î½3

ï£¶
ï£¸ = (cid:20) (I3 âˆ’ R)Ï‰Ã— + Ï‰Ï‰âŠº

kÏ‰k

(cid:21)âˆ’1

t.

Î¾Ã— is conveniently mapped to a vector form Î¾ = (cid:18)Ï‰
Î½(cid:19).

1

m1

(cid:0)Î¾1

âŠº(cid:1)âŠº

âŠº, Â· Â· Â· , Î¾1

âŠº, Â· Â· Â· , Î¾K
1

âŠº, Â· Â· Â· , Î¾K
mK

The above formalizes the procedure of recasting a
skeletal pose as a se(3) parameterized vector, p =
. K denotes the
number of kinematic chains, mk the number of joints in
chain k, and Î¾k
the Lie algebra parameter vector of joint
i
i in chain k. Considering the bone length invariance, the
3 translational DoFs for all bones except the ï¬rst bone are
ï¬xed. If a bone is anatomically constrained to rotate along
one axis, our scheme obtains zero variance of the rotational
parameters along the other two axis. This nails down on
the exact DoF of the joint and we explicitly encode this by
ï¬xing its non-rotational elements as constants.

3.2. Hierarchical Motion Context Modeling

The future pose prediction problem can now be formu-
lated as follows: Given as input the sequence of Lie-algebra

1The inverse transformation is given by the exponential map exp :

Î¾Ã— âˆˆ se(3) 7â†’ (cid:18)R t

1(cid:19) âˆˆ SE(3).

0

10006

(a)

(b)

Figure 1: (a) A display of the three articulate objects, with their respective joints and skeletons. The ï¬rst bones of the
skeletons are of 6 DoFs, while all other bones in the ï¬sh or mouse skeleton have 2 DoFs. The ï¬rst bone amounts to the
bone located in the spine and starts from the root joint. (b) An illustration of a simpliï¬ed ï¬sh kinematic chain. bi and Jk
stands for the ith bone and kth joint, respectively. Each bone is assigned with a local coordinate system describing its rigid
transformation relative to its parent (preceding) bone, the sequence of rigid transformations characterizes a pose. Speciï¬cally,
the rigid transformation of the ï¬rst bone is relative to the global coordinate system.

Figure 2: The proposed neural network unfolded over recurrent steps. Local hidden state hn
hnâˆ’1
jâˆ’1 , hnâˆ’1

, and pj . Global state gn is updated as a function of gnâˆ’1 and hnâˆ’1

j+1 , gnâˆ’1, cnâˆ’1

, hnâˆ’1

1

j

j

, Â· Â· Â· , hnâˆ’1
tâˆ’1 .

j is updated as a function of

parameterized poses, hp1, Â· Â· Â· , pti, generate predictions for
hpt+1, Â· Â· Â· , pt+T i. In conventional motion prediction mod-
els, the encoder and decoder usually consist of single or
stacked layers of LSTM or GRU cells. Poses are input suc-
cessively into the encoder cells to model motion contexts
into hidden states. The inputs must be processed sequen-
tially and the ï¬nal hidden state is largely affected by the
inputs at recent frames [25], which cannot properly capture
long-term dependencies [2]. To avoid this issue, we con-
sider a new encoder-decoder architecture, where a Hierar-
chical Motion Recurrent (HMR) network is proposed as the
encoder, and the entire input sequence of poses is fed one-
shot instead of successively. Motion contexts are jointly
modeled by a hierarchical state S consisting of local states
hj for individual frames and an overall sequence-level state
g. At each recurrent step n, the jth frame updates its motion
context hn
j by exchanging information with its neighboring
local states hn
jâˆ’1, as well as with the global state
gn. As the number of recurrent steps increases, the number
of frames that have information exchange with hn
j becomes
larger, which enriches state representations incrementally.

j+1 and hn

Fig. 2 illustrates the proposed encoder network unfolded

over recurrent steps. At recurrent step 0, the network is
initialized with h0
g =

j = W pj + b and g0 = c0

j = c0

j , where c0
h0

j and c0

g respectively denote the cell

1

tâˆ’1

tâˆ’1

Pj=1

states of h0
j and g0. Matrix W and vector b are network
parameters. Subsequently, at each recurrent step n, the state
transition process is performed to update hn
g as
functions of hnâˆ’1
. Fig. 3 illustrates the
one-step state transition process with equations formulating
the process and ï¬gures visualizing it.

, gnâˆ’1, cnâˆ’1

j , gn, cn

, cnâˆ’1

j , cn

g

j

j

j

j , cn

jâˆ’1 , hnâˆ’1

j(cid:1) As illustrated in

Update frame-level state (cid:0)hn
the left panel of Fig. 3, at recurrent step n, hnâˆ’1
is updated
j ) by exchanging information with hnâˆ’1
(to hn
j+1 , and
gnâˆ’1. There are a total of 4 types of forget gates: f n, ln, rn,
and qn (forward, left, right, and global forget gates), which
respectively control the information ï¬‚ows from the current
jâˆ’1 , right cell state cnâˆ’1
cell state cnâˆ’1
j+1 ,
and global cell state cnâˆ’1
j . The
input gate in controls the information ï¬‚ow from the pose
input pj . Finally, the jth frame hidden state hn
j is obtained
by a Hadamard product of the output gate on
j with the tanh

to the ï¬nal cell state cn

, left cell state cnâˆ’1

g

j

10007

Root Joint Intermediate Joint End Joint 25 joints; 57 DoFs 21 joints; 44 DoFs 5 joints; 12 DoFs ğ’™ğŸğ‰2ğ‰1ğ’›ğŸğ’šğŸğ‰3ğ’™ğŸğ’›ğŸğ’šğŸâ€¦ğ‰ğ‘šğ‰ğ‘šâˆ’1Global Coordinate System b1 b2 b3 J1 J2 J3 V0 Pose 1 Pose 2 J4 EncoderDecoder(à´¤c,ğ¡ğ )(à´¥ğœ,à´¥ğ¡)recurrent steptimeğ‘¡âˆ’1ğ‘—+1ğ‘—âˆ’1ğ‘—1â€¦â€¦â€¦ğ¡ğ‘—ğ‘›âˆ’1â€¦ğ ğ’âˆ’ğŸğ¡1ğ‘›âˆ’1ğ¡ğ‘—âˆ’1ğ‘›âˆ’1ğ¡ğ‘—+1ğ‘›âˆ’1ğ¡ğ‘¡âˆ’1ğ‘›âˆ’1ğ¡ğ‘—ğ‘›ğ¡1ğ‘›ğ¡ğ‘¡âˆ’1ğ‘›â€¦â€¦ğ ğ‘›ğ©1ğ©ğ‘—âˆ’1ğ©ğ‘—ğ©ğ‘—+1ğ©ğ‘¡âˆ’1ğ‘¡timeğ‘¡+1ğ‘¡+ğ‘‡âˆ’1ğ‘¡+ğ‘‡â€¦LSTMLSTMLSTMLSTMâ€¦â€¦LSTMLSTM...LSTMLSTMğ©ğ‘¡à·ğ©ğ‘¡+1à·ğ©ğ‘¡+2à·ğ©ğ‘¡+ğ‘‡âˆ’1à·ğ©ğ‘¡+ğ‘‡Figure 3: The left panel shows the update process of frame-level state (cid:0)hn
sequence-level state (cid:0)gn, cn

j(cid:1), the right panel shows the update process of
g(cid:1). The equations in the two panels formulate the process while the ï¬gures visualize the gates.

j , cn

activated cell state cn
are parameters to be learned where k âˆˆ {f, l, r, q, i, o}.

j . Matrices Uk, Wk, Zk and biases bk

j

g

and cnâˆ’1

g and Ëœf n

j with the tanh activated cn

Update sequence-level state (cid:0)gn, cn
g(cid:1)
The update
process from gnâˆ’1 to gn is demonstrated in the right panel
of Fig. 3. Ëœf n
j are the respective forget gates that
ï¬lter information from cnâˆ’1
to global cell state
g . The global state Ëœgn is obtained by a Hadamard product
cn
of the output gate Ëœon
g . Matri-
ces ËœWk, ËœZk and biases Ëœbk with index k âˆˆ {g, f, o} are the
parameters to be learned.
Encoder & Decoder

In the proposed HMR approach,
our encoder learns a two-level representation of the entire
input sequence.
It is subsequently passed to the decoder
that recursively outputs the future motion sequence. As dis-
played in Fig. 2, our decoder engages a two-layer stacked
LSTM network. For both layers of the decoder, the cell
state input is c = 1
j , namely the average over
all frame-level cell states at the ï¬nal recurrent step n. In
particular, for the ï¬rst layer, its hidden state input is set as
h = 1
j . Similarly, the hidden state input of the
second layer is conï¬gured as hg = 1
Finally, the pose pt at time t serves as the initial input pose
to the decoder. The decoder is executed following the di-
rected links shown in Fig. 2, producing pose predictions in
a recursive manner.

tâˆ’1 Ptâˆ’1
j=1 cn

tâˆ’1 Ptâˆ’1

t (cid:16)Ptâˆ’1

j + gn(cid:17).

j=1 hn

j=1 hn

Loss function Given a kinematic chain of m joints
m)âŠº, the

with prescribed Lie algebra pose p = (Î¾ âŠº

1 , Â· Â· Â· , Î¾ âŠº

location of joint Ji can be obtained by forward kinematics

1(cid:19) = ï£®
(cid:18)Ji
ï£°

i

Yj=1

exp(Î¾jÃ—)ï£¹
ï£»(cid:18)0
1(cid:19) .

(2)

Existing works such as [17, 21] adopted a simple L2 loss
function for training, which unfortunately treats all joints
equally, and ignores this important kinematic chain hierar-
chy. One immediate consequence is that Lie algebraic pa-
rameter estimation errors will accumulate rapidly down the
chain. To account for this, we propose the following loss:

Loss(p, Ë†p) =

(m âˆ’ i)likÎ¾i âˆ’ Ë†Î¾ik2.

(3)

denotes the predicted pose, and

where Ë†p = (cid:16) Ë†Î¾ âŠº

1 , Â· Â· Â· , Ë†Î¾ âŠº

li denotes the length of bone i. Now higher losses would
be incurred if there are errors in the preceding joints of a
chain. As will be illustrated in Subsection 4.5, this setting
improves prediction performance.

mâˆ’1

Xi=1
m(cid:17)âŠº

4. Experiments

4.1. Experimental Settings

Datasets

Experiments are conducted on three large
and complex datasets of distinct articulate objects, namely
human, ï¬sh, and mouse. For human, the 3D human full-
body motion dataset H3.6m [16] is used. H3.6m contains

10008

ğ©ğ‘—ğœğœğœğœğğ‘—ğ‘›âˆ’1ğ ğ‘›âˆ’1ğœğ‘—ğ‘›âˆ’1ğœğœğ‘—âˆ’1ğ‘›âˆ’1ğœğ‘—+1ğ‘›âˆ’1ğœğ‘”ğ‘›âˆ’1à·¤ğœğ‘—ğ‘›ğ«ğ‘›ğ¥ğ‘›ğŸğ‘›ğªğ‘›ğ¢ğ‘›tanhğœğ¨ğ‘›ğœğ‘—ğ‘›ğ¡ğ‘—ğ‘›â¨€â¨€â¨€â¨€â¨€tanhâ¨â¨€enâˆ’1j=(cid:16)hnâˆ’1jâˆ’1âŠº,hnâˆ’1jâŠº,hnâˆ’1j+1âŠº(cid:17)âŠºfn=Ïƒ(cid:0)Ufpj+Wfenâˆ’1j+Zfgnâˆ’1+bf(cid:1)ln=Ïƒ(cid:0)Ulpj+Wlenâˆ’1j+Zlgnâˆ’1+bl(cid:1)rn=Ïƒ(cid:0)Urpj+Wrenâˆ’1j+Zrgnâˆ’1+br(cid:1)qn=Ïƒ(cid:0)Uqpj+Wqenâˆ’1j+Zqgnâˆ’1+bq(cid:1)in=Ïƒ(cid:0)Uipj+Wienâˆ’1j+Zignâˆ’1+bi(cid:1)Ëœcnj=tanh(cid:0)Ucpj+Wcenâˆ’1j+Zcgnâˆ’1+bc(cid:1)cnj=lnâŠ™cnâˆ’1jâˆ’1+fnâŠ™cnâˆ’1j+rnâŠ™cnâˆ’1j+1+qnâŠ™cnâˆ’1g+inâŠ™Ëœcnjonj=Ïƒ(cid:0)Uopj+Wonnâˆ’1j+Zognâˆ’1+bo(cid:1)hnj=onjâŠ™tanh(cid:0)cnj(cid:1).ğˆğ§ğ©ğ®ğ­ğ¬ğ©ğ‘—:poseinputframeğ‘—ğ¡ğ‘—ğ‘›âˆ’1:hiddenstateğ‘—atstepğ‘›âˆ’1ğ ğ‘›âˆ’1:globalstateğ‘—atstepğ‘›âˆ’1ğœğ‘—ğ‘›âˆ’1:localcellstateğ‘—atstepğ‘›âˆ’1ğœğ‘”ğ‘›âˆ’1:globalcellstateatstepğ‘›âˆ’1ğ†ğšğ­ğğ¬ğŸğ¨ğ«ğ¡ğ‘—ğ‘›ğ®ğ©ğğšğ­ğğ¢ğ’:inputgateğªğ‘›:globalforgetğŸğ‘›:forwardforgetgateğ¥ğ‘›:leftforgetgateğ«ğ‘›:rightforgetgateğ¨ğ‘›:outputgateğ†ğšğ­ğğ¬ğŸğ¨ğ«ğ ğ‘›ğ®ğ©ğğšğ­ğáˆšğŸğ’‹ğ‘›:forgetgateforhiddenstateğ‘—áˆšğŸğ‘”ğ‘›:forgetgateforglobalstateà·¥ğ¨ğ‘›:outputgateOperationsğœâ¨€â¨tanhSigmoid activationtanh activationHadamard productSumğğ®ğ­ğ©ğ®ğ­ğ¬ğ¡ğ‘—ğ‘›:hiddenstateğ‘—atstepğ‘›ğ ğ‘›:globalstateğ‘—atstepğ‘›ğœğ‘—ğ‘›:cellstateğ‘—atstepğ‘›ğœğ‘”ğ‘›:globalcellstateatstepğ‘›Legendsğ¡1ğ‘›âˆ’1,â€¦,ğ¡ğ‘¡âˆ’1ğ‘›âˆ’1ğœğœğœğœğœ1ğ‘›âˆ’1ğœğ‘—ğ‘›âˆ’1ğœğ‘¡âˆ’1ğ‘›âˆ’1ğœğ‘”ğ‘›âˆ’1áˆšğŸğ‘¡âˆ’1ğ‘›áˆšğŸğ‘”ğ‘›ğœğ‘”ğ‘›ğ ğ‘›â¨€â¨€â¨€â€¦â€¦â€¦â€¦ğ ğ‘›âˆ’1ğœtanhâ¨€à·¥ğ¨ğ‘›áˆšğŸğ‘—ğ‘›áˆšğŸ1ğ‘›â¨â¨€Ëœgnâˆ’1=1tâˆ’1tâˆ’1Xj=1hnâˆ’1jËœfnj=Ïƒ(cid:16)ËœWfhnâˆ’1j+ËœZfgnâˆ’1+Ëœbf(cid:17)Ëœfng=Ïƒ(cid:16)ËœWgËœgnâˆ’1+ËœZggnâˆ’1+Ëœbg(cid:17)cng=ËœfngâŠ™cnâˆ’1g+tâˆ’1Xj=1Ëœfnâˆ’1jâŠ™cnâˆ’1jËœoni=Ïƒ(cid:16)ËœWoËœgnâˆ’1+ËœZognâˆ’1+Ëœbo(cid:17)gn=ËœoniâŠ™tanh(cng).3.6 million 3D human poses with 15 activities performed
by 7 subjects. Following existing works [25, 17], we down-
sampled the motion sequence by 2 to 25 frames per second
(FPS). For animals, we consider the ï¬sh and mouse datasets
of [32], which contain 14 ï¬sh videos (50 FPS) of 6 different
ï¬sh, and 8 mouse videos (25 FPS) of 4 lab mice. In general,
the continuous sequences in these videos vary from 2,250
frames to 24,000 frames. For all datasets, comparison with
existing methods were done with pose sequences parame-
terized using our Lie algebra representation.

Parameter Settings

The hidden state size, i.e. length
of state vectors h and g is set to 300, 800, and 100 respec-
tively for human, ï¬sh, and mouse motion prediction. All
other settings and hyperparameters are constant across dif-
ferent objects. The default number of recurrent steps is set
to 10 and neighboring context window size 3. Following
previous works [21, 25], we do not model global transla-
tion and utilize t = 50 observed frames as inputs to predict
future T = 10 frames in training. The Adam optimizer is
employed with an intial learning rate of 0.001 which decays
by 10% every 10,000 iterations. A batch size of 16 is used
and the gradient clipping threshold is set to 5.

4.2. Evaluation on H3.6m dataset

First, we benchmark our approach against state-of-the-
art methods on the H3.6m dataset [16] with the mean angle
error (MAE) metric adopted in previous works [17, 25, 21].
In Table 1, the performance of different methods are pre-
sented in terms of MAE for 4 complex activities, namely
â€œDiscussionâ€, â€œGreeting, â€œPosingâ€ and â€œWalking Dogâ€. A
including ERD [9],
total of 10 methods are compared,
LSTM-3LR [9], SRNN [17], Res-GRU [21], zero-velocity
[25], MHU [25], our HMR network, and 3 variants of HMR.
Zero-velocity is a baseline that constantly outputs the last
observed pose. We reproduced the existing methods fol-
lowing their released codes on GitHub2. For all methods,
training is done over all activity types with a training out-
put window size of T = 10 frames. From the quantitative
results in Table 1, the ï¬rst observation is that our HMR net-
work delivers state-of-the-art results for both short-term and
long-term predictions on complex activities. Interestingly,
many existing methods, such as ERD and SRNN, tend to be
outperformed by the simple baseline zero-velocity, which
reconï¬rms the ï¬ndings reported in [21]. We can also ob-
serve that prediction error increases as we predicts deeper
into the future. We further conducted full experiments on
all other 11 activities, which reconï¬rms that our methods
consistently outperforms existing methods. We put them in
the supplementary ï¬les due to space limitations.

Besides quantitative evaluation, we further compare the
performance of state-of-the-art methods visually. Exem-

2The implementation of [25] is not available and we report the experi-

mental results in their paper.

plar visual results for long-term forecasting on the walk-
ing activity are demonstrated in Fig. 4, where the predicted
poses for future 50 seconds (1,250 frames) are presented.
A XYZ baseline is engaged in the comparison, which em-
ploys 3 stacked LSTM layers as the encoder and uses raw
3D joint coordinates instead of Lie algebra parameters as
inputs. Here, training for all methods is done over a sin-
gle activity type for a longer training output window size
of T = 100 frames. Our insights are that it is unrealistic
to expect accurate forecasting in the long-term and a more
reasonable goal is to achieve human-like motion. As shown
in Fig. 4 as well as in the supplementary video, LSTM-3LR
converges to a motionless state within 1 sec; ERD exhibits
jittery (nonsmooth) and unrealistic motion; Res-GRU con-
verges to a motionless pose after 5 sec. XYZ yields good
short term predictions but suffers from bone length defor-
mation (e.g., longer or shorter arms), leading to horrendous
predictions in the long term. HMR is capable of produc-
ing natural pose predictions throughout the entire forecast
window. In this regard, an important highlight of our archi-
tecture is the capability to generate long-term natural, bone-
length-invariant, and human-like motions.

For raw 3D joint coordinates representation as inputs,
besides the XYZ baseline presented, we also tried other en-
coders such as those in Res-GRU [21], ERD [9], and HMR.
Empirical evidences demonstrate similar severe body dis-
tortions in long-term prediction. Slight deformation at ear-
lier predictions propagate out of control, resulting in ex-
treme deterioration in long term performance. This suggests
the limitations (or challenges) of raw coordinates represen-
tation and the necessity of explicitly encoding the kinematic
body structure and anatomical constraints.

4.3. Evaluation on Fish and Mouse datasets

Whereas the human dataset poses the challenge of hav-
ing to model multiple kinematic chains simultaneously, the
ï¬sh and mouse datasets of [32] raise different issues such as
1) long kinematic chain of 21 joints for ï¬sh and 2) stochas-
tic nature of the animal motions and lack of activity type
classiï¬cation in the datasets.

For both datasets, we evaluate the performance of state-
of-the-art methods with quantitative results reported in Ta-
ble 2. HMR consistently and signiï¬cantly outperforms
other methods on both datasets.

It would be more instructive to look at the visual results.
A sample forecasting result for the ï¬sh dataset is shown in
Fig. 5. The long kinematic chain in the ï¬sh skeletal anatomy
resulted in modeling difï¬culties for the competing methods.
In both ERD and Res-GRU, the predicted ï¬sh pose demon-
strate unnatural distortions and a zigzagged contour, which
is especially the case for ERD. LSTM-3LR suffers from the
issue of quickly converging to a motionless state. In con-
trast, HMR retains streamlined shapes and the curvature of

10009

Methods

ERD [9]
LSTM-3LR [9]
SRNN [17]
Res-GRU [21]
Zero-velocity [21]
MHU [25]
HMR (Remove l, r from update of h)
HMR (Remove g)
HMR (Remove 2nd decoder layer)
HMR

Methods

ERD [9]
LSTM-3LR [9]
SRNN [17]
Res-GRU [21]
Zero-velocity [21]
MHU [25]
HMR (Remove l, r from update of h)
HMR (Remove g)
HMR (Remove 2nd decoder layer)
HMR

80ms
2.22
1.80
1.16
0.31
0.31
0.31
0.30
0.30
0.30
0.29

80ms
2.42
1.22
1.74
0.41
0.28
0.33
0.27
0.25
0.30
0.24

Discussion

Greeting

160ms

320ms

400ms

560ms

640ms

720ms

1000ms

2.38
2.00
1.40
0.69
0.67
0.66
0.59
0.57
0.60
0.55

2.58
2.13
1.75
1.03
0.97
0.93
0.93
0.87
0.94
0.83

2.69
2.13
1.85
1.12
1.04
1.00
1.02
0.96
1.03
0.94

2.89
2.29
2.06
1.52
1.41
1.37
1.42
1.38
1.40
1.35

Posing

2.93
2.32
2.07
1.61
1.56
1.51
1.56
1.54
1.55
1.49

2.94
2.36
2.08
1.70
1.71
1.66
1.67
1.69
1.69
1.61

3.11
2.44
2.19
1.87
1.96
1.88
1.80
1.89
1.86
1.72

160ms

320ms

400ms

560ms

640ms

720ms

1000ms

2.77
1.89
1.89
0.84
0.57
0.64
0.56
0.54
0.59
0.53

3.26
3.02
2.23
1.53
1.13
1.22
1.22
1.19
1.26
1.12

3.39
3.53
2.43
1.81
1.38
1.47
1.52
1.48
1.49
1.42

3.43
4.25
2.67
2.06
1.81
1.82
1.76
1.93
1.87
1.75

3.42
4.57
2.73
2.21
2.14
2.11
1.91
2.10
2.04
1.89

3.45
4.83
2.79
2.24
2.23
2.17
2.08
2.23
2.20
2.02

3.87
4.60
3.42
2.53
2.78
2.51
2.60
2.65
2.66
2.50

80ms
1.70
0.93
1.33
0.52
0.54
0.54
0.54
0.53
0.57
0.52

80ms
1.58
0.76
1.57
0.56
0.60
0.56
0.56
0.56
0.60
0.55

160ms

320ms

400ms

560ms

640ms

720ms

1000ms

2.04
1.51
1.60
0.86
0.89
0.87
0.89
0.86
0.92
0.85

2.60
2.27
1.83
1.30
1.30
1.27
1.30
1.28
1.33
1.25

160ms

320ms

1.78
1.29
1.73
0.95
0.98
0.88
0.89
0.87
0.90
0.87

2.02
1.91
1.93
1.33
1.36
1.21
1.33
1.23
1.24
1.20

2.81
2.54
1.98
1.47
1.49
1.45
1.44
1.45
1.49
1.40

3.29
2.97
2.27
1.78
1.79
1.75
1.68
1.71
1.75
1.65
Walking Dog
560ms

400ms

3.47
3.05
2.28
1.75
1.74
1.71
1.66
1.70
1.79
1.62

3.55
3.12
2.30
1.82
1.77
1.74
1.70
1.76
1.82
1.67

3.43
3.09
2.31
1.96
1.80
1.87
1.85
1.95
1.88
1.73

640ms

720ms

1000ms

2.10
2.18
1.96
1.48
1.50
1.37
1.49
1.42
1.48
1.36

2.31
2.72
2.13
1.78
1.74
1.67
1.73
1.84
1.79
1.65

2.37
3.01
2.17
1.81
1.80
1.72
1.82
1.90
1.86
1.70

2.48
3.30
2.23
1.88
1.87
1.81
1.90
1.94
1.94
1.77

2.60
3.78
2.20
1.96
1.96
1.90
2.00
2.06
2.07
1.84

Table 1: Performance evaluation (in MAE) of comparison methods over 4 different action types on the H3.6m dataset.

Ground 
Truth

ERD
(2015)

LSTM-3LR 
(2015)

Res-GRU 
(2017)

XYZ 
Baseline

HMR 
(Ours)

25

50

75

100

1,000

1,025

1,050

1,075

1,100

1,125

1,150

1,175

1,200

1,225

1,250

â€¦

Frame

Figure 4: Long-term motion forecasting of walking activity by the comparison methods on the H3.6m dataset. 25 frames correspond to 1 sec. The
complete visual results can be found in the supplementary video ï¬le.

Methods

ERD [9]
LSTM-3LR [9]
Res-GRU [21]
HMR (ours)

80ms
0.62
0.91
0.52
0.40

Fish

Mouse

160ms

320ms

400ms

560ms

640ms

720ms

1000ms

0.59
0.59
0.56
0.48

0.54
0.45
0.52
0.44

0.69
0.39
0.39
0.28

0.79
0.25
0.26
0.13

0.85
0.26
0.25
0.12

0.87
0.30
0.26
0.13

1.20
0.29
0.26
0.11

80ms
0.77
0.68
0.40
0.39

160ms

320ms

400ms

560ms

640ms

720ms

1000ms

0.62
0.61
0.48
0.44

0.67
0.81
0.66
0.56

0.77
0.84
0.70
0.63

0.86
0.85
0.74
0.69

0.83
0.81
0.71
0.68

0.88
0.85
0.72
0.67

0.91
0.80
0.74
0.69

Table 2: Performance evaluation (in MAE) of the comparison methods for the Fish and Mouse datasets of [32].

the predicted pose remains smooth and natural.

4.4. Computational Efï¬ciency

The mouse motion is highly stochastic, leading to difï¬-
culties in accurate forecasting. Compared methods all tend
to converge to motionless states. A sample forecasting se-
quence on the mouse dataset is displayed in Fig. 6. ERD,
LSTM-3LR and Res-GRU converge to motionless states af-
ter 30, 15, 45 frames respectively. HMR remains in mo-
tion throughout with fairly accurate prediction for the ï¬rst
40 frames. In particular, it can be seen that the HMR pre-
dicted mouse body orientation remains in alignment with
the ground truth whereas compared methods all wrongly
predict the mouse orientation.

The training and testing time as well as number of train-
ing parameters required for different methods are shown
in Table 3. Our architecture is implemented using Ten-
sorFlow 1.8 [1]. All experiments were performed on a
Nvidia GeForce GTX TITAN X GPU. In brief, HMR re-
quires less parameters than existing methods, and its com-
putation speed is signiï¬cantly faster.

4.5. Loss Function Study

In this subsection we compare our loss function pre-
sented in Eq. (3) against conventional L2 loss. In addition

10010

Ground 
Truth

ERD
(2015)

LSTM-3LR 
(2015)

Res-GRU 
(2017)

HMR 
(Ours)

Ground 
Truth

ERD
(2015)

LSTM-3LR 
(2015)

Res-GRU 
(2017)

HMR 
(Ours)

0

20

40

60

80

0

15

30

45

60

Frame

Frame

Figure 5: Motion forecasting on Fish dataset. The head of the ï¬sh
is rendered wider (for resemblance with the actual zebraï¬sh).

Figure 6: Motion forecasting on Mouse dataset. The mouse shape
is rendered in gray color with joints marked out along the spine.

Methods
ERD
LSTM-3LR
SRNN
Res-GRU
HMR (Ours)

# Parameters Train time / 1,000 iterations (s) Test FPS
17,348,054
20,831,054
22,817,888
6,684,726
4,422,654

52.4
33.7
14.4
173.5
406.1

428
632
947
65
33

Table 3: Number of training parameters and efï¬ciency.

to the MAE metric used, we also use the mean joint error
(MJE) as a supplementary metric. Experiments were con-
ducted on H3.6m dataset with results averaged over all 15
activities. In the experiments, we kept the network struc-
ture ï¬xed as HMR, while using different losses. As shown
in Table 4, the proposed loss consistently outperforms the
L2 loss. By respecting the hierarchical nature of kine-
matic chains, our proposed loss reduces errors in the root
joint predictions, which translates to a better estimate of
the global orientation and thus signiï¬cant improvements on
the MJE metric. Experimental results on animals (see Sub-
section 4.3) also indicate that employing the proposed loss
function (in our work) instead of the L2 loss (employed
in compared works) is more successful in capturing the
anatomical features of the skeleton and its motion dynam-
ics, such as better prediction of the mouse orientation.

MAE

Time(ms)
L2
Our
L2
Our

MJE

80
0.34
0.33
67.6
9.3

160
0.57
0.55
78.3
17.1

320
0.87
0.83
83.1
28.1

400
0.96
0.93
86.4
32.7

560
1.15
1.13
97.6
40.9

640
1.24
1.21
105.7
43.5

1000
1.40
1.36
113.4
46.4

Table 4: Comparison of our loss function against L2 loss in
terms of MAE and MJE, respectively.

4.6. Extensions and Reductions in HMR Network

By default in HMR the number of neighboring context
window size is ï¬xed to 3. As the number of steps increases,
the number of local states that have information exchange
with hn
j becomes larger. It is interesting to see the effect
of enlarging or reducing neighboring window size, which
we report in Table 5. We observe that enlarging the neigh-
boring context window size does not necessarily leads to
improvement in accuracy. The optimal number of recurrent

steps and hidden state size on the H3.6m dataset is also em-
pirically determined. This allows us to settle on the default
hidden state size 300 and number of recurrent steps 10.

Hidden Size Val. Loss

Rec. Steps Val. Loss Neigh. Win. Val. Loss

100
200
300
400
500

0.177
0.165
0.151
0.172
0.175

1
5
10
15
20

0.169
0.162
0.151
0.159
0.161

1
3
5
7
11

0.169
0.151
0.155
0.159
0.158

Table 5: Validation Loss on H3.6m on varying internal parameters, hid-
den states size, number of recurrent steps n, and context window size.

We also report 3 variants of our HMR network: 1) Re-
moval of the left and right forget gates in the update of the
frame level states hj from the encoder; 2) Removal of the
sequence level state g from the encoder; and 3) Removal
of the second decoder layer in Table 1. Removal of the se-
quence level state g from our HMR encoder is observed not
to severely affect forecasting before 400 ms (i.e. 10 frames)
but performance beyond 400 ms declines noticeably. Pre-
diction accuracy also suffered upon removal of the left and
right forget gates or the removal of the second decoder layer.

5. Conclusion

We have proposed a hierarchical motion recurrent net-
work, which can effectively model motion contexts and sig-
niï¬cantly surpasses existing work in both short-term and
long-term motion predictions. The proposed network incor-
porates our Lie algebra representation naturally preserves
the skeletal articulation of the underlying objects. Exten-
sive results on human, ï¬sh and mouse datasets demonstrate
the competency of our approach. Future work includes fur-
ther investigation into group-level motion predictions.

6. Acknowledgments

This paper is partly supported by the National Key R&D
Program of China (No. 2017YFB1401304), the Natural
Science Foundation of Zhejiang Province, China (Grant No.
LQ19F020001), the JCO grants of A*STAR Singapore, and
the support from University of Alberta-Huawei Joint Inno-
vation Center in Canada. We thank Yunphant Ltd. for their
constructive suggestions.

10011

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-
ï¬‚ow: A system for large-scale machine learning.
In 12th
{USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 16), pages 265â€“283, 2016. 7

[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate. CoRR,
abs/1409.0473, 2014. 2, 3, 4

[3] E. Barsoum, J. Kender, and Z. Liu. HP-GAN: proba-
bilistic 3d human motion prediction via GAN. CoRR,
abs/1711.09561, 2017. 2

[4] J. Butepage, M. Black, D. Kragic, and H. Kjellstrom. Deep
representation learning for human motion prediction and
classiï¬cation. In CVPR, 2017. 1, 2

[5] A. A. Chaaraoui, J. R. Padilla-LÂ´opez, P. Climent-PÂ´erez, and
F. FlÂ´orez-Revuelta. Evolutionary joint selection to improve
human action recognition with RGB-D devices. ESWA,
41(3):786â€“794, 2014. 2

[6] K. Cho, B. van Merrienboer, CÂ¸ . GÂ¨ulcÂ¸ehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase
representations using RNN encoder-decoder for statistical
machine translation.
In EMNLP, pages 1724â€“1734, 2014.
2

[7] P. DollÂ´ar, P. Welinder, and P. Perona. Cascaded pose regres-

sion. In CVPR, pages 1078â€“1085, 2010. 2

[8] Y. Du, W. Wang, and L. Wang. Hierarchical recurrent neu-
ral network for skeleton based action recognition. In CVPR,
pages 1110â€“1118, 2015. 2

[9] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik. Recurrent
network models for human dynamics. In ICCV, 2015. 1, 2,
6, 7

[10] F. S. Grassia. Practical parameterization of rotations using
the exponential map. J. Graphics, GPU, & Game Tools,
3(3):29â€“48, 1998. 2

[11] F. Han, B. Reily, W. Hoff, and H. Zhang. Space-time rep-
resentation of people based on 3d skeletal data: A review.
CVIU, 158:85â€“105, 2017. 2

[12] D. Hoiem and S. Savarese. Representations and Techniques
for 3D Object Recognition and Scene Interpretation. Synthe-
sis Lectures on Artiï¬cial Intelligence and Machine Learning.
Morgan & Claypool Publishers, 2011. 2

[13] D. Holden, J. Saito, and T. Komura. A deep learning frame-
work for character motion synthesis and editing. ACM Trans.
Graph., 35(4):138:1â€“138:11, 2016. 2

[14] W. Hong, A. Kennedy, X. Burgos-Artizzu, M. Zelikowsky,
S. Navonne, P. Perona, and D. Anderson. Automated
measurement of mouse social behaviors using depth sens-
ing, video tracking, and machine learning.
PNAS,
112(38):E5351â€“E5360, 2015. 2

[15] Z. Huang, C. Wan, T. Probst, and L. V. Gool. Deep learn-
ing on lie groups for skeleton-based action recognition. In
CVPR, pages 1243â€“1252, 2017. 2

[16] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Hu-
man3.6m: Large scale datasets and predictive methods for
3d human sensing in natural environments.
IEEE TPAMI,
36(7):1325â€“39, 2014. 5, 6

[17] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena. Structural-
In CVPR,

rnn: Deep learning on spatio-temporal graphs.
pages 5308â€“5317, 2016. 1, 2, 5, 6, 7

[18] A. M. Lehrmann, P. V. Gehler, and S. Nowozin. Efï¬cient
nonlinear markov models for human motion. In CVPR, pages
1314â€“1321, 2014. 1, 2

[19] J. Luo, W. Wang, and H. Qi. Group sparsity and geometry
constrained dictionary learning for action recognition from
depth maps. In ICCV, pages 1809â€“1816, 2013. 2

[20] Y. Ma, S. Soatto, J. Koseck, and S. Sastry. An Invitation to
3-D Vision: From Images to Geometric Models. Springer,
2010. 3

[21] J. Martinez, M. Black, and J. Romero. On human motion
prediction using recurrent neural networks. In CVPR, 2017.
1, 2, 5, 6, 7

[22] A. Mathis, P. Mamidanna, K. M. Cury, T. Abe, V. N. Murthy,
M. W. Mathis, and M. Bethge. Deeplabcut: markerless pose
estimation of user-deï¬ned body parts with deep learning.
Technical report, Nature Publishing Group, 2018. 2

[23] G. Salem, J. Krynitsky, M. H. H. III, T. Pohida, and X. P.
Burgos-Artizzu. Cascaded regression for 3d pose estima-
tion for mouse in ï¬sheye lens distorted monocular images.
In 2016 IEEE Global Conference on Signal and Information
Processing, GlobalSIP 2016, Washington, DC, USA, Decem-
ber 7-9, 2016, pages 1032â€“1036, 2016. 2

[24] J. Shotton, R. Girshick, A. Fitzgibbon, T. Sharp, M. Cook,
M. Finocchio, R. Moore, P. Kohli, A. Criminisi, A. Kipman,
and A. Blake. Efï¬cient human pose estimation from single
depth images. TPAMI, 35(12):2821â€“2840, 2013. 2

[25] Y. Tang, L. Ma, W. Liu, and W. Zheng. Long-term human
motion prediction by modeling motion context and enhanc-
ing motion dynamic. CoRR, abs/1805.02513, 2018. 1, 2, 3,
4, 6, 7

[26] G. W. Taylor, G. E. Hinton, and S. T. Roweis. Modeling
human motion using binary latent variables. In NIPS, pages
1345â€“1352, 2006. 1, 2

[27] R. Vemulapalli, F. Arrate, and R. Chellappa. Human action
recognition by representing 3d skeletons as points in a lie
group. In CVPR 2014, pages 588â€“595, 2014. 2, 3

[28] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet en-
semble for action recognition with depth cameras. In CVPR,
pages 1290â€“1297, 2012. 2

[29] J. M. Wang, D. J. Fleet, and A. Hertzmann. Gaussian process

dynamical models. In NIPS, pages 1441â€“1448, 2005. 1, 2

[30] A. Wiltschko, M. Johnson, G. Iurilli, R. Peterson, J. Ka-
ton, S. Pashkovski, V. Abraira, R. Adams, and S. Datta.
Mapping sub-second structure in mouse behavior. Neuron,
88(6):1121â€“1135, 2015. 2

[31] J. Xie, S. Zhu, and Y. N. Wu. Synthesizing dynamic patterns
In CVPR, pages

by spatial-temporal generative convnet.
1061â€“1069, 2017. 1

[32] C. Xu, L. Govindarajan, Y. Zhang, and L. Cheng. Lie-X:
Depth image based articulated object pose estimation, track-
ing, and action recognition on Lie groups. IJCV, 123(3):454â€“
78, 2017. 2, 3, 6, 7

10012

