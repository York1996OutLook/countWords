VERI-Wild: A Large Dataset and a New Method for

Vehicle Re-IdentiÔ¨Åcation in the Wild

Yihang Lou1,4

Yan Bai1,5

Jun Liu2

Shiqi Wang3

Ling-Yu Duan1,4, ‚àó

1Peking University, Beijing, China

3City University of Hong Kong, Hongkong, China

2Nanyang Technological University, Singapore
4Peng Cheng Laboratory, Shenzhen, China

{yihanglou, yanbai, lingyu}@pku.edu.cn, jliu029@ntu.edu.sg, shiqwang@cityu.edu.hk

5Hulu, Beijing, China

Abstract

Re-Identify Vehicles In the Wild

Hard Negative

Vehicle Re-identiÔ¨Åcation (ReID) is of great signiÔ¨Åcance
to the intelligent transportation and public security. How-
ever, many challenging issues of Vehicle ReID in real-world
scenarios have not been fully investigated, e.g., the high
viewpoint variations, extreme illumination conditions, com-
plex backgrounds, and different camera sources. To pro-
mote the research of vehicle ReID in the wild, we collect
a new dataset called VERI-Wild with the following dis-
tinct features: 1) The vehicle images are captured by a
large surveillance system containing 174 cameras covering
a large urban district (more than 200km2). 2) The camera
network continuously captures vehicles for 24 hours in each
day and lasts for 1 month. 3) It is the Ô¨Årst vehicle ReID
dataset that is collected from unconstrained conditions1.
VERI-Wild contains more than 400 thousand images of 40
thousand vehicle IDs. In this paper, we also propose a new
method for vehicle ReID, in which, the ReID model is cou-
pled into a Feature Distance Adversarial Network (FDA-
Net), and a novel feature distance adversary scheme is de-
signed to online generate hard negative samples in feature
space to facilitate ReID model training. The comprehensive
results show the effectiveness of our method on the proposed
dataset2 and the other two existing datasets.

1. Introduction

Vehicle Re-IdentiÔ¨Åcation (ReID) aims to retrieve im-
ages of a query vehicle from a large-scale vehicle database,
which is of great signiÔ¨Åcance to the urban security and
city management [9][31]. The straightforward method is to
identify vehicles by the recognition of license plates [10][4].

‚àóLing-Yu Duan is the corresponding author.
1The unconstrained condition arises from the data collection in a real
surveillance camera network of a city-scale district, covering huge diver-
sity of viewpoints, resolutions, illuminations, camera sources, weathers,
occlusions, backgrounds, vehicle models in the wild, etc.

2The dataset is available at https://github.com/PKU-IMRE/

VERI-Wild.

CNN

Positive

Negative Hard Negative

Figure 1. Left: Our dataset is collected with a large-scale real
video surveillance system consisting of 174 cameras distributed
in a urban district (>200km2). Right: A hard negative generation
method is proposed to boost the vehicle ReID performance.

However, in many circumstances, the license plates cannot
be clearly captured, sometimes even removed, occluded, or
faked. As a result, there is an exponential increase in the
demand for the visual appearance based vehicle ReID tech-
niques. The development of deep learning and existing an-
notated datasets have greatly facilitated the vehicle ReID re-
search. However, the diversity in terms of viewpoint, back-
ground, and illumination variations present great challenges
to the vehicle ReID models in real-world applications.

In vehicle ReID, the dataset is crucial to comprehen-
sively and fairly evaluate the performance of the ReID
methods. However, to the best of our knowledge, all of the
existing vehicle ReID datasets [9][26][10] are captured un-
der constrained conditions, and generally have limitations
in the following aspects: 1) The number of vehicle identi-
ties and images are not large enough to the needs of practi-
cal application. 2) The limited camera numbers and cover-
ing areas do not involve complex and variant backgrounds
in a variety of real-world scenarios. 3) The camera views
are highly restricted. For most vehicle datasets, the sam-
ples are collected from checkpoint cameras that only cap-
ture the front and rear views, and the severe occlusion is also

13235

not taken into consideration. 4) Most of current datasets
are constructed from short-time surveillance videos without
signiÔ¨Åcant illumination and weather changes. These limita-
tions may oversimplify the practical challenges of the ReID
task, and the ReID models developed and evaluated on such
datasets could be inevitably questioned regarding the gen-
eralization capability in the wild.

The above issues motivate us to create a new vehicle
ReID dataset in the Wild (VERI-Wild) with the following
distinctive features: 1) The dataset is captured via a large
Closed Circuit Television (CCTV) system, which contains
174 surveillance cameras and covers a large urban district
of more than 200km2. 2) The unconstrained capture con-
ditions involve complex backgrounds, various viewpoints
and occlusion in the wild. 3) The 174 cameras capture
for 24h√ó30days, such that various weathers and illumina-
tion conditions are considered. 4) Cleaning from 12 mil-
lion vehicle images, VERI-Wild contains 416,314 images
of 40,671 IDs. VERI-Wild is currently the most challeng-
ing dataset for vehicle ReID in real scenarios (see Fig. 1).

Due to the large quantity of vehicle IDs and images, the
proposed VERI-Wild dataset poses signiÔ¨Åcant challenges
to vehicle ReID. One of the challenges is the similar ve-
hicle problem, where many vehicles with different IDs can
have very similar appearances, especially when these ve-
hicles belong to the same vehicle model (see Fig. 1). As
such, the remaining visual clues to differentiate such sim-
ilar appearances are the local characteristic details such as
the decorations and customized marks. To promote the ca-
pability of the model in capturing subtle differences, it is a
wise choice to provide such hard negative pairs for training.
Previous attempts [6, 28] focus on seeing more hard nega-
tives by mining them from training set. However, selecting
them from the whole training set leads to high computa-
tional cost. Moreover, the hard negative samples are lim-
ited, and iteratively training them may lead to over-Ô¨Åtting.

In this paper, we propose a Feature Distance Adversar-
ial Network (FDA-Net), in which a novel adversary scheme
on feature distance is designed in the embedding space.
Within such scheme, a generator aims to online generate
hard negative samples from both visual appearance and fea-
ture distance perspectives to cheat the embedding discrim-
inator, while the embedding discriminator tries to discrimi-
nate them. A similarity constraint is imposed on the gener-
ator to make the generated hard negative to be visually sim-
ilar to the real input, and meanwhile an extra attention reg-
ularization is further designed to enforce it to present sub-
tle differences. Besides, the feature representation model
(feature extractor) for vehicle ReID is seamlessly coupled
into FDA-Net as the embedding discriminator, and end-to-
end optimization can be achieved. As the adversary train-
ing proceeds, the generated hard negatives would become
harder, which in turn promotes the discriminator to become

more discriminative. The key idea of generating hard nega-
tive samples has signiÔ¨Åcantly improved the state-of-the-art
performance on vehicle ReID benchmarks.

Our main contributions are summarized as follows:
(1) A large-scale challenging dataset, VERI-Wild, is pro-
posed for vehicle ReID evaluation in the wild. VERI-Wild
is the Ô¨Årst vehicle ReID dataset captured from an uncon-
strained large-scale real-world camera network.

(2) We design a FDA-Net to facilitate the ReID model
learning by incorporating a novel feature distance adver-
sary. In FDA-Net, the hard negatives are continuesly online
generated to facilitate the learning of more discriminative
embedding discriminator.

(3) The FDA-Net achieves superior performance over
the state-of-the-art approaches on all the evaluated vehicle
ReID datasets. The VERI-Wild dataset and the feature dis-
tance adversary scheme is expected to facilitate the large-
scale vehicle ReID research from the perspective of Ô¨Åguring
out the ReID performance bottleneck in the wild.

2. Related Work

Vehicle ReID Datasets. Recent vehicle ReID methods
are mainly evaluated on two public datasets, VehicleID [9]
and VeRI-776 [10]. Although impressive results have been
achieved on these datasets, the vehicle ReID problem is still
far from being addressed in the real-world scenarios. The
practical challenging factors have not been fully considered
in VehicleID [9] or its extension [26], since they both con-
tain very limited viewpoints (only two views, namely, front
and rear). Moreover, they do not contain complex back-
ground. Almost no occlusion or illumination changes are
considered by them. The samples in VeRI-776 [10] are cap-
tured by 18 cameras in a circular road of 1.0 km2 areas for
a short time period (4:00 pm to 5:00 pm in only one day).
Again, the limitations of VeRI-776 also lie in the small num-
ber of vehicle IDs, simple scenarios, low resolution, etc.

Vehicle Re-IdentiÔ¨Åcation. Vehicle ReID has attracted
more research efforts in past two years. Liu et al. [10] pro-
posed a ‚ÄúPROVID‚Äù ReID model that employed visual fea-
ture, license plate and spatial-temporal information to ex-
plore the ReID task. Shen et al. [18] proposed a two-stage
framework that incorporates complex spatial-temporal in-
formation for effectively regularizing the ReID results. Re-
cent methods [9][29][10] focus on learning an embedding
model which map the samples into an embedding space
where the samples of the same ID are closer than those of
the different, and the similarities between vehicles are mea-
sured by the feature distances. Liu et al. [9] introduced a
mixed difference network using vehicle model and ID in-
formation to strengthen the feature representation. Zhou et
al. [31] designed a multi-view inference scheme to gener-
ate global-view feature representation to improve the vehi-
cle ReID. Different from the above methods, our work aims

23236

to explore generating hard negatives in the feature space to
improve the discriminative capability of the ReID model.

GAN and GAN in ReID. GANs have achieved
great success in many tasks, such as image generation
[15][5] and translation [32][2][3]. Recent ReID meth-
ods also explore GAN both in vehicle and person ReID
Ô¨Åelds [30][22][13][12]. Zheng et al. [30] adopted the DC-
GAN [15] by using Gaussian noises to generate unlabeled
person images before training. Wei et al. [22] proposed a
PTGAN to transfer person images between different styles
for reducing domain gap. Zhou et al.
[25] designed a
GAN model to generate cross-view vehicle images to im-
prove cross-view ReID. Lou et al. [12] proposed to gener-
ate desired vehicle images from same-view and cross-view
to facilitate ReID model training. Some other methods fo-
cus on image transferring between different datasets [22][3]
or generating different human poses [13], but they are not
suitable for vehicles.

Hard Example Learning. Learning from hard exam-
ples has always been a hot research topic [11][12][28].
Loshchilov et al. [11] proposed to online select hard ex-
amples according to loss in SGD optimization. Yuan et al.
[28] proposed a hard-aware cascaded method to select hard
examples for efÔ¨Åcient training. However, the diversity of
the hard examples in training set is insufÔ¨Åcient compared to
those in the real-world. Wang et al. [20] proposed to add
mask to obtain hard positives for improving the robustness
against occlusion in detection.

3. VERI-Wild Dataset

3.1. Description of VERI Wild

We collect a large-scale vehicle ReID dataset in the
wild (VERI-Wild), which is captured from an existing large
CCTV camera system consisting of 174 cameras across one
month (30 √ó 24h) under unconstrained scenarios. The cam-
eras are distributed in a large urban district of more than
200km2. The YOLO-v2 [16] is used to detect the bound-
ing box of vehicles. Our raw vehicle image set contains
12 million vehicle images, and 11 volunteers are invited to
clean the dataset for 1 month. After data cleaning and an-
notation, 416,314 vehicle images of 40,671 identities are
collected. We present the statistics of VERI-Wild in Fig. 3,
and the sample images from VERI-Wild are also compared
in Fig. 2. For privacy consideration, the license plates are
masked in our dataset. The distinctive features of VERI-
Wild are summarized into the following aspects:

Unconstrained capture conditions in the wild. The
VERI-Wild dataset is collected from a real CCTV camera
system consisting of 174 surveillance cameras, in which the
unconstrained capture conditions pose great challenges.

Complex capture conditions. The 174 surveillance
cameras are distributed in an urban district over 200km2,

D
I

e
l
c
i
h
e
V

6
7
7
-
I
R
e
V

d
l
i

W

-
I
R
E
V

Viewpoint Changes

Illumination Variations

Background Variations

Occlusion

#1

#2

#3

#4

#5

#6

#7

#8

#46

Extreme Case (Across 46 Cameras)

‚Ä¶

Figure 2. Comparison among the samples of VehicleID [9], VeRI-
776 [10] and VERI-Wild datasets. Our collected VERI-Wild
dataset poses many more practical challenges for vehicle ReID,
e.g., signiÔ¨Åcant viewpoint, illumination, and background varia-
tions, and severe occlusion. Another challenge in our dataset is
that one vehicle may appear across numerous cameras, e.g., in an
extreme case, the same vehicle appears in 46 surveillance cameras.

Table 1. Comparisons among the VehicleID [9], the VeRI-776
[10], and the created VERI-Wild datasets for vehicle ReID.

Dataset

Images
Identities
Cameras

Capture Time

Views

Spatio-temporal

Relation Annotation

Tracks Across

Cameras

Camera ID
Timestamp

Occlusion

Complex Background

Morning
Afternoon

Night

Rainy Weather
Foggy Weather

VehicleID

VeRI-776

VERI-Wild

221,763
26,267

12
N/A

49,360

776
18
18h

2

√ó

√ó

√ó

√ó

√ó

√ó

X

X

√ó

√ó

√ó

6

X

√ó

√ó

√ó

√ó

√ó

√ó

X

√ó

√ó

√ó

416,314
40,671

174

125,280h

Unconstrained

X

X

X

X

X

X

X

X

X

X

X

presenting various backgrounds, resolutions, viewpoints,
and occlusion in the wild, as shown in Fig. 2. In extreme
cases, one vehicle even appears in more than 40 different
cameras, which is very challenging for ReID algorithms.

Large time span involving severe illumination and
weather changes. The VERI-Wild is collected from a du-
ration of 174 √ó 24 √ó 30 = 125, 280 video hours. Fig. 3 (b)
shows the vehicle distributions in 4 time slots of 24h, i.e.,
morning, noon, afternoon, evening across 30 days. Also the
VERI-Wild contains poor weather conditions, such as rainy,

33237

3%2%

4%

4%

5%

6%

21%

Sedan

SUV

Van

MPV

Bus

Truck

Pickup

Others

55%

3%2%1%

3%

3%

6%

29%

8%

9%

15%

21%

White

Black

Silver

Red

Yellow

Grey

Gold

Blue

Green

Brown

Purple

(a)

(b)

(c)

(d)

Figure 3. The statistics of VERI-Wild dataset. (a) The number of identities across different cameras, i.e., 1-174 cameras; (b) The number
of IDs captured in each day; (c) The distribution of vehicle types; (d) The distribution of vehicle colors.

foggy, etc, which are not contained in previous datasets.

Rich Context Information. We provide rich context in-
formation such as camera IDs, timestamp, tracks relation
across cameras, which are potential to facilitate the research
on behavior analysis in camera networks, like vehicle be-
havior modeling [14], cross-camera tracking [7] and graph-
based retrieval [24].

3.2. Evaluation Protocol

The VERI-Wild is randomly divided into two parts for
training and testing, as shown in Table 2. To better eval-
uate ReID methods, we further split the test set into three
subsets, as shown in Table 3.

Table 2. The splitting for training and testing sets. (IDs/Images)

Dataset

Train

Probe

Gallery

VehicleID [9]
VeRI-776 [10]

13,164/100,182

576/37,778

2,400/2,400
200/1,678

2,400/17,638
200/11,579

VeRI-Wild

30,671/277,797

10,000/10,000

10,000/128,517

Table 3. Descriptions of the subset of the test set.

Test Size

Small Medium

Large

Identities
Images

3,000
41,816

5,000
69,389

10,000
138,517

In the ReID process, for each given query, a candidate
list sorted by the feature distances between the query and
reference images is returned from the database. The mean
Average Precision (mAP) and Cumulative Matching Char-
acteristics (CMC) are used as performance metrics.
Mean Average Precision: The mAP evaluates the overall
performance for ReID, and is deÔ¨Åned as follows:

AP =

Pn

k=1 P (k) √ó gt(k)

Ngt

, mAP =

PQ

q=1 AP (q)

Q

(1)

where k is the rank in the recall list of size n, and Ngt is the
number of relevant vehicles. P (k) is the precision at cut-
off k and gt(k) indicates whether the k-th recall is correct
or not. Q is the number of total query images. Moreover,
Top K match rate is also reported in the experiments.
Cumulative Match Characteristics: The CMC curve
shows the probability that a query identity appears in
different-sized candidate lists. The cumulative match char-
acteristics at rank k can be calculated as:

CM C@k =

PQ

q=1 gt(q, k)

Q

,

(2)

Figure 4. An example of the real hard negative pair. The two vehi-
cles look very similar, and only subtle differences can be observed,
e.g., the details behind the windscreen.

where gt(q, k) equals 1 when the groundtruth of q image
appears before rank k.

4. Proposed Method

In learning an embedding model, the hard negative sam-
ples play the predominant roles in facilitating the embed-
ding model‚Äôs discriminative capability [6][20]. The simi-
larity metrics in the embedding space is represented by the
feature distance. As shown in Fig. 4, in general, two sam-
ples in each real hard negative pair are often similar (with
only subtle differences observed). Inspired by this, we de-
sign a novel feature distance adversary scheme to gener-
ate hard negative samples for enhancing the vehicle ReID
model, which consists of two parts, i.e., a similarity con-
straint and an attention regularization. Given an input vehi-
cle, the similarity constraint is designed to enforce the gen-
erated hard negative to be visually similar to the input. To
further improve the manipulating capability on subtle dif-
ferences, an attention regularization is proposed to constrain
the attentive regions of the input vehicle and the generated
hard negative to be dissimilar. In this manner, the gener-
ated hard negatives tend to present visually similar but with
subtle differences to the input. As the opposite of the ad-
versary scheme, the discriminator is promoted to be more
discriminative with more available hard negatives. Accord-
ingly, a Feature Distance Adversary Network (FDA-Net) is
designed in this paper, which includes a hard negative gen-
erator G and an embedding discriminator D.

4.1. Hard Negative Generator

Similarity Constraint. To get a visually similar neg-
ative to the input, we aim to constrain the generated hard
negative closer to its positive than its real sampled negative.

43238

ùë•

ùë•&

ùë•‚Äô

ùëì (ùë•)

ùëì (ùê∫(ùë•))

ùëì(ùë•&)

ùëì(ùë•‚Äô)

ùê∫(ùë•)

Convs

G

Feature

Extraction

Attention
Module

F
C

Similarity
Constraint

Attention

Regularization

Feature Distance

Adversary

Training Generator

Training Discriminator

G

Real Training Set

Pos ... Neg...

Convs

Attention
Module

F
C

Triplet Distance

Constraint

Figure 5. Illustration of the proposed FDA-Net. The feature distance adversary scheme is imposed between the generator G and the
embedding discriminator D. The G tries to generate a hard negative sample under similarity constraint and attention regularization, while
the D tries to discriminate them. The generator and discriminator are alternatively optimized. We use the generated hard negative G(x) as
well as training set to train a more discriminative embedding model D.

Besides, the generated samples should be different from the
input. Given a real input vehicle image x, to generate such
hard negative sample G(x), the similarity constraint for G
can be formulated as follows:

kH(x), H(xp)k2

2 + Œ≤ ‚â§ kH(x), H(G(x))k2

2

‚â§ kH(x), H(xn)k2

2 ‚àí Œ≤,

(3)

where H(¬∑) denotes the feature representation in the em-
bedding space. xp and xn represent the real positive and
negative of input x, respectively. Such a constraint can be
explained as follows: the generated G(x) is constrained to
be away from x at a minimum margin gap Œ≤, and meanwhile
it is also constrained to be away from the real negative xn at
a minimum margin gap Œ≤, as shown in Fig. 6. The right part
in Eq (3) enforces G(x) to be more similar to x, compared
to the sampled real negative, while the left part in Eq (3)
constrains G(x) to avoid present the same appearance as x.

ùêª(ùë•‚Äô)

ùõΩ

ùêª(ùë•)

ùêª(ùê∫(ùë•))

ùõΩ

ùêª(ùë•()

Figure 6. Illustration of the similarity constraint.

The desired G(x) is located in an annular belt region
around x. Such characteristics allow the generated G(x) to
be more likely to break the distance constraint when opti-
mizing the embedding model, which is also the crucial role
of the real hard negatives in training. Finally, the loss for
similarity constraint on G can be formulated as:

Ex[max{kH(x), H(xp)k2
+Ex[max{kH(x), H(G(x))k2

LG sim =
2 ‚àí kH(x), H(G(x))k2
2 ‚àí kH(x), H(xn)k2

2 + Œ≤, 0}]
2 + Œ≤, 0}].

(4)
Attention Regularization. To strengthen the manipu-
lating capability on local subtle differences (see Fig. 4),

an attention regularization is further imposed. To promote
differences in local regions, the attentive regions between
x and G(x) should be relatively far away in feature dis-
tance. We design an attention module AT T to perform fea-
ture response selection for the intermediate feature f (x) ‚àà
Rh√ów√óc, then the output attention map is formulated as:

A(x) = AT T (f (x); Œ∏att), A ‚àà Rh√ów√ó1.

(5)
Each patch ai,j(x) in A(x) indicates the attention value
(importance score) at (i, j) for f (x). A(x) is normalized
with softmax function to be non-negative. The f (x) and
f (G(x)) are then weighted by A(x) and fed into Fully Con-
nected (FC) layers to get attentive feature representation as:

F (x) = F C(f (x) ‚äô A(x)),

F (G(x)) = F C(f (G(x)) ‚äô A(x)).

(6)

To ensure the attentive regions are consistent on x and
G(x), both f (x) and f (G(x)) are weighted by A(x). The
regularization enforce the F (x) and F (G(x)) to be larger
than a minimum margin Œ≥ as follows:

LG reg = Ex[max{ Œ≥ ‚àí kF (x), F (G(x))k2

2 , 0}].

(7)

For clarity, we use H(¬∑) and F (¬∑) to denote the obtained
feature with/without attention regularization. By using the
attention regularization, the generator is explicitly promoted
to make the subtle differences in some local regions.

4.2. Embedding Discriminator

The embedding discriminator D is Ô¨Åxed to compute fea-
ture distance for similarity measurement in training the gen-
erator. In contrast, during training the discriminator, G is
Ô¨Åxed to generate hard negatives for D. Therefore, D aims
to distinguish the hard negative G(x) from x by enlarging
their distances via triplet distance constraint, and the loss
for LD emb can be formulated as:

LD emb = Ex[max{k(H(x), H(xp)k2

2 ‚àí {kH(x), H(G(x))k2

2

+Œ±, 0}], Œ± ‚â• 2Œ≤,

(8)
where Œ± is the minimum margin gap.
In order to make
the training more efÔ¨Åcient and stable, we mix the generated

53239

G(x) and real negative xn from the training set to optimize
the embedding discriminator D. In addition, we incorporate
the extra softmax loss to our training procedure, which is
widely used in ReID. Thus, the overall loss LD emb for D
in Eq. (8) is given by:

LD emb = Ex [‚àí log Dcls(I|x)] +

Ex (cid:2)max{kH(x), H(xp)k2

2 ‚àí kH(x), H(z)k2

2

+Œ±, 0}, z ‚àà {G(x) ‚à™ xn},

(9)

where I is the ID label of real input sample x and Dcls is
another classiÔ¨Åcation objective of D. During the training of
D, the sample z is alternatively selected from the union set
of G(x) and xn. We also apply LD emb to the F (x) to train
the attention module. The embedding discriminator can be
considered as a feature extractor for vehicle ReID.

4.3. Real/Fake Adversary

Besides satisfying the distance constraints, the generated
hard negatives should appear as realistic vehicles. Thus, the
real/fake adversarial scheme is further imposed. The output
of real/fake discriminator Drf (x) indicates the probability
of an image x to be a real one. The loss of Drf (x) can be
formulated as a standard cross-entropy loss as:

Lrf = Ex[logDrf (x) + log(1 ‚àí Drf (G(x)))].

(10)

4.4. Overall Loss Function

Finally, the overall loss functions for optimizing the gen-

erator and discriminator can be represented as follows:

LG = ŒªLrf + LG sim + LG reg

LD = ‚àíŒªLrf + LD emb.

(11)

where the Œª is a hyper parameter to balance the two adver-
sarial schemes. For the whole network, the G and D are
alternatively optimized in an adversarial way.

4.5. Training and Testing Details

The feature distance adversarial learning allows us to
couple the ReID model and discriminator as an embedding
discriminator in FDA-Net. During updating G, the G is op-
timized to generate a negative sample that satisÔ¨Åes the hard
negative‚Äôs distance constraint with input in the embedding
space. When updating D, for each input sample, a hard
negative sample is generated by G, which is further com-
bined with real training samples, then their feature distances
are optimized by D for discrimination. So the generator is
able to continuously generate hard negatives to adapt the it-
eratively updated embedding discriminator during training,
and meanwhile these generated hard negatives can be used
to further facilitate the training of D.

Therefore, in testing stage, the vehicle samples are fed
into embedding discriminator D to get vehicle feature rep-
resentation F (¬∑) to perform feature matching in ReID.

4.6. Implementation Details

Network Architecture. For the generator network, two
stride-2 convolutions, 9 residual blocks, and two stride 1/2
deconvolutions are used. The size of training images is
224 √ó 224. There are two subnetworks for discriminator
network. For the real-fake discriminator, we use a Patch-
GAN of size 70 √ó 70 as in [32]. For the embedding dis-
criminator, we use the V GG CN N M 1024 (VGGM) as
the base network for fair comparison, which is also adopted
in [9]. The attention module is constructed with a 2-layer
CNN with 1 √ó 1 Ô¨Ålters and ReLU activation at the top.

Hyper Parameters. For the discriminator, Œ± in triplet
margin constraint is set to 0.6. For the generator, the Œ≤
in similarity constraint is set to 0.3 and the Œ≥ in attention
regularization is set to 0.7. The loss weight Œª is set to 1.
Learning rate starts from 0.001 for embedding discrimina-
tor, and starts from 0.0002 for other discriminator and gen-
erator. The learning rate is kept constant for the Ô¨Årst 50
epochs and decay to zero in the next 50 epochs.

5. Experimental Results

5.1. Experiment setup

We conduct experiments on our proposed VERI-Wild
dataset and two existing datasets, VehicleID and VeRI-776,
by following the evaluation protocols in [10] and [9], re-
spectively. For match rate computation in VERI-Wild, we
follow the standard CMC protocol that all the references of
a given query are in gallery. But in VehicleID [9] dataset,
there are only one reference of a given query in gallery. In
VeRI-776 [10], only cross-camera search is performed. For
better evaluation, we perform comparisons as follows:

1) EN: This network is a conventional embedding net-

work with triplet and softmax loss.

2) FDA-Net: This is the proposed FDA-Net.
3) FDA-Net ‚äñ Att: This structure is similar to FDA-Net

but without attention regularization.

5.2. Quantitative Results

5.2.1 Evaluation on VERI-Wild

To verify the proposed VERI-Wild is challenging and close
to the real scenarios, we test the existing methods published
in recent 2 years, and use the codes or models provided
by their authors to evaluate the performance.
In vehicle
ReID, after the release of VehicleID and VeRI-776 datasets,
the VGG M usually serves as a baseline model for com-
parison, the performance of which is also provided. We
present the experimental results in Tables 4 and 5. Clearly,
the performances of these methods all dramatically drop on
VERI-Wild compared to their results on existing VehicleID
and VeRI datasets. For example, HDC [28] achieves mAP
of 63.1 %, which is the best result on VehicleID dataset.

63240

(a)

(b)

(c)

Figure 7. The CMC comparisons on VERI-Wild (TestSize = 10000), VeRI-776 and VehicleID (TestSize = 2400) test sets.

Table 4. The mAP performance on the VERI-Wild dataset.

Table 6. Performance on the VehicleID dataset.

Settings

Small Medium

GoogLeNet [27]

Triplet [17]

Softmax [10]

CCL [9]

HDC [28]

GSTE [1]

Unlabled GAN [32]

EN

FDA-Net ‚äñ Att

FDA-Net

24.27

15.69

26.41

22.50

29.14

31.42

29.86

28.77

32.40

35.11

24.15

13.34

22.66

19.28

24.76

26.18

24.71

24.63

27.10

29.80

Large

21.53

9.93

17.62

14.81

18.30

19.50

18.23

19.48

21.13

22.78

However, it only achieves the 29.14% mAP on VERI-Wild,
signiÔ¨Åcantly lower than its results on VehicleID. Such per-
formance change indicates that VERI-Wild is a challenging
dataset and valuable for the vehicle ReID research. Note
that the match rate protocol on VERI-Wild and VehicleID
are different, it can‚Äôt be fairly compared.

Our proposed FDA-Net outperforms the other compari-
son methods. Compared to the baseline EN, the incremen-
tal improvements on FDA-Net ‚äñ Att and FDA-Net demon-
strate the feature distance adversary scheme can signiÔ¨Å-
cantly facilitate the discriminative capability of the embed-
ding model. More speciÔ¨Åcally, the FDA-Net outperforms
FDA-Net ‚äñ Att, indicating the effectiveness of the atten-
tion regularization. Compared to the Unlabled GAN [30]
that uses Gaussian noises to randomly generate negatives,
our FDA-Net presents much more improvements by explor-
ing the potential of the hard negatives. The CMC curves on

Table 5. Match rate on the VERI-Wild dataset.

Settings

Methods

GoogLeNet [27]

Triplet [17]

Softmax [10]

CCL [9]

HDC [28]

GSTE [1]

Unlabled GAN [32]

EN

FDA-Net ‚äñ Att

FDA-Net

Small

Medium

Large

R = 1

57.16

44.67

53.4

R = 5

75.13

63.33

75.03

56.96

75.0

57.1

60.46

58.06

57.13

61.93

64.03

78.93

80.13

79.6

77.33

80.48

82.8

R = 1

53.16

40.34

46.16

51.92

49.64

52.12

51.58

52.86

55.62

57.82

R = 5

71.1

58.98

69.88

70.98

72.28

74.92

74.42

73.18

75.64

78.34

R = 1

44.61

33.46

37.94

44.6

43.97

45.36

43.63

43.02

46.48

49.43

R = 5

63.55

51.36

59.89

60.95

64.89

66.5

65.52

66.3

68.36

70.48

‚àó All the references of any given query are in gallery.

Settings

Methods

LOMO [8]
DGD [23]
GoogLeNet [27]
FACT [10]
XVGAN [25]
CCL [9]
Mixed Diff [9]
HDC [28]
VAMI [31]

EN
FDA-Net ‚äñ Att
FDA-Net

Test Size=1600

Test Size=2400

mAP

-
-
42.85
-
-
44.8
48.1
63.1
-

55.78
62.71
65.33

R=1

18.85
40.25
43.40
44.59
49.55
39.94
45.05
-
52.87

51.73
57.23
59.84

R=5

29.18
65.31
63.86
64.57
71.39
62.98
68.85
-
75.12

73.08
76.14
77.09

mAP

-
-
40.39
-
-
38.6
45.5
57.5
-

52.20
59.26
61.84

R=1

15.32
37.33
38.27
39.92
44.89
35.68
41.05
-
47.34

47.62
52.06
55.53

R=5

25.29
57.82
59.39
60.32
66.65
56.24
63.38
-
70.29

67.81
72.41
74.65

‚àó For CMC only one reference of any given query is in gallery.
VERI-Wild large test set are shown in Fig. 7(a). Our method
achieves higher rank 1 values than the compared methods.

5.2.2 Evaluation on VehicleID

The results on VehicleID are shown in Table 6. Our FDA-
Net consistently obtains better performance over other ap-
proaches in both 1600 and 2400 test size. Both VAMI [31]
and XVGAN [25] involve GANs, and they focus on gen-
erating cross-view vehicle images from the input view of a
vehicle for improving cross-view ReID. However, our ap-
proach achieves superior performance from the perspective
of hard negatives generation. HDC [28] also pays attention
to the hard negatives, while they focus on mining hard neg-
atives in the training set. Compared to the HDC, our gen-
eration scheme achieves more superior performance. It is
worth mentioning that HDC cascades a set of GoogleNet
which is a much deeper network than VGG M used in
our model, which further proves the effectiveness of our
method. The CMC curves comparison on VehicleID dataset
are shown in Fig. 7(b).

Other GAN relevant ReID methods focus on person im-
age style transferring [22][3] or pose generation [13], which
are not suitable for fair comparison or applied to vehicles.

5.2.3 Evaluation on VeRI

Table 7 shows the results on the VeRI-776 dataset. The
proposed method outperforms the state-of-the-art method
VAMI [31] by 5.36% mAP. In particular, VAMI used GAN

73241

Real Input

Attention Map

Generated
Hard Neg

Real Hard Neg
(Different ID)

Real Input

Generated

Hard Negative

Real Input

Generated

Hard Negative

Figure 8. Examples of hard negative generation results. The gener-
ated hard negatives (3rd column) are similar to the real inputs (1st
column), yet with differences in certain regions. These regions
with differences are learned from the attention module with large
responses in the corresponding attention map (2nd column).

to infer feature for multi-view feature representation, while
we focus on improving the capability of representing sub-
tle details via hard negative scheme. Compared with OIFE
[21], which used key-point alignment in vehicle feature rep-
resentation, the proposed FDA-Net achieves much better
performance. The CMC curves on VeRI-776 dataset are
also provided in Fig. 7(c). Our method has also achieved
much superior performance, especially in the rank 1 match
rate (84.27% v.s. state-of-the-art 77.03% VAMI), meaning
that the adversary scheme can signiÔ¨Åcantly improve the dis-
crimination capability of ReID model on subtle differences.

Table 7. Performance comparisons on the VeRI-776 dataset.

Methods

LOMO [8]
DGD [23]
GoogLeNet [27]
FACT [10]
XVGAN [25]
OIFE [21]
SiameseVisual [18]
FACT +Plate + STR [10]
VAMI [31]

EN
FDA-Net ‚äñ Att
FDA-Net

mAP

9.64
17.92
17.81
18.73
24.65
48.00
29.48
27.77
50.13

47.85
53.46
55.49

r = 1

25.33
50.70
52.12
51.85
60.20
65.92
41.12
61.44
77.03

79.67
83.97
84.27

r = 5

46.48
67.52
66.79
67.16
77.03
87.66
60.31
78.78
90.82

89.45
91.59
92.43

5.3. Qualitative Results

Visualization of Hard Negatives. In Fig. 8 and Fig. 9,
the input and generated hard negatives are pair-wisely
shown.
It can be observed that the hard negatives have
very similar appearance with the input vehicles, and minor
modiÔ¨Åcations have been harmoniously made for distinction.
Compared with real hard negatives, our generated hard neg-
atives also present challenging discrimination difÔ¨Åculties.

6. Discussion

The signiÔ¨Åcance of real/fake loss. The ‚Äúreal/fake‚Äù loss
allows the generated images to look more like a real one,
also used in [22][30][3][13]. Without real/fake loss, FDA-
Net would still introduce blur, deformation after complete

Figure 9. Comparison examples of generated hard negative pairs.

training. Such vehicles do not exist in the real-world, when
the discriminator is strong enough, such samples are of less
signiÔ¨Åcance to facilitate model training.

The choice of Œ±, Œ≤ and Œ≥. We set Œ± = 0.6 in triplet
margin which is a widely used setting in ReID. We also ex-
periment with Œ± from 0.4 to 0.7, and the performances are
close. According to Eq(3) and Eq(8), kH(x), H(xp)k2
2 +
2Œ≤ ‚â§ kH(x), H(xn)k2
2 + Œ± ‚â§
kH(x), H(xn)k2
2, it is known that 2Œ≤ ‚â§ Œ±, thus we set
Œ≤ = 0.3. Different from margin constraint like Œ± and Œ≤, the
constraint of Œ≥ is similar to veriÔ¨Åcation loss [19], we Ô¨Ånd
that Œ≥ = 0.7 works well in our experiments.

2 and kH(x), H(xp)k2

7. Conclusion

In this work, we contribute a large-scale VERI-Wild
dataset, which presents rich variants on backgrounds, illu-
mination, occlusion and viewpoints, etc. The VERI-Wild is
expected to facilitate the development and evaluation of the
ReID methods in realistic scenarios.

In particular, we present a novel FDA-Net for vehicle
ReID, which is able to elegantly generate hard negative
samples in the embedding space for training a more discrim-
inative ReID model. The ReID model coupled into FDA-
Net, can be end-to-end optimized with feature distance ad-
versarial scheme. With more available hard negatives, the
embedding model‚Äôs discriminative capability can be further
facilitated, which has been well validated.
Acknowledgement: This work was supported by the Na-
tional Natural Science Foundation of China under Grant
61661146005 and Grant U1611461, and in part by the
National Basic Research Program of China under Grant
2015CB351806, and in part by Hong Kong RGC Early Ca-
reer Scheme under Grant 9048122 (CityU 21211018).

References

[1] Y. Bai, Y. Lou, F. Gao, S. Wang, Y. Wu, and L. Duan. Group
sensitive triplet embedding for vehicle re-identiÔ¨Åcation.
IEEE Transactions on Multimedia, 2018.

[2] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo.
Stargan: UniÔ¨Åed generative adversarial networks for multi-

83242

domain image-to-image translation. arXiv preprint, 1711,
2017.

[3] W. Deng, L. Zheng, G. Kang, Y. Yang, Q. Ye, and
J. Jiao.
Image-image domain adaptation with preserved
self-similarity and domain-dissimilarity for person re-
identiÔ¨Åcation.
In Proc. of Computer Vision and Pattern
Recognition, 2018.

[4] S. Du, M. Ibrahim, M. Shehata, and W. Badawy. Auto-
matic license plate recognition (alpr): A state-of-the-art re-
view. IEEE Transactions on Circuits and Systems for Video
Technology, 23(2):311‚Äì325, 2013.

[5] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In Proc. of International Con-
ference on Neural Information Processing Systems, pages
2672‚Äì2680, 2014.

[6] A. Gordo, J. Almaz¬¥an, J. Revaud, and D. Larlus. Deep image
retrieval: Learning global representations for image search.
In Proc. of European Conference on Computer Vision, pages
241‚Äì257, 2016.

[7] O. Javed, K. ShaÔ¨Åque, Z. Rasheed, and M. Shah. Model-
ing inter-camera space-time and appearance relationships for
tracking across non-overlapping views. Computer Vision and
Image Understanding, 109(2):146‚Äì162, 2008.

[8] S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identiÔ¨Åcation
by local maximal occurrence representation and metric
learning.
In Proc. of Computer Vision and Pattern Recog-
nition, pages 2197‚Äì2206, 2015.

[9] H. Liu, Y. Tian, Y. Wang, L. Pang, and T. Huang. Deep
relative distance learning: Tell the difference between sim-
ilar vehicles. In Proc. of the Computer Vision and Pattern
Recognition, pages 2167‚Äì2175, 2016.

[10] X. Liu, W. Liu, T. Mei, and H. Ma. A deep learning-based
approach to progressive vehicle re-identiÔ¨Åcation for urban
surveillance. In Proc. of European Conference on Computer
Vision, pages 869‚Äì884. Springer, 2016.

[11] I. Loshchilov and F. Hutter.

for faster training of neural networks.
arXiv:1511.06343, 2015.

Online batch selection
arXiv preprint

[12] Y. Lou, Y. Bai, J. Liu, S. Wang, and L.-Y. Duan. Embed-
ding adversarial learning for vehicle re-identiÔ¨Åcation. IEEE
Transactions on Image Processing, 2019.

[13] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and
M. Fritz. Disentangled person image generation. In Proc.
of Computer Vision and Pattern Recognition, pages 99‚Äì108,
2018.

[14] B. T. Morris and M. M. Trivedi.

Learning, modeling,
and classiÔ¨Åcation of vehicle track patterns from live video.
IEEE Transactions on Intelligent Transportation Systems,
9(3):425‚Äì437, 2008.

[15] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. Computer Science, 2015.

[16] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

arXiv preprint, 2017.

[17] F. Schroff, K. Dmitry, and J. Philbin. Facenet: A uniÔ¨Åed
embedding for face recognition and clustering. In Computer
Vision and Pattern Recognition, pages 815‚Äì823, 2015.

[18] Y. Shen, T. Xiao, H. Li, S. Yi, and X. Wang. Learning deep
neural networks for vehicle re-id with visual-spatio-temporal
path proposals. In Proc. of IEEE International Conference
on Computer Vision, pages 1918‚Äì1927. IEEE, 2017.

[19] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiÔ¨Åcation-veriÔ¨Åcation.
In Proc.
of Advances in neural information processing systems, pages
1988‚Äì1996, 2014.

[20] X. Wang, A. Shrivastava, and A. Gupta. A-fast-rcnn: Hard
In

positive generation via adversary for object detection.
Computer Vision and Pattern Recognition, 2017.

[21] Z. Wang, L. Tang, X. Liu, Z. Yao, S. Yi, J. Shao, J. Yan,
S. Wang, H. Li, and X. Wang. Orientation invariant fea-
ture embedding and spatial temporal regularization for vehi-
cle re-identiÔ¨Åcation. In Proc. of Computer Vision and Pattern
Recognition, pages 379‚Äì387, 2017.

[22] L. Wei, S. Zhang, W. Gao, and Q. Tian. Person transfer gan
to bridge domain gap for person re-identiÔ¨Åcation. In Proc. of
Computer Vision and Pattern Recognition, 2018.

[23] T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep fea-
ture representations with domain guided dropout for person
re-identiÔ¨Åcation.
In Proc. of Computer Vision and Pattern
Recognition, pages 1249‚Äì1258, 2016.

[24] J. Xu, V. Jagadeesh, Z. Ni, S. Sunderrajan, and B. Manju-
nath. Graph-based topic-focused retrieval in distributed cam-
era network. IEEE Transactions on Multimedia, 15(8):2046‚Äì
2057, 2013.

[25] Z. Y. and S. L. Cross-view gan based vehicle generation for
re-identiÔ¨Åcation. In Proc. of British Machine Vision Confer-
ence (BMVC), 2017.

[26] K. Yan, Y. Tian, Y. Wang, W. Zeng, and T. Huang. Exploit-
ing multi-grain ranking constraints for precisely searching
visually-similar vehicles.
In Proc. of Conference on Com-
puter Vision, pages 562‚Äì570, 2017.

[27] L. Yang, P. Luo, C. Change Loy, and X. Tang. A large-scale
car dataset for Ô¨Åne-grained categorization and veriÔ¨Åcation.
In Proc. of Computer Vision and Pattern Recognition, pages
3973‚Äì3981, 2015.

[28] Y. Yuan, K. Yang, and C. Zhang. Hard-aware deeply cas-

caded embedding. arXiv preprint arXiv:1611.05720, 2016.

[29] X. Zhang, F. Zhou, Y. Lin, and S. Zhang. Embedding la-
bel structures for Ô¨Åne-grained feature representation. arXiv
preprint arXiv:1512.02895, 2015.

[30] Z. Zheng, L. Zheng, and Y. Yang. Unlabeled samples gener-
ated by gan improve the person re-identiÔ¨Åcation baseline in
vitro. arXiv preprint arXiv:1701.07717, 3, 2017.

[31] Y. Zhou and L. Shao. Viewpoint aware attentive multi-view
In Computer Vision

inference for vehicle re-identiÔ¨Åcation.
and Pattern Recognition, pages 6489‚Äì6498, 2018.

[32] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. arXiv preprint arXiv:1703.10593, 2017.

93243

