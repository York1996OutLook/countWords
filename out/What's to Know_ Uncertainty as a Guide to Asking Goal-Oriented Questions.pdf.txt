Uncertainty as a Guide to Asking Goal-oriented Questions

What‚Äôs to know?

Ehsan Abbasnejad, Qi Wu, Javen Shi, Anton van den Hengel

{ehsan.abbasnejad,qi.wu01,javen.shi,anton.vandenhengel}@adelaide.edu.au
Australian Institute of Machine Learning & The University of Adelaide, Australia

Abstract

One of the core challenges in Visual Dialogue problems
is asking the question that will provide the most useful infor-
mation towards achieving the required objective. Encour-
aging an agent to ask the right questions is difÔ¨Åcult because
we don‚Äôt know a-priori what information the agent will need
to achieve its task, and we don‚Äôt have an explicit model of
what it knows already. We propose a solution to this prob-
lem based on a Bayesian model of the uncertainty in the im-
plicit model maintained by the visual dialogue agent, and
in the function used to select an appropriate output. By se-
lecting the question that minimises the predicted regret with
respect to this implicit model the agent actively reduces am-
biguity. The Bayesian model of uncertainty also enables
a principled method for identifying when enough informa-
tion has been acquired, and an action should be selected.
We evaluate our approach on two goal-oriented dialogue
datasets, one for visual-based collaboration task and the
other for a negotiation-based task. Our uncertainty-aware
information-seeking model outperforms its counterparts in
these two challenging problems.

1. Introduction

One of the fundamental problems in any challenge that
requires actively seeking the information required to carry
out a task is that of identifying the information that will best
enable the agent to achieve its objective. Identifying the in-
formation needed, and how to get it, is inherently complex,
not least because the space of all possibly useful informa-
tion is so large. We propose a solution to this problem here
that is applicable to reinforcement learning in general, and
that we demonstrate on the challenging problem of goal-
oriented visual dialogue.

Goal-oriented visual dialogue requires the participants to
engage in a natural language conversation towards a speci-
Ô¨Åed objective. The objectives of the two participants might
be collaborative, such as communicating the identity of a
speciÔ¨Åc object in an image [14], or they may be adversar-

In GuessWhat [14] one player knows the correct ob-
Figure 1.
ject (here shown in a red box), and the other must ask questions
to identify it. Traditionally an agent would generate questions by
sequentially selecting words with the highest conditional probabil-
ity, even though knowing the answer might be uninformative (in
this case ‚Äòshirt‚Äô in the Baseline histogram). Our solution, however,
selects ‚Äòblue‚Äô, which corresponds to the highest sum of the proba-
bility and standard deviation (likely to be the most ‚Äòinformative‚Äô).

ial (see Sec. 4.2).

The primary technological challenge in goal-oriented vi-
sual dialogue is to devise natural language interactions that
are directed towards achieving the required objective. This
is in contrast to the more traditional approach that aims only
to keep the other participant talking for as long as possi-
ble [22, 27, 37]. Note that the performance criteria in these
two approaches are opposite, as in goal-directed visual dia-
logue success is indicated by achieving the shortest possible
conversation.

Inspired by the success of deep learning in both com-
puter vision and natural language processing (NLP), most
recent goal-oriented dialogue studies rely on sequence-to-
sequence (seq2seq) deep learning models [34, 37]. Ob-
taining the large datasets this approach requires is chal-
lenging, however. As a partial solution, a combination of
seq2seq and deep reinforcement learning [35] are are com-
monly used to train a model (i.e. agent) with unlimited self-
generated data in a self-play environment. Even if this was
achieved ideally, however, it is unlikely that it would lead

14155

Q: Is it a person?             A: Yes Is he wearing a ‚Ä¶ContextToken HistoryshirtredbluehatblackshirtredbluehatblackDistribution of Predicted TokensOur ModelBaseline ModelSelectedTokenshirtblue‚Ä¶‚Ä¶to an agent capable of carrying out the complex reasoning
needed to devise the next interaction that will recover ex-
actly the information required to achieve an as yet unspeci-
Ô¨Åed objective.

Ideally, rather than learning to generate questions solely
by reinforcement learning, the method should calculate the
question that, when answered, will provide the most useful
information for achieving the agent‚Äôs objective. The direct
approach would require enumerating everything the agent
might ever need to know, and the value of each such piece
of information towards achieving its as yet unspeciÔ¨Åed ob-
jective. This would allow the identiÔ¨Åcation of the missing
piece of information that is most critical to achieving the
agent‚Äôs objective, and the formulation of a corresponding
question.

This direct approach is infeasible because the agent has
the capacity to store all of the information it might need
to hold about the task, the intention of its counterpart, the
image, and so on. Additionally, in the current state of the
art approaches, this information is stored implicitly in the
weights of a neural network. DeÔ¨Åning the scope of such
an information store is impossible, which makes measuring
its information content infeasible. Explicitly relating the
information stored to the agent‚Äôs objective is similarly in-
feasible. This makes it impossible to directly identify the
question that will provide the most useful information to-
wards achieving the agent‚Äôs objective.

Visual dialogue models trained using reinforcement
learning already learn to estimate the value of a particular
question as a step towards achieving their objective. This
is represented in the model‚Äôs value function. All that is re-
quired is a method for identifying the gaps in the model‚Äôs
internal information. We could then combine these informa-
tion gaps with the learned policy to identify the most useful
question.

Given that the models in question represent their internal
information implicitly, a good approximation of the model‚Äôs
information gaps is available in the uncertainty of its in-
ternal state. By propagating the model‚Äôs internal uncer-
tainty through the question generation process we can thus
identify questions that best reÔ¨Çect the model‚Äôs ambiguity in
achieving its objective. This is as compared to the current
process that selects the question the model is most certain
about (see Fig. 1).

We thus propose an information-seeking decoder (see
Fig. 2) that chooses each word in a question based on its
uncertainty about the environment and conditioned on the
history of the conversations. We prove this leads to the
minimum expected regret. An additional beneÔ¨Åt of having
an accessible estimate of a model‚Äôs uncertainty is that it al-
lows a more systematic identiÔ¨Åcation of the point at which
enough information has been gathered to make the required
decision.

We evaluate our model primarily on the well-known col-
laborative goal oriented visual dialogue problem Guess-
What [14]. To demonstrate that it is equally applicable to
(non-collaborative) negotiation tasks we also relate its per-
formance on Deal or No Deal [21]. GuessWhat is a visual
dialogue game between two agents in which they cooper-
ate to identify one of many objects in an image. Deal or
No Deal challenges two players to partition a collection of
items such that each is assigned to one only player. In con-
trast to GuessWhat, this game is semi-cooperative in that
one player can win more than their counterpart. Our ap-
proach signiÔ¨Åcantly outperforms the baseline on both tasks.
Our framework is summarised in Fig. 2.

Overall, our contributions are fourfold:

‚Ä¢ We propose a Bayesian Deep Learning method for
quantifying the uncertainty in the internal represen-
tation of a Reinforcement Learning model. This is
signiÔ¨Åcant as it provides a theoretically sound method
for propagating uncertainty to the output space of the
model.

‚Ä¢ We describe an uncertainty-aware information-seeking
decoder for goal-oriented conversation that actively
formulates questions that will provide the information
the agent needs to achieve its objective.

‚Ä¢ We devise a method that exploits the conÔ¨Ådence of
the predictor as a measure to indicate if the model
has enough information to produce an accurate output.
We show this approach is effective and leads to fewer
rounds of conversation for a goal to be achieved.

‚Ä¢ We show that in both visual and textual dialogue chal-
lenges, whether cooperation or adversarial behaviour
is desired, our approach outperforms the baselines. To
the best of our knowledge, this is the Ô¨Årst approach that
works well across domains and tasks.

2. Related Work

Goal-oriented dialogue Dialogue generation [22, 23, 29,
3] has been studied for many years in the NLP literature,
and has many applications. Dialogue generation is typi-
cally viewed as a Seq2Seq problem, or formulated as a sta-
tistical machine translation problem [26, 29, 2]. Recently,
dialogue systems have been extended to the visual domain.
For example, Das et al. [11] proposed a visual dialogue task
that allows a machine to chat with a human about the con-
tent of a given image. Goal-oriented dialogue requires the
agent understand a user request and complete a related task
with a clear goal within a limited number of turns. Early
goal-oriented dialogue systems [38, 41] model conversation
as partially observable Markov Decision Processes (MDP)
with many hand-crafted features for the state and action
space representations, which restrict their usage to narrow
domains. Bordes et al. [8] propose a goal-oriented dialogue

4156

Figure 2. The framework in two applications in this paper: we develop a generic information-seeking decoder for dialogue systems. Our
decoder selects each word optimistically with an upper bound on the reward to maximise the information obtained (details in Sec. 3.2).
Samples from the reward posterior is taken by applying dropout to the context variable of the RNN which provably performs variational
inference (see Eq. 1 and the Supplements for details).

test-bed that requires a user chat with a bot to book a table
at a restaurant. In visual goal-oriented dialogue De Vries et
al. [14] propose a guess-what game style dataset, where one
person asks questions about an image to guess which object
has been selected, and the second person answers questions
as yes/no/NA.

RL in dialogue generation Reinforcement learning (RL)
has been applied in many dialogue settings. Li et al. [22]
simulate two virtual agents and hand-craft three rewards to
train the response generation model. Recently, some works
[6, 32] make an effort to integrate the Seq2Seq model and
RL. RL has also been widely used to improve dialogue man-
agers, which manage transitions between dialogue states
[25, 28]. In visual dialogue, Das et al. [11] use reinforce-
ment learning to improve cooperative bot-bot dialogues,
and Wu et al. [40] combine reinforcement learning and
generative adversarial networks (GANs) to generate more
human-like visual dialogues. In [12], Das et al. introduce a
reinforcement learning mechanism for visual dialogue gen-
eration. They establish two RL agents corresponding to
question and answer generation respectively, to Ô¨Ånally lo-
cate an unseen image from a set of images. The question
agent predicts the feature representation of the image and
the reward function is given by measuring how close the
representation is compared to the true feature.

Uncertainty There are typically two sources of uncer-
tainty to be considered: Aleatoric and Epistemic [19]. The
former addresses the noise inherent in the observation while
the later captures our ignorance about which model gener-
ated our data. Both sources can be captured with Bayesian
deep learning approaches, where a prior distribution over
the model weights is considered. However, performing
Bayesian inference on a deep neural network is challeng-
ing and computationally expensive. One simple technique

that has recently gained attention is to use Monte Carlo [1]
dropout sampling which places a Bernoulli distribution over
network weights [16, 17, 5, 4].

Most recently, Lipton et al. [24] proposed a Bayes-by-
Backprop Q-network (BBQ-network) to approximate the
Q-function and the uncertainty in its approximation. It en-
courages a dialogue agent to explore state-action regions
in which the agent is relatively uncertain in its action se-
lection. However, the BBQ-network only uses Thompson
sampling to model the distribution of rewards for the words
in a Bayesian manner and ignores the uncertainty in the es-
timators. This can lead to very uncertain decisions about
conÔ¨Ådent actions or vice versa. Our method, on the other
hand, models both the uncertainty in the actions (i.e. word
choices), and the estimators, by directly incorporating the
variance in the sampling procedure. We also provide a the-
oretical justiÔ¨Åcation for the selection which is guaranteed to
minimise regret.

3. Goal-oriented Dialogue Systems

We ground our goal-oriented dialogue problem as an in-
teractive game between two agents for a collection of items.
The items are either 1) multiple objects in an image for one
agent to identify by asking the other questions, or 2) ob-
jects for the agents to split by negotiation. Conditioned on
this game, once enough information is gathered, a Guesser
takes the dialogue history and predicts the goal. The game
is a success when the goal is achieved. The game between
these two agents effectively simulates real natural language
based conversation to achieve a particular goal, e.g. uncov-
ering an unknown object, or an agreed split.

Each game is deÔ¨Åned as a tuple (I, D, O, o‚àó), where I
is the observed collection, D is the dialogue with Tdialogue
rounds of conversation pairs (Wj, W ‚Ä≤
and Wj =

j)Tdialogue

j=1

4157

Dialogue HistoryContextRNNDeal or No DealInformation-Seeking DecoderI‚Äôd like the books and the ballOpponentI want the ballContextContextInformation-Seeking DecoderYou can have one of the ballsInput ImageDialogue HistoryIs it a book? No.Is it on top? Yes.RNNCNNInformation-Seeking DecoderContextIs this a Mouse?OracleNoContextRNNContextInformation-Seeking DecoderIs this a Laptop?OracleyesContextGuesserGuessWhatInformation-Seeking DecoderSamples of word distributionsSample    s from Reward Posteriormean, variance        ,Select word from  Eq. 7:  Select word Select word Isthis‚Ä¶ùëìùúÉùí≤(ùë°+1)ContextContext(w(t))Mj
t=1 is a sequence of Mj tokens w(t) with a a prede-
j is the response. O = (on)No
Ô¨Åned vocabulary V , and W ‚Ä≤
n=1
is the list of objects, where No is the number of candidate
objects in the collection. o‚àó is the target or a list of targets.
In the GuessWhat game [14], o‚àó is a target object that the
dialogue refers to. In the Deal or No Deal [21], it is a list of
target objects that the negotiator agent is interested in.

Given an input collection I, an initial statement W1 is
generated by sampling from the model until the stop to-
ken is encountered. Then the counterpart agent receives
the statement W1, and generates the answer W ‚Ä≤
1, the pair
(W1, W ‚Ä≤
1) is appended to the dialogue history. We repeat
this loop until the end of dialogue token is sampled, or the
number of questions reaches the maximum. Finally, the
Guesser takes the whole dialogue D and the object list O
as inputs to predict the goal. We consider the goal reached
if o‚àó is selected.

3.1. RL for Dialogue Generation

We model dialogue generation as a Markov Decision
Process (MDP) to be solved by using a reinforcement learn-
ing (RL) agent [35]. The agent interacts with the environ-
ment over a sequence of discrete steps in which we have
the dialogue generated based on the collection I at time
step t in round T , the state of agent with the history of
conversation pairs and the tokens of current question gen-

j)T ‚àí1

j=1 , (w(t)

T )m

erated so far: St = (cid:0)I, (Wj, W ‚Ä≤
t=1(cid:1), where
t = PT ‚àí1
k=1 Mk + m. The action of agent is to choose the
subsequent token w(t+1)
from the vocabulary V (we drop
T for brevity). Depending on the action the agent takes, the
transition between two states falls into one of the following:
1) wt+1 = end of statement: The current statement is

T

Ô¨Ånished, it is the other agent‚Äôs turn.

2) wt+1 = end of dialogue: The dialogue is Ô¨Ånished, the

Guesser selects the output from list O.

3) Otherwise, the newly generated token wt+1 is ap-
pended to the current statement, the next state St+1 =

(cid:0)I, (Wj, W ‚Ä≤

j)T ‚àí1

j=1 , (w(t)

T )m+1
T =1(cid:1).

The maximum length of a statement Wj is Mmax, and
the maximum number of rounds in a dialogue is Tdialogue.
Therefore, the number of time steps t of any dialogue are
t ‚â§ Mmax ‚àóTdialogue. We use the stochastic policy œÄŒ∏(w|S),
where Œ∏ represents the parameters of the deep neural net-
work that produces the probability distributions for each
state. The goal of the policy learning is to estimate the pa-
rameter Œ∏. At the end of the dialogue, a decision about the
unknown goal is made for which a reward is given by the
environment. RL seeks to maximise the expected reward.

After a complete dialogue is generated, we update the
RL agent‚Äôs parameters based on the outcome of the dia-
logue. Let rw(t) be the reward for achieving the goal after
completing the dialogue, Œ≥ be a discount factor, and b be

a bias function estimating the running average of the com-
pleted dialogue rewards so far1. Let future reward R for an

action w(t) be R(w(t)) = E(cid:2)P‚àû

i=0 Œ≥i(rwt+i ‚àí b(wt+i))(cid:3)

where expectation is with respect to the policy œÄ. The pa-
rameters of this model comprising of the policy and the
bias function are then optimised using gradient policy the-
orem [36] and REINFORCE [39]. The policy determines
how a statement is made in a dialogue system. Note that
at each step there is an estimation of the reward (which is
never directly observed) for each word in the RL and the
observable reward is only given to the complete dialogue.
Upon receiving the reward for the complete dialogue the
parameters are accordingly updated. Utilising this estima-
tion of the reward at each stage and a particular choice of
the word strategy, a sequence of words is generated.

In the subsequent section we discuss a particular strategy
that utilises the uncertainty in the policy (model) and seeks
to provide a better approach for exploration of the space
of possible dialogues. Moreover, since REINFORCE is a
Monte Carlo estimate that is known to have a high variance,
there is an additional source of uncertainty in evaluation of
the expected rewards. As such, it is essential to consider
uncertainty in policies manifesting in word choices.

3.2. Information seeking Decoder

The decoder‚Äôs objective is, given the dialogue thus far, to
choose the subsequent word such that the resulting response
is most ‚Äúinformative‚Äù. To that end, we assume there is an
underlying reward for each word rw(t) at step t that we seek
to uncover by exploring the space of actions (tokens in the
vocabulary). A common practice is to model this value as
the output of a deterministic function fŒ∏(w(t)) : V ‚Üí R
parameterised by Œ∏ such as a neural network for sequential
problems (e.g. LSTMs [18] or GRUs [9]). To select the
subsequent action using this function one can greedily se-
lect the action with highest value or sample from a softmax
(categorical distribution) built from this function.

However, this approach does not account for the uncer-
tainty in the prediction of the reward rw(t) . This uncertainty
has two main sources, (1) model uncertainty which is due
to the imperfections in the parameters and (2) prediction
uncertainty which is due to the lack of information about
each action and its consequence. We choose a prior for the
parameters and update them with the likelihood of the dia-
logue observations to obtain the posterior distribution in a
Bayesian manner. The posterior at round T is:

p(Œ∏|Rt, Dt, I) =

1
Z Y

t

p(w(t)|(Wj , W ‚Ä≤

j)T ‚àí1

j=1 , Rt, I, fŒ∏)p(Œ∏)

(1)
where Z is the normaliser and p(Œ∏) is a prior for the pa-
rameters. Here, Rt = rw(1) , . . . , rw(t) is the set of rewards
collected up to step t in the dialogue. This formulation has a

1This bias function reduces the variance of the estimator.

4158

self-regularising behaviour that, unlike likelihood maximi-
sation, is less susceptible to a local optima and performs
better in practice. The predictive distribution of rewards
from which each word is chosen becomes2:

p(rw(t+1) |w(t+1), Rt, Dt, I)

= Z p(rw(t+1) |w(t+1), fŒ∏)p(Œ∏|Rt, Dt, I)dŒ∏

(2)

‚âà

1
N

N

Xi=1

p(rw(t+1) |w(t+1), f (i)

Œ∏ ), f (i)

Œ∏ ‚àº p(Œ∏|Rt, Dt, I)

where N is the number of samples for the Monte Carlo
estimation of the integral and

1

|V |

), . . . , w(t+1)

p(rw(t+1) |w(t+1), fŒ∏) = softmax(fŒ∏(w(t+1)

)
(3)
where |V | is the size of the dictionary. However, the poste-
rior p(Œ∏|Rt, Dt, I) in Eq.1 does not have a closed-form so-
lution. Thus, we resort to variational inference [15], the de-
tails of which is provided in the Supplements. In a nutshell,
inspired by [16, 17] we show that the posterior is approx-
imated by a particular mixture model which is equivalent
to performing typical MAP with dropout regularisation for
dialogue generation. Further, the Monte Carlo estimate in
Eq.2 is efÔ¨Åciently computed by applying dropout N times
in the RNN network (note we take N context variables in
Fig. 2). Hence, the mean and variance of the rewards com-
puted from the posterior are:

ÀÜ¬µ(w(t)) =

1
N

N

X

i=1

f (i)
Œ∏ (w(t))

(4)

ÀÜœÉ2(w(t)) =

1
N

N

X

i=1

(cid:16)f (i)

Œ∏ (w(t)) ‚àí ¬µ(w(t))(cid:17)2

+ œÑ ‚àí1

(5)

where œÑ is the precision parameter. Using Chebyshev‚Äôs
inequality, we have:

which means for Œ≤t > 0, with high probability we have

p(cid:16)(cid:12)(cid:12)fŒ∏(w(t)) ‚àí ÀÜ¬µ(w(t))(cid:12)(cid:12) < Œ≤t ÀÜœÉ(w(t))(cid:17) ‚â• 1 ‚àí
(cid:12)(cid:12)fŒ∏(w(t)) ‚àí ÀÜ¬µ(w(t))(cid:12)(cid:12) < Œ≤t ÀÜœÉ(w(t)) for a random function

fŒ∏(w(t)). Hence we have an upper bound on the random
function fŒ∏ with high probability:

(6)

1
Œ≤2
t

fŒ∏(w(t)) < ÀÜ¬µ(w(t)) + Œ≤t ÀÜœÉ(w(t))

(7)

Selecting an action (word) with this upper bound both
accounts for the estimation of the high‚Äìreward values by
ÀÜ¬µ(w(t)) and the uncertainty in this estimation for the given
word ÀÜœÉ(w(t)). In this bound, Œ≤t controls how much the un-
certainty is taken into account for selecting a word. Fur-
thermore, it is clear that with Œ≤t ‚Üí 0 this upper bound

2An alternative view is that we model fŒ∏ as a stochastic function and
choose words accounting for their uncertainty. fŒ∏ is fully realised by its
parameters Œ∏, hence we use the uncertainty in the functional and the pa-
rameters interchangeably.

approaches greedy selection. In the reinforcement learning
context, this approach mediates the exploration-exploitation
dilemma by changing Œ≤t.

This upper-bound is inspired by the Upper ConÔ¨Ådence
Bound (UCB) which is popular in multi-armed bandit prob-
lems [10]. A similar upper bound for Gaussian processes
was proposed in [30]. However, this bound for neural net-
works, in particular for dialogues systems, is novel.

Expected Regret and Information For a dialogue agent,
a metric for evaluating performance is cumulative regret,
that is the loss due to not knowing the best word to choose
at a given time. Suppose the best action at round t is w(t)
‚àó
for our choice w(t) , we incur instantaneous expected regret,

œÅt = EfŒ∏(cid:2)fŒ∏(w(t)
T = PT

‚àó ) ‚àí fŒ∏(w(t))(cid:3). The cumulative regret œÅ‚Ä≤

T
after T rounds is the sum of instantaneous expected regrets:
œÅ‚Ä≤
T are ever re-
vealed during dialogues generation. Our expected regret at
each round is bounded as

t=1 œÅt. Note that neither œÅt nor œÅ‚Ä≤

œÅt < EfŒ∏(cid:2)ÀÜ¬µ(w(t)) + Œ≤t ÀÜœÉ(w(t)) ‚àí fŒ∏(w(t))(cid:3) < 2Œ≤t ÀÜœÉ(w(t))
(8)
where the Ô¨Årst inequality is due to fŒ∏(w(t)
‚àó ) < ÀÜ¬µ(w(t)) +

Œ≤t ÀÜœÉ(w(t)) and the second one is because (cid:12)(cid:12)fŒ∏(w(t)) ‚àí
ÀÜ¬µ(w(t))(cid:12)(cid:12) < Œ≤t ÀÜœÉ(w(t)), then ‚àífŒ∏(w(t)) < ‚àíÀÜ¬µ(w(t)) +

Œ≤t ÀÜœÉ(w(t)). Therefore, we have

T < 2X
œÅ‚Ä≤

t

Œ≤t ÀÜœÉ(w(t))

(9)

As such, the expected regret for each word selected is
bounded by the standard deviation of the predicted reward.
When we choose words with high standard deviation, we
actively seek to gain more information about the uncertain
words to effectively reduce our expected regret.

Further, let‚Äôs assume the predictive distribution is near
Gaussian with mean and variance ÀÜ¬µ(w(t)), ÀÜœÉ2(w(t)) (which
considering the central limit theorem is natural). The en-
tropy is then 1
2 log(2œÄeÀÜœÉ2(w(t))). Hence, selecting ac-
tions with higher uncertainty is also justiÔ¨Åed from an
information-theoretic perspective as means of selecting in-
formative words. In a dialogue system, when uttering a sen-
tence with length T the information we can obtain is at most
(using the union bound) 1
t log(2œÄeÀÜœÉ2(w(t))). An alter-
native to using the approach in Eq. 7 is to choose the words
with highest entropy (the most informative words). How-
ever, that is an extreme case that will lead the RL algorithm
to continuously explore the dialogue space.

2 PT

3.3. Stopping Dialogue

One of the key challenges in a goal-oriented dialogue
system is to identify the point at which the agent has sufÔ¨Å-
cient information to make the required decision. We speci-
Ô¨Åed above that the probability of the unknown goal given
the dialogue thus far is p(ot+1|Dt). The uncertainty in

4159

Algorithm 1 Training information-seeking dialogues
1: for Each update do
2:
3:
4:
5:
6:

Pick target objects o‚àó
Set Dt to initial input collection
for j = 1 to Tdialogue do

# Generate trajectories
for k = 1 to K do

‚ä≤ Generate (Wj , W ‚Ä≤

k ‚àà Ok

‚ä≤ Select K of the objects/items

j ) pairs

while w(t+1) not <stop> do

(n)
Œ∏ ‚àº p(fŒ∏|Wj , Rt), n = 1, . . . , N

Sample f
Set ÀÜ¬µ(w(t)), ÀÜœÉ(w(t)) from f
w(t+1) = arg maxw ÀÜ¬µ(w) + Œ≤t ÀÜœÉ2(w)

(n)
Œ∏

end while
W ‚Ä≤
if <stop> ‚àà Wj or H(ot+1|Dt) ‚â§ Œ∑ then ‚ä≤ Sec. 3.3

j = SuperviseAgent(Wj , Dt)

New Object

Sampling Greedy Beam Search Avg. Ques

Supervised [14]

RL [31]
TPG [43]

Ours

Ours (Œ∑ = 0.05)
Ours (Œ∑ = 0.01)

Ours+MN

Supervised [14]

RL [31]

Ours

Ours (Œ∑ = 0.05)
Ours (Œ∑ = 0.01)

Ours+MN

41.6
58.5
62.6
61.4
58.5
59.8
68.3

39.2
56.5
59.0
56.7
58.0
66.3

43.5
60.3

-

62.1
59.5
59.3
69.2

New Image

40.8
58.4
59.82
56.5
57.5
67.1

47.1
60.2

-

63.6
59.6
60.4

-

44.6
58.4
60.6
57.3
58.5

-

5
5
5
5
4.2
4.5
5

5
5
5
4.3
4.5
5

Table 1. Accuracy in identifying the goal object in the GuessWhat
dataset (higher is better). The numbers in parentheses is the thresh-
old used in questions for guessing the object in the image. Average
number of questions is shown at the last column (lower is better).

‚ä≤ Predict the goal

7:

8:

9:

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

21:

delete (Wj , W ‚Ä≤

j ) and break;

else

append w(t+1) to Wj

end if
append (Wj , W ‚Ä≤

j ) to Dt

end for
ok = argmaxo p(o|Dt)

reward =(1 If ok = o‚àó

k
0 Otherwise

end for
Evaluate policy and update parameters Œ∏

22:
23:
24: end for

this measure reÔ¨Çects the agent‚Äôs conÔ¨Ådence in its prediction,
and thus provides a natural measure for the stopping crite-
ria of the conversation. Intuitively, the agent stops when it
feels conÔ¨Ådent in its prediction of the goal. Hence, we have
H(ot+1|Dt) ‚â§ Œ∑ where H is the entropy and Œ∑ is an appro-
priately chosen hyper-parameter for the conÔ¨Ådence. When
Œ∑ is larger, we allow for less conÔ¨Ådent predictions leading
to shorter dialogues. See Alg. 1 for the full algorithm.

4. Experiments

To evaluate the performance of the proposed approach
we conducted experiments on two different goal-oriented
dialogue tasks: GuessWhat [14] and Deal or No Deal [21].
Our approach outperforms the baseline in both cases.
In
both experiments we pre-train the networks using the su-
pervised model and reÔ¨Åne using reinforcement learning.
To that end, we employ a two stage algorithm in which
we learn to imitate the human dialogue behaviour in a su-
pervised learning task and subsequently Ô¨Åne-tune for bet-
ter generalisation and goal discovery using reinforcement
learning. In both experiments, our decoder takes the his-
tory of the dialogue in addition to input collection (e.g. an
image) and, guided by the uncertainty of each word, pro-
duces a question. Similar two-stage approaches are taken
in [13, 14, 21]. Without using supervised learning Ô¨Årst, the
dialogue model may diverge from human language.

4.1. GuessWhat

from the image. The task of the other player, the ques-
tioner, is to locate the unknown object by asking a series
of yes/no questions. After enough information is gathered
by the questioner, it then guesses what the selected object
was. If the questioner guesses the correct object the game
is successfully concluded. It is desirable for the questioner
to guess the correct answer in as few rounds of question-
ing as possible. The dataset includes 155, 281 dialogues
of 821, 955 pairs of question/answers with vocabulary size
11, 465 on 66, 537 unique images and 134, 074 objects.

Implementation Details We follow the same experimen-
tal setup as [14] in which three main components are built:
a yes/no answering agent, a guesser and a questioner. The
questioner is a recurrent neural network (RNN) that pro-
duces a sequence of state vectors for a given input sequence
by applying long-short term memory (LSTM) as a transition
function. The output of this LSTM network is the internal
estimate of the reward with size 1024. To obtain a distribu-
tion over tokens, a softmax is applied to this output.

The samples of the reward estimate in the questioner are
taken utilising dropout with parameter 0.5. Subsequently,
the upper bound in Eq. 7 is calculated to choose words. For
this experiment we set Œ≤t = 13.

Once the questioner is trained using our information
seeking decoder in RL, we take three approaches to evaluat-
ing the performance of the questioner: (1) sampling where
the subsequent word is sampled from the multinomial dis-
tribution in the vocabulary, (2) greedy where the word with
maximum probability is selected and (3) beam search keep-
ing the K-most promising candidate sequences at each time
step (we choose K = 20 in all experiments). During train-
ing the baseline uses the greedy approach to select the se-

In GuessWhat [14] a visually rich image with several ob-
jects is shown to two players. One player selects an object

3We observed marginal performance improvement by using a larger Œ≤

on Guesswhat, despite the additional training overhead.

4160

is it a person?

is he wearing a brown coat?

is he wearing a white shirt?

is he wearing a blue shirt?

are they sitting down?

Yes

No

No

No

No

is it the guy in the orange shirt

Yes

to the left?

e
g
n
a
r
O
n
i
n
a
M

Figure 3. Sample dialogue from the GuessWhat dataset. The agent
asks about a brown coat and then changes it to orange in anticipa-
tion of wrong identiÔ¨Åcation.

quence of words as in [14].
Overall Results We compare two cases, labelled New
Object and New Image. In the former the object sought is
new, but the image has been seen previously. In the latter
the image is also previously unseen. We report the predic-
tion accuracy for the guessed objects.
It is clear that the
accuracies are generally higher for the new objects as they
are obtained from the already seen images.

The results are summarised in Tab. 1. As shown, simply
applying REINFORCE improves the output of the system
signiÔ¨Åcantly, in particular in the new image case where the
generalisation is tested. This improvement is because the
question generator has the chance to better explore possible
questions. Additionally, the greedy approach outperforms
others in the RL baseline in [31]. This illustrates that the
distribution of the words obtained from the softmax in the
question generator is not very peaked and the difference be-
tween the best and second best word is often small. This
indicates that the prediction at test time is very uncertain
and supports our approach.

Since our approach seeks uncertain words, those words
are exploited at training time, which leads to lower vari-
ance (a more peaked distribution) and better performance
of the greedy selection. Beam search signiÔ¨Åcantly increases
performance when we carry out 5 rounds (as in [14, 31])
of question-answering. This is because the most informa-
tive words are selected by our approach which, combined
with the beam-search‚Äôs mechanism for forward exploration,
leads to better performance.

Note that our approach is generic enough that can be
used in combination with other architectures (e.g. [20, 42]).
For instance, in Tab. 1 ‚ÄúOurs+MN‚Äù uses the Memory Net-
work [33] and Attention mechanism [7] in the Guesser (sim-
ilar to that of [43]) which leads to better question genera-
tion. Fig. 3 shows one example produced by our dialogue
generator. More examples can be found in the supplements.

Ablation Study on Early Stopping
In goal-oriented dia-
logue systems, it is desirable to make a decision as soon as
possible. In this experiment, we control the dialogue length
by changing the threshold Œ∑ (see Sec. 3.3 for more details).
When Œ∑ is larger, we accept less conÔ¨Ådent predictions lead-
ing to shorter dialogues. As shown in the Tab. 1, our mod-
els achieve a comparable performance to the baseline even

0.6

0.5

0.4

0.3

0.2

1

2

3

4

5

Baseline

RL

Ours

Figure 4. The proportion of dialogues successful in identifying the
goal object at each round in GuessWhat.

using shorter rounds of question answering. The Fig. 4
shows the proportion of dialogues successful in identifying
the goal object at each round. Our model achieves higher
accuracy even in the earlier rounds, e.g. at the round three.
Human Study To evaluate how well humans can guess
the target object based on the questions generated by our
models, we conduct a human study. Following [42], we
show human subjects 50 images with generated question-
answer pairs from our model, and let them guess the objects.
We ask three human subjects to play on the same split and
the game is recognised as successful if at least two of them
give the right answer. In our experiment, the average perfor-
mance of humans was 79% compared to 52% and 70% for
the supervised [14] and RL [31] models. We are even better
than a model proposed in [42] (76%), which has three com-
plex hand-crafted rewards. These results indicate that our
agent can provide more useful information that can beneÔ¨Åt
a human in achieving the Ô¨Ånal goal.

4.2. Deal or No Deal

Here two agents receive a collection of items, and are
instructed to divide them so that each item is assigned to one
agent. This problem is, unlike the GuessWhat game, semi-
cooperative game in that the goals are adversarial. Each
agent‚Äôs goal is to maximise its own rewards which may be
in direct contradiction with its opponents goals.

Each item has a different random non-negative value for
each agent. These random values are constrained so that:

Score

% Agreed

e
n
i
l
e
s
a
B

s
r
u
O

Supervised [21]

RL [21]

RL+Rollouts [21]

Œ≤t = 1
Œ≤t = 10
Œ≤t = 1000
Œ≤t = 10+Rollouts

5.4 vs. 5.5
7.1 vs. 4.2

8.3 vs. 4.2
8.09 vs 4.08

8.27 vs 4.23
8.21 vs 4.33
8.58 vs 4.13

87.9
89.9

94.4
92.02

94.79
94.65

95.75

% Selection
50.78 vs 49.23
55.81 vs 44.19

60.02 vs 39.98
77.13 vs 22.87

88.56 vs 11.44
87.05 vs 12.95
93.62 vs 6.38

Table 2. Prioritising words with greater uncertainty leads to better
performance in negotiations.‚Äò% Selection‚Äô represents the percent-
age of trials in which the Ô¨Ånal decision is made by each agent.

4161

(1) the sum of values for all items for each agent is 10;
(2) each item has a non-zero value for at least one agent;
and (3) there are items with non-zero value for both agents.
These constraints are to ensure both agents cannot receive
a maximum score, and that no item is worthless to both
agents. After 10 turns, agents are given the option to com-
plete the negotiation with no agreement, which is worth 0
points to each. There are 3 item types (books, hats, balls) in
the dataset and between 5 and 7 total items in the collection.

Implementation Details The supervised learning model
comprises 4 recurrent neural networks implemented as
GRUs. The agent‚Äôs input goal is encoded as the hidden
state of a GRU with size 64. The tokens are generated
by sampling from the distribution of tokens. Simple max-
imum likelihood often leads to accepting an offer because
it is more often than proposing a counter offer. To rem-
edy this problem, similar to the previous GuessWhat exper-
iment, we perform goal-oriented reinforcement learning to
Ô¨Åne-tune the model. In addition, following [21] we exper-
imented with rollouts. That is, considering the future ex-
pected reward in the subsequent dialogue, which is similar
to the beam search in the previous experiment.

Results & Ablation Analysis Results are shown in
Tab. 2. We report the average reward for each agent and
the percentage of agreed upon negotiations. We see that our
approach signiÔ¨Åcantly outperforms the baseline RL. This is
due to the information-seeking behaviour of our approach
that leads to the agent learning to perform better negotia-
tions and achieve agreements when the deals are acceptable.
We also evaluate the inÔ¨Çuence of Œ≤t (in Eq. 7), which
controls how much the uncertainty is taken into account in
selecting a word, in turn controling the extent of exploration
in dialogue generation. Increasing Œ≤t leads to more explo-
ration and more conÔ¨Ådence in the actions at the expense of
later convergence. From Tab. 2, we can see that a larger Œ≤t
leads to better performance. We also observed that if Œ≤t is
too high, say 1000, it diminishes performance as the agent
continues exploring (by uttering risky statements that may
lead to better understanding of the agent‚Äôs counterpart at the
cost of losing the deal) rather than exploiting its knowledge
about the best word choices at each step of the negotiation.
Tab. 3 shows examples of the negotiations generated us-
ing our model. The baseline model sometimes refuses an
option that could lead to a desirable deal. She learns to be
forceful in some cases, and consistent. This is because our
model uncovers that by taking this risk, the counterpart may
change his strategy. This is in part due to the fact that the
supervised case the agent is willing to compromise quickly
and our approach exploits that. This is achieved by repeat-
ing the same proposition by our model. Furthermore, our
model learns to give her counterpart an option to give him
a false sense of control over the negotiation, thus deceiv-
ing him. While she seems to have given-up in favour of the

Alice : book=(2, 0) hat=(2, 5) ball=(1, 0)

Bob : book=(2, 2) hat=(2, 2) ball=(1, 2)

Our Approach vs Baseline

Baseline vs Baseline

Alice: i would like the hats
and the books.
Bob: i need the hats and the
books.
Alice: you can have the ball if
i can have the rest

Bob: ok deal

Alice: i‚Äôd like the hats and the ball.
Bob: you can have the ball , but i
need the hats and the books .
Alice: i need the hats and a book .

Bob: no deal . i can give you the ball

and both books

Alice: 10 , Bob: 2 points

Alice: 0 , Bob: 4 points

Alice : book=(1, 0) hat=(1, 7) ball=(3, 1)

Bob : book=(1, 9) hat=(1, 1) ball=(3, 0)

Bob: i would like the book and
the hat .
Alice : you can have the book
if i can have the rest

Bob: ok , deal

Bob: i want the book and 2 balls
Alice: i need the hat and the balls
Bob: i need the book and one ball
Alice: how about i take the hat and 1
ball?
Bob: sorry i cant make a deal without
the book

Alice: then we will need the hat and

the book

Alice: 10, Bob: 9 points

Alice: 0 (7)‚àó, Bob: 0 points

Table 3. Samples from the negotiation experiments: Our approach
is Alice and Bob is the baseline. ‚àó is the potential reward.

other‚Äôs beneÔ¨Åt, she enforces her choice and is consistent.

5. Conclusion

One of the primary limitations of current goal-directed
dialogue systems is their limited ability to identify the infor-
mation required to achieve their goal, and the steps required
to obtain it. This limitation inherent in any reinforcement
learning-based system that needs to learn to acquire the in-
formation required to achieve a goal. We have described a
simple extension to reinforcement learning that overcomes
this limitation, and enables an agent to select the action that
is most likely to provide the information required to meet
their objective. The selection process is simple, and con-
trollable, and minimises the expected regret. It also enables
a principled approach to identifying the appropriate point at
which to stop seeking more information, and act.

The approach we propose is based on a principled
Bayesian formulation of the uncertainty in both the inter-
nal state of the model, and the process used to select actions
using this state information. We have demonstrated the per-
formance of the approach when applied to generating goal-
oriented dialogue, which is one of the more complex prob-
lems in its class due to the generality of the actions involved
(natural language), and the need to adapt to the unknown
intentions of the other participant. The proposed approach
none the less outperforms the comparable benchmarks.

Acknowledgment: We gratefully acknowledge the sup-
port of NVIDIA Corporation with the donation of the Titan
Xp GPU used for this research

4162

References

[1] E. Abbasnejad, J. Domke, and S. Sanner. Loss-
calibrated monte carlo action selection.
In Proceed-
ings of the Twenty-Ninth AAAI Conference on Ar-
tiÔ¨Åcial Intelligence, pages 3447‚Äì3453. AAAI Press,
2015. 3

[2] E. Abbasnejad, S. Sanner, E. V. Bonilla, and
P. Poupart. Learning community-based preferences
via dirichlet process mixtures of gaussian processes.
In Proceedings of
the Twenty-Third International
Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI ‚Äô13,
pages 1213‚Äì1219, 2013. 2

[3] E. Abbasnejad, Q. Wu, I. Abbasnejad, J. Shi, and
A. van den Hengel. An active information seeking
model for goal-oriented vision-and-language tasks.
arXiv preprint arXiv:1812.06398, 2018. 2

[4] M. E. Abbasnejad, A. Dick, Q. Sh i, and A. van den
Hengel. Active learning from noisy tagged images.
2018. 3

[5] M. E. Abbasnejad, Q. Shi, I. Abbasnejad, A. van den
Hengel, and A. R. Dick. Bayesian conditional gen-
erative adverserial networks. CoRR, abs/1706.05477,
2017. 3

[6] N. Asghar, P. Poupart, J. Xin, and H. Li. On-
line sequence-to-sequence reinforcement learning for
open-domain conversational agents. arXiv preprint
arXiv:1612.03929, 2016. 3

[7] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate.
CoRR, abs/1409.0473, 2014. 7

[8] A. Bordes, Y.-L. Boureau, and J. Weston. Learn-
ing end-to-end goal-oriented dialog. arXiv preprint
arXiv:1605.07683, 2016. 2

[9] K. Cho, B. van Merrienboer, D. Bahdanau, and
Y. Bengio. On the properties of neural machine trans-
lation: Encoder-decoder approaches. In Eighth Work-
shop on Syntax, Semantics and Structure in Statistical
Translation (SSST-8), 2014, 2014. 4

[10] V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic

linear optimization under bandit feedback. 2008. 5

Conference on Computer Vision (ICCV), pages 2970‚Äì
2979, 2017. 6

[14] H. de Vries, F. Strub, S. Chandar, O. Pietquin,
H. Larochelle, and A. C. Courville. Guesswhat?! vi-
sual object discovery through multi-modal dialogue.
In Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2017. 1, 2, 3, 4, 6, 7

[15] C. W. Fox and S. J. Roberts. A tutorial on varia-
tional bayesian inference. ArtiÔ¨Åcial Intelligence Re-
view, 38(2):85‚Äì95, Aug 2012. 5

[16] Y. Gal and Z. Ghahramani. Dropout as a Bayesian
approximation: Representing model uncertainty in
deep learning. Proceedings of the 33th International
Conference on International Conference on Machine
Learning, 2016. 3, 5

[17] Y. Gal and Z. Ghahramani. A theoretically grounded
application of dropout in recurrent neural networks.
In Proceedings of the 30th International Conference
on Neural Information Processing Systems, NIPS‚Äô16,
USA, 2016. Curran Associates Inc. 3, 5

[18] S. Hochreiter and J. Schmidhuber. Long short-term

memory. 9:1735‚Äì80, 12 1997. 4

[19] A. Kendall and Y. Gal. What uncertainties do we
need in bayesian deep learning for computer vision?
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems
30, pages 5574‚Äì5584. Curran Associates, Inc., 2017.
3

[20] S. Lee, Y. Heo, and B. Zhang. Answerer in ques-
tioner‚Äôs mind for goal-oriented visual dialogue. CoRR,
abs/1802.03881, 2018. 7

[21] M. Lewis, D. Yarats, Y. N. Dauphin, D. Parikh, and
D. Batra. Deal or No Deal? End-to-End Learning for
Negotiation Dialogues. ArXiv e-prints, 2017. 2, 4, 6,
7, 8

[22] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and
D. Jurafsky. Deep reinforcement learning for dialogue
generation. arXiv preprint arXiv:1606.01541, 2016.
1, 2, 3

[11] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M.
Moura, D. Parikh, and D. Batra. Visual dialog. 2017.
2, 3

[23] J. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky.
Adversarial learning for neural dialogue generation.
arXiv preprint arXiv:1701.06547, 2017. 2

[12] A. Das, S. Kottur,

J. M. Moura, S. Lee, and
D. Batra. Learning cooperative visual dialog agents
with deep reinforcement learning.
arXiv preprint
arXiv:1703.06585, 2017. 3

[24] Z. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and
L. Deng. Bbq-networks: EfÔ¨Åcient exploration in deep
reinforcement learning for task-oriented dialogue sys-
tems. arXiv preprint arXiv:1711.05715, 2017. 3

[13] A. Das, S. Kottur, J. M. F. Moura, S. Lee, and D. Batra.
Learning cooperative visual dialog agents with deep
reinforcement learning.
In 2017 IEEE International

[25] O. Pietquin, M. Geist, S. Chandramohan, and
H. Frezza-Buet.
Sample-efÔ¨Åcient batch reinforce-
ment learning for dialogue management optimization.

4163

the 12th International Conference on Neural Informa-
tion Processing Systems, NIPS‚Äô99, Cambridge, MA,
USA, 1999. MIT Press. 4

[37] O. Vinyals and Q. Le. A neural conversational model.

06 2015. 1

[38] Z. Wang and O. Lemon. A simple and generic belief
tracking mechanism for the dialog state tracking chal-
lenge: On the believability of observed information. In
Proceedings of the SIGDIAL 2013 Conference, pages
423‚Äì432, 2013. 2

[39] R. J. Williams. Simple statistical gradient-following
algorithms for connectionist reinforcement learning.
Machine Learning, 8, May 1992. 4

[40] Q. Wu, P. Wang, C. Shen, I. Reid, and A. van den
Hengel. Are you talking to me? reasoned visual di-
alog generation through adversarial learning. arXiv
preprint arXiv:1711.07613, 2017. 3

[41] S. Young, M. Ga≈°i¬¥c, B. Thomson, and J. D. Williams.
Pomdp-based statistical spoken dialog systems: A re-
view. Proceedings of the IEEE, 101(5):1160‚Äì1179,
2013. 2

[42] J. Zhang, Q. Wu, C. Shen, J. Zhang, J. Lu, and A. Van
Den Hengel. Goal-oriented visual question generation
via intermediate rewards. In European Conference on
Computer Vision, pages 189‚Äì204. Springer, 2018. 7

[43] R. Zhao and V. Tresp. Improving goal-oriented visual
dialog agents via advanced recurrent nets with tem-
pered policy gradient. In IJCAI, 2018. 6, 7

ACM Transactions on Speech and Language Process-
ing (TSLP), 7(3):7, 2011. 3

[26] A. Ritter, C. Cherry, and W. B. Dolan. Data-driven
response generation in social media. pages 583‚Äì593.
Association for Computational Linguistics, 2011. 2

[27] I. V. Serban, A. Sordoni, Y. Bengio, A. C. Courville,
and J. Pineau. Building end-to-end dialogue systems
using generative hierarchical neural network models.
In AAAI, volume 16, pages 3776‚Äì3784, 2016. 1

[28] S. Singh, D. Litman, M. Kearns, and M. Walker.
Optimizing dialogue management with reinforcement
learning: Experiments with the njfun system. Journal
of ArtiÔ¨Åcial Intelligence Research, 16:105‚Äì133, 2002.
3

[29] A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji,
M. Mitchell, J.-Y. Nie, J. Gao, and B. Dolan. A
neural network approach to context-sensitive gener-
ation of conversational responses.
arXiv preprint
arXiv:1506.06714, 2015. 2

[30] N. Srinivas, A. Krause, S. Kakade, and M. Seeger.
Gaussian process optimization in the bandit setting:
no regret and experimental design.
In Proceedings
of the 27th International Conference on International
Conference on Machine Learning, 2010. 5

[31] F. Strub, H. De Vries, J. Mary, B. Piot, A. Courville,
and O. Pietquin. End-to-end optimization of goal-
driven and visually grounded dialogue systems. arXiv
preprint arXiv:1703.05423, 2017. 6, 7

[32] P.-H. Su, M. Gasic, N. Mrksic, L. Rojas-Barahona,
S. Ultes, D. Vandyke, T.-H. Wen, and S. Young. Con-
tinuously learning neural dialogue management. arXiv
preprint arXiv:1606.02689, 2016. 3

[33] S. Sukhbaatar, a. szlam, J. Weston, and R. Fergus.
End-to-end memory networks.
In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Gar-
nett, editors, Advances in Neural Information Process-
ing Systems 28, pages 2440‚Äì2448. Curran Associates,
Inc., 2015. 7

[34] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to
sequence learning with neural networks. In Proceed-
ings of the 27th International Conference on Neural
Information Processing Systems - Volume 2, NIPS‚Äô14,
pages 3104‚Äì3112, Cambridge, MA, USA, 2014. MIT
Press. 1

[35] R. S. Sutton and A. G. Barto. Reinforcement learn-
ing: An introduction. IEEE Transactions on Neural
Networks, 16:285‚Äì286, 1998. 1, 4

[36] R. S. Sutton, D. McAllester, S. Singh, and Y. Man-
sour. Policy gradient methods for reinforcement learn-
ing with function approximation.
In Proceedings of

4164

