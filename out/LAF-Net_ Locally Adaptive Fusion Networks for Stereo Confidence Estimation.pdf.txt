LAF-Net: Locally Adaptive Fusion Networks for Stereo Conï¬dence Estimation

Sunok Kim1,2, Seungryong Kim1,2, Dongbo Min3, Kwanghoon Sohn1*

1Yonsei University 2 Â´Ecole polytechnique fÂ´edÂ´erale de Lausanne (EPFL)

3Ewha Womans University

{kso428,khsohn}@yonsei.ac.kr seungryong.kim@epfl.ch dbmin@ewha.ac.kr

ConceptualÂ figure.Â (fig1)

Abstract

We present a novel method that estimates conï¬dence map
of an initial disparity by making full use of tri-modal in-
put, including matching cost, disparity, and color image
through deep networks. The proposed network, termed
as Locally Adaptive Fusion Networks (LAF-Net), learns
locally-varying attention and scale maps to fuse the tri-
modal conï¬dence features. The attention inference net-
works encode the importance of tri-modal conï¬dence fea-
tures and then concatenate them using the attention maps
in an adaptive and dynamic fashion. This enables us to
make an optimal fusion of the heterogeneous features, com-
pared to a simple concatenation technique that is commonly
used in conventional approaches.
In addition, to encode
the conï¬dence features with locally-varying receptive ï¬elds,
the scale inference networks learn the scale map and warp
the fused conï¬dence features through convolutional spatial
transformer networks. Finally, the conï¬dence map is pro-
gressively estimated in the recursive reï¬nement networks to
enforce a spatial context and local consistency. Experimen-
tal results show that this model outperforms the state-of-
the-art methods on various benchmarks.

1. Introduction

Stereo matching for reconstructing geometric conï¬gura-
tion of a scene is one of the fundamental and essential prob-
lems in computer vision ï¬elds [36]. For decades, numer-
ous methods have been proposed for this task by leveraging
handcrafted [43, 10] and/or machine learning based [45, 38]
techniques. However, because of its challenging elements
such as reï¬‚ective surfaces, textureless regions, repeated pat-
tern regions, occlusions [23, 13, 6], and photometric de-
formations incurred by illumination and camera speciï¬ca-
tion variations [44, 9], the stereo matching still remains an
unsolved problem. To alleviate these inherent challenges,

This research was supported by Next-Generation Information Com-
puting Development Program through the National Research Founda-
tion of Korea(NRF) funded by the Ministry of Science and ICT (NRF-
2017M3C4A7069370). âˆ—Corresponding author

t
s
o
C
g
n
h
c
t
a
M

i

y
t
i
r
a
p
s
i
D

e
g
a
m

i
Â 
r
o
o
C

l

t
e
N
â€
F
A
L

e
c
n
e
d
i
f
n
o
C

Figure 1. Illustration of LAF-Net: using tri-modal input, consist-
ing of matching cost, disparity, and color image, LAF-Net esti-
mates conï¬dence of disparity.

most methods [39, 27, 29, 20, 18, 21] have adopted the
conï¬dence estimation step that detects unreliable disparities
and reï¬nes them for improving the quality of stereo match-
ing results.

Formally, the conï¬dence estimation pipeline involves
ï¬rst extracting the conï¬dence features and then training the
conï¬dence classiï¬ers using ground-truth conï¬dences [39,
27, 30]. Conventionally, there exist several handcrafted
conï¬dence measures using different input modalities, such
as matching cost, disparity, and color image [12, 28]. Since
any single conï¬dence measures cannot handle all failure
cases in stereo matching, various combination of hand-
designed conï¬dence measures extracted from the tri-modal
input [8, 39, 27, 29, 20] has been used to learn shallow clas-
siï¬ers, such as random decision forest [2, 22]. Despite per-
formance improvement by the joint usage of the tri-modal
input, they still show a limited performance due to their low
discriminative power.

Recent approaches have attempted to estimate the con-
ï¬dence by leveraging deep convolutional neural networks
(CNNs) thanks to their high robustness [30, 37, 18, 21],
demonstrating the substantial accuracy gain over the hand-
crafted approaches. However, unlike handcrafted ap-
proaches [8, 39, 27] that make full use of the tri-modal in-
put, CNN-based approaches have been formulated by par-

1205

tially using single- or bi-modal input, e.g., matching cost
only [38], disparity only [30, 37], matching cost and dis-
parity [18, 21], or disparity and color [7, 40]. Moreover, a
simple concatenation technique [16] is commonly used to
fuse multi-modal conï¬dence features, disregarding that the
fusion weights may vary for each pixel depending on the
characteristic of conï¬dence features.

Meanwhile, the receptive ï¬elds for conï¬dence features
can vary for each pixel. This assumption has been used in
conventional handcrafted methods [39, 27, 29, 20] in a way
of extracting multi-scale conï¬dence features. For instance,
it was reported in [27] that the median disparity deviation
value in different scales is the most important conï¬dence
features for both outdoor [24] and indoor database [34].
A similar idea has also been adopted in some conï¬dence
estimation approaches based on deep CNNs. In [21], the
multi-scale disparity feature extraction networks have been
proposed to learn the conï¬dence features from disparity in
different scales. Also, the dilated convolution that extracts
local contextualized information with different dilation fac-
tors was proposed by Fu et al. [7]. Tosi et al.
[40] pro-
posed local-global conï¬dence networks to effectively com-
bine both local and global context from the input images.
However, there is still no mechanism that explicitly consid-
ers locally-varying scale ï¬elds.

On the other hand, in order to consider the spatial con-
text and local consistency, the output conï¬dence map was
reï¬ned using joint ï¬ltering [20] or using deep CNNs [31],
generating more reliable conï¬dence map.

In this paper, we propose novel conï¬dence estimation
networks, called Locally Adaptive Fusion Networks (LAF-
Net), that utilize tri-modal input consisting of matching
cost, disparity, and color image as illustrated in Fig. 1.
The networks consist of conï¬dence feature extraction net-
works, attention inference networks, scale inference net-
works, and recursive conï¬dence reï¬nement networks.
In
the attention inference networks, we fuse the tri-modal in-
put adaptively with locally-varying attention maps to bene-
ï¬t from the joint usage of the tri-modal conï¬dence features.
In the scale inference networks, locally adaptive scale pa-
rameters are learned for all pixels, which enables the net-
works to extract the conï¬dence features within locally op-
timal receptive ï¬elds.
In addition, the output conï¬dence
is further reï¬ned through the recursive conï¬dence reï¬ne-
ment networks. The proposed method is extensively eval-
uated through an ablation study and comparison with con-
ventional handcrafted and CNNs-based methods on various
benchmarks, including Middlebury 2006 [34], Middlebury
2014 [33], and KITTI 2015 [24].

2. Related Works

Handcrafted approaches.
there have
been extensive literatures in conï¬dence estimation, mainly

In last decades,

based on handcrafted conï¬dence measures [6, 5, 25]. In a
comprehensive study of conï¬dence measures has been pre-
sented by Hu and Mordohai [12]. Various single conï¬dence
measures have been analyzed and categorized according to
different input by Park et al. [28]. From matching cost, the
peak ratio of the matching costs [11] and naive peak ra-
tio [12] have been widely used to remove unreliable pix-
els. The maximum margin [12] and winner margin [35]
were computed with the difference of matching costs. From
disparity, a left-right consistency [5] has been most widely
used for ï¬nding the correctness of matched pixels. The vari-
ances of the disparity (VAR) [8] and the median disparity
deviation (MDD) [8] in a local window were also measured
to estimate unreliable pixels. Several conï¬dence measures
extracted from image have been introduced in [28]. The
variance of intensities might be used, especially separating
the homogeneous regions from the well-textured regions as
well as the magnitude of the image gradients. A distance-
to-edge measure incorporated the texturedness of a pixel.

Since there is no single conï¬dence feature that yields
stably optimal performance, various approaches to beneï¬t
from the feature combination among a different set of sin-
gle conï¬dence measures have been proposed [8, 39] which
trained a shallow classiï¬er such as random decision for-
est [1, 22]. However, the performance of the aforemen-
tioned methods is still limited since the selected conï¬dence
features are not optimal. To select the set of (sub-)optimal
conï¬dence features among multiple conï¬dence features,
Park and Yoon [27] utilized the permutation importance
measures to select important set of conï¬dence feautres.
In [27], they found the MDD in different scales are impor-
tant to measure unrelibale pixels. Similarly, Poggi and Mat-
toccia [29] employed the set of conï¬dence features from
only disparity map that can be computed in O(1) complex-
ity without losing the conï¬dence estimation performance.
While the aforementioned methods detect unconï¬dent pix-
els in a pixel-level, Kim et al. [20] leveraged a spatial con-
text to estimate conï¬dence in a superpixel-level. In [20],
the resulting conï¬dence map was further reï¬ned through
hierarchical conï¬dence map aggregation. However, all of
these methods used handcrafted conï¬dence features, and
they may not be optimal to detect unrelibale pixels on chal-
lenging scenes.

Deep CNN-based approaches. Recent approaches have
tried to measure the conï¬dence through deep CNNs [30, 37,
31, 18, 21]. A quantitative evaluation of conï¬dence mea-
sures that use machine learning approaches has been per-
formed in [32]. Formally, these CNN-based methods ï¬rst
extract the conï¬dence features from single- or bi-modal in-
put and then predict the conï¬dence by jointly learning the
feature extractor and classiï¬er. Various methods have been
proposed that use the single- or bi-modal input, i.e., a left
disparity [30], both left and right disparity [37], a matching

206

FeatureÂ ExtractionÂ Networks

AttentionÂ InferenceÂ Networks

ScaleÂ InferenceÂ Networks

Â 

t
s
o
C
K
â€
p
o
T

y
t
i
r
a
p
s
i
D

l

r
o
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

ğ‘‹(cid:3004)

ğ‘‹(cid:3005)

ğ‘‹(cid:3010)

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

Â 

Â 

N
B
+
v
n
o
C

x
a
m

t
f
o
S

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

ğ‘Œ

Â 

Â 

N
B
+
v
n
o
C

i

d
o
m
g
S

i

BilinearÂ samplerÂ &Â Conv.

*
RecursiveÂ RefinementÂ Networks

e
c
n
e
d
i
f
n
o
C

i

d
o
m
g
S

i

Â 

Â 

N
B
+
v
n
o
C

U
L
e
R
+

Â 

Â 
Â 

Â 

Â 

N
B
+
v
n
o
C

ğ‘„(cid:4593)

ğ‘Œ(cid:3020)

ğ‘

Figure 2. The network conï¬guration of LAF-Net which consists of four sub-networks, including feature extraction networks, attention
inference networks, scale inference network, and recursive reï¬nement networks. Given matching cost, disparity, and color image as input,
our networks output conï¬dence of the disparity. The detail of scale inference network is illustrated in Fig. 4.

cost [38], matching cost and disparity [18, 21], and dispar-
ity and color [7, 40]. In order to extract conï¬dence features
from matching cost and disparity, Kim et al. [21] proposed
the top-K pooling layer to normalize matching cost and im-
proved the discriminative power to classify the unrelibale
pixels. Although these methods improved the conï¬dence
estimation performance, they did not make full use of the
tri-modal input.

In [21], the multi-scale disparity feature extractor was
proposed, while dilation convolution was applied in [7] to
gain local contexualized information effectively.
In [40],
they proposed global conï¬dence measure using encoder-
decoder networks by looking at the whole image and dispar-
ity content. By using the output of global conï¬dence, they
proposed local-global approach by fusing the local conï¬-
dence, the global conï¬dence, and disparity. All of these
methods considered only ï¬xed and pre-deï¬ned scale ranges
and did not estimate a scale that varying for each pixel.
On the other hand, the conï¬dence reï¬nement networks [31]
were also developed, which can improve the accuracy of the
estimated conï¬dence map by leveraging a local consistency
within the conï¬dence map.

3. Proposed Method

3.1. Problem Statement and Motivation

Let us deï¬ne a pair of stereo images as I l and I r, re-
spectively. The objective of stereo matching is to estimate a
disparity Di between the stereo image pairs that is deï¬ned

i and I r

for each pixel i = [ix, iy]T . The matching costs Ci,d be-
iâ€² , where iâ€² = i âˆ’ [d, 0]T , among disparity
tween I l
candidates d = {1, ..., dmax} are ï¬rst measured, and then
aggregated and optimized for computing the disparity Di.
Most existing methods for stereo matching [10, 17, 45] can-
not provide fully reliable results due to its challenging ele-
ments, thus several approaches [39, 27, 37, 18, 20] have
presented an additional module to predict a conï¬dence Qi
of the disparity Di. By leveraging the conï¬dence Qi, they
reï¬ne the initial disparity Di through subsequent disparity
reï¬nement pipeline.

To realize this, we design a novel network architecture
that estimates the conï¬dence by fully exploiting match-
ing cost C, disparity map D, and color image I. The
overall networks consist of four sub-networks, including
conï¬dence feature extraction networks, attention inference
networks, scale inference networks, and recursive conï¬-
dence reï¬nement networks, as illustrated in Fig. 2.
In
feature extraction networks, conï¬dence features are ï¬rst
extracted from tri-modal input. The intermediate features
from this network are then fed to learn locally-varying at-
tention maps in attention inference networks. The atten-
tion maps are used to adaptively concatenate the tri-modal
conï¬dence features, unlike existing approaches [18, 21,
7, 40] that use a simple concatenation technique. Then,
locally-varying scale ï¬elds are learned for extracting conï¬-
dence features within geometrically-aligned receptive ï¬elds
through scale inference networks, different from conven-
tional approaches [30, 37, 21] with a ï¬xed-size convolution.

207

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3. The visulization of the attention maps: (a) top-1 match-
ing cost, (b) initial disparity, (c) left color image, (d)-(f) the atten-
tion maps for matching cost, disparity, and color, respectively.

Finally, the conï¬dence is progressively reï¬ned in recursive
conï¬dence reï¬nement networks to enforce a spatial context
and local consistency inspired by [20, 31].

3.2. Conï¬dence Feature Extraction Networks

The conï¬dence feature extraction networks are designed
to extract the tri-modal conï¬dence features denoted as X C ,
X D, and X I from matching cost C, disparity D, and left
color image1 I l by feed-forward processes such that X C =
F(C; W C), X D = F(D; W D), and X I = F(I l; W I )
with network parameters W C , W D, and W I , respectively.
The network parameters for each network are seperately
learned, not shared, to encode the heterogeneous charater-
istics of the tri-modal input. The size and absolute value
of raw matching cost Craw vary depending on the search
range of stereo image pairs and stereo matching methods.
Additionally, its distribution is often non-discriminative as
mentioned in [37, 7]. To alleviate these limitations, the in-
put matching cost Craw is converted into a top-K match-
ing probabiliry2 C as in [18, 21], which enables the search
range-invariant convolutions.

The conï¬dence feature extraction networks consist of 3
convolutional layers (Conv) with 3 Ã— 3 kernels producing
64 feature channel, followed by batch normalization (BN)
and rectiï¬ed linear units (ReLU).

3.3. Attention Inference Networks

Due to their heterogeneous attributes, a direct concate-
nation of these tri-modal input does not provide an optimal
performance [7]. Alternatively, some methods [18, 7, 40,
21] ï¬rst extract the bi-modal conï¬dence features and then
concatenate them. However, such a simple approach that
ï¬xes the fusion weights at inference often fails to perform

1We use a left color image only to estimate the conï¬dence of left dis-
parity and a right image can be used when estimating the conï¬dence of
right disparity.

2We denote this as the matching cost for the sake of clarity.

an optimal feature fusion.

i , AD

i , and AI

To alleviate this limitation, inspired by [15], we build the
attention inference networks for inferring an optimal fusion
weight among the tri-modal features, i.e., X C , X D, and
X I . The locally-varying attention for each modality at pixel
i is deï¬ned as AC
i for matching cost, disparity,
and color image, respectively. These attentions are learned
such that AC
D ), and
AI
i = F(X I
C , W A
D ,
and W A
I , and these attentions then undergoes a softmax
function to make the sum of attentions for each pixel to be 1,
i ) = 1. Note that the attention inference
D , and
I ) are not shared but independently learned depending

network parameters for each modality (i.e., W A
W A
on their attributes.

i ; W A
I ) with the network parameters W A

i.e., Pâˆ—âˆˆC,D,I (Aâˆ—

i = F(X C
i ; W A

i = F(X D

C ), AD

C , W A

i ; W A

The learned attentions are then applied to the conï¬dence

feature as

Yi = Î (cid:0)X C

i âŠ™ AC

i , X D

i âŠ™ AD

i , X I

i âŠ™ AI
i(cid:1) ,

(1)

where Î (Â·) is a concatenation operator and âŠ™ is an element-
wise multiplication operator. Note that unlike methods [7,
21, 40] that use the ï¬xed fusion weights, the attentions AC ,
AD, and AI , are estimated conditioned on input and varies
locally, thus enabling the data-adaptive fusion more effec-
tively. The visualization of attention maps for different in-
put modalities is exempliï¬ed in Fig. 3. The attention of
top-K matching cost is high for pixels having high matching
probability. On the other hand, the attention of disparity has
high value in noisy region, indicating informative features
can be extracted from the differnet disparity assignments,
as considered similar to VAR or MDD [8] in handcafted
features. In color image, the attentions near image bound-
ary are high and this indicates a image texture can give a
useful cue to estimate conï¬dence. By adaptively weighting
the conï¬dence features with these attention maps, we can
obtain more discriminative conï¬dence features.

The attention learning networks consist of 2 Conv with
3 Ã— 3 kernels. The ï¬rst Conv produces 64 channel feature,
followed by BN and ReLU, and the second Conv produces
1 channel feature followed by only BN.

3.4. Scale Inference Networks

The optimal receptive ï¬elds for conï¬dence features can
vary at each pixel. In order to encode conï¬dence features of
different scales, some approaches [27, 7, 21, 40] have been
proposed, but they consider only ï¬xed and pre-deï¬ned scale
ranges and do not estimate scales that vary for each pixel.

To determine the optimal receptive ï¬elds for conï¬dence
features at each pixel, we present the scale inference net-
works that learn locally-varying scale ï¬elds. It ï¬rst infers
the scale ï¬elds through subsequent convolutions such that
Si = F(Yi; W S) with network parameters W S. With these
scale ï¬elds Si, the intermediate features are warped through

208

Â 

Â 

(cid:1861)(cid:1845)(cid:3036)

a

(cid:1862)
(cid:1851)

(cid:1862)(cid:3020)

Conv.

(cid:1851)(cid:3020)

(cid:1852)

Figure 4. Illustration of a bilinear sampler in the scale inferent net-
works: for each pixel i in the feature Y can be warped as enlarged
size feature Y S . The neighbors j S is convolved as Z with stride.

ï‚´

ï‚´

an image sampling on a parameterized grid, similar to spa-
tial transformer networks (STNs) [14].

However, a spatially-varying parameterized sampling
grid cannot be directly realized with the original STNs [14]
that is designed for a global geometric ï¬eld. To deal with
locally-varying scale ï¬elds, we ï¬rst build a locally-varying
sampling grid for N Ã— N neighbors j âˆˆ Ni indenpendently,
and then warp the convolutional activation for each sam-
pling grid as used in [4, 19]. Concretely, the locally-varying
sampling grid jS = [jS

x , jS

y ]T is deï¬ned such that
jy âˆ’ iy (cid:21) +(cid:20) ix
iy (cid:21) ,

0 Si (cid:21)(cid:20) jx âˆ’ ix

0

(cid:20) jS

y (cid:21) = (cid:20) Si

jS

x

(2)

for all pixels i and their neighbors j within receptive ï¬elds
on the regular grid. For each grid sample jS = [jS
y ]T ,
receptive ï¬elds for convolutional layers are warped through
the bilinear sampler [14] independently such that

x , jS

Y S

i,j = Xi

Yimax(0, 1 âˆ’ |jS

x âˆ’ ix|)max(0, 1 âˆ’ |jS

y âˆ’ iy|),

(3)
where Y S
i,j is the warped convolutional activation of Yi,j .
Since this scale-varying convolutional features are deï¬ned
for all i and j independently, the spatial size of Y S is en-
larged as |N | times of the size of Y without overlap as il-
lustrated in Fig. 4. Then, Y S passes through a subsequent
convolution with the stride N to convolve the warped fea-
tures independently and generate the scale-adaptive conï¬-
dence features Z. We chose N as 3 since the kernel size of
following convolutional layer is 3 Ã— 3.

The scale learning networks consist of 2 Conv with 3 Ã— 3
kernels. The ï¬rst Conv produces 64 channel feature, fol-
lowed by BN and ReLU, and the second Conv produces
1 channel feature followed by only BN. The output passes
through the sigmoid layer to generate the scale parameter
for each pixel.

(a)

(c)

(e)

(b)

(d)

(f)

Figure 5. The effectiveness of the proposed recursive conï¬dence
reï¬nement networks: (a) left color image, (b) initial disparity, (c)
estimated conï¬dence map without recursive module, (d) thresh-
olded disparity with (c), (e) estimated conï¬dence map with recur-
sive module, (f) thresholded disparity with (e). The mismatched
pixels in the red boxes are reliably detected with the proposed re-
cursive conï¬dence reï¬nement networks.

networks. From the conï¬dence feature Zi, we ï¬nally for-
mulate the conï¬dence prediction networks to estimate the
conï¬dence Qi such that Qi = F(Zi; W P ) with the pre-
diction parameters W P . The iterative reï¬nement proce-
dure of output conï¬dence can improve the conï¬dence es-
timation accuracy as studied in the handcrafted approach
using joint ï¬ltering [20] and CNNs-based approach [31].
Inspired by this, we propose the recursive conï¬dence reï¬ne-
ment networks, where the previously estimated conï¬dence
serves as a guidance of the current conï¬dence estimation.
To realize this recursive module, we formulate the networks
such that Qt
are
the estimated conï¬dences at tth and (t âˆ’ 1)th iteration, re-
spectively. The initial conï¬dence Q0
i is deï¬ned as zeros.
As evolving the iterations, the conï¬dence accuracy is im-
proved gradually and the ï¬nal conï¬dence map is obtained
as Qâ€² = Qtmax . The effectiveness of the recursive conï¬-
dence reï¬nement networks is shown in Fig. 5. Here, we set
0.9 to threshold. With the recursive module, the ability to
predict mismatched pixels on initial disparity is improved.
The recursive conï¬dence reï¬nement networks consist of
2 Conv and ï¬nal sigmoid layer similar to the scale learning
networks. For the number of iteration, we set tmax to 3.

i = F(Zi, Qtâˆ’1

; W P ) where Qt

i and Qtâˆ’1

i

i

The proposed method employs the cross-entropy loss
function [38, 21] with respect to the ground-truth conï¬-
dence Qâˆ— and the estimated conï¬dence Qâ€²
i.

4. Experimental Results

3.5. Recursive Conï¬dence Reï¬nement Networks

4.1. Experimental Settings

So far we introduce our networks that fuse tri-modal con-
ï¬dence features through the attention and scale inference

The proposed method was implemented in MATLAB
with VLFeat MatConvNet toolbox [42] and simu-

209

Match. cost
Disparity
Color
MID 2006
MID 2014
KITTI 2015

X

X

X

X

X

X

X

X

X

0.0431 0.0392 0.0381 0.0375 0.0364
0.0762 0.0703 0.0687 0.0685 0.0683
0.0347 0.0245 0.0237 0.0231 0.0225

0.25

0.2

0.15

0.1

0.05

]

%
[
e
t
a
r
 
l
e
x
i
p

 

d
a
B

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

]

%
[
e
t
a
r
 
l
e
x
i
p

 

d
a
B

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

0

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

10

20

30

40

50

60

70

80

90

100

Table 1. Ablation study for the various combination of input
modalities in LAF-Net on MID 2006 [34], MID 2014 [33], and
KITTI 2015 [24] dataset, when the raw matching cost is obtained
using MC-CNN [45].

Attention
Scale
Recursive
MID 2006
MID 2014
KITTI 2015

X

X

X

X

X

X

X

X

0.0374 0.0375 0.0372 0.0371 0.0364
0.0686 0.0688 0.0685 0.0685 0.0683
0.0235 0.0236 0.0231 0.0229 0.0225

Table 2. Ablation study for the effectivness of each sub-networks
in LAF-Net on MID 2006 [34], MID 2014 [33], and KITTI
2015 [24] dataset, when the raw matching cost is obtained using
MC-CNN [45]. The average AUC values for simple concatenation
without fusion methods are 0.0386, 0.0689, and 0.0238 for MID
2006, MID 2014, and KITTI 2015, respectively.

lated on a PC with TitanX GPU. We make use of the
stochastic gradient descent with momentum, and set the
learning rate to 1 Ã— 10âˆ’6 and the batch size to 16. To com-
pute a raw matching cost, we used a census transform with a
5Ã—5 local window and MC-CNN [45], respectively. For the
census transform, we applied SGM [10] on estimated cost
volumes by setting P1 = 0.008 and P2 = 0.126 as in [27].
For computing the MC-CNN, â€˜KITTI 2012 fast networkâ€™
was used, provided at the authorâ€™s website [46]. We set
Ïƒ as 100 and 0.05 for census-SGM and MC-CNN, respec-
tively, as in [21]. We trained our networks using MPI Sintel
dataset [3] and KITTI 2012 dataset [24], and evaluated each
model on Middlebury 2006 (MID 2006) [34], Middlebury
2014 (MID 2014) [33], and KITTI 2015 dataset [24]. In ad-
dition, we used the half-sized KITTI database due to mem-
ory constraints, so we measured the error rates and AUC
values in the half-sized resolution. For Middlebury, we
used the third-sized images provided by [34]. The ground-
truth conï¬dence maps are obtained by thresholding an ab-
solute difference between estimated disparity and ground-
truth disparity to 1. In inference, the LAF-Net takes about
0.912s, 2.413s, and 0.783s for MID 2006 (368Ã—424), MID
2014 (496Ã—792), and KITTI 2015 (608Ã—184), respectively,
while [40] takes 0.750s, 1.628s, and 0.552s in the same set-
tings. Due to the bilinear sampler and recursive procedure,
the LAF-Net takes longer than [40]. In contrast, the number
of parameters in LAF-Net and [40] is 1,337K and 9,289K,
proving that LAF-Net is lighter while achieving a better ac-
curacy.

0.14

0.12

0.1

0.08

0.06

0.04

0.02

]

%
[
e
t
a
r
 
l
e
x
i
p

 

d
a
B

0

0

0.25

0.2

]

%
[
e
t
a
r
 
l
e
x
i
p
 
d
a
B

0.15

0.1

0.05

0

0

10

20

30

40

50

60

70

80

90

100

Sparsification[%]

Sparsification[%]

(a)

(b)

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

10

20

30

40

50

60

70

80

90

100

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

]

%
[
e
t
a
r
 
l
e
x
i
p

 

d
a
B

0

0

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

10

20

30

40

50

60

70

80

90

100

Sparsification[%]

Sparsification[%]

(c)

(d)

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

0.35

0.3

0.25

0.2

0.15

0.1

0.05

]

%
[
e
t
a
r
 
l
e
x
i
p
 
d
a
B

0

0

Haeusler et al.
Spyropoulos et al.
Park and Yoon
O(1)
Kim et al. (TIP'17)
CCNN
PBCP
Shaked et al. (Conf)
Kim et al. (Conf)
LFN
ConfNet
LGC-Net
LAF-Net
Optimal

10

20

30

40

50

60

70

80

90

100

0

0

10

20

30

40

50

60

70

80

90

100

Sparsification[%]

Sparsification[%]

(e)

(f)

Figure 6. The sparsiï¬cation curves of selected images for MID
2006 [34], MID 2014 [33], and KITTI 2015 dataset [24] using (a),
(c), (e) census-SGM and (b), (d), (f) MC-CNN. The sparsiï¬cation
curve for the ground-truth conï¬dence map is described as â€˜opti-
malâ€™.

In the following, we evaluated the proposed method in
comparison to conventional handcrafted approaches, such
as Haeusler et al. [8], Spyropoulos et al. [39], Park and
Yoon [27], Poggi and Mattoccia [29], Kim et al. [20]. Sev-
eral CNNs-based approaches using single- or bi-modal in-
put are also compared, where using disparity only, such
as Poggi and Mattoccia (CCNN) [30], Seki and Pollefey
(PBCP) [37], matching cost only, such as Shaked et al. [38],
both disparity and matching cost, such as Kim et al. [21],
and both color and disparity, such as Fu et al. (LFN) [7]
and the global measures of Tosi et al. (ConfNet) [40] and
local and global measures (LGC-Net) [40]. We obtained
the results of [27], [20], and [21] by using the author-
provided code, while the results of [8], [39], [37], [38],
and [7] were obtained by our own implementation. We re-
implemented methods of [29], [30], and [40] based on the
author-provided code.

To evaluate the performance of conï¬dence estimation
quantitatively, we used the sparsiï¬cation curve and its area
under curve (AUC) as used in [8, 39, 27, 37, 21]. The spar-

210

Datasets

Haeusler et al. [8]
Spyropoulos et al. [39]
Park and Yoon [27]
Poggi et al. [29]
Kim et al. [20]
CCNN [30]
PBCP [37]
Shaked et al. (Conf) [38]
Kim et al. (Conf) [21]
LFN [7]
ConfNet [40]
LGC-Net [40]
LAF-Net
Optimal

MID 2006 [34]

MID 2014 [33]

KITTI 2015 [24]

Census-SGM MC-CNN

Census-SGM MC-CNN

Census-SGM MC-CNN

0.0454
0.0447
0.0438
0.0439
0.0430
0.0454
0.0462
0.0464
0.0419
0.0416
0.0451
0.0413
0.0405
0.0340

0.0417
0.0420
0.0426
0.0413
0.0409
0.0402
0.0413
0.0495
0.0394
0.0393
0.0428
0.0389
0.0364
0.0323

0.0841
0.0839
0.0802
0.0791
0.0772
0.0769
0.0791
0.0806
0.0749
0.0752
0.0783
0.0735
0.0718
0.0569

0.0750
0.0752
0.0734
0.0707
0.0701
0.0716
0.0718
0.0736
0.0694
0.0692
0.0721
0.0685
0.0683
0.0527

0.0585
0.0536
0.0527
0.0461
0.0430
0.0419
0.0439
0.0531
0.0407
0.0405
0.0486
0.0392
0.0385
0.0348

0.0308
0.0323
0.0303
0.0263
0.0294
0.0258
0.0272
0.0292
0.0250
0.0253
0.0277
0.0236
0.0225
0.0170

Table 3. The average AUC values for MID 2006 [34], MID 2014 [33], and KITTI 2015 [24] dataset. The AUC value of ground truth
conï¬dence is measured as â€˜Optimalâ€™. The result with the lowest AUC value in each experiment is highlighted.

(a)

(b)

Figure 7. Comparisons of AUC values for (a) census-based SGM
and (b) MC-CNN for the KITTI 2015 dataset [24]. We sort the
AUC values in the ascending order according to the AUC values.

siï¬cation curve draws a bad pixel rate while successively
removing pixels in descending order of conï¬dence values
in the disparity map, thus it enables us to observe the ten-
dency of estimation errors. For the higher accuracy of the
conï¬dence measure, AUC value is lower and the optimal
AUC is measured using ground-truth conï¬dence.

4.2. Ablation Study

We analyzed our conï¬dence estimation networks with
the ablation evaluations, with respect to various combina-

tion of different modalities and the effectiveness of the pro-
posed sub-networks.

The effects on tri-modal input.
In Table 1, ablation ex-
periments to validate the effects of multi-modal input show
the necessity of using the tri-modal input. Note that the
attention inference module is not used for input of single
modality. Although the bi-modal input improved the ability
to predict reliable pixels, the full usage of tri-modal input
shows the best performance.

The effects on various fusion methods.
In Table 2, abla-
tion experiments to validate the effects of the proposed fu-
sion methods. Compared to the simple concatenation tech-
nique, the conï¬dence estimator is improved with the atten-
tion and scale obtained from the attention and scale infer-
ence networks. Also, the recursive conï¬dence reï¬nement
networks show the additional improvement.

4.3. Conï¬dence Estimation Analysis

In order to measure the performance of the conï¬dence
estimator in comparison to other methods, we compared
the average AUC values of our method with conventional
learning-based approaches using handcrafted conï¬dence
measures [8, 39, 27, 29, 20] and CNNs-based methods [37,
30, 7, 40]. For fair comparison, we also evaluated the conï¬-
dence estimation performance only for [38, 21], i.e., Shaked
et al. (Conf) [38] and Kim et al. (Conf) [21].

Sparsiï¬cation curves for MID 2006 [34], MID 2014 [33],
and KITTI 2015 [24] with census-based SGM and MC-
CNN are shown in Fig. 6. Fig. 7 describes the AUC
values, which are sorted in ascending order, for the KITTI
2015 [24] with census-based SGM and MC-CNN, respec-
tively. The results have shown that the proposed conï¬-
dence estimator exhibits a better performance than both

211

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 8. The conï¬dence maps on MID 2006 dataset [34] (ï¬rst two rows) and MID 2014 dataset [33] (last two rows) using census-SGM
and MC-CNN. (a) color images, (b) initial disparity map, (c)-(f) are estimated conï¬dence maps by (c) Kim et al. [21], (d) LFN [7], (e)
LGC-Net [40], (f) LAF-Net, and (g) ground-truth conï¬dence map.

Figure 9. The conï¬dence maps on KITTI 2015 dataset [24] using census-SGM (ï¬rst two rows), and MC-CNN (last two raws). (From top to
bottom, left to right) color images, initial disparity map, estimated conï¬dence maps by CCNN [30], PBCP [37], Kim et al. [21], LFN [7],
LGC-Net [40], and LAF-Net.

conventional handcrafted approaches and CNN-based ap-
proaches. The average AUC with census-based SGM and
MC-CNN for MID 2006, MID 2014, and KITTI 2015
datasets were summarized in Table 3. The handcrafted ap-
proaches showed inferior performance than the proposed
method due to low discriminative power. CNNs-based
methods [30, 37, 38, 7] have improved conï¬dence estima-
tion performance compared to existing handcrafted meth-
ods such as [8, 39, 27, 29, 20], but they are still infe-
rior to our method as they rely on single- [30, 38] or bi-
modal [37, 21, 7, 40] input rather than tri-modal input. The
estimated conï¬dence maps are shown in Fig. 8 and Fig. 9.

5. Conclusion

We presented LAF-Net that estimates conï¬dence with
tri-modal input,
including matching cost, disparity, and
color image through deep networks. The key idea of the
proposed method is to design locally adaptive attention
and scale inference networks to generate optimal fusion
weights.
In addition, the conï¬dence estimation perfor-
mance is further improved with recursive conï¬dence reï¬ne-
ment networks. A direction for further study is to examine
how conï¬dence estimation networks could be learned in an
unsupervised manner as proposed in [26, 41].

212

References

[1] C. Biernacki, G. Celeux, and G. Govaert. Assessing a mix-
ture model for clustering with the integrated completed like-
lihood. IEEE Trans. Pattern Anal. Mach. Intell., 22(7):719â€“
725, 2000.

[2] L. Breiman. Random forests. Mach. Learn., 63(4):5â€“32,

2001.

[3] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ï¬‚ow evaluation. in
Proc. Eur. Conf. Comput. Vis., pages 611â€“625, Oct. 2012.

[4] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker. Uni-
versal correspondence network. in Proc. Advances in Neural
Inf. Process. Syst., pages 2414â€“2422, Dec. 2016.

[5] G. Egnal, M. Mintz, and R. Wildes. A stereo conï¬dence
metric using single view imagery with comparison to ï¬ve
alternative approaches.
Image. Vis. Comput., 22(12):943â€“
957, 2004.

[6] G. Egnal and R. P. Wildes. Detecting binocular half-
occlusions: Empirical comparisons of ï¬ve approaches. IEEE
Trans. Pattern Anal. Mach. Intell., 24(8):1127â€“1133, 2002.

[7] Z. Fu and M. A. Fard.

Learning conï¬dence measures
by multi-modal convolutional neural networks.
in Proc.
IEEE Winter Conf. Applicat. Comput. Vis., pages 1321â€“
1330, 2018.

[8] R. Haeusler, R. Nair, and D. Kondermann. Ensemble learn-
ing for conï¬dence measrues in stereo vision. in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., pages 305â€“312, Jun.
2013.

[9] Y. Heo, K. Lee, and S. Lee. Robust stereo matching using
adaptive normalized cross correlation. IEEE Trans. Pattern
Anal. Mach. Intell., 33(4):807â€“822, 2011.

[10] H. Hirschmuller. Stereo processing by semiglobal matching
and mutual information. IEEE Trans. Pattern Anal. Mach.
Intell., 30(2):328â€“341, 2008.

[11] H. Hirschmuller, P. Innocent, and J. Garibaldi. Real-time
correlation-based stereo vision with reduced border errors.
Int. J. Comput. Vis., 47(1â€“3):229â€“246, 2002.

[12] X. Hu and P. Mordohai. A quantitative evaluation of conï¬-
dence measures for stereo vision. IEEE Trans. Pattern Anal.
Mach. Intell., 34(11):2121â€“2133, 2012.

[13] M. Humenberger, C. Zinner, M. Weber, W. Kubinger, and
M. Vincze. A fast stereo matching algorithm suitable for em-
bedded real-time systems. Comput. Vis. Image. Understand.,
114(11):1180â€“1202, 2010.

[14] M. Jaderberg, K. Simonyan, and A. Zisserman. Spatial trans-
former networks. in Proc. Advances in Neural Inf. Process.
Syst., pages 2017â€“2025, Dec. 2015.

[15] X. Jia, B. D. Brabandere, T. Tuytelaars, and L. V. Gool. Dy-
namic ï¬lter networks. in Proc. Advances in Neural Inf. Pro-
cess. Syst., pages 667â€“675, Dec. 2016.

[16] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiï¬cation with convo-
lutional neural networks. in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., pages 1725â€“1732, Jun. 2014.

[17] S. Kim, B. Ham, B. Kim, and K. Sohn. Mahalanobis
distance cross-correlation for illumination invariant stereo

matching. IEEE Trans. Circ. Syst. Vid. Techn., 24(11):1844â€“
1859, 2014.

[18] S. Kim, D. Min, B. Ham, S. Kim, and K. Sohn. Deep stereo
in Proc. IEEE

conï¬dence prediction for depth estimation.
Conf. Image. Process., Sep. 2017.

[19] S. Kim, D. Min, B. Ham, S. Lin, and K. Sohn. Fcss: Fully
convolutional self-similarity for dense semantic correspon-
dence. IEEE Trans. Pattern Anal. Mach. Intell., 2017.

[20] S. Kim, D. Min, S. Kim, and K. Sohn. Feature augmentation
for learning conï¬dence measure in stereo matching. IEEE
Trans. Image Process., 26(12):6019â€“6033, 2017.

[21] S. Kim, D. Min, S. Kim, and K. Sohn. Uniï¬ed conï¬dence
estimation networks for robust stereo matching. IEEE Trans.
Image Process., 28(3):1299â€“1313, 2019.

[22] A. Liaw and M. Wiener. Classiï¬cation and regression by

random forest. R news, 2(3):18â€“22, 2002.

[23] X. Mei, X. Sun, M. Zhou, S. Jiao, H. Wang, and X. Zhang.
On building an accurate stereo matching system on graphics
hardware. in Proc. IEEE Int. Conf. Comput. Vis. Work., pages
467â€“474, Nov. 2011.

[24] M. Menze and A. Geiger. Object scene ï¬‚ow for autonomous
vehicles. in Proc. IEEE Conf. Comput. Vis. Pattern Recog-
nit., pages 3061â€“3070, Jun. 2015.

[25] P. Mordohai. The self-aware matching measure for stereo. in
Proc. IEEE Int. Conf. Comput. Vis., pages 1841â€“1848, Sep.
2009.

[26] C. Mostegel, M. Rumpler, F. Fraundorfer, and H. Bischof.
Using self-contradiction to learn conï¬dence measures in
stereo vision.
in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., pages 4067â€“4076, Jun. 2016.

[27] M. Park and K. Yoon. Leveraging stereo matching with
in Proc. IEEE Conf.

learning-based conï¬dence measures.
Comput. Vis. Pattern Recognit., pages 101â€“109, Jun. 2015.

[28] M. Park and K. Yoon. Learning and selecting conï¬dence
IEEE Trans. Pattern

measures for robust stereo matching.
Anal. Mach. Intell., 2018.

[29] M. Poggi and S. Mattoccia. Learning a general-purpose con-
ï¬dence measure based on o(1) features and a smarter aggre-
gation strategy for semi global matching. in Proc. IEEE Int.
Conf. 3D Vis., pages 509â€“518, Oct. 2016.

[30] M. Poggi and S. Mattoccia. Learning from scratch a con-
in Proc. Brit. Mach. Vis. Conf., 10, Sep.

ï¬dence measure.
2016.

[31] M. Poggi and S. Mattoccia. Learning to predict stereo re-
liability enforcing local consistency of conï¬dence maps. in
Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jul. 2017.

[32] M. Poggi, F. Tosi, and S. Mattoccia. Quantitative evaluation
of conï¬dence measures in a machine learning world. in Proc.
IEEE Int. Conf. Comput. Vis., Oct. 2017.

[33] D. Scharstein, H. Hirschmuller, Y. Kitajima, G. Krathwohl,
N. Nesic, X. Wang, and P. Westling. High-resolution stereo
datasets with subpixel-accurate ground truth. in Proc. Ger-
man Conf. Pattern Recognit., pages 31â€“42, Sep. 2014.

[34] D. Scharstein and C. Pal. Learning conditional random ï¬elds
for stereo. in Proc. IEEE Conf. Comput. Vis. Pattern Recog-
nit., pages 1â€“8, Jun. 2007.

213

[35] D. Scharstein and R. Szeliski. Stereo matching with non-

linear diffusion. Int. J. Comput. Vis., 28(2):155â€“174, 1998.

[36] D. Scharstein and R. Szeliski. A taxonomy and evaluation
of dense two-frame stereo correspondence algorithms. Int. J.
Comput. Vis., 47(1â€“3):7â€“42, 2002.

[37] A. Seki and M. Pollefeys. Patch based conï¬dence prediction
for dense disparity map. in Proc. Brit. Mach. Vis. Conf., 10,
Sep. 2016.

[38] A. Shaked and L. Wolf. Improved stereo matching with con-
stant highway networks and reï¬‚ective conï¬dence learning. in
Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2017.
[39] A. Spyropoulos, N. Komodakis, and P. Mordohai. Learning
to detect ground control points for improving the accuracy of
stereo matching. in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., pages 1621â€“1628, Jun. 2014.

[40] F. Tosi, M. Poggi, A. Benincasa, and S. Mattoccia. Beyond
local reasoning for stereo conï¬dence estimation with deep
learning. in Proc. Eur. Conf. Comput. Vis., Sep. 2018.

[41] F. Tosi, M. Poggi, A. Tonioni, L. D. Stefano, and S. Mattoc-
cia. Learning conï¬dence measures in the wild. in Proc. Brit.
Mach. Vis. Conf., page 2, Sep. 2017.

[42] A. Vedaldi and K. Lnc. Matconvnet: Convolutional neural
in Proc. ACM Int. Conf. Multimedia,

networks for matlab.
pages 689â€“692, Oct. 2015.

[43] K. J. Yoon and I. S. Kweon. Adaptive support-weight ap-
proach for correspondence search. IEEE Trans. Pattern Anal.
Mach. Intell., 28(4):650â€“656, 2006.

[44] R. Zabih and J. Woodï¬ll. Non-parametric local transforms
in Proc. Eur. Conf.

for computing visual correspondence.
Comput. Vis., pages 151â€“158, May 1994.

[45] J. Zbontar and Y. LeCun. Computing the stereo matching
cost with a convolutional neural network.
in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1592â€“1599, Jun.
2015.

[46] J. Zbontar and Y. LeCun. Stereo matching by training a con-
volutional neural network to compare image patches. Jour-
nal of Machine Learning Research, 17:1â€“32, 2016.

214

