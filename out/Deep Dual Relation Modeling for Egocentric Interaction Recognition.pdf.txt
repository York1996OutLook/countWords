Deep Dual Relation Modeling for Egocentric Interaction Recognition

Haoxin Li1,3,4, Yijun Cai1,4, Wei-Shi Zheng2,3,4, âˆ—

1School of Electronics and Information Technology, Sun Yat-sen University, China

2School of Data and Computer Science, Sun Yat-sen University, China

3Peng Cheng Laboratory, Shenzhen 518005, China

4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China

lihaoxin05@gmail.com, caiyj6@mail2.sysu.edu.cn, wszheng@ieee.org

Abstract

Egocentric interaction recognition aims to recognize the
camera wearerâ€™s interactions with the interactor who faces
the camera wearer in egocentric videos. In such a human-
human interaction analysis problem, it is crucial to explore
the relations between the camera wearer and the interactor.
However, most existing works directly model the interac-
tions as a whole and lack modeling the relations between
the two interacting persons. To exploit the strong relations
for egocentric interaction recognition, we introduce a dual
relation modeling framework which learns to model the re-
lations between the camera wearer and the interactor based
on the individual action representations of the two persons.
Speciï¬cally, we develop a novel interactive LSTM module,
the key component of our framework, to explicitly model
the relations between the two interacting persons based on
their individual action representations, which are collabo-
ratively learned with an interactor attention module and a
global-local motion module. Experimental results on three
egocentric interaction datasets show the effectiveness of our
method and advantage over state-of-the-arts.

1. Introduction

Egocentric interaction recognition [11, 25, 31, 35, 39]
attracts increasing attention with the popularity of wearable
cameras and broad applications including human machine
interaction [2, 18] and group events retrieval [3, 4]. Dif-
ferent from exocentric (third-person) videos, in egocentric
videos, the camera wearers are commonly invisible and the
videos are usually recorded with dynamic ego-motion (see
Figure 1). The invisibility of the camera wearer hampers ac-
tion recognition learning of the camera wearer, and the ego-
motion hinders direct motion description of the interactor,
which make egocentric interaction recognition challenging.

âˆ—Corresponding author

(a) Invisibility of the camera wearer

(b) Ego-motion of the camera wearer

Figure 1. Illustration of camera-wearerâ€™s invisibility and ego-
motion. (a) compares the person (in red boxes) receiving some-
thing in exocentric (left) and egocentric (right) videos from NUSF-
PID dataset [25].
(b) shows adjacent frames with obvious ego-
motion in an egocentric video from UTokyo PEV dataset [39].

An egocentric interaction comprises the actions of the
camera wearer and the interactor that inï¬‚uence each other
with relations. So modeling the relations between the two
interacting persons is important for interaction analysis. To
model the relations between the the two interacting per-
sons explicitly, we need to obtain individual action repre-
sentations of the two persons primarily. Therefore, we for-
mulate the egocentric interaction recognition problem into
two interconnected subtasks, individual action representa-
tion learning and dual relation modeling.

In recent years, various works attempt to recognize in-
teractions from egocentric videos. Existing methods inte-
grated motion information using statistical properties of tra-
jectories and optical ï¬‚ows [25, 31, 38] or utilized face ori-
entations descriptors [11] with SVM classiï¬ers for recog-
nition. Deep neural network was also adopted to aggre-
gate short-term and long-term information for classiï¬cation
[35]. However, some of them [31] aimed to recognize in-

7932

Video

Action representation

1

2

3

M

.
.
.

Frame ğ‘°ğ’âˆ’ğŸ
Frame ğ‘°ğ’

Motion 
Module

Feature 

Extraction

Parameters 

shared

Feature 

Extraction

Attention 
Module

(a)

.
.
.

(b)

.
.
.

(c)

.
.
.

(d)

.
.
.

Relation modeling

...

Interactive 

LSTM

step n-1

Interactive 

LSTM
step n

.
.
.

Interactive 

LSTM

step n+1

...

(a) global motion features; (b) local motion features;
(c) global appearance features; (d) local appearance features.

Figure 2. Proposed framework. Frames I i(i = 1, ..., N ) are sam-
pled from the video as input. The Feature Extraction Module ex-
tracts basic visual features of sampled frames. The Attention Mod-
ule localizes the interactor and learns appearance features. The
Motion Module estimates global and local motions for motion fea-
tures learning. The Interaction Module models the relations for
better interaction recognition based on the learned individual fea-
tures (a), (b), (c) and (d) explained in the blue box.

teractions from a static observerâ€™s view, which is imprac-
tical for most applications. Others [11, 25, 35] directly
learned interaction as a whole through appearance and mo-
tion learning as done in common individual action analysis.
They didnâ€™t learn individual action representations of the
interacting persons, and thus failed to model the relations
explicitly. The ï¬rst-person and second-person features were
introduced to represent the actions of the camera wearer and
the interactor in [39]. But they learned the individual action
representations from multiple POV (point-of-view) videos
and still lacked explicit relation modeling.
Overview of the framework. In this paper, we focus on
the problem of recognizing human-human interactions from
single POV egocentric videos. Considering the relations
in egocentric interactions, we develop a dual relation mod-
eling framework, which integrates the two interconnected
subtasks, namely individual action representation learning
and dual relation modeling, for recognition as shown in Fig-
ure 2. Speciï¬cally, for dual relation modeling, we develop
an interaction module, termed interactive LSTM, to model
the relations between the camera wearer and the interactor
explicitly based on the learned individual action representa-
tions. For individual action representations learning, we in-
troduce an attention module and a motion module to jointly
learn action features of the two interacting persons. We ï¬-
nally combine these modules into an end-to-end framework
and train them with human segmentation loss, frame recon-
struction loss and classiï¬cation loss as supervision. Exper-
imental results indicate the effectiveness of our method.
Our contributions. In summary, the main contribution of
this paper is three-fold. (1) An interactive LSTM module is
developed to model the relations between the camera wearer
and the interactor from single POV egocentric videos. (2)
An interactor attention module and a global-local motion

module are designed to jointly learn individual action rep-
resentations of the camera wearer and the interactor from
single POV egocentric video. (3) By integrating individual
action representations learning and dual relation modeling
into an end-to-end framework, our method shows its effec-
tiveness and outperforms existing state-of-the-arts on three
egocentric interaction datasets.

2. Related Work

Egocentric action recognition aims to recognize camera
wearerâ€™s actions from ï¬rst-person videos. Since the ego-
motion is a dominant characteristic of egocentric videos,
most methods used dense ï¬‚ow or trajectory based statisti-
cal features [1, 13, 17, 23, 24] to recognize the actions of
the camera wearer.
In some object-manipulated actions,
some works extracted hands and objects descriptors for
recognition [10, 20, 28, 43], and others further explored
gaze information according to hand positions and motions
[12, 19]. Recently, deep neural networks have also been ap-
plied to egocentric action recognition. Frame-based feature
series analysis showed their promising results [16, 32, 40].
CNN networks with multiple information streams were also
trained on recognition task [22, 34]. However, these meth-
ods target on individual actions which are a bit different
from human-human interactions.
Egocentric interaction recognition speciï¬cally focuses on
ï¬rst-person human-human interactions. Ryoo et al. recog-
nized what the persons in the videos are doing to the static
observer [30, 31], but it is unrealistic in most daily life sce-
narios. Some works used face orientations, individual loca-
tions descriptors and hand features to recognize interactions
[5, 11]. Others used motion information based on the mag-
nitudes or clusters of trajectories and optical ï¬‚ows [25, 38].
A convLSTM was utilized to aggregate features of succes-
sive frames for recognition [35]. These methods commonly
learned interaction descriptors by direct appearance or mo-
tion learning, but didnâ€™t considered explicit relation mod-
eling with individual action representations of the camera
wearer and the interactor. Yonetani et al. learned individual
action features of the two persons but also lacked explicit
relations modeling [39].

Different from the existing methods above, our frame-
work jointly learns individual actions of the camera wearer
and the interactor from single POV egocentric videos and
further explicitly models the relations between them by an
interactive LSTM.

3. Individual Action Representation Learning

To model the relations, we ï¬rst need individual action
representations of the camera wearer and the interactor.
Here, we learn interactor masks to separate the interactor
from background and learn appearance features with an at-

7933

Figure 3. Structure of attention module. The module takes feature
f n as input and generates attention weighted features and multi-
scale masks.

For

tention module. In the meanwhile, a motion module is in-
tegrated to learn motion cues, so that we can jointly learn
the individual appearance and motion features of the two
persons, which are the basis to model relations in Section 4.
two consecutive sampled frames I nâˆ’1, I n âˆˆ
RHÃ—W Ã—3, we use a feature extraction module composed of
ResNet-50 [14] to extract basic features f (I nâˆ’1), f (I n) âˆˆ
RH0Ã—W0Ã—C , which encode the scene or human information
with multidimensional representations for further modeling
on top of them. In the following, we denote f (I nâˆ’1) and
f (I n) as f nâˆ’1 and f n respectively for convenience.

3.1. Attention Appearance Features Learning

Egocentric videos record the actions of the camera
wearer and the interactor simultaneously. To learn individ-
ual action features of the two persons, we wish to separate
the interactor from background based on the feature f n.

Pose-guided or CAM-guided strategy [8, 9, 27] is used
for person attention learning. Similarly, we introduce an
attention module to localize the interactor with human seg-
mentation guidance. We employ a deconvolution structure
[26] on top of the basic feature f n to generate the masks
of the interactor as shown in Figure 3. Mask M (0) âˆˆ
RH0Ã—W0 serves to weight the corresponding feature maps
for attention features learning. Multi-scale masks M (k) âˆˆ
RHkÃ—Wk (k = 1, 2, 3) are applied to localize the interactor
at different scales for ï¬ner masks generation and explicit
motion estimation later in Subsection 3.2.
Mask of the Interactor. To localize the interactor, we in-
troduce a human segmentation loss to guide the learning of
our attention module. Given a reference mask M RF , the
human segmentation loss is a pixel-wise cross entropy loss:

Lseg = âˆ’

3

Hk

Wk

X

X

X

1

Hk Ã— Wk

[M RF

i,j logM (k)

i,j +

j=1

k=1

i=1
(1 âˆ’ M RF

i,j )log(1 âˆ’ M (k)

i,j )],

where k indexes the mask scales and the reference mask is
resized to the corresponding shape for calculation. Here,
the reference masks are obtained using JPPNet[21].

Attention Features. An optimized attention module could
localize the interactor, so the mask M (0) has higher val-
ues at the positions corresponding to the interactor, which
indicates concrete appearance information of the interactor.
Then the local appearance feature describing the action of
the interactor from its appearance can be calculated with
weighted pooling as follows:

f n

l,a =

1

|M (0)|

H0

W0

X

X

i=1

j=1

M (0)

i,j Â· f n

i,j,1:C ,

(2)

i=1 PW0

where |M (0)| = PH0
i,j . Accordingly, the
global appearance feature, which describes the action of the
camera wearer from what is observed, is calculated using
global average pooling:

j=1 M (0)

f n

g,a =

1

H0 Ã— W0

H0

W0

X

X

f n

i,j,1:C .

i=1

j=1

(3)

The attention module learns local appearance features to
provide a concrete description instead of a global descrip-
tion of the interactor, and thus assists the relation modeling
later. Meanwhile, the interactor masks localizing the inter-
actor plays an important role in separating global and local
motion, which is employed in Subsection 3.2.

3.2. Global local Motion Features Learning

Motion features are vital for action analysis. To learn
individual action representations of the two interacting per-
sons, we wish to describe the ego-motion (global motion)
of the camera wearer and the local motion of the interac-
tor explicitly based on the basic features f n, f nâˆ’1 and the
interactor masks M (k)(k = 0, 1, 2, 3).

Differentiable warping scheme [15] is used for ego-
motion estimation with a frame reconstruction loss [36, 42].
Inspired by them, we design a self-supervised motion mod-
ule with the differentiable warping mechanism to jointly es-
timate the two types of motion from egocentric videos.
Global-local Motion Formulation by Reconstruction. To
separate the global and local motions in egocentric videos,
we reuse the interactor mask M (3) generated in Subsection
3.1 with the same scale as the input frames to formulate
the transformation between two adjacent frames. With the
learnable parameters T and D denoting transformation ma-
trix and dense motion ï¬eld, we can formulate the transfor-
mation from homogeneous coordinates X n to X nâˆ’1 con-
cisely as:

(1)

Ë†X nâˆ’1 = T (X n + M (3) âŠ™ D),

(4)

where âŠ™ is element-wise multiplication, X n and X nâˆ’1 are
homogeneous coordinates of frame I n and I nâˆ’1.

In Equation (4), M (3) âŠ™ D is the local dense motion
ï¬eld of the interactor, and T describes the ego-motion of

7934

convs

convolutional layers

deconvs

deconvolutional layers

pooling

global average pooling

fc

fully connected layers

âŠ› multiplicative 

elementwise
multiplication

patch comparison

âŠ›ğ’‡ğ’âˆ’ğŸ
ğ’‡ğ’

convs

ğ‘´(ğŸ)

ğ’‡ğ’,ğ’ğ’

0
0

.
.
.

ğ‘»
ğ‘«

ğ’‡ğ’ˆ,ğ’ğ’

...

pooling

fc

deconvs

convs

p
o
o

l
i

n
g

...

Figure 4. Structure of motion module. The module takes basic
features f n, f nâˆ’1 and mask M (0) as inputs and estimates global
and local motion parameters in two branches in which global mo-
tion feature f n
l,m are extracted. âˆ— in
red circle is a multiplicative patch comparison [7] to calculate the
correlations between two feature maps, which captures the relative
motions between them for dense ï¬‚ow estimation.

g,m and local motion feature f n

the camera wearer, so Equation (4) jointly formulates global
and local motion explicitly by point set reconstruction.
Self-supervision. To learn the parameters in Equation (4),
we use view synthesis objective [42] as supervision:

Lrec = X

|I n(x) âˆ’ Ë†I n(x)|,

(5)

x

where x indexes over pixel coordinates X n. And Ë†I n is the
reconstructed frame warped from frame I nâˆ’1 according to
the transformed point set Ë†X nâˆ’1, which is employed with
the bilinear sampling mechanism [15] as

Ë†I n(x) = X

wij I nâˆ’1(Ë†xij),

(6)

iâˆˆ{t,b},jâˆˆ{l,r}

where Ë†x indexes over projected coordinates Ë†X nâˆ’1, Ë†xij is
the neighboring coordinate of Ë†x, wij is proportional to
the spatial proximity between Ë†xij and Ë†x, and subject to
Pi,j wij = 1. In addition, we regularize the local dense
motions with a smoothness loss for robust learning [36].

With the reconstruction loss in Equation (5), we design
a motion module illustrated in Figure 4 with two branches
learning the parameters of global ego-motion and local mo-
tion in Equation (4), from which global motion feature
f n
l,m are extracted from the
embedding layers.

g,m and local motion feature f n

The motion module jointly estimates explicit motions of
the camera wearer and the interactor by reusing the interac-
tor masks, from which we learn concrete individual motion
features of the two interacting persons hence aid relation
modeling in Section 4.

3.3. Ego feature and Exo feature.

For each frame pair {I nâˆ’1, I n}, we obtain global ap-
g,a and local appearance feature f n

pearance feature f n

l,a

g,m and local motion feature f n

from the attention module, and also global motion feature
f n
l,m from the motion mod-
ule. The global features describe overall scene context and
ego-motion of the camera wearer, which could represent the
action of the camera wearer. While the local features, ob-
tained with the interactor masks, describe the concrete ap-
pearance and motion of the interactor, which could repre-
sent the action of the interactor. Thus we obtain the individ-
ual action representations of the two interacting persons.

(
a
)
 

l

G
o
b
a

l
 

(
b
)
 
L
o
c
a

l
 

For further exploring the relations between the two per-
g,m]
exo =
l,m] describing the interactor. With them we could

sons, we deï¬ne the ego-feature f n
describing the camera wearer, and exo-feature f n
[f n
model the relations in Section 4.

ego = [f n

g,a, f n

l,a, f n

4. Dual Relation Modeling by Interactive

LSTM

Given the action representations, a classiï¬er may be
trained for recognition as done in most previous works.
However, as discussed before, a distinguishing property of
egocentric human-human interactions is the relations be-
tween the camera wearer and the interactor, which deserves
further exploration for better interaction representations.

We notice that only the ego-feature or exo-feature may
not exactly represent an interaction. For the example shown
in Figure 6, two interactions consist of similar individual
actions: the camera wearer turning his head and the interac-
tor pointing somewhere. In this case, neither the features of
any action can identify an interaction sufï¬ciently. However,
some relations would clearly tell the differences of the two
interactions, such as the sequential orders and the motion di-
rections of the individual actions. To utilize the relations for
recognition, we develop an interaction module to model the
relations between the two persons based on the ego-feature
and exo-feature deï¬ned in Subsection 3.3.

4.1. Symmetrical Gating and Updating

We deï¬ne ego-state F n

To model the relations such as the synchronism or com-
plementarity between the two interacting persons, we inte-
grate their action features using LSTM structure.
ego and exo-state F n

exo to denote
the latent states till the n-th step to encode the evolution of
the two actions, which correspond to ego-feature and exo-
feature introduced in Subsection 3.3, respectively. We wish
to mutually incorporate the action context of each interact-
ing person at each time step to explore the relations such
as the synchronism and complementarity. Thus, we uti-
lize exo-state to ï¬lter out the irrelevant parts, enhance the
relevant parts and complement the absent parts of the ego-
state. Meanwhile, the exo-state is also ï¬ltered, enhanced
and complemented by the ego-state. This symmetrical gat-
ing and updating mechanism is realized with two symmet-

7935

rical LSTM blocks where each block works as follows:

[in; on; gn; an] =Ïƒ(W f n + U F nâˆ’1+

J nâˆ’1 + b),

âˆ— + v),

J n =Ï†(V F n
cn =inan + gncnâˆ’1,
F n =ontanh(cn).

(7)

(8)

(9)

(10)

Here, the input gate, output gate, forget gate and update can-
didate are denoted as in, on, gn and an respectively. Ïƒ is
tanh activation function for update candidate and sigmoid
activation function for other gates. F âˆ— is the latent state
from the dual block, Ï† is ReLU activation function, and J n
is the modulated dual state. {W , U , V , b, v} are parame-
ters of each LSTM block.

It is noted that the current ego-state integrates the histor-
ical information of the ego-actions and also the exo-actions
into itself, and vice versa for exo-state. The ego-state and
exo-state describe the interaction from the view of the cam-
era weearer and the interactor respectively. In this symmet-
rical gating and updating manner, the symmetrical LSTM
blocks model the interactive relations instead of a raw com-
bination of two actions.

4.2. Explicit Relation Modelling

Besides the symmetrical LSTM blocks introduced above
for implicitly encoding the dual relations into the ego-state
and exo-state, we further explicitly model the dual relation.
To this end, we introduce relation-feature rn to explicitly
calculate the relations with a nonlinear additive operation
on the ego-state and exo-state:

Ego 
block

ğ’‡ğ’†ğ’ˆğ’ğŸ
ğ‘­ğ’†ğ’ˆğ’ğŸ
ğ‘­ğ’†ğ’™ğ’ğŸ
ğ’‡ğ’†ğ’™ğ’ğŸ

Exo
block

ğ‘¹ğŸ

Ego 
block

ğ’‡ğ’†ğ’ˆğ’ğŸ
ğ‘­ğ’†ğ’™ğ’ğŸ
ğ‘­ğ’†ğ’ˆğ’ğŸ
ğ‘­ğ’†ğ’™ğ’ğŸ
ğ‘­ğ’†ğ’ˆğ’ğŸ
ğ’‡ğ’†ğ’™ğ’ğŸ

Exo
block

ğ‘¹ğŸ

Ego 
block

ğ’‡ğ’†ğ’ˆğ’ğŸ
ğ‘­ğ’†ğ’™ğ’ğŸ
ğ‘­ğ’†ğ’ˆğ’ğŸ
ğ‘­ğ’†ğ’™ğ’ğŸ
ğ‘­ğ’†ğ’ˆğ’ğŸ
ğ’‡ğ’†ğ’™ğ’ğŸ

Exo
block

ğ‘¹ğŸ

Ego 
block

ğ’‡ğ’†ğ’ˆğ’ğ‘µ
ğ‘­ğ’†ğ’™ğ’ğ‘µâˆ’ğŸ
ğ‘­ğ’†ğ’ˆğ’ğ‘µ
ğ‘­ğ’†ğ’™ğ’ğ‘µ
ğ‘­ğ’†ğ’ˆğ’ğ‘µâˆ’ğŸ
ğ’‡ğ’†ğ’™ğ’ğ‘µ

Exo
block

ğ‘¹ğ‘µ

Figure 5. Diagram of Interactive LSTM. The unrolled symmetrical
LSTM blocks mutually gate and update each other as the green
arrows depict. The unrolled relation LSTM branch is highlighted
in red. All LSTM blocks contains N time steps.

between the two actions, which provides a better represen-
tation of the interaction.

The posterior probability of an interaction category given

the ï¬nal relation-state RN can be deï¬ned as

p(y|RN ) = Î´(W RN + b),

(15)

where W and b are parameters of classiï¬er and Î´ is softmax
function. Then a cross entropy loss function is employed to
supervise parameters optimization as follow:

Lcls = âˆ’

K

X

k=1

yklog[p(yk|RN )],

(16)

where K is the number of class.

rn = tanh(F n

ego + F n

exo).

(11)

Combining the loss functions of each module above, we

train our model end-to-end with the ï¬nal objective:

With the relation-feature rn at each time step, we further
model the time variant relations with another LSTM branch
to integrate the historical relations into the relation-states
Rn, which can be formulated as follows:

Lf inal = Lcls + Î±Lseg + Î²Lrec + Î³Lsmooth,

(17)

where Î±, Î², Î³ are weights of segmentation loss, frame re-
construction loss and smooth regularization, respectively.

[in; on; gn; an] = Ïƒ(W rn + U Rnâˆ’1 + b),

cn = inan + gncnâˆ’1,
Rn = ontanh(cn).

(12)

(13)

(14)

5. Experiments

5.1. Datasets

In the equations above, the gates and parameters are simi-
larly denoted as those in the symmetrical LSTM blocks. In
Equation (14), Rn integrates historical and current relations
information to explicitly represent the relations of the two
actions at n-th time step during the interaction.

Combining the two components above, i.e. the symmet-
rical LSTM blocks and the relation LSTM branch, our in-
teraction module is illustrated in Figure 5, which we term
interactive LSTM. It captures the evolution or synchronism
of the two actions and further explicitly models the relations

We evaluate our method on three egocentric human-

human interaction datasets.
UTokyo Paired Ego-Video (PEV) Dataset contains 1226
paired egocentric videos recording dyadic human-human
interactions [39]. It consists of 8 interaction categories and
was recorded by 6 subjects. We split the data into train-test
subsets based on the subject pairs as done in [39] and the
mean accuracy of the three splits is reported.
NUS First-person Interaction Dataset contains 152 ï¬rst-
person videos and 133 third-person videos of both human-
human and human-object interactions [25]. We evaluate our

7936

Methods

PEV NUS(ï¬rst h-h) NUS(ï¬rst)

RMF[1]
Ryoo and Matthies[31]
Narayan et al. [25]
Yonetani et al. [39] (single POV)
convLSTM[35] (raw frames)
convLSTM[35] (difference of frames)
LRCN[6]
TRN[41]
Two-stream[33]
Our method
Yonetani et al. [39] (multiple POV)
Our method (multiple POV)

-
-
-

60.4

-
-

45.3
49.3
58.5
64.2
69.2
69.7

-
-

74.8

-
-
-

65.4
66.7
78.6
80.2

-
-

-
-

77.9

-

69.4
70.0
70.6
74.7
80.6
81.8

-
-

JPL

86.0
89.6
96.7
75.0
70.6
90.1
78.5
84.2
93.4
98.4

-
-

Table 1. State-of-the-art comparison (%) with existing methods. NUS(ï¬rst h-h) denotes the ï¬rst-person human-human interaction subset
of NUS dataset and NUS(ï¬rst) denotes the ï¬rst-person subset. It is notable that only PEV dataset provides multiple POV videos so that no
multiple POV result of other datasets is reported.

method on ï¬rst-person human-human interaction subset to
verify the effectiveness of our method. To further test our
method in human-object interaction cases, we also evaluate
on the ï¬rst-person subset. Random train-test split scheme is
adopted and the mean accuracy is reported.
JPL First-Person Interaction Dataset consists of 84
videos of humans interacting with a humanoid model with
a camera mounted on its head [31]. It consists of 7 different
interactions. We validate our methodâ€™s effectiveness in this
static observer setting and report the mean accuracy over
random train-test splits.

5.2. Implementation Details

Network Details. In the motion module, we set 5 as the
maximum displacement for the multiplicative patch com-
parisons. In the interaction module, we reduce the size of
ego-feature and exo-feature to 256 and set 256 as the hidden
size of LSTM blocks. 20 equidistant frames are sampled as
input as done in [35].
Data Augmentation. We adopt several data augmentation
techniques to ease overï¬tting due to the absence of large
amount of training data. (1) Scale jittering [37]. We ï¬x the
size of sampled frames as 160Ã—320 and randomly crop a
region, then resize it to 128Ã—256 as input. (2) Each video
is horizontally ï¬‚ipped randomly. (3) We adjust the hue and
saturation in HSV color space of each video randomly. (4)
At every sampling of a video, we randomly translate the
frame index to obtain various samples of the same video.
Training setup. The whole network is hard to converge
if we train all the parameters together. Hence, we sepa-
rate the training process into two stages. At the ï¬rst stage,
we initialize feature extraction module with ImageNet [29]
pretrained parameters and train attention module, motion
module and interaction module successively while freezing
other parameters. At the second stage, the three modules are

ï¬netuned together in an end-to-end manner. We use Adam
optimizer with initial learning rate 0.0001 to train our model
using TensorFlow on Tesla M40, and decrease the learning
rate when the loss saturates. To deal with overï¬tting, we
further employ large-ratio dropout, high weight regulariza-
tion and early stop strategies during training.

5.3. Comparison to the State of the art Methods

We compare our method with state-of-the-arts and the
results are shown in Table 1. The ï¬rst part lists the meth-
ods using hand-crafted features. The second part presents
some deep learning based action analysis methods (reim-
plemented by us except convLSTM). The third part reports
the results of our method and the fourth part compares the
performance using multiple POV videos on PEV dataset.

As shown, our method outperforms existing methods.
Most previous methods directly learn interaction repre-
sentations without relation modeling, while ours explicitly
models the relations between the two interacting persons.
The results show that relation modeling is useful for inter-
action recognition.

Among the compared deep learning methods, we ob-
tain clear improvement over convLSTM[35], LRCN[6] and
TRN[41], since they mainly capture the temporal changes
of appearance features, but ours further explicitly captures
motions and models the relation between the two interacting
persons. Two-stream network [33] with the same backbone
CNN as ours integrates both appearance and motion fea-
tures but obtains inferior performance to ours, perhaps due
to the lack of relation modeling.

On PEV dataset, Yonetani et al. [39] achieves 69.2% of
accuracy with paired videos, certainly surpassing others us-
ing single POV video. We use our interactive LSTM to fuse
the features from paired videos since there also exist some
relations between the actions recorded by the paired videos.

7937

Features

Ego-features
Exo-features

Concat(no relation)

Interaction with sym. blocks
Interaction with rel. branch

Interaction with both

PEV NUS(ï¬rst h-h)

55.2
53.1
60.8
62.7
63.0
64.2

67.9
76.1
77.9
78.1
79.0
80.2

Table 2. Recognition accuracy comparison (%) about interacion.
Concat(no relation) means concatenation of ego-features and exo-
features without any relation modeling.
Interaction with sym.
blocks means only symmetrical LSTM blocks are used. Interac-
tion with rel. branch means only relation LSTM branch is used.
Interaction with both means both components are used.

We achieve comparable result (69.7%) which further proves
the relation modeling ability of our interactive LSTM.

In terms of inference time, our framework takes around
0.15 seconds per video with 20 sampled frames, which is
still towards real time. TRN[41] takes 0.04 seconds per
video but it has clear lower recognition performance than
ours. Although Two-stream[33] obtains slightly inferior
performance to ours, it takes 0.9 seconds per video since
it spends much more time on extracting optical ï¬‚ows.

5.4. Further Analysis

5.4.1 Study on Interaction Module.

Table 2 compares recognition performance about inter-
action.
It shows that our interactive LSTM clearly im-
proves the performance, since it models the relations and
also drives feature learning of other modules. On differ-
ent datasets, the relation modeling obtains different per-
formance gains. We obtain clearer improvements on PEV
dataset since it contains more samples dependent on rela-
tions. While in NUS(ï¬rst h-h) dataset, most samples have
weaker relations between the two interacting persons.

As discussed in Section 4, the main differences between
the two interaction samples shown in Figure 6 might be the
sequential orders and motion directions. We further com-
pare the recognition results on them of different methods.
It is observed that both two-stream[33] and simple con-
catenation cannot sufï¬ciently model the two interactions.
While with explicit relation modeling, the two interactions
are correctly distinguished, which indicates that our inter-
active LSTM models the relations to distinguish confusing
samples for better interaction recognition.

(a) Interaction category: none

(b) Interaction category: attention

Figure 6. Comparison of recognition results of two interaction
samples. w/o relation means concatenation of two action features
without any relation modelling is used for recognition. The bar
graphs on the right present the probabilities of each category.

overall features, which are more related to the interaction.
Furthermore, relation modeling performs better than con-
catenation since it enhances the features through symmetri-
cal gating or updating and relation modeling.

Figure 7 visualizes some learned masks. (See more ex-
amples in supplementary material.) As shown, the attention
module learns to localize the interactor with the JPPNet ref-
erence masks as supervision. With additional classiï¬cation
loss, it could localize some objects around the interactor and
strongly related to the interactions such as the hat in the ex-
ample, which leads to around 2% accuracy boost for local
appearance features. This shows the advantage of using the
designed attention module in our framework over using the
JPPNet masks directly in this recognition task. In addition,
with only the classiï¬cation loss, our attention module fails
to localize the interactor at all, indicating the necessity of
reference masks for interactor localization.

The attention module is an indispensible part of our
framework for individual action representation learning. It
not only learns concrete appearance features, but also sev-
ers to separate the global and local motion for explicit mo-
tion features learning. Without attention module, our frame-
work could only capture the global appearance and motion
cues, and fails to model the relations between the camera
wearer and the interactor, which leads to 9.0% and 12.3%
accuracy degradation on PEV and NUS(ï¬rst h-h) dataset,
demonstrating the importance of attention module.

5.4.2 Study on Attention Module.

5.4.3 Study on Motion Module.

We compare recognition accuracy of different appearance
features in Table 3.
It is observed that local appearance
features slightly improve the performance since it provides
concrete descriptions of the interactor instead of general or

We show accuracy comparisons of different motion features
in Table 4.
It is seen that two-stream (ï¬‚ow) is a power-
ful method, but it is computational inefï¬cient. Our method
explicitly captures motions of the camera wearer and in-

7938

Features

PEV NUS(ï¬rst h-h)

Features

PEV NUS(ï¬rst h-h)

Two-stream[33](RGB)

Global appearance
Local appearance
Concat(no relation)

Interaction

40.7
40.7
43.2
44.2
45.9

63.8
63.8
65.1
66.8
68.2

Two-stream[33](ï¬‚ow)

Global motion
Local motion

Concat(no relation)

Interaction

54.0
51.9
51.0
53.2
56.6

73.2
52.3
69.6
73.4
75.0

Table 3. Recognition accuracy comparison (%) using appearance
features. Concat(no relation) means simple concatenation of
global and local appearance features. Interaction means relations
modeling is used with global and local appearance features.

Table 4. Recognition accuracy comparison (%) using motion fea-
tures. Concat(no relation) means simple concatenation of global
and local motion features. Interaction means relations modeling is
used with global and local motion features.

(a) Frame

(b) JPPNet mask

(a) Frame I nâˆ’1

(b) Frame I n

(c) Mask

(d) Mask

(c) Local dense motion

(d) Global motion

Figure 7. Example of learned mask with different supervision. (a)
is original frame; (b) is the JPPNet mask; (c) is the learned mask
trained with only human segmentation loss; (d) is the learned mask
trained with human segmentation loss and classiï¬cation loss.

teractor and reaches comparable results with two-stream
(ï¬‚ow), which indicates the effectiveness of our motion mod-
ule. Furthermore, our method could achieve higher accu-
racy with relation modeling. On different datasets, global
motion and local motion contribute differently to recogni-
tion, probably because global motion is important to distin-
guish interactions such as positive and negtive response on
PEV dataset, but such interactions highly relevant to global
motion are not included in NUS(ï¬rst h-h) dataset.

In Figure 8, we show the reconstructed frame and local
dense motion ï¬eld. (See more examples in supplementary
material.) From the reconstructed frame, it is seen that the
the slight head motion to the right is captured, which leaves
a strip on the left highlighted in blue. The local dense mo-
tion ï¬eld shows the motion of the interactor reaching out the
hand towards the right. This example shows that the motion
module could learn the global and local motion jointly.

Our motion module explicitly estimates global and local
motions of the camera wearer and the interactor individu-
ally, which is important for relation modeling. Without the
motion module, our method fails to capture motion infor-
mation and can only use appearance features, which leads
to 18.3% and 12.0% accuracy drop on PEV and NUS(ï¬rst
h-h) dataset, showing the necessity of motion modeling.

Figure 8. Illustration of global and local motion. (a) and (b) are
two consecutive sampled frames. (c) Local dense motion shows
the amplitudes of the horizontal motion vectors in the interactor
mask, the amplitudes to the right are proportional to the brightness
of the motion ï¬eld. The motion vectors outside the interacor mask
is discarded. (d) Global motion shows the slight head motion to
the right, which reï¬‚ects on the strip highlighted in blue.

6. Conclusion

In this paper, we propose to learn individual action rep-
resentations and model the relations of the camera wearer
and the interactor for egocentric interaction recognition. We
construct a dual relation modeling framework by develop-
ing a novel interactive LSTM to explicitly model the rela-
tions. In addition, an attention module and a motion module
are designed to jointly model the individual actions of the
two persons for helping modeling the relations. Our dual re-
lation modeling framework shows promising results in the
experiments. In the future, we would extend our method to
handle more complex scenarios such as multi-person inter-
actions, which are not considered in this paper.

Acknowledgement

This work was supported partially by the National
Key Research and Development Program of China
(2018YFB1004903), NSFC(61522115), and Guangdong
Province Science and Technology Innovation Leading Tal-
ents (2016TX03X157).

7939

References

[1] Girmaw Abebe, Andrea Cavallaro, and Xavier Parra. Robust
multi-dimensional motion features for ï¬rst-person vision ac-
tivity recognition. Computer Vision and Image Understand-
ing, 149:229â€“248, 2016.

[2] Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Ma-
lik, and Sergey Levine. Learning to poke by poking: Expe-
riential learning of intuitive physics. In Advances in neural
information processing systems, pages 5074â€“5082. 2016.

[3] Stefano Alletto, Giuseppe Serra, Simone Calderara, and Rita
Cucchiara. Understanding social relationships in egocentric
vision. Pattern Recognition, 48(12):4082â€“4096, 2015.

[4] Stefano Alletto, Giuseppe Serra, Simone Calderara,
Francesco Solera, and Rita Cucchiara. From ego to nos-
vision: Detecting social relationships in ï¬rst-person views.
In IEEE Conference on Computer Vision and Pattern Recog-
nition Workshops, pages 580â€“585, 2014.

[5] Sven Bambach, Stefan Lee, David J Crandall, and Chen Yu.
Lending a hand: Detecting hands and recognizing activities
in complex egocentric interactions.
In IEEE International
Conference on Computer Vision (ICCV), pages 1949â€“1957,
2015.

[6] J. Donahue, L. A. Hendricks, M. Rohrbach, S. Venugopalan,
S. Guadarrama, K. Saenko, and T. Darrell. Long-term recur-
rent convolutional networks for visual recognition and de-
scription. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 39(4):677â€“691, 2017.

[7] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der
Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learn-
ing optical ï¬‚ow with convolutional networks.
In IEEE In-
ternational Conference on Computer Vision (ICCV), pages
2758â€“2766, 2015.

[8] Wenbin Du, Yali Wang, and Yu Qiao. Rpan: An end-to-
end recurrent pose-attention network for action recognition
in videos. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 3745â€“3754, 2017.

[9] Wenbin Du, Yali Wang, and Yu Qiao. Recurrent spatial-
temporal attention network for action recognition in videos.
IEEE Transactions on Image Processing, 27(3):1347â€“1360,
2018.

[10] A. Fathi, A. Farhadi, and J. M. Rehg. Understanding ego-
centric activities. In IEEE International Conference on Com-
puter Vision (ICCV), pages 407â€“414, 2011.

[11] Alircza Fathi, Jessica K Hodgins, and James M Rehg. Social
interactions: A ï¬rst-person perspective. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
1226â€“1233, 2012.

[12] Alireza Fathi, Yin Li, and James M Rehg. Learning to recog-
nize daily actions using gaze. In The European Conference
on Computer Vision (ECCV), pages 314â€“327, 2012.

[13] A. Fathi and G. Mori. Action recognition by learning mid-
level motion features. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 1â€“8, 2008.

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 770â€“778, 2016.

[15] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in Neural Infor-
mation Processing Systems, pages 2017â€“2025, 2015.

[16] Reza Kahani, Alireza Talebpour, and Ahmad Mahmoudi-
A correlation based feature representation
arXiv preprint

Aznaveh.
for ï¬rst-person activity recognition.
arXiv:1711.05523, 2017.

[17] Kris M Kitani, Takahiro Okabe, Yoichi Sato, and Akihiro
Sugimoto. Fast unsupervised ego-action learning for ï¬rst-
person sports videos.
In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 3241â€“3248,
2011.

[18] J. Lee and M. S. Ryoo. Learning robot activities from ï¬rst-
person human videos using convolutional future regression.
In IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pages 1497â€“1504, 2017.

[19] Y. Li, A. Fathi, and J. M. Rehg. Learning to predict gaze
In IEEE International Conference on

in egocentric video.
Computer Vision (ICCV), pages 3216â€“3223, 2013.

[20] Yin Li, Zhefan Ye, and James M Rehg. Delving into egocen-
In IEEE Conference on Computer Vision and

tric actions.
Pattern Recognition (CVPR), pages 287â€“295, 2015.

[21] X. Liang, K. Gong, X. Shen, and L. Lin. Look into person:
Joint body parsing amp; pose estimation network and a new
benchmark. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 41(4):871â€“885, 2019.

[22] Minghuang Ma, Haoqi Fan, and Kris M Kitani. Going deeper
In IEEE Conference
into ï¬rst-person activity recognition.
on Computer Vision and Pattern Recognition (CVPR), pages
1894â€“1903, 2016.

[23] Yang Mi, Kang Zheng, and Song Wang. Recognizing actions
in wearable-camera videos by training classiï¬ers on ï¬xed-
camera videos. In Proceedings of the 2018 ACM on Interna-
tional Conference on Multimedia Retrieval, pages 169â€“177,
2018.

[24] T. P. Moreira, D. Menotti, and H. Pedrini. First-person ac-
tion recognition through visual rhythm texture description.
In IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 2627â€“2631, 2017.

[25] Sanath Narayan, Mohan S Kankanhalli, and Kalpathi R Ra-
makrishnan. Action and interaction recognition in ï¬rst-
person videos. In IEEE Conference on Computer Vision and
Pattern Recognition Workshops, pages 526â€“532, 2014.

[26] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmenta-
tion. In IEEE International Conference on Computer Vision
(ICCV), pages 1520â€“1528, 2015.

[27] Y. Peng, Y. Zhao, and J. Zhang. Two-stream collaborative
learning with spatial-temporal attention for video classiï¬ca-
tion. IEEE Transactions on Circuits and Systems for Video
Technology, 29(3):773â€“786, 2019.

[28] H. Pirsiavash and D. Ramanan. Detecting activities of daily
living in ï¬rst-person camera views.
In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
2847â€“2854, 2012.

[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,

7940

In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 1904â€“1913, 2016.

Imagenet large
Aditya Khosla, Michael Bernstein, et al.
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211â€“252, 2015.

[30] MS Ryoo, Thomas J Fuchs, Lu Xia, Jake K Aggarwal, and
Larry Matthies. Robot-centric activity prediction from ï¬rst-
person videos: What will they do to me? In ACM/IEEE In-
ternational Conference on Human-Robot Interaction, pages
295â€“302, 2015.

[31] Michael S Ryoo and Larry Matthies. First-person activity
recognition: What are they doing to me? In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 2730â€“2737, 2013.

[32] M. S. Ryoo, B. Rothrock, and L. Matthies. Pooled motion
features for ï¬rst-person videos. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 896â€“
904, 2015.

[33] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos.
In
Advances in Neural Information Processing Systems, pages
568â€“576, 2014.

[34] Suriya Singh, Chetan Arora, and CV Jawahar. First per-
son action recognition using deep learned descriptors.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 2620â€“2628, 2016.

[35] Swathikiran Sudhakaran and Oswald Lanz. Convolutional
long short-term memory networks for recognizing ï¬rst per-
son interactions. In IEEE International Conference on Com-
puter Vision Workshops, pages 2339â€“2346, 2017.

[36] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia
Schmid, Rahul Sukthankar, and Katerina Fragkiadaki. Sfm-
net: Learning of structure and motion from video. arXiv
preprint arXiv:1704.07804, 2017.

[37] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment net-
works: Towards good practices for deep action recognition.
In The European Conference on Computer Vision (ECCV),
pages 20â€“36, 2016.

[38] Lu Xia, Ilaria Gori, Jake K Aggarwal, and Michael S Ryoo.
Robot-centric activity recognition from ï¬rst-person rgb-d
videos. In IEEE Winter Conference on Applications of Com-
puter Vision, pages 357â€“364, 2015.

[39] Ryo Yonetani, Kris M. Kitani, and Yoichi Sato. Recognizing
micro-actions and reactions from paired egocentric videos.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 2629â€“2638, 2016.

[40] Hasan FM Zaki, Faisal Shafait, and Ajmal Mian. Model-
ing sub-event dynamics in ï¬rst-person action recognition. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1619â€“1628, 2017.

[41] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor-
ralba. Temporal relational reasoning in videos. In The Euro-
pean Conference on Computer Vision (ECCV), 2018.

[42] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 6612â€“6619, 2017.

[43] Y. Zhou, B. Ni, R. Hong, X. Yang, and Q. Tian. Cascaded
interactional targeting network for egocentric video analysis.

7941

