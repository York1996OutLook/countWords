Crowd Counting and Density Estimation by Trellis Encoder-Decoder Networks

Xiaolong Jiang1‚àó, Zehao Xiao1‚àó, Baochang Zhang3, Xiantong Zhen4,

Xianbin Cao1,2‚Ä†, David Doermann5, Ling Shao4

1School of Electronic and Information Engineering, Beihang University, Beijing, China

2Key Laboratory of Advanced technology of Near Space Information System (Beihang University),

Ministry of Industry and Information Technology of China, Beijing, China

3School of Automation Science and Electrical Engineering, Beihang University, Beijing, China

4Inception Institute of ArtiÔ¨Åcial Intelligence, UAE

5Department of Computer Science and Engineering, University at Buffalo, New York, USA
jasperj1tmac@163.com, zhxiao@buaa.edu.cn, bczhang@buaa.edu.cn, zhenxt@gmail.com,

xbcao@buaa.edu.cn, doermann@buffalo.edu, ling.shao@ieee.org

Abstract

Spatial

Semantic

Crowd counting has recently attracted increasing inter-
est in computer vision but remains a challenging problem.
In this paper, we propose a trellis encoder-decoder network
(TEDnet) for crowd counting, which focuses on generating
high-quality density estimation maps. The major contribu-
tions are four-fold. First, we develop a new trellis architec-
ture that incorporates multiple decoding paths to hierarchi-
cally aggregate features at different encoding stages, which
improves the representative capability of convolutional fea-
tures for large variations in objects. Second, we employ
dense skip connections interleaved across paths to facili-
tate sufÔ¨Åcient multi-scale feature fusions, which also helps
TEDnet to absorb the supervision information. Third, we
propose a new combinatorial loss to enforce similarities in
local coherence and spatial correlation between maps. By
distributedly imposing this combinatorial loss on intermedi-
ate outputs, TEDnet can improve the back-propagation pro-
cess and alleviate the gradient vanishing problem. Finally,
on four widely-used benchmarks, our TEDnet achieves the
best overall performance in terms of both density map qual-
ity and counting accuracy, with an improvement up to 14%
in MAE metric. These results validate the effectiveness of
TEDnet for crowd counting.

1. Introduction

With the rapid pace of urbanization, crowds tend to
gather more frequently, increasing requirements for effec-
tive safety monitoring, disaster relief, urban planning, and

‚àóThese authors contribute equally.
‚Ä†This author is the corresponding author.

r
e
d
o
c
n
E

r
e
d
o
c
e
D

1

2

3

4

256 16

128 32

128 64

128 128

128 256

1

32

54

76

98

D

128 32

128 64

L1

Z2,D
128 1

D

D

128 128

128 32

128 64

Z3,D
128 1

Z4,4
128 32

Z4,D
128 1

Z
512 1

L2

L3

L4

D

D

D

U

Multi-scale 
Encoding 

Block

Max 

Pooling 

D

Decoding 

Block

U

Up-Sampling

Block

Combinatorial

Loss

1

2

3

4

Spatial

Figure 1. An illustration of the Trellis Encoder-Decoder network
(TEDnet) with distributed combinatorial losses. The horizontal
and vertical axes indicate the spatial-semantic-spatial feature hi-
erarchy established within TEDnet. We instantiate this hierarchy
into a feature grid, whose rows and columns are indexed on the
margin. The spatial and channel dimensions of each feature map
is denoted by its side.

crowd management. As a fundamental technique to sup-
port these applications, crowd counting has been investi-
gated and has resulted in advanced solutions. Most crowd
counting methods are based on detection [18, 9, 45], re-
gression [28, 4, 10], and density estimation [17, 53, 2, 11].

6133

Gt Count:1068

Est Count:1074

Gt Count:271

Est Count:269

Figure 2. An illustration of estimated density maps and crowd counts generated by the proposed approach and other state-of-the-arts. The
Ô¨Årst column shows two samples drawn from ShanghaiTech Part A. The second column shows corresponding ground truth maps with Ô¨Åxed
Gaussian kernels. From the third to the last column we show the density maps estimated by MCNN [53], SANet [2], and the proposed
TEDNet, respectively. TEDnet generates density maps closer to the ground truth, and more accurate crowd counts.

Density estimation based methods, in particular, have re-
ceived increasing research focus. These techniques have the
ability to localize the crowd by generating a density estima-
tion map using pixel-wise regression. The crowd count is
then calculated as the integral of the density map. To gen-
erate maps with a retained spatial size as the inputs, deep
encoder-decoder convolutional neural network (CNN) ar-
chitectures are widely applied [6, 42, 54, 33, 8, 20, 25, 12].
In particular, encoder-decoder methods also play an impor-
tant role in localization-oriented tasks to facilitate accurate
pixel-wise regression [19, 31, 43, 30, 24], given that the
convolution itself is essentially a pixel-wise feature local-
ization using traversal template matching. Thus motivated,
we propose the trellis encoder-decoder network (TEDnet)
for density estimation to address the crowd counting prob-
lem. Our approach improves both the encoding and decod-
ing processes for more accurate pixel-wise estimations.

Feature encoding facilitates accurate pixel-wise estima-
tions by extracting features, while preserving the pixel-wise
localization precision in the feature maps. In scenes with
severe occlusions and scale variations, CNN features are
widely employed to enhance the feature encoding perfor-
mance [53, 1, 27, 40, 2, 22]. It is worth noting that most cur-
rent counting methods adopt CNNs that were originally de-
signed for classiÔ¨Åcation tasks, such as VGG-16 [40, 38, 22],
Inception [2], and DenseNet [11]. Despite their previous
success, these networks build deep hierarchies to transform
low-level spatial information into high-level semantic infor-
mation. Consequentially, the resolution of feature maps is
gradually degraded due to down-sampling operations, and
thus, the localization precision is lowered. It is desirable
to maintain a favorable balance between spatial resolution
preservation and semantic feature extraction.

Feature decoding generates density maps by aggregat-
ing encoded feature maps. The pixel-wise accuracy per-
formance for an estimated map is guaranteed by sufÔ¨Åcien-
t fusions of multi-scale decoding features that incorporate
low-level spatial precision and high-level semantic depth.

In hourglass encoder-decoder networks with a single de-
coding path [33, 54, 6, 42], features must endure excessive
down-sampling and up-sampling operations, which degrade
the pixel-wise precision.
In addition, rich low-level spa-
tial and high-level semantic information residing in multi-
scale feature maps at the two ends of the hourglass are sep-
arated by the gap between the encoder and decoder. Al-
though attempts have been made to enhance the hourglass
networks with skip connections [20, 25, 8, 50], they are not
designed to generate high-quality density estimation maps
due to the lack of hierarchical fusions between multi-scale
features. From a more fundamental perspective, the widely
adopted mean square error (MSE) loss in crowd counting
assumes pixel-wise independence, while neglecting the lo-
cal coherence and spatial correlation in density maps. It is
therefore inadequate for facilitating the generation of high-
quality density maps.

To address these issues in existing encoder-decoder net-
works and improve the counting performance with an en-
hanced architecture, we propose the trellis encoder-decoder
network (TEDnet) to generate high-quality density maps.
TEDnet achieves sufÔ¨Åcient aggregation and fusion of multi-
scale features within an established trellis-like feature hier-
archy. In the encoding process, multi-scale convolutional
kernels are used to obtain scale adaptation, where down-
sampling strides are cut to four to preserve pixel-wise spa-
tial precision. In the decoding process, multiple paths are
deployed at corresponding encoding stages, each of which
aggregates the encoded multi-scale features. Across paths,
features containing diverse spatial and semantic information
are integrated using dense skip connections, which guaran-
tees thorough multi-scale feature fusions. Our multi-path
trellis network is similar in spirit to an ensemble of multiple
hourglass networks with different feature scales, establish-
ing a feature learning hierarchy resides in a trellis structure,
as highlighted in Figure 1. Each path in TEDnet gener-
ates an intermediate output map that intrinsically enables
the deployment of distributed supervision within each path.

6134

This alleviates the gradient vanishing problem and boosts
the gradient Ô¨Çow through the network. Each distributed loss
in TEDnet is a combinatorial loss deÔ¨Åned based on the pro-
posed spatial abstraction loss (SAL) and the spatial correla-
tion loss (SCL). SAL and SCL relieve the pixel-wise inde-
pendence assumption posed by the MSE loss, and improve
the density map quality, as well as counting performance,
by enforcing similarities in local coherence and spatial cor-
relation between maps.

TEDnet takes full images, rather than image patches, as
the input and outputs full-resolution density maps. This fur-
ther ensures the density map quality (qualitatively demon-
strated in Figure 2) by avoiding the tedious patch-wise oper-
ation, which induces boundary artifacts. The main contribu-
tions of the proposed approach are summarized as follows:

‚Ä¢ We propose a new deep learning architecture for ac-
curate density estimation and crowd counting, called
trellis encoder-decoder network (TEDnet), which as-
sembles multiple encoding-decoding paths hierarchi-
cally to generate high-quality density map for accurate
crowd counting.

‚Ä¢ We establish a multi-path decoder that pervasively ag-
gregates the spatially-endowed features within a de-
coding feature hierarchy and progressively fuses multi-
scale features with dense skip connections interleaved
in the hierarchy.

‚Ä¢ We introduce a combinatorial loss comprising of the
newly designed SAL and SCL to supervise local coher-
ence and spatial correlation in density maps. Distribut-
ed supervision, in conjunction with the combinatorial
loss, is deployed on intermediate multi-path outputs to
improve the optimization of the network.

‚Ä¢ We achieve the best overall performance on four
commonly-used benchmark datasets, largely surpass-
ing the state-of-the-art methods by up to 14% for the
MAE metric. We obtain the best quality of estimated
density maps, in terms of both PSNR and SSIM mea-
sures.

2. Related Work

In this section, we provide a brief review of the most
related work and refer to comprehensive surveys for crowd
counting [35, 36, 41, 13].

2.1. Detection and Regression based Methods

Detection-based counting methods deploy a detector to
traverse the image, which localizes and counts the tar-
gets along the way [5, 9, 18, 45]. These methods are
surpassed by the regression-based alternatives, as the de-
tection performance is affected in the presence of over-
crowded scenes. The successes of regression-based meth-

ods [3, 34, 10, 51, 15] can thus be attributed to their abili-
ty of circumventing explicit detection and directly mapping
the input images to scalar values. Nevertheless, regression-
based methods forfeit localization capability such that they
cannot perceive crowd distributions. To recover the lost
localization capability, crowd counting methods based on
density estimation are therefore developed by conducting
pixel-wise regressions.

2.2. Density Estimation based Methods

Initially introduced in [17], density estimation based
methods avoid explicitly detecting each individual and re-
tain the ability to localize the crowd. Earlier approaches
strove to compute density maps with hand-crafted features
[17, 7] and random forest regressions [7, 29, 49]. More
recent methods appeal to CNN based feature extraction to
supply scale and perspective invariant features. In partic-
ular, MCNN [53], Crowdnet [1], Hydra CNN [27], CNN-
boost [44], CP-CNN[40], and Switching CNN [37] all con-
form to an ensemble design approach to enable multi-scale
adaptation, where multiple CNN branches with different re-
ceptive Ô¨Åelds are jointly maintained. The extra computa-
tional expense introduced by these methods is to some de-
gree wasted on inefÔ¨Åcient and un-Ô¨Çexible branching [19].
As a remedy, single-branch counting networks with scale
adaptations were proposed in [2, 11, 47]. Notably, most
of these methods follow a patch-based counting mechanism
[52, 44, 27, 1, 37, 21, 11, 2], where the full density map is
obtained by concatenating discrete density patches. More
importantly, methods such as MCNN, Hydra CNN, and
CNN-boost output density maps with reduced resolution
due to excessive down-sampling strides. This inevitably
sacriÔ¨Åces pixel-wise details and damages the density map
quality. Comparatively, CP-CNN [40] focuses on generat-
ing high-quality full-resolution maps with the help of global
and local semantic information. In [2], researchers comput-
ed high-quality full-resolution maps with a new encoder-
decoder network, as well as a SSIM local pattern consistent
loss. In order to limit the down-sampling stride in the en-
coding process, CSRNet [19] adopts dilated convolutional
layers to substitute pooling layers.

Unlike other approaches, the proposed trellis encoder-
decoder architecture attempts to generate high-quality den-
sity estimation maps by preserving the spatial information
in the encoding feature hierarchy. More importantly, it in-
corporates a multi-path decoder to enhance the aggrega-
tion and fusion of multi-scale features with rich spatial and
semantic information. As a result, pixel-wise regression
accuracy in the estimated map is enhanced.
In a broad-
er view, density estimation is similar to other localization-
oriented tasks, such as tracking [32, 30, 23] and detec-
tion [24], which also generate localization estimation map-
s as outputs. These tasks are inter-correlated with den-
sity estimations such that the resulting localization maps

6135

can be fused to integrate task-speciÔ¨Åc localization response
[13, 11]. Moreover, semantic segmentation also relies on
powerful encoder-decoder architecture to integrate multi-
scale features and to improve localization precision. Con-
sequently, efforts have been made to enhance the hourglass
architecture. In [8], SDN stacks multiple single-path hour-
glass networks into a deeper sequence to improve the fea-
ture fusion and guarantee Ô¨Åne recovery of localization in-
formation. In [20, 25], the single-path hourglass network
is extended by adding residual units inside the skip connec-
tions.

3. Trellis Encoder-Decoder Networks

As shown in Figure 1, the goal of TEDnet is to achieve
improved counting performance by generating density map-
s with high pixel-wise density estimations. In the encoder,
the localization property of a density estimation conforms
to the nature of a convolutional layer operation. Here, the
convolutional kernels are the feature templates that are lo-
calized in the feature maps via template-matching. In the
decoder, encoded feature maps are aggregated to represent
the locality of crowded objects. Our TEDnet can establish
a feature hierarchy within the trellis architecture, where re-
liable multi-scale features are encoded with well-preserved
spatial information. These are then decoded into accurate
density maps, with a great capacity for precise localization.
In what follows, we explain in detail the multi-scale en-
coder, the multi-path decoder, and the distributed supervi-
sion with combinatorial loss in TEDnet.

3.1. Multi Scale Encoder

We design the multi-scale encoder to extract reliable fea-
tures relevant to crowded human objects, while being able
to localize these features with pixel-wise precision. The
multi-scale encoding block is capable of overcoming occlu-
sions and scale variations present in crowd counting scenes,
as elaborated below.

As shown in Figure 3, a multi-scale encoding block is
implemented with kernels of different sizes, which enables
the encoder to extract multi-scale features. As indicated in
Figure 1, a total of nine encoding blocks are implement-
ed and grouped into Ô¨Åve encoding stages. To preserve fea-
ture localization precision, we limit the application of pool-
ing operations. Consequently, only two 2 √ó 2 max pooling
layers are inserted at the Ô¨Årst two encoding stages, each of
which has a down-sampling stride of 2. To further enlarge
the receptive Ô¨Åelds, dilated convolutional kernels with dila-
tion rates of 2 and 4 are employed in the last two encoding
blocks [19].

3.2. Multi Path Decoder

We design a new multi-path decoder to hierarchically ag-
gregate the spatially-preserved features and restore the spa-
tial resolution in the density map. As the component that

1√ó1 
Conv

1√ó1 
Conv

1√ó1 
Conv

1√ó1 
Conv

C

3√ó3 
Conv

5√ó5 
Conv

7√ó7 
Conv

Multi-scale  Encoding  Block

Figure 3. An illustration of the multi-scale encoding block. Circled
C in the Ô¨Ågure represents channel-wise concatenation.

directly generates the density maps, the decoder has a vi-
tal inÔ¨Çuence on the density map quality. Unfortunately, less
emphasis had been placed on the decoder in the literature
for crowd counting and density estimation. In CSRNet [19],
the density maps are simply generated by applying bilinear
interpolation to up-sample the encoded feature maps.

In Crowdnet [1], a 1 √ó 1 convolutional layer is imple-
mented as the decoder. SANet [2] advocates the idea of
reÔ¨Ånement in a single-path hourglass decoder. To the best
of our knowledge, it is by far the most sophisticated design
in the context of density estimation. Alternatively, efforts
have been made in other tasks using the hourglass architec-
ture, such as image segmentation [33, 54, 8, 20] and super-
resolution [6, 42]. Nevertheless, as explained in Section 1,
these architectures are not optimal for density estimation.
They suffer from prolonged single-path feature transforma-
tion hierarchy with heavy parameterization, as well as in-
sufÔ¨Åcient feature aggregations and fusions.

To remedy the defects of the existing decoder, we pro-
pose a multi-path decoder in TEDnet, which assembles a
set of single-path hourglass architectures with multi-scale
features. As depicted in Figure 1, three decoding paths are
exploited on the feature maps, computed from the last three
encoding stages. Within each path, a decoding feature hi-
erarchy is established to aggregate feature representations
at the same semantic level, in a progressive way. Among
different paths, feature maps from different levels are fused
with dense skip connections. Both aggregation and fusion
for features are implemented in densely interleaved decod-
ing blocks. The decoder implementation is realized by s-
tacking decoding blocks into the trellis structure, such that a
feature hierarchy is established. As shown in Figure 1, such
a feature hierarchy is pinpointed into the trellis architecture
with a grid representation, where each column indicates one
decoding path and each row presents the depth within each
path.

Decoding Block. As shown in Figure 4(a), each decoding
block takes two inputs. The right input feature is passed
from the same decoding path and it possesses deeper se-
mantic information whose channels doubles those of the
left input feature.
It is aggregated via a deconvolutional
layer F 2
i,j with 3 √ó 3 kernels, which halves the channels.

6136

ùíÅùüí,ùüí

Nearest Neighbor 

Interpolation

3 √ó 3 Conv
Stride = 1

Nearest Neighbor 

Interpolation

3 √ó 3 Conv
Stride = 1

Up-sampling Block

(b)

ùíÅ

1√ó1 Conv

ùë≠ùüè(‚àô)

Decoding 

Block

3√ó3 DeConv

C

ùë≠ùüê(‚àô)ùë≠ùüë(‚àô)

1√ó1 Conv

(a)

Figure 4. An illustration of the decoding block (a) and the up-
sampling block (b). Circled C in the Ô¨Ågure represents channel-
wise concatenation.

The left input feature is aggregated by a convolutional lay-
er F 1
i,j deploying 1 √ó 1 kernels, with its depth unchanged.
These two aggregated features are fused through channel-
wise concatenation, followed by a convolutional layer F 3
i,j
with 1 √ó 1 Ô¨Ålters.

In (1), Zi,j denotes the decoded feature at the i-th row
and j-th column within the feature grid, computed by the
decoding block Di,j , as follows:

Zi,j = Di,j(Zi‚àí1,j‚àí1, Zi‚àí1,j)
i,j(Zi‚àí1,j‚àí1), F 2

i,j([F 1

= F 3

i,j(Zi‚àí1,j)]),

(1)

where F (¬∑) indicates a convolutional operations, and [¬∑] de-
notes a channel-wise concatenation.

Within the established feature hierarchy as shown in Fig-
ure 1, the decoded features enable the aggregation and fu-
sion of multi-scale features. As a result, the decoded feature
map Z4,4, at the end of the rightmost decoding path, con-
tains the richest spatial and semantic information. Thus,
the Ô¨Ånal output density map Z is generated from these fea-
ture maps by restoring the spatial dimension through the
up-sampling block.
Up-sampling Block. As illustrated in Figure 4(b), the
design of the up-sampling block is inspired by a super-
resolution technique [26], where the nearest neighbor in-
terpolation is followed by a 3 √ó 3 convolutional layer with a
stride of 1. The overall down-sampling stride of TEDnet is
4. We restore the spatial size of the density map by repeat-
ing the up-sampling operations twice in the up-sampling
block.

Overall, a spatial-semantic-spatial feature hierarchy is
In Figure 1, the proposed ar-
fully exploited in TEDnet.
chitecture is established to host the feature hierarchy. As
indicated by the horizontal axis, the feature maps on the
right in the hierarchy have more semantic information than
the ones on the left. Those on the left, however, contain
richer spatial details. Vertically, spatial information is grad-
ually recovered through skip connections, which transmit
low-level spatial features from left to right, top to bottom.
It is worth noting that, for a simple single-path hourglass

Spatial Abstraction Loss

Spatial Correlation Loss

 

M
a
x
P
o
o
l
i

n
g

Y

Z

MSE

 

M
a
x
P
o
o
l
i

n
g

 

M
a
x
P
o
o
l
i

n
g

MSE

MSE

MSE

Y

Z

1‚àí
√óùùÄ

L

LSA

LSC

Figure 5. An illustration of the combinatorial loss L. Z indi-
cates one of the distributed outputs, while Y is the corresponding
ground truth map, resized to be the same as Z.

encoder-decoder, spatial information cannot be recovered
in the decoder as indicated vertically in Figure 1. Although
sparsely linked skip connections can alleviate inadequate
feature fusion to a certain extent in single-path hourglass
encoder-decoders, the pervasive feature fusions as realized
in TEDnet can still not be reached.

3.3. Distributed Supervision

The multi-path architecture of TEDNet produces inter-
mediate output estimation maps, i.e., Z2,D, Z3,D, Z4,D,
Z, at the ends of decoding paths as illustrated in Figure 1.
This design naturally enables distributed supervision, such
that multiple losses can be applied at each intermediate out-
put. Previous attempts have been made to provide multi-
supervision, where losses are computed between interme-
diate feature maps and the ground truths [48, 16, 8].
In
contrast, the proposed distributed supervision implemented
in TEDnet computes multiple losses between intermediate
density estimation maps and ground truth maps. From the
ensemble point of view, each distributed loss is calculated
to supervise the corresponding path representing a single-
path hourglass network. In particular, to compute the losses
at Z2,D, Z3,D, Z4,D, each of them is aggregated from its
previous feature map using a convolutional layer with 1 √ó 1
Ô¨Ålter size. The ground truth density map is down-sampled
to 128 √ó 128 with average pooling operations. Each of
these intermediate outputs is separately decoded on differ-
ent feature levels along its own path. Meanwhile, informa-
tion from different paths are integrated through dense skip
connections. As a result, the supervision at each output is
meaningful and can help better optimize the network.

Due to the distributed supervision, in conjunction with
the dense skip connections, the gradient vanishing phe-
nomenon, which indicates weaker gradients at the earlier
stage of the network, is substantially alleviated. Consider
the convolutional block 1 for instance. During the back-
propagation process, the gradients Ô¨Çow is a summation of
propagated Ô¨Çows, starting at each distributed supervision,
such that the gradient is boosted. Moreover, for each Ô¨Çow o-
riginating at its corresponding supervision, instead of Ô¨Çow-
ing backward along just one decoding path, the interleaved
dense skip connections provide more diffused Ô¨Çow paths at

6137

Gt Count:920

Gt Count:470

Gt Count:2731

Gt Count:681

Gt Count:20

Est Count:919

Est Count:468

Est Count:2367

Est Count:749

Est Count:19

Figure 6. From left to right, we display the density maps generated by TEDnet on ShanghaiTech Part A, ShanghaiTech Part B, UCF-QNRF,
UCF CC 50, and WorldExpo‚Äô10 datasets. The second row shows ground truth density map, the third row depicts our estimated maps.

each fork junctions, thus further boosting the gradient Ô¨Çow.

3.4. Combinatorial Loss

As shown in Figure 5, the loss function distributed at
each decoding output is a combination of two losses.
In
general, the pixel-wise mean square error (MSE) loss has
dominated the training of density estimation based crowd
counting approaches [53, 1, 11]. As advocated in [2, 19],
the MSE loss assumes pixel-wise isolation and indepen-
dence. As a result, it is incapable of enforcing spatial cor-
relation and coherence among pixels in the estimated maps,
which, however, plays an important role in inÔ¨Çuencing the
quality of the density map. To compensate the limited MSE
loss, we deÔ¨Åne a spatial abstraction loss (SAL) and a spatial
correlation loss (SCL), resulting in a combinatorial loss.
Spatial Abstraction Loss. SAL progressively computes
the MSE losses on multiple abstraction levels between the
predicted map and the ground truth. These spatial abstrac-
tions are instantiated by cascading max pooling layers with
down-sampling strides, leading to a gradually enlarged re-
ceptive Ô¨Åeld on each level. At each level, the pixel value in
an abstracted map is non-linearly sampled from a receptive
Ô¨Åeld at the corresponding location in the preceding abstrac-
tion level. By computing MSE on each abstraction level,
SAL can supplement the pixel-wise MSE loss with patch-
wise supervision. In our experiments, after a normal MSE
loss, we implement three levels of abstraction (K = 3 in
the following equation) with 2 √ó 2 max pooling layers, each
with a stride of 2. The computation of SAL is formalized
as:

K

LSA =

Xk=1

1
Nk

kœïk(Z) ‚àí œïk(Y )k2
2,

(2)

where œïk(¬∑) denotes the abstraction computation on the k-
th abstraction level. Nk is the number of pixels within a
map on the k-th abstraction level.
Spatial Correlation Loss. Beyond the patch-wise super-
vision enforced by SAL, SCL further complements the
pixel-wise MSE loss with map-wise computation. SCL

represents the difference between two density maps based
on normalized cross correlation (NCC) similarity. This is
less sensitive to linear changes in the density map intensity.
In addition, SCL is easier to compute and experimentally
friendly compared to the MSE loss. The computation of
SCL deÔ¨Åned on two maps is:

P

Q

LSC = 1 ‚àí

(Zpq ¬∑ Ypq)

Pq
Pp
s P
Pq
Pp

Q

Zpq

2 ¬∑

P

Q

Pp

Pq

,

(3)

2

Ypq

where Ypq and Zpq represent the pixels in the ground truth
density map and the predicted density map, respectively. p
and q are the row and column indexes in the map, and P √óQ
denotes the total number of pixels. The Ô¨Ånal combinatorial
loss L is formulated as a weighted sum of SAL and SCL as:

L = LSA + ŒªLSC ,

(4)

where Œª is a factor to balance the contributions of SAL and
SCL. The selection of Œª is explained in Section 4.2.

4. Experiments and Results

4.1. Implementation details

Following [47], we generate our ground truth maps by
Ô¨Åxed size Gaussian kernels and augment the training data
with an online sampling strategy (more details can be found
in [47]). We train our TEDnet in an end-to-end manner from
scratch, and optimize the network parameters based on the
Adam optimizer [14]. We use batch size of 8, Xavier initial-
ization, and an initial learning rate of 1e ‚àí 3. The learning
rate is step-wise and decreased by a factor of 0.8 every 10K
iterations. In regard to the efÔ¨Åciency performance of TED-
net, it takes 2500 epochs for training to convergence, and
0.027s for testing each image on ShanghaiTech Part A.

Image-wise Operation. To generate high-quality full-
resolution density maps, TEDnet takes full-size images as

6138

Figure 7. Illustration of convergence performance for different net-
work structures and supervision losses.

Œª

inputs and outputs the same size density maps. Our ap-
proach differs from methods that adopt patch-wise opera-
tions [52, 44, 27, 1, 37, 21, 11, 2]. Notably, patch-wise op-
erations induce boundary artifacts that negatively affect the
localization precision. Moreover, the patch-wise counting
accuracy suffers from statistical shifts across patches [2].
Counting Accuracy. To evaluate the counting accuracy,
we adopt the mean average error (MAE) and the mean
squared error (MSE) metrics, which are deÔ¨Åned as:

MAE =

1

M

M

X

i=1

|Ci ‚àí C gt

i |, MSE = vuut

1

M

M

X

i=1

|Ci ‚àí C gt

i |2

(5)

where M is the number of images in the test set, and C gt
i
and Ci represent the ground truth and the predicted count
of the i-th image, computed as the integral of the density
maps.

Density Map Quality. To evaluate the quality of the es-
timated density maps, we also calculate the PSNR (Peak
Signal-to-Noise Ratio) and SSIM (Structural Similarity in
Image) indices, as described in [40]. In particular, the SSIM
index is normally adopted in image quality assessment [46],
and it computes the similarity between two images from the
mean, variance and covariance statistics.
4.2. Ablation Study

The ablation study results are shown in Table 1. The
table is partitioned row-wise into three groups, with Ô¨Åve
conÔ¨Ågurations. Each group contains the indexed conÔ¨Ågu-
rations corresponding to one main contribution of TEDnet.
These include the trellis network with multi-path decoding,
the distributed supervision, and the combinatorial loss with
SAL and SCL. In different columns, we report the count-
ing accuracy of each conÔ¨Åguration, using the MAE met-
ric. We also illustrate the quality of the density map using
the PSNR metrics. In Figure 7, the left picture illustrates
the convergence performance of conÔ¨Ågurations 1, 2, and 3,
demonstrating the beneÔ¨Åts for convergence introduced by
the dense skip connections and the distributed supervision.
The right picture in Figure 7 shows that when Œª = 1, i.e.
the SAL and SCL are equally weighted in the combinatori-
al loss, the best counting accuracy is reported.

The Ô¨Årst group of conÔ¨Ågurations shown in Table 1 com-
pare the performance of the multi-path trellis decoder and
single-path hourglass decoder. Two conÔ¨Ågurations in this

Table 1. Ablation study results on ShanghaiTech Part A dataset.
Best performance is bolded. Arrows in all tables indicate the fa-
vorable directions of the metric values.

ConÔ¨Ågurations

Network
Structure

Supervision
Methodology

Loss Function

1

2

2

3

3
4
5

Trellis Encoder +

Single path Decoder

Trellis Encoder + Multi-

path Trellis Decoder

Single

Supervision
Distributed
Supervision
Normal MSE

SAL

SAL + SCL

MAE‚Üì

PSNR‚Üë

73.1

22.51

71.2

24.24

71.2

24.24

69.2

69.2
67.8
64.2

24.71

24.71
24.94
25.88

group adopt the same trellis encoder, and single supervi-
sion is applied on density map Z with the normal MSE
loss. The results in this group show that the multi-path de-
coder improves the counting accuracy by 2.6% in terms of
MAE metric, and enhance the density map quality by 7.1%
in PSNR metric. Furthermore, the yellow curve in Figure
7 demonstrates faster convergence thanks to the dense skip
connections implemented in the multi-path decoder. The
second group of conÔ¨Ågurations are all set up with TED-
net, using the normal MSE loss. The results show that
distributed supervision improves the MAE by 2.8% and P-
SNR by 1.9%, and the green curve shows further improved
convergence speed and performance. In the last group, we
compare the performance of distributedly deploying differ-
ent losses. The combinatorial loss with both SAL and S-
CL (Œª = 1) stands out, with 7.2% improvements in MAE
and 4.5% in PSNR, which conÔ¨Årms that higher density map
quality can improve the counting accuracy. Overall, the best
result is reported by conÔ¨Åguration 5, which incorporates all
three contributions.

4.3. Performance and Comparison

We compare the performance of our TEDnet with eight
state-of-the-art approaches, on four challenging dataset-
s, including the ShanghaiTech [53], the UCF CC 50 [10],
WorldExpo‚Äô10 [52] and the UCF-QNRF [11]. We explain
the superior performance of TEDnet in terms of both count-
ing accuracy (MAE and MSE as shown in Table 2 and
3) and density map quality measures (SSIM and PSNR as
shown in Table 4).

4.3.1 Counting Accuracy

ShanghaiTech. The ShanghaiTech dataset is one of the
largest datasets which includes Part A and Part B subsets.
As shown in Table 2, on Part A, our method achieves the
lowest MAE and a competitive MSE. In terms of MAE, we
lead the second best by 4.2%. On Part B, we report the best
performance in terms of both two metrics, where MSE is
improved by 5.9%. The signiÔ¨Åcant improvements on this
dataset validate the effectiveness of TEDnet.

UCF CC 50. The UCF CC 50 dataset introduced by Idrees

6139

Table 2. Estimation errors on the ShanghaiTech dataset, the UCF CC 50 and the UCF-QNRF dataset

Method

Zhang et al.[52]
MCNN [53]
Cascaded-MTL [39]
Switching-CNN [37]
CP-CNN [40]
CSRNet [19]
SANet [2]
Idrees et al. [11]
Ours

ShanghaiTech Part A
MAE‚Üì

MSE‚Üì

ShanghaiTech Part B
MAE‚Üì

MSE‚Üì

UCF CC 50

UCF-QNRF

MAE‚Üì MSE‚Üì MAE‚Üì MSE‚Üì

181.8
110.2
101.3
90.4
73.6
68.2
67.0

-

64.2

277.7
173.2
152.4
135.0
106.4
115.0
104.5

-

109.1

32.0
26.4
20.0
21.6
20.1
10.6
8.4

-

8.2

49.8
41.3
31.1
33.4
30.1
16.0
13.6

-

12.8

467.0
377.6
322.8
318.1
295.8
266.1
258.4

-

498.5
509.1
397.9
439.2
320.9
397.5
334.9

-

249.4

354.5

-

277
252
228

-
-
-

132
113

-

426
514
445

-
-
-

191
188

Table 3. The MAE of the WorldExpo‚Äô10 dataset, S is short for
Scene.

Table 4. Quality of density map on ShanghaiTech Part A dataset
and parameter studies, M stands for millions.

Method

Zhang et al. [52]
MCNN [53]
Switching-CNN [37]
CP-CNN [40]
CRSNet [19]
SANet [2]
Ours

S1

9.8
3.4
4.4
2.9
2.9
2.6
2.3

S2

14.1
20.6
15.7
14.7
11.5
13.2
10.1

S3

14.3
12.9
10.0
10.5
8.6
9.0
11.3

S4

22.2
13.0
11.0
10.4
16.6
13.3
13.8

S5

3.7
8.1
5.9
5.8
3.4
3.0
2.6

Ave.

12.9
11.6
9.4
8.9
8.6
8.2
8.0

et al.[10] contains 50 images of varying resolutions, with a
wide range of densities. To settle the sample scarcity prob-
lem, we perform a 5-fold cross-validation, following the s-
tandard setting in [10]. As shown in Table 2, we achieve an
improvement of 3.5% in terms of the MAE metric.

UCF-QNRF. The UCF-QNRF is a new dataset with one
of the highest number of high-count crowd images and an-
notations. We compare our result with four state-of-the-
art methods and our method achieves the best performance
in terms of both MAE and MSE. We beat the second best
approach by a 14.4% improvement in MAE and 1.6% im-
provement in MSE, as shown in Table 2.

WorldExpo10. The WorldExpo10 dataset was introduced
by Zhang et al. [52], containing 3980 frames from 108 dif-
ferent scenes from the Shanghai 2010 WorldExpo. Table 3
shows that TEDnet delivers the lowest MAE in 3 out of 5
test scenes, and reports up to 13.3% improvement in Scene
5 over others. Overall, we achieve the best average MAE
performance, outperforming the second best by 2.4%.

4.3.2 Density Map Quality

As mentioned in Section 2, CP-CNN [40] and CSRnet [19]
also emphasize generating high-quality density maps. MC-
NN [53] is one of the most representative methods in densi-
ty estimation based crowd counting. We compare the qual-
ity of density maps estimated by TEDnet and these three
state-of-the-art systems. Quantitatively, as demonstrated
in Table 4, our method outperforms the other methods in
both PSNR and SSIM metrics on the ShanghaiTech Part A
dataset. Particularly, we obtain 8.1% and 8.4% improve-
ments over the second best method, in terms of PSNR and
SSIM metrics. Qualitatively, we visualize the maps gener-
ated by MCNN, SANet, and TEDnet on the ShanghaiTech

Method
MCNN [53]
CP-CNN [40]
CRSNet [19]
Ours

PSNR‚Üë

SSIM‚Üë

Parameters

21.4
21.72
23.79
25.88

0.52
0.72
0.76
0.83

0.13M
68.4M
16.26M
1.63M

Part A in Figure 2. In addition, we also display the density
maps generated by TEDnet on other datasets in Figure 6.

Our TEDnet introduces an enhanced multi-path decoder
architecture, which, however, is still lightweight compared
to other state-of-the-art methods, which also strives to gen-
erate high-quality density maps. As shown in Table 4, the
number of parameters in TEDnet is only equal to 10% of
those in CRSNet and 2.4% of CP-CNN. More important-
ly, we demonstrate the best overall performance in density
map quality as well as counting accuracy. MCNN is most
lightweight, yet we show signiÔ¨Åcant improvement in PSNR
by 17.3% and 36% in SSIM. Moreover, we also outperform
MCNN on all datasets in terms of MAE and MSE.

5. Conclusion

In this paper, we have presented a new deep learning ar-
chitecture, called the trellis encoder-decoder network (T-
EDnet) for crowd counting.
It consists of a multi-scale
encoder and a multi-path decoder to generate high-quality
density estimation maps. It preserves the localization preci-
sion in the encoded feature maps, upon which a multi-path
decoder with dense skip connections is adopted to achieve
thorough aggregation and fusion of multi-scale features.
The TEDnet is trained with the distributed supervision im-
plemented with the proposed combinatorial loss. Experi-
ments on four benchmarks show that the TEDnet achieves
new state-of-the-art performance in terms of both density
map quality and crowd counting accuracy.

6. Acknowledgment

This paper was supported by the National Science Fund
for Distinguished Young Scholars under Grant 61425014,
the National Key ScientiÔ¨Åc Instrument and Equipment De-
velopment Project under Grant 61827901, and the Natu-
ral Science Foundation of China under Grant 91538204,
61871016.

6140

References

[1] L. Boominathan, S. S. Kruthiventi, and R. V. Babu. Crowd-
net: A deep convolutional network for dense crowd counting.
In Proceedings of the 2016 ACM on Multimedia Conference,
pages 640‚Äì644. ACM, 2016. 2, 3, 4, 6, 7

[2] X. Cao, Z. Wang, Y. Zhao, and F. Su. Scale aggregation
network for accurate and efÔ¨Åcient crowd counting. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 734‚Äì750, 2018. 1, 2, 3, 4, 6, 7, 8

[3] A. B. Chan and N. Vasconcelos. Bayesian poisson regression
for crowd counting.
In Computer Vision, 2009 IEEE 12th
International Conference on, pages 545‚Äì551. IEEE, 2009. 3

[4] K. Chen, C. C. Loy, S. Gong, and T. Xiang. Feature mining
for localised crowd counting. In BMVC, volume 1, page 3,
2012. 1

[5] P. Dollar, C. Wojek, B. Schiele, and P. Perona. Pedestri-
an detection: An evaluation of the state of the art.
IEEE
transactions on pattern analysis and machine intelligence,
34(4):743‚Äì761, 2012. 3

[6] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-
resolution convolutional neural network. In European Con-
ference on Computer Vision, pages 391‚Äì407. Springer, 2016.
2, 4

[7] L. Fiaschi, U. K¬®othe, R. Nair, and F. A. Hamprecht. Learn-
ing to count with regression forest and structured labels. In
Pattern Recognition (ICPR), 2012 21st International Confer-
ence on, pages 2685‚Äì2688. IEEE, 2012. 3

[8] J. Fu, J. Liu, Y. Wang, and H. Lu. Stacked deconvolution-
al network for semantic segmentation. arXiv preprint arX-
iv:1708.04943, 2017. 2, 4, 5

[9] W. Ge and R. T. Collins. Marked point processes for crowd
counting.
In Computer Vision and Pattern Recognition,
2009. CVPR 2009. IEEE Conference on, pages 2913‚Äì2920.
IEEE, 2009. 1, 3

[10] H. Idrees, I. Saleemi, C. Seibert, and M. Shah. Multi-source
In
multi-scale counting in extremely dense crowd images.
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2547‚Äì2554, 2013. 1, 3, 7, 8

[11] H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. Al-Maadeed,
N. Rajpoot, and M. Shah. Composition loss for counting,
density map estimation and localization in dense crowds.
arXiv preprint arXiv:1808.01050, 2018. 1, 2, 3, 4, 6, 7, 8

[12] X. Jiang, P. Li, X. Zhen, and X. Cao. Model-free tracking
with deep appearance and motion features integration.
In
2019 IEEE Winter Conference on Applications of Computer
Vision (WACV), pages 101‚Äì110. IEEE, 2019. 2

[13] D. Kang, Z. Ma, and A. B. Chan. Beyond counting: com-
parisons of density maps for crowd analysis tasks-counting,
detection, and tracking. IEEE Transactions on Circuits and
Systems for Video Technology, 2018. 3, 4

[14] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 6

[15] S. Kumagai, K. Hotta, and T. Kurita. Mixture of count-
ing cnns: Adaptive integration of cnns specialized to spe-
ciÔ¨Åc appearance for crowd counting. arXiv preprint arX-
iv:1703.09393, 2017. 3

[16] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-
In ArtiÔ¨Åcial Intelligence and Statistics,

supervised nets.
pages 562‚Äì570, 2015. 5

[17] V. Lempitsky and A. Zisserman. Learning to count objects
In Advances in neural information processing

in images.
systems, pages 1324‚Äì1332, 2010. 1, 3

[18] M. Li, Z. Zhang, K. Huang, and T. Tan. Estimating the num-
ber of people in crowded scenes by mid based foreground
segmentation and head-shoulder detection. In Pattern Recog-
nition, 2008. ICPR 2008. 19th International Conference on,
pages 1‚Äì4. IEEE, 2008. 1, 3

[19] Y. Li, X. Zhang, and D. Chen. Csrnet: Dilated convolution-
al neural networks for understanding the highly congested
scenes. In Proceedings of the IEEE Conference on Comput-
er Vision and Pattern Recognition, pages 1091‚Äì1100, 2018.
2, 3, 4, 6, 8

[20] G. Lin, A. Milan, C. Shen, and I. D. Reid. ReÔ¨Ånenet: Multi-
path reÔ¨Ånement networks for high-resolution semantic seg-
mentation. In Cvpr, volume 1, page 5, 2017. 2, 4

[21] J. Liu, C. Gao, D. Meng, and A. G. Hauptmann. Decidenet:
Counting varying density crowds through attention guided
detection and density estimation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 5197‚Äì5206, 2018. 3, 7

[22] X. Liu, J. van de Weijer, and A. D. Bagdanov. Leveraging
unlabeled data for crowd counting by learning to rank. arXiv
preprint arXiv:1803.03095, 2018. 2

[23] E. Lu, W. Xie, and A. Zisserman. Class-agnostic counting.

arXiv preprint arXiv:1811.00472, 2018. 3

[24] Z. Ma, L. Yu, and A. B. Chan. Small instance detection by
integer programming on object density maps.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3689‚Äì3697, 2015. 2, 3

[25] V. Nekrasov, C. Shen, and I. Reid. Light-weight reÔ¨Ånenet
for real-time semantic segmentation. arXiv preprint arX-
iv:1810.03272, 2018. 2, 4

[26] A. Odena, V. Dumoulin, and C. Olah. Deconvolution and

checkerboard artifacts. Distill, 2016. 5

[27] D. Onoro-Rubio and R. J. L¬¥opez-Sastre.

Towards
perspective-free object counting with deep learning. In Eu-
ropean Conference on Computer Vision, pages 615‚Äì629.
Springer, 2016. 2, 3, 7

[28] N. Paragios and V. Ramesh. A mrf-based approach for real-
time subway monitoring.
In Computer Vision and Pattern
Recognition, 2001. CVPR 2001. Proceedings of the 2001
IEEE Computer Society Conference on, volume 1, pages I‚ÄìI.
IEEE, 2001. 1

[29] V.-Q. Pham, T. Kozakaya, O. Yamaguchi, and R. Okada.
Count forest: Co-voting uncertain number of targets using
random forest for crowd density estimation. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 3253‚Äì3261, 2015. 3

[30] W. Ren, D. Kang, Y. Tang, and A. B. Chan. Fusing crowd
density maps and visual object trackers for people tracking
in crowd scenes.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5353‚Äì
5362, 2018. 2, 3

6141

[45] M. Wang and X. Wang. Automatic adaptation of a generic
pedestrian detector to a speciÔ¨Åc trafÔ¨Åc scene. In Computer
Vision and Pattern Recognition (CVPR), 2011 IEEE Confer-
ence on, pages 3401‚Äì3408. IEEE, 2011. 1, 3

[46] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncel-
li. Image quality assessment: from error visibility to struc-
tural similarity.
IEEE transactions on image processing,
13(4):600‚Äì612, 2004. 7

[47] Z. Wang, Z. Xiao, K. Xie, Q. Qiu, X. Zhen, and X. Cao.
In defense of single-column networks for crowd counting.
arXiv preprint arXiv:1808.06133, 2018. 3, 6

[48] S. Xie and Z. Tu. Holistically-nested edge detection. In Pro-
ceedings of the IEEE international conference on computer
vision, pages 1395‚Äì1403, 2015. 5

[49] B. Xu and G. Qiu. Crowd density estimation based on rich
features and random projection forest.
In Applications of
Computer Vision (WACV), 2016 IEEE Winter Conference on,
pages 1‚Äì8. IEEE, 2016. 3

[50] J. Yang, Q. Liu, and K. Zhang. Stacked hourglass network
for robust facial landmark localisation. In Computer Vision
and Pattern Recognition Workshops (CVPRW), 2017 IEEE
Conference on, pages 2025‚Äì2033. IEEE, 2017. 2

[51] C. Zhang, H. Li, X. Wang, and X. Yang. Cross-scene crowd
counting via deep convolutional neural networks.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 833‚Äì841, 2015. 3

[52] C. Zhang, H. Li, X. Wang, and X. Yang. Cross-scene crowd
In Pro-
counting via deep convolutional neural networks.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 833‚Äì841, 2015. 3, 7, 8

[53] Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma. Single-
image crowd counting via multi-column convolutional neu-
ral network. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pages 589‚Äì597, 2016.
1, 2, 3, 6, 7, 8

[54] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene
In IEEE Conf. on Computer Vision and

parsing network.
Pattern Recognition (CVPR), pages 2881‚Äì2890, 2017. 2, 4

[31] M. Rodriguez,

I. Laptev, J. Sivic, and J.-Y. Audibert.
Density-aware person detection and tracking in crowds. In
Computer Vision (ICCV), 2011 IEEE International Confer-
ence on, pages 2423‚Äì2430. IEEE, 2011. 2

[32] M. Rodriguez,

I. Laptev, J. Sivic, and J.-Y. Audibert.
Density-aware person detection and tracking in crowds. In
Computer Vision (ICCV), 2011 IEEE International Confer-
ence on, pages 2423‚Äì2430. IEEE, 2011. 3

[33] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
In
lutional networks for biomedical image segmentation.
International Conference on Medical image computing and
computer-assisted intervention, pages 234‚Äì241. Springer,
2015. 2, 4

[34] D. Ryan, S. Denman, C. Fookes, and S. Sridharan. Crowd
counting using multiple local features.
In Digital Image
Computing: Techniques and Applications, 2009. DICTA‚Äô09.,
pages 81‚Äì88. IEEE, 2009. 3

[35] D. Ryan, S. Denman, S. Sridharan, and C. Fookes. An eval-
uation of crowd counting methods, features and regression
models. Computer Vision and Image Understanding, 130:1‚Äì
17, 2015. 3

[36] S. A. M. Saleh, S. A. Suandi, and H. Ibrahim. Recent survey
on crowd density estimation and counting for visual surveil-
lance. Engineering Applications of ArtiÔ¨Åcial Intelligence,
41:103‚Äì114, 2015. 3

[37] D. B. Sam, S. Surya, and R. V. Babu. Switching convolution-
al neural network for crowd counting. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, volume 1, page 6, 2017. 3, 7, 8

[38] Z. Shi, L. Zhang, Y. Liu, X. Cao, Y. Ye, M.-M. Cheng, and
G. Zheng. Crowd counting with deep negative correlation
learning. In Proceedings of the IEEE Conference on Comput-
er Vision and Pattern Recognition, pages 5382‚Äì5390, 2018.
2

[39] V. A. Sindagi and V. M. Patel. Cnn-based cascaded multi-
task learning of high-level prior and density estimation for
crowd counting.
In Advanced Video and Signal Based
Surveillance (AVSS), 2017 14th IEEE International Confer-
ence on, pages 1‚Äì6. IEEE, 2017. 8

[40] V. A. Sindagi and V. M. Patel. Generating high-quality crowd
density maps using contextual pyramid cnns. In 2017 IEEE
International Conference on Computer Vision (ICCV), pages
1879‚Äì1888. IEEE, 2017. 2, 3, 7, 8

[41] V. A. Sindagi and V. M. Patel. A survey of recent advances
in cnn-based single image crowd counting and density esti-
mation. Pattern Recognition Letters, 107:3‚Äì16, 2018. 3

[42] X. Tao, H. Gao, R. Liao, J. Wang, and J. Jia. Detail-revealing
deep video super-resolution. In Proceedings of the IEEE In-
ternational Conference on Computer Vision, Venice, Italy,
pages 22‚Äì29, 2017. 2, 4

[43] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and
P. H. Torr. End-to-end representation learning for correlation
Ô¨Ålter based tracking. In Computer Vision and Pattern Recog-
nition (CVPR), 2017 IEEE Conference on, pages 5000‚Äì5008.
IEEE, 2017. 2

[44] E. Walach and L. Wolf. Learning to count with cnn boosting.
In European Conference on Computer Vision, pages 660‚Äì
676. Springer, 2016. 3, 7

6142

