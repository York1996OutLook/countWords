Revisiting Local Descriptor based Image-to-Class Measure

for Few-shot Learning

Wenbin Li1, Lei Wang2,

Jinglin Xu3,

Jing Huo1, Yang Gao1,

Jiebo Luo4

1Nanjing University, China,

2University of Wollongong, Australia

3Northwestern Polytechnical University, China,

4University of Rochester, USA

Abstract

Few-shot learning in image classiï¬cation aims to learn
a classiï¬er to classify images when only few training
examples are available for each class. Recent work has
achieved promising classiï¬cation performance, where
an image-level
feature based measure is usually used.
In this paper, we argue that a measure at such a level
may not be effective enough in light of the scarcity of
examples in few-shot learning.
Instead, we think a local
descriptor based image-to-class measure should be taken,
inspired by its surprising success in the heydays of local
invariant features. Speciï¬cally, building upon the recent
episodic training mechanism, we propose a Deep Nearest
Neighbor Neural Network (DN4 in short) and train it in an
end-to-end manner. Its key difference from the literature is
the replacement of the image-level feature based measure
in the ï¬nal layer by a local descriptor based image-to-class
measure. This measure is conducted online via a k-nearest
neighbor search over the deep local descriptors of convo-
lutional feature maps. The proposed DN4 not only learns
the optimal deep local descriptors for the image-to-class
measure, but also utilizes the higher efï¬ciency of such
a measure in the case of example scarcity, thanks to the
exchangeability of visual patterns across the images in
the same class. Our work leads to a simple, effective,
and computationally efï¬cient
framework for few-shot
learning.
Experimental study on benchmark datasets
consistently shows its superiority over the related state-
of-the-art, with the largest absolute improvement of 17%
over the next best. The source code can be available from
https://github.com/WenbinLee/DN4.git.

1. Introduction

Few-shot learning aims to learn a model with good gen-
eralization capability such that it can be readily adapted to
new unseen classes (concepts) by accessing only one or few

examples. However, the extremely limited number of ex-
amples per class can hardly represent the class distribution
effectively, making this task truly challenging.

To tackle the few-shot learning task, a variety of meth-
ods have been proposed, which can be roughly divided into
two types, i.e., meta-learning based [16, 14, 13] and metric-
learning based [9, 17, 25]. The former type introduces
a meta-learning paradigm [18, 21] to learn an across-task
meta-learner for generalizing to new unseen tasks. They
usually resort to recurrent neural networks or long short
term memory networks to learn a memory network [24, 12]
to store knowledge. The latter type adopts a relatively sim-
pler architecture to learn a deep embedding space to transfer
representation (knowledge). This type of methods usually
relies on the metric learning and episodic training mecha-
nism [22]. Both types of methods have greatly advanced
the development of few-shot learning.

These existing methods mainly focus on making knowl-
edge transfer [22, 2], concept representation [17, 4] or re-
lation measure [25], but have not paid sufï¬cient attention
to the way of the ï¬nal classiï¬cation. They generally take
the common practice, i.e., using the image-level pooled fea-
tures or fully connected layers designed for large-scale im-
age classiï¬cation, for the few-shot case. Considering the
unique characteristic of few-shot learning (i.e., the scarcity
of examples for each training class), such a common prac-
tice may not be appropriate anymore.

In this paper, we revisit

the Naive-Bayes Nearest-
Neighbor (NBNN) approach [1] published a decade ago,
and investigate its effectiveness in the context of the latest
few-shot learning research. The NBNN approach demon-
strated a surprising success when the bag-of-features model
with local invariant features (i.e., SIFT) was popular. That
work provides two key insights. First, summarizing the lo-
cal features of an image into a compact image-level rep-
resentation could lose considerable discriminative informa-
tion. It will not be recoverable when the number of training
examples is small. Second, in this case, directly using these
local features for classiï¬cation will not work if an image-to-
image measure is used. Instead, an image-to-class measure

17260

should be taken, by exploiting the fact that a new image can
be roughly â€œcomposedâ€ using the pieces of other images in
the same class. The above two insights inspire us to review
the way of the ï¬nal classiï¬cation in the existing methods
for few-shot learning and reconsider the NBNN approach
for this task with deep learning.

Speciï¬cally, we develop a novel Deep Nearest Neighbor
Neural Network (DN4 in short) for few-shot learning.
It
follows the recent episodic training mechanism and is fully
end-to-end trainable. Its key difference from the related ex-
isting methods lies in that it replaces the image-level fea-
ture based measure in the ï¬nal layer with a local descriptor
based image-to-class measure. Similar to NBNN [1], this
measure is computed via a k-nearest neighbor search over
local descriptors, with the difference that these descriptors
are now trained deeply via convolutional neural networks.
Once trained, applying the proposed network to new few-
shot learning tasks is straightforward, consisting of local de-
scriptor extraction and then a nearest neighbor search. Inter-
estingly, in terms of computation, the scarcity of examples
per class now turns out to be an â€œadvantageâ€ making NBNN
more appealing for few-shot learning. It mitigates the com-
putation of searching for the nearest neighbors from a huge
set of local descriptors, which is one factor of the lower
popularity of NBNN in large-scale image classiï¬cation.

Experiments are conducted on multiple benchmark
datasets to compare the proposed DN4 with the original
NBNN and the related state-of-the-art methods for the task
of few-shot learning. The proposed method again demon-
strates a surprising success. It improves the 1-shot and 5-
shot accuracy on miniImageNet from 50.44% to 51.24%
and from 66.53% to 71.02%, respectively. Particularly, on
ï¬ne-grained datasets, it achieves the largest absolute im-
provement over the next best method by 17%.

2. Related Work

Among the recent literature of few-shot learning, the
transfer learning based methods are most relevant to the
proposed method. Therefore, we brieï¬‚y review two main
branches of this kind of methods as follows.

Meta-learning based methods. As shown by the rep-
resentative work [16, 14, 3, 2, 5], the meta-learning based
methods train a meta-learner with the meta-learning or the
learning-to-learn paradigm [18, 19, 21] for few-shot learn-
ing. This is beneï¬cial for identifying how to update the
parameters of the learnerâ€™s model. For instance, Santoro et
al. [16] trained an LSTM as a controller to interact with an
external memory module. And the work [14] adopted an
LSTM-based meta-learner as an optimizer to train another
classiï¬er as well as learning a task-common initialization
for this classiï¬er. The work of MM-Net [2] constructed a
contextual learner to predict the parameters of an embed-
ding network for unlabeled images by using memory slots.

it

Although the meta-learning based methods can achieve
excellent results for few-shot classiï¬cation,
is difï¬-
cult to train their complicated memory-addressing architec-
ture because of the temporally-linear hidden state depen-
dency [13]. Compared with the methods in this branch,
the proposed framework DN4 can be trained more easily
in an end-to-end manner from scratch, e.g., by only using a
common single convolutional neural networks (CNN), and
could provide quite competitive results.

Metric-learning based methods. The metric-learning
based methods mainly depend on learning an informative
similarity metric, as demonstrated by the representative
work [9, 22, 20, 17, 4, 25, 11]. Speciï¬cally, to introduce
the metric-based method into few-shot learning, Koch et
al. [9] originally utilized a Siamese Neural Network to learn
powerful discriminative representations and then general-
ized them to unseen classes. And then, Vinyals et al. [22]
introduced the episodic training mechanism into few-shot
learning and proposed the Matching Nets by combining at-
tention and memory together. In [17], a Prototypical Net-
work was proposed by taking the mean of each class as
its corresponding prototype representation to learn a met-
ric space. Recently, Sung et al. considered the relation
between query images and class images, and presented a
Relation Network [25] to learn a deep non-linear measure.
The proposed framework DN4 belongs to metric-
learning based methods. However, a key difference from
them is that the above methods mainly adopt the image-
level features for classiï¬cation, while the proposed DN4 ex-
ploits deep local descriptors and the image-to-class measure
for classiï¬cation, as inspired by the NBNN approach [1].
As will be shown in the experimental part, the proposed
DN4 can clearly outperform the several state-of-the-art
metric-learning based methods.

3. The Proposed Method

3.1. Problem Formulation

Let S denote a support set, which contains C different
image classes and K labeled samples per class. Given a
query set Q, few-shot learning aims to classify each unla-
beled sample in Q according to the set S. This setting is also
called C-way K-shot classiï¬cation. Unfortunately, when S
only has few samples per class, it will be hard to effectively
learn a model to classify the samples in Q. Usually, the
literature resorts to an auxiliary set A to learn transferable
knowledge to improve the classiï¬cation on Q. Note that
the set A can contain a large number of classes and labeled
samples, but it has a disjoint class label space with respect
to the set S.

The episodic training mechanism [22] has been demon-
strated in the literature as an effective approach to learn-
ing the transferable knowledge from A, and it will also

7261

Figure 1. Illustration of the proposed Deep Nearest Neighbor Neural Network (DN4 in short) for a few-shot learning task in the 5-way
and 1-shot setting. As shown, this framework consists of a CNN-based embedding module Î¨(Â·) for learning deep local descriptors and an
image-to-class module Î¦(Â·) for measuring the similarity between a given query image X and each of the classes, ci (i = 1, 2, Â· Â· Â· , 5).

be adopted in this work. Speciï¬cally, at each iteration, an
episode is constructed to train the classiï¬cation model by
simulating a few-shot learning task. The episode consists
of a support set AS and a query set AQ that are randomly
sampled from the auxiliary set A. Generally, AS has the
same numbers of ways (i.e., classes) and shots as S.
In
other words, there are exactly C classes and K samples per
class in AS. During training, tens of thousands of episodes
will be constructed to train the classiï¬cation model, namely
the episodic training. In the test stage, with the support set
S, the learned model can be directly used to classify each
image in Q.

3.2. Motivation from the NBNN Approach

This work is largely inspired by the Naive-Bayes
Nearest-Neighbor (NBNN) method in [1]. The two key ob-
servations of NBNN are described as follows, and we show
that they apply squarely to few-shot learning.

First, for the (then-popular) bag-of-features model in im-
age classiï¬cation, local invariant features are usually quan-
tized into visual words to generate the distribution of words
(e.g., a histogram obtained by sum-pooling) in an image.
It is observed in [1] that due to quantization error, such
an image-level representation could signiï¬cantly lose dis-
criminative information. If there are sufï¬cient training sam-
ples, the subsequent learning process (e.g., via support vec-
tor machines) can somehow recover from such a loss, still
showing satisfactory classiï¬cation performance. Neverthe-
less, when training samples are insufï¬cient, this loss is un-
recoverable and leads to poor classiï¬cation.

Few-shot learning is impacted more signiï¬cantly by the
issue of example scarcity than NBNN. And the existing
methods usually pool the last convolutional feature maps
(e.g., via the global average pooling or fully connected
layer) to an image-level representation for the ï¬nal classiï¬-

cation. In this case, such an information loss will also occur
and is unrecoverable.

Second, as further observed in [1], using the local invari-
ant features of two images, instead of their image-level rep-
resentations, to measure an image-to-image similarity for
classiï¬cation will still incur a poor result. This is because
such an image-to-image similarity does not generalize be-
yond training samples. When the number of training sam-
ples is small, a query image could be different from any
training samples of the same class due to intra-class varia-
tion or background clutter. Instead, an image-to-class mea-
sure should be used. Speciï¬cally, the local invariant fea-
tures from all training samples in the same class are col-
lected into one pool. This measure evaluates the proximity
(e.g., via nearest-neighbor search) of the local features of a
query image to the pool of each class for classiï¬cation.

Again,

this observation applies to few-shot learning.
Essentially, the above image-to-class measure breaks the
boundaries of training images in the same class, and uses
their local features collectively to provide a richer and more
ï¬‚exible representation for a class. As indicated in [1], this
setting can be justiï¬ed by a fact that a new image can be
roughly â€œcomposedâ€ by using the pieces of other images in
the same class (i.e., the exchangeability of visual patterns
across the images in the same class).

3.3. The Proposed DN4 Framework

The above analysis motivates us to review the way of
the ï¬nal classiï¬cation in few-shot learning and reconsider
the NBNN approach. This leads to the proposed framework
Deep Nearest Neighbor Neural Network (DN4 in short).

As illustrated in Figure 1, DN4 mainly consists of two
components: a deep embedding module Î¨ and an image-
to-class measure module Î¦. The former learns deep local
descriptors for all images. With the learned descriptors,

7262

CNNSupport setQuery image0.10.40.9-0.7-0.4Local Features Î¨(ğ‘‹ğ‘‹)Image-to-Class module Î¦MaxÎ¦(Î¨(ğ‘‹ğ‘‹),ğ‘ğ‘1)ğ’…ğ’…ğ’˜ğ’˜ğ’™ğ’™ğ‘–ğ‘–ğ’‰ğ’‰Class:ğ‘ğ‘1Class:ğ‘ğ‘2Class:ğ‘ğ‘3Class:ğ‘ğ‘4Class:ğ‘ğ‘5EmbeddingÎ¨Î¦(Î¨(ğ‘‹ğ‘‹),ğ‘ğ‘2)Î¦(Î¨(ğ‘‹ğ‘‹),ğ‘ğ‘3)Î¦(Î¨(ğ‘‹ğ‘‹),ğ‘ğ‘4)Î¦(Î¨(ğ‘‹ğ‘‹),ğ‘ğ‘5)the latter calculates the aforementioned image-to-class mea-
sure. Importantly, these two modules are integrated into a
uniï¬ed network and trained in an end-to-end manner from
scratch. Also, note that the designed image-to-class module
can readily work with any deep embedding module.

Deep embedding module. The module Î¨ routinely
learns the feature representations for query and support im-
ages. Any proper CNN can be used. Note that Î¨ only con-
tains convolutional layers but has no fully connected layer,
since we just need deep local descriptors to compute the
image-to-class measure. In short, given an image X, Î¨(X)
will be an hÃ—wÃ—d tensor, which can be viewed as a set of
m (m = hw) d-dimensional local descriptors as

Î¨(X) = [x1, . . . , xm] âˆˆ RdÃ—m ,

(1)

where xi is the i-th deep local descriptor. In our experi-
ments, given an image with a resolution of 84 Ã— 84, we can
get h = w = 21 and d = 64. It means that each image has
441 deep local descriptors in total.

Image-to-Class module. The module Î¦ uses the deep
local descriptors from all training images in a class to con-
struct a local descriptor space for this class. In this space,
we calculate the image-to-class similarity (or distance) be-
tween a query image and this class via k-NN, as in [1].

Speciï¬cally, through the module Î¨, a given query image
q will be embedded as Î¨(q) = [x1, . . . , xm] âˆˆ RdÃ—m. For
each descriptor xi, we ï¬nd its k-nearest neighbors Ë†x
j=1
in a class c. Then we calculate the similarity between xi
and each Ë†xi, and sum the mk similarities as the image-to-
class similarity between q and the class c. Mathematically,
the image-to-class measure can be easily expressed as

i |k

j

Î¦(cid:0)Î¨(q), c(cid:1) =

m

k

X

X

i=1

j=1

cos(xi, Ë†x

j
i )

cos(xi, Ë†xi) =

xâŠ¤
i

Ë†xi

kxik Â· k Ë†xik

,

(2)

where cos(Â·) indicates the cosine similarity. Other similar-
ity or distance functions can certainly be employed.

Note that

in terms of computational efï¬ciency,

the
image-to-class measure seems more suitable for few-shot
classiï¬cation than the generic image classiï¬cation focused
in [1]. The major computational issue in NBNN caused by
searching for k-nearest neighbors from a huge pool of lo-
cal descriptors has now been substantially weakened due to
the much smaller number of training samples in the few-
shot setting. This makes the proposed framework compu-
tationally efï¬cient. Furthermore, compared with NBNN, it
will be more promising, by beneï¬ting from the deep fea-
ture representations that are much more powerful than the
hand-crafted features used in NBNN.

cation model is non-parametric if not considering the em-
bedding module Î¨. Since a non-parametric model does not
involve parameter learning, the over-ï¬tting issue in para-
metric few-shot learning methods (e.g., learning a fully con-
nected layer over image-level representation) can also be
mitigated to some extent.

3.4. Network Architecture

For a fair comparison with the state-of-the-art methods,
we take a commonly used four-layer convolutional neural
network as the embedding module. It contains four convo-
lutional blocks, each of which consists of a convolutional
layer, a batch normalization layer and a Leaky ReLU layer.
Besides, for the ï¬rst two convolutional blocks, an additional
2Ã—2 max-pooling layer is also appended, respectively. This
embedding network is named Conv-64F, since there are 64
ï¬lters of size 3 Ã— 3 in each convolutional layer. As for the
image-to-class module, the only hyper-parameter is the pa-
rameter k, which will be discussed in the experiment.

At each iteration of the episodic training, we feed a sup-
port set S and a query image q into our model. Through the
embedding module Î¨, we obtain all the deep local repre-
sentations for all these images. Then via the module Î¦, we
calculate the image-to-class similarity between q and each
class by Eq. (2). For a C-way K-shot task, we can get a
similarity vector z âˆˆ RC . The class corresponding to the
largest component of z will be the prediction for q.

4. Experimental Results

The main goal of this section is to investigate two inter-
esting questions: (1) How does the pre-trained deep features
based NBNN without episodic training perform on the few-
shot learning? (2) How does our proposed DN4 framework,
i.e., a CNN based NBNN in an end-to-end episodic training
manner, perform on the few-shot learning?

4.1. Datasets

We conduct all the experiments on four benchmark

datasets as follows.

miniImageNet. As a mini-version of ImageNet [15],
this dataset [22] contains 100 classes with 600 images per
class, and has a resolution of 84 Ã— 84 for each image. Fol-
lowing the splits used in [14], we take 64, 16 and 20 classes
for training (auxiliary), validation and test, respectively.

Stanford Dogs. This dataset [7] is originally used for
the task of ï¬ne-grained image classiï¬cation, including 120
breeds (classes) of dogs with a total number of 20, 580 im-
ages. Here, we conduct ï¬ne-grained few-shot classiï¬cation
task on this dataset and take 70, 20 and 30 classes for train-
ing (auxiliary), validation and test, respectively.

Finally, it is worth mentioning that the image-to-class
module in DN4 is non-parametric. So the entire classiï¬-

Stanford Cars. This dataset [10] is also a benchmark
dataset for ï¬ne-grained classiï¬cation task, which consists of

7263

196 classes of cars with a total number of 16, 185 images.
Similarly, 130, 17 and 49 classes in this dataset are split for
training (auxiliary), validation and test.

CUB-200. This dataset [23] contains 6033 images from
200 bird species. In a similar way, we select 130, 20 and 50
classes for training (auxiliary), validation and test.

For the last three ï¬ne-grained datasets, all the images in

these datasets are resized to 84 Ã— 84 as miniImageNet.

4.2. Experimental Setting

All experiments are conducted around the C-way K-shot
classiï¬cation task on the above datasets. To be speciï¬c,
5-way 1-shot and 5-shot classiï¬cation tasks will be con-
ducted on all these datasets. During training, we randomly
sample and construct 300, 000 episodes to train all of our
models by employing the episodic training mechanism. In
each episode, besides the K support images (shots) in each
class, 15 and 10 query images will also be selected from
each class for the 1-shot and 5-shot settings, respectively.
In other words, for a 5-way 1-shot task, there will be 5 sup-
port images and 75 query images in one training episode.
To train our model, we adopt Adam algorithm [8] with an
initial learning rate of 1Ã—10âˆ’3 and reduce it by half of every
100, 000 episodes.

During test, we randomly sample 600 episodes from the
test set, and take the top-1 mean accuracy as the evaluation
criterion. This process will be repeated ï¬ve times, and the
ï¬nal mean accuracy will be reported. Moreover, the 95%
conï¬dence intervals are also reported. Notably, all of our
models are trained from scratch in an end-to-end manner
and do not need ï¬ne-tuning in the test stage.

4.3. Comparison Methods

Baseline methods. To illustrate the basic classiï¬cation
performance on the above datasets, we implement a base-
line method k-NN (Deep global features). Particularly, we
adopt the basic embedding network Conv-64F and append
three additional FC layers to train a classiï¬cation network
on the corresponding training (auxiliary) dataset. During
test, we use this pre-trained network to extract features from
the last FC layer and use a k-NN classiï¬er to get the ï¬-
nal classiï¬cation results. Also, to answer the ï¬rst question
at the beginning of Section 4, we re-implement the NBNN
algorithm [1] by using the pre-trained Conv-64F truncated
from the above k-NN (Deep global features) method. This
new NBNN algorithm employing the deep local descriptors
instead of the hand-crafted descriptors (i.e., SIFT), is called
NBNN (Deep local features).

Metric-learning based methods. As our method be-
longs to the metric-learning branch, we mainly compare
our model with four state-of-the-art metric-learning based
models, including Matching Nets FCE [22], Prototypical
Nets [17], Relation Net [25] and Graph Neural Network

(GNN) [4]. Note that we re-run the GNN model by using
the Conv-64F as its embedding module because the origi-
nal GNN adopts a different embedding module Conv-256F,
which also has four convolutional layers but with 64, 96,
128 and 256 ï¬lters for the corresponding layers, respec-
tively. Also, we re-run the Prototypical Nets via the same
5-way training setting instead of the 20-way training setting
in the original work for a fair comparison.

Meta-learning based methods. Besides the metric-
learning based models, ï¬ve state-of-the-art meta-learning
based models are also picked for reference. These models
include Meta-Learner LSTM [14], Model-agnostic Meta-
learning (MAML) [3], Simple Neural AttentIve Learner
(SNAIL) [13], MM-Net [2] and Dynamic-Net [5]. As SNAIL
adopts a much more complicated ResNet-256F (a smaller
version of ResNet [6]) as its embedding module, we will ad-
ditionally report its results based on the Conv-32F provided
in its appendix for a fair comparison. Note that Conv-32F
has the same architecture with Conv-64F, but with 32 ï¬lters
per convolutional layer, which has also been employed by
Meta-Learner LSTM and MAML to reduce over-ï¬tting.

4.4. Few shot Classiï¬cation

The generic few-shot classiï¬cation task is conducted on
miniImageNet. The results are reported in Table 1, where
the hyper-parameter k is set as 3. From Table 1, it is amaz-
ing to see that NBNN (Deep local features) can achieve
much better results than k-NN (Deep global features), and
it is even better than Matching Nets FCE, Meta-Learner
LSTM, and SNAIL (Conv-32F). This not only veriï¬es that
the local descriptors can perform better than the image-level
features (i.e., FC layer features used by k-NN), but also
shows that the image-to-class measure is truly promising.
However, NBNN (Deep local features) still has a large per-
formance gap compared with the state-of-the-art Prototyp-
ical Nets, Relation Net and GNN. The reason is that, as a
lazy learning algorithm, NBNN (Deep local features) does
not have a training stage and also lacks the episodic training.
So far, the ï¬rst question has been answered.

On the contrary, our proposed DN4 embeds the image-
to-class measure into a deep neural network, and can learn
the deep local descriptors jointly by employing the episodic
training, which indeed obtains superior results. Compared
with the metric-learning based models, our DN4 (Conv-
64F) gains 7.68%, 2.22%, 2.79% and 0.8% improvements
over Matching Nets FCE, GNNâ€¡ (Conv-64F), Prototypi-
cal Netsâ€¡ (i.e., via 5-way training setting) and Relation
Net on the 5-way 1-shot classiï¬cation task, respectively.
On the 5-way 5-shot classiï¬cation task, we can even get
15.71%, 7.52%, 4.49% and 5.7% signiï¬cant improvements
over these models. The reason is that these methods usually
use image-level features whose number is too small, while
our DN4 adopts learnable deep local descriptors which are

7264

Table 1. The mean accuracies of the 5-way 1-shot and 5-shot tasks on the miniImageNet dataset, with 95% conï¬dence intervals. The
second column refers to which kind of embedding module is employed, e.g., Conv-32F and Conv-64F etc. The third column denotes the
type of this method, i.e., meta-learning based or metric-learning based. âˆ— Results reported by the original work. â€¡ Results re-implemented
in the same setting for a fair comparison.

Model

Embedding

Type

5-Way Accuracy (%)

k-NN (Deep global features)
NBNN (Deep local features)

Matching Nets FCEâˆ— [22]
Prototypical Netsâ€¡ [17]
Prototypical Netsâˆ— [17]
GNNâ€¡ [4]
GNNâˆ— [4]
Relation Netâˆ— [25]

Our DN4 (k=3)

Meta-Learner LSTMâˆ— [14]
SNAILâˆ— [13]
MAMLâˆ— [3]
MM-Netâˆ— [2]
SNAILâˆ— [13]
Dynamic-Netâˆ— [5]
Dynamic-Netâˆ— [5]

Conv-64F
Conv-64F

Conv-64F
Conv-64F
Conv-64F
Conv-64F
Conv-256F
Conv-64F

Conv-64F

Metric
Metric

Metric
Metric
Metric
Metric
Metric
Metric

Metric

1-shot

27.23Â±1.41
44.10Â±1.17

43.56Â±0.84
48.45Â±0.96
49.42Â±0.78
49.02Â±0.98
50.33Â±0.36
50.44Â±0.82

5-shot

49.29Â±1.56
58.84Â±1.10

55.31Â±0.73
66.53Â±0.51
68.20Â±0.66
63.50Â±0.84
66.41Â±0.63
65.32Â±0.70

51.24Â±0.74

71.02Â±0.64

To take a whole picture of the-state-of-art methods

Conv-32F
Conv-32F
Conv-32F
Conv-64F

ResNet-256F
ResNet-256F

Conv-64F

Meta
Meta
Meta
Meta
Meta
Meta
Meta

43.44Â±0.77

45.10

48.70Â±1.84
53.37Â±0.48
55.71Â±0.99
55.45Â±0.89
56.20Â±0.86

60.60Â±0.71

55.20

63.11Â±0.92
66.97Â±0.35
68.88Â±0.92
70.13Â±0.68
72.81Â±0.62

Table 2. The mean accuracies of the 5-way 1-shot and 5-shot tasks on three ï¬ne-grained datasets, i.e., Stanford Dogs, Stanford Cars and
CUB-200, with 95% conï¬dence intervals. For each setting, the best and the second best methods are highlighted.

Model

Embed.

5-Way Accuracy (%)

Stanford Dogs

Stanford Cars

CUB-200

1-shot

5-shot

1-shot

5-shot

1-shot

5-shot

k-NN (Deep global features) Conv-64F
NBNN (Deep local features) Conv-64F

26.14Â±0.91
31.42Â±1.12

Matching Nets FCEâ€¡ [22]
Prototypical Netsâ€¡ [17]
GNNâ€¡ [4]

35.80Â±0.99
Conv-64F
37.59Â±1.00
Conv-64F
Conv-64F 46.98Â±0.98

43.14Â±1.02
42.17Â±0.99

47.50Â±1.03
48.19Â±1.03
62.27Â±0.95

23.50Â±0.88
28.18Â±1.24

34.80Â±0.98
40.90Â±1.01
55.85Â±0.97

34.45Â±0.98
38.27Â±0.92

44.70Â±1.03
52.93Â±1.03
71.25Â±0.89

25.81Â±0.90
35.29Â±1.03

45.30Â±1.03
37.36Â±1.00
51.83Â±0.98

45.34Â±1.03
47.97Â±0.96

59.50Â±1.01
45.28Â±1.03
63.69Â±0.94

Our DN4 (k=1)
Our DN4-DA (k=1)

Conv-64F
Conv-64F 45.73Â±0.76 66.33Â±0.66 61.51Â±0.85 89.60Â±0.44 53.15Â±0.84 81.90Â±0.60

63.51Â±0.62 59.84Â±0.80 88.65Â±0.44

74.92Â±0.64

45.41Â±0.76

46.84Â±0.81

more abundant especially in the 5-shot setting. On the other
hand, local descriptors enjoy the exchangeability character-
istic, making the distribution of each class built upon the
local descriptors more effective than the one built upon the
image-level features. Therefore, the second question can
also be answered.

Net, a two-stage model, it pre-trains its model with all
classes together before conducting the few-shot training,
while our DN4 does not. More importantly, our DN4 only
has one single uniï¬ed network, which is much simpler than
these meta-learning based methods with additional compli-
cated memory-addressing architectures.

To take a whole picture of the few-shot learning area, we
also report the results of the state-of-the-art meta-learning
based methods. We can see that our DN4 is still competi-
tive with these methods. Especially in the 5-way 5-shot set-
ting, our DN4 gains 15.82%, 10.42%, 7.91% and 4.05% im-
provements over SNAIL (Conv-32F), Meta-Learner LSTM,
MAML and MM-Net, respectively. As for the Dynamic-

4.5. Fine grained Few shot Classiï¬cation

Besides the generic few-shot classiï¬cation, we also con-
duct ï¬ne-grained few-shot classiï¬cation tasks on three ï¬ne-
grained datasets, i.e., Stanford Dogs, Stanford Cars and
CUB-200. Two baseline models and three state-of-the-
art models are implemented on these three datasets, i.e.,

7265

k-NN (Deep global features), NBNN (Deep local fea-
tures), Matching Nets FCE [22], Prototypical Nets [17] and
GNN [4]. The results are shown in Table 2.
In general,
the ï¬ne-grained few-shot classiï¬cation task is more chal-
lenging than the generic one due to the smaller inter-class
and larger intra-class variations of the ï¬ne-grained datasets.
It can be seen by comparing the performance of the same
methods between Tables 1 and 2. The performance of the
k-NN (Deep global features), NBNN (Deep local features)
and Prototypical Nets on the ï¬ne-grained datasets is worse
than that on miniImageNet.
It can also be observed that
NBNN (Deep local features) performs consistently better
than k-NN (Deep global features).

Due to the small inter-class variation of the ï¬ne-grained
task, we choose k = 1 for our DN4 to avoid introducing
noisy visual patterns. From Table 2, we can see that our
DN4 performs surprisingly well on these datasets under the
5-shot setting. Especially on the Stanford Cars, our DN4
gains the largest absolute improvement over the second best
method, i.e., GNN, by 17%. Under the 1-shot setting, our
DN4 does not perform as well as in the 5-shot setting. The
key reason is that our model relies on the k-nearest neighbor
algorithm, which is a lazy learning algorithm and its per-
formance depends largely on the number of samples. This
characteristic has been shown in Table 5, i.e., the perfor-
mance of DN4 gets better and better as the number of shots
increases. Another reason is that these ï¬ne-grained datasets
are not sufï¬ciently large (e.g., CUB-200 only has 6033 im-
ages), resulting in over-ï¬tting when training deep networks.
To avoid over-ï¬tting, we perform data augmentation on
the training (auxiliary) sets by cropping and horizontally
ï¬‚ipping randomly. Then, we re-train our model, i.e., DN4-
DA, on these augmented datasets but test on the original test
sets. It can be observed that our DN4-DA can obtain nearly
the best results for both 1-shot and 5-shot tasks. The ï¬ne-
grained recognition largely relies on the subtle local visual
patterns, and they can be naturally captured by the learnable
deep local descriptors emphasized in our model.

4.6. Discussion

Ablation study. To further verify that the image-to-class
measure is more effective than the image-to-image measure,
we perform an ablation study by developing two image-to-
image (IoI for short) variants of DN4. Speciï¬cally, the ï¬rst
variant named DN4-IoI-1 concatenates all local descriptors
of an image as a high-dimensional (h Ã— w Ã— d) feature vec-
tor and uses the image-to-image measure. As for the second
variant (DN4-IoI-2 for short), it keeps the local descriptors
like DN4 without concatenation. The only difference be-
tween DN4-IoI-2 and DN4 is that DN4-IoI-2 restricts the
search for the k-NN of a queryâ€™s local descriptor within
each individual support image, while DN4 can search from
one entire support class. Under the 1-shot setting, DN4-IoI-

Table 3. The results of the ablation study on miniImageNet.

Model

DN4-IoI-1
DN4-IoI-2
DN4

5-Way Accuracy (%)

1-shot

5-shot

37.39Â±0.82
51.14Â±0.79
51.24Â±0.74

50.47Â±0.66
69.52Â±0.62
71.02Â±0.64

Table 4. The 5-way 5-shot mean accuracy (%) of our DN4 by vary-
ing the value of k âˆˆ {1, 3, 5, 7} during training on miniImageNet.

Model

5-way 5-shot Accuracy (%)

k = 1

k = 3

k = 5

k = 7

DN4

71.95

71.02

70.20

68.56

2 is identical with DN4. Both variants still adopt the k-NN
search, and use k = 1 and k = 3 for 1-shot setting and
5-shot setting, respectively.

The results on miniImageNet are reported in Table 3. As
seen, DN4-IoI-1 performs clearly the worst by using the
concatenated global features with the image-to-image mea-
sure. In contrast, DN4-IoI-2 performs excellently on both
1-shot and 5-shot tasks, which veriï¬es the importance of lo-
cal descriptors and the exchangeability (within one image).
Notably, DN4 is superior to DN4-IoI-2 on the 5-shot task,
which shows that utilizing the exchangeability of visual pat-
terns within a class indeed helps to gain performance.

Inï¬‚uence of backbone networks. Besides the com-
monly used Conv-64F, we also evaluate our model by us-
ing another deeper embedding module, i.e., ResNet-256F
used by SNAIL [13] and Dynamic-Net [5]. The details
of ResNet-256F can refer to SNAIL [13]. When using
ResNet-256F as the embedding module, the accuracy of
DN4 reaches 52.44 Â± 0.80% for the 5-way 1-shot task
and 72.53 Â± 0.62% for the 5-shot task. As seen, with a
deeper backbone network, DN4 can perform better than
the case of using the shallow Conv-64F. Moreover, when
using the same ResNet-256F as the embedding module,
our DN4 (ResNet-256F) can gain 2.4% improvements over
Dynamic-Net (ResNet-256F) (i.e., 70.13 Â± 0.68%) under
the 5-shot setting (see Table 1).

Inï¬‚uence of neighbors.

In the image-to-class mod-
ule, we need to ï¬nd the k-nearest neighbors in one support
class for each local descriptor of a query image. Next, we
measure the image-to-class similarity between a query im-
age and a speciï¬c class. How to choose a suitable hyper-
parameter k is thus a key. For this purpose, we perform a
5-way 5-shot task on miniImageNet by varying the value of
k âˆˆ {1, 3, 5, 7}, and show the results in Table 4. It can be
seen that the value of k has a mild impact on performance.
Therefore, in our model, k should be selected according to
the speciï¬c task.

Inï¬‚uence of shots. The episodic training mechanism is

7266

Table 5. The 5-way K-shot mean accuracy (%) of our DN4 by
varying the number of shots (K = 1, 2, 3, 4, 5) during training on
miniImageNet. For each test setting, the best result is highlighted.

Train

1-shot
2-shot
3-shot
4-shot
5-shot

Test

1-shot

2-shot

3-shot

4-shot

5-shot

51.24
50.69
53.22
52.43
53.85

58.13
58.53
60.74
60.90
61.78

62.10
62.31
64.95
65.33
66.16

64.22
64.84
67.52
67.93
68.92

66.10
66.49
69.35
69.70
71.02

popular in current few-shot learning methods. The basic
rule is the matching condition between training and test. It
means that, in the training stage, the numbers of ways and
shots should keep consistent with those adopted in the test
stage. In other words, if we want to perform a 5-way 1-shot
task, the same 5-way 1-shot setting should be maintained
in the training stage. However, in the real training stage,
we still want to know the inï¬‚uence of mismatching con-
ditions, i.e., under-matching condition and over-matching
condition. We ï¬nd that the over-matching condition can
achieve better performance than the matching condition,
and much better than the under-matching condition.

Basically, for the under-matching condition, we use a
smaller number of shots in the training stage, and con-
versely, use a larger number of shots for the over-matching
condition. We ï¬x the number of ways but vary the num-
ber of shots during training to learn several different mod-
els. Then we test these models under different shot settings,
where the number of shots is changed but the number of
ways is ï¬xed. A 5-way K-shot (K = 1, 2, 3, 4, 5) task is
conducted on miniImageNet by using our DN4. The results
are presented in Table 5, where the entries on the diagonal
are the results of the matching condition. The results in the
upper triangle are the results of the under-matching con-
dition. Also, the lower triangle contains the results of the
over-matching condition. It can be seen that the results in
the lower triangle are better than those on the diagonal, and
the results on the diagonal are better than those in the upper
triangle. This exactly veriï¬es our statement made above.
It is also worth mentioning that if we use a 5-shot trained
model and test it on the 1-shot task, we can obtain an ac-
curacy of 53.85%. This result is quite high in this task,
and much better than 51.24% obtained by the 1-shot trained
model using our DN4 under a matching condition.

Visualization. We visualize the similarity matrices
learned by NBNN (Deep local features) and our DN4 un-
der the 5-way 5-shot setting on miniImageNet. Both of
them are image-to-class measure based models. We select
20 query images from each class (i.e., 100 query images
in total), calculate the similarity between each query image
and each class, and visualize the 5Ã—100 similarity matrices.

1
2
3
4
5

20

60

40

80
(a) NBNN

00.10.20.30.40.50.60.70.80.91

1
2
3
4
5

100

00.10.20.30.40.50.60.70.80.91

1
2
3
4
5

100

0.40.50.60.70.80.91
0.3
00.10.2

40

20
100
(c) Ground Truth

60

80

60

40

20
80
(b) Our DN4

Figure 2. Similarity matrices of NBNN (Deep local Features), our
DN4 and the ground truth on miniImageNet under the 5-way 5-
shot setting. Vertical axis denotes the ï¬ve classes in the support
set. Horizontal axis denotes 20 query images per class. The
warmer colors indicate higher similarities.

From Figure 2, it can be seen that the results of DN4 are
much closer to the ground truth than those of NBNN, which
demonstrates that the end-to-end manner is more effective.
Runtime. Although NBNN performs successfully in the
literature [1], it did not become popular. One key reason is
the high computational complexity of the nearest-neighbor
search, especially in large-scale image classiï¬cation tasks.
Fortunately, under the few-shot setting, our framework can
enjoy the excellent performance of NBNN without being
signiï¬cantly affected by its computational issue. Gener-
ally, during training for a 5-way 1-shot or 5-shot task, one
episode (batch) time is 0.31s or 0.38s with 75 or 50 query
images on a single Nvidia GTX 1080Ti GPU and a single
Intel i7-3820 CPU. During test, it will be more efï¬cient, and
only takes 0.18s for one episode. Moreover, the efï¬ciency
of our model can be further improved with optimized paral-
lel implementation.

5. Conclusions

In this paper, we revisit the local descriptor based image-
to-class measure and propose a simple and effective Deep
Nearest Neighbor Neural Network (DN4) for few-shot
learning. We emphasize and verify the importance and
value of the learnable deep local descriptors, which are
more suitable than image-level features for the few-shot
problem and can well boost the classiï¬cation performance.
We also verify that the image-to-class measure is superior to
the image-to-image measure, owing to the exchangeability
of visual patterns within a class.

Acknowledgements

This work is partially supported by the NSF awards
(Nos.
1704309, 1722847, 1813709), National NSF of
China (Nos. 61432008, 61806092), Jiangsu Natural Sci-
ence Foundation (No. BK20180326), the Collaborative
Innovation Center of Novel Software Technology and In-
dustrialization, and Innovation Foundation for Doctor Dis-
sertation of Northwestern Polytechnical University (No.
CX201814).

7267

[22] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and
D. Wierstra. Matching networks for one shot learning. In
NIPS, pages 3630â€“3638, 2016.

[23] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-
longie, and P. Perona. Caltech-UCSD Birds 200. Technical
Report CNS-TR-2010-001, California Institute of Technol-
ogy, 2010.

[24] J. Weston, S. Chopra, and A. Bordes. Memory networks.

ICLR, 1410.3916, 2015.

[25] F. S. Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M.
Hospedales. Learning to compare: Relation network for few-
shot learning. CVPR, 2018.

References

[1] O. Boiman, E. Shechtman, and M. Irani.

In defense of
nearest-neighbor based image classiï¬cation. In CVPR, pages
1â€“8. IEEE, 2008.

[2] Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei. Memory match-
In CVPR,

ing networks for one-shot image recognition.
pages 4080â€“4088, 2018.

[3] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-
In ICML,

learning for fast adaptation of deep networks.
pages 1126â€“1135, 2017.

[4] V. Garcia and J. Bruna. Few-shot learning with graph neural

networks. ICLR, 2018.

[5] S. Gidaris and N. Komodakis. Dynamic few-shot visual
In CVPR, pages 4367â€“4375,

learning without forgetting.
2018.

[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770â€“778, 2016.

[7] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ï¬ne-grained image categorization: Stanford dogs.
In CVPR Workshop, volume 2, page 1, 2011.

[8] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. ICLR, 2015.

[9] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural
In ICML Work-

networks for one-shot image recognition.
shop, volume 2, 2015.

[10] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ï¬ne-grained categorization. In ICCV Work-
shop, pages 554â€“561, 2013.

[11] W. Li, J. Xu, J. Huo, L. Wang, G. Yang, and J. Luo. Dis-
tribution consistency based covariance metric networks for
few-shot learning. In AAAI, 2019.

[12] A. Miller, A. Fisch, J. Dodge, A.-H. Karimi, A. Bordes, and
J. Weston. Key-value memory networks for directly reading
documents. EMNLP, 2016.

[13] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A sim-

ple neural attentive meta-learner. ICLR, 2018.

[14] S. Ravi and H. Larochelle. Optimization as a model for few-

shot learning. ICLR, 2017.

[15] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. IJCV, 115(3):211â€“252, 2015.

[16] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and
T. P. Lillicrap. Meta-learning with memory-augmented neu-
ral networks. In ICML, pages 1842â€“1850, 2016.

[17] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks

for few-shot learning. In NIPS, pages 4080â€“4090, 2017.

[18] S. Thrun. Lifelong learning algorithms. In Learning to learn,

pages 181â€“209. Springer, 1998.

[19] S. Thrun and L. Pratt. Learning to learn: Introduction and
overview. In Learning to learn, pages 3â€“17. Springer, 1998.
[20] E. Triantaï¬llou, R. Zemel, and R. Urtasun. Few-shot learn-
In NIPS, pages

ing through an information retrieval lens.
2255â€“2265, 2017.

[21] R. Vilalta and Y. Drissi. A perspective view and survey of

meta-learning. AIR, 18(2):77â€“95, 2002.

7268

