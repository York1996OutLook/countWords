Multispectral and Hyperspectral Image Fusion by MS/HS Fusion Net

Qi Xie1, Minghao Zhou1, Qian Zhao1, Deyu Meng1,âˆ—, Wangmeng Zuo2, Zongben Xu1

1Xiâ€™an Jiaotong University; 2Harbin Institute of Technology

xq.liwu@stu.xjtu.edu.cn

woshizhouminghao@stu.xjtu.edu.cn timmy.zhaoqian@gmail.com

dymeng@mail.xjtu.edu.cn

wmzuo@hit.edu.cn

zbxu@mail.xjtu.edu.cn

Abstract

Hyperspectral imaging can help better understand the
characteristics of different materials, compared with tradi-
tional image systems. However, only high-resolution mul-
tispectral (HrMS) and low-resolution hyperspectral (LrHS)
images can generally be captured at video rate in practice.
In this paper, we propose a model-based deep learning ap-
proach for merging an HrMS and LrHS images to generate
a high-resolution hyperspectral (HrHS) image. In speciï¬c,
we construct a novel MS/HS fusion model which takes the
observation models of low-resolution images and the low-
rankness knowledge along the spectral mode of HrHS im-
age into consideration. Then we design an iterative algo-
rithm to solve the model by exploiting the proximal gradi-
ent method. And then, by unfolding the designed algorithm,
we construct a deep network, called MS/HS Fusion Net,
with learning the proximal operators and model parameters
by convolutional neural networks. Experimental results on
simulated and real data substantiate the superiority of our
method both visually and quantitatively as compared with
state-of-the-art methods along this line of research.

1. Introduction

A hyperspectral (HS) image consists of various bands of
images of a real scene captured by sensors under different
spectrums, which can facilitate a ï¬ne delivery of more faith-
ful knowledge under real scenes, as compared to traditional
images with only one or a few bands. The rich spectra of HS
images tend to signiï¬cantly beneï¬t the characterization of
the imaged scene and greatly enhance performance in dif-
ferent computer vision tasks, including object recognition,
classiï¬cation, tracking and segmentation [10, 37, 35, 36].

In real cases, however, due to the limited amount of inci-
dent energy, there are critical tradeoffs between spatial and
spectral resolution. Speciï¬cally, an optical system usually
can only provide data with either high spatial resolution but
a small number of spectral bands (e.g., the standard RGB
image) or with a large number of spectral bands but re-
duced spatial resolution [23]. Therefore, the research issue

âˆ—Corresponding author.

Figure 1. (a)(b) The observation models for HrMS and LrHS im-
ages, respectively. (c) Learning bases Ë†Y by deep network, with
HrMS Y and LrHS Z as the input of the network. (d) The HrHSI
X can be linearly represented by Y and to-be-estimated Ë†Y , in a
formulation of X â‰ˆ Y A + Ë†Y B, where the rank of X is r.

on merging a high-resolution multispectral (HrMS) image
and a low-resolution hyperspectral (LrHS) image to gener-
ate a high-resolution hyperspectral (HrHS) image, known
as MS/HS fusion, has attracted great attention [47].

The observation models for the HrMS and LrHS images

are often written as follows [12, 24, 25]:

Y = XR + Ny,
Z = CX + Nz,

(1)
(2)
where X âˆˆ RHW Ã—S is the target HrHS image1 with H, W
and S as its height, width and band number, respectively,
Y âˆˆ RHW Ã—s is the HrMS image with s as its band number
(s < S), Z âˆˆ RhwÃ—S is the LrHS image with h, w and S
as its height, width and band number (h < H, w < W ),
R âˆˆ RSÃ—s is the spectral response of the multispectral sen-
sor as shown in Fig. 1 (a), C âˆˆ RhwÃ—HW is a linear op-
erator which is often assumed to be composed of a cyclic
convolution operator Ï† and a down-sampling matrix D as
shown in Fig. 1 (b), Ny and Nz are the noises contained in

1The target HS image can also be written as tensor X âˆˆ RHÃ—W Ã—S .
We also denote the folding operator for matrix to tensor as: fold(X) = X .

1585

+ZZXYRDeep netâˆˆâ„Ã—Ã—Ã—âˆˆâ„Ã—foldâˆˆâ„Ã—Ã—âˆˆâ„Ã—(c)(d)â„âˆ’Ã—âˆˆâˆˆâ„Ã—Ã—(âˆ’)foldâˆˆâ„Ã—Ã—Ã—âˆˆâ„Ã—âˆˆâ„Ã—Ã—(a)(b)âŠ—+downsamplingband linearcombinationâˆˆâ„Ã—YYXAYAYBBâˆˆâ„Ã—CHrMS and LrHS images, respectively. Many methods have
been designed based on (1) and (2), and achieved good per-
formance [40, 14, 24, 25].

method that integrates the observation models and image
prior learning into a single network architecture. This work
mainly contains the following three-fold contributions:

Since directly recovering the HrHS image X is an ill-
posed inverse problem, many techniques have been ex-
ploited to recover X by assuming certain priors on it. For
example, [54, 2, 11] utilize the prior knowledge of HrHS
that its spatial information could be sparsely represented un-
der a dictionary trained from HrMS. Besides, [27] assumes
the local spatial smoothness prior on the HrHS image and
uses total variation regularization to encode it in their opti-
mization model. Instead of exploring spatial prior knowl-
edge from HrHS, [52] and [26] assume more intrinsic spec-
tral correlation prior on HrHS, and use low-rank techniques
to encode such prior along the spectrum to reduce spectral
distortions. Albeit effective for some applications, the ratio-
nality of these techniques relies on the subjective prior as-
sumptions imposed on the unknown HrHS to be recovered.
An HrHS image collected from real scenes, however, could
possess highly diverse conï¬gurations both along space and
across spectrum. Such conventional learning regimes thus
could not always ï¬‚exibly adapt different HS image struc-
tures and still have room for performance improvement.

Methods based on Deep Learning (DL) have outper-
formed traditional approaches in many computer vision
tasks [34] in the past decade, and have been introduced to
HS/MS fusion problem very recently [28, 30]. As com-
pared with conventional methods, these DL based ones are
superior in that they need fewer assumptions on the prior
knowledge of the to-be-recovered HrHS, while can be di-
rectly trained on a set of paired training data simulating the
network inputs (LrHS&HrMS images) and outputs (HrHS
images). The most commonly employed network structures
include CNN [7], 3D CNN [28], and residual net [30]. Like
other image restoration tasks where DL is successfully ap-
plied to, these DL-based methods have also achieved good
resolution performance for MS/MS fusion task.

However, the current DL-based MS/HS fusion meth-
ods still have evident drawbacks. The most critical one is
that these methods use general frameworks for other tasks,
which are not speciï¬cally designed for MS/HS fusion. This
makes them lack interpretability speciï¬c to the problem.
In particular, they totally neglect the observation models
(1) and (2) [28, 30], especially the operators R and C,
which facilitate an understanding of how LrHS and HrMs
are generated from the HrHS. Such understanding, how-
ever, should be useful for calculating HrHS images. Besides
this generalization issue, current DL methods also neglect
the general prior structures of HS images, such as spectral
low-rankness. Such priors are intrinsically possessed by all
meaningful HS images, which implies that DL-based meth-
ods still have room for further enhancement.

In this paper, we propose a novel deep learning-based

Firstly, we propose a novel MS/HS fusion model, which
not only takes the observation models (1) and (2) into con-
sideration but also exploits the approximate low-rankness
prior structure along the spectral mode of the HrHS im-
age to reduce spectral distortions [52, 26]. Speciï¬cally, we
prove that if and only if observation model (1) can be sat-
isï¬ed, the matrix of HrHS image X can be linearly rep-
resented by the columns in HrMS matrix Y and a to-be-
estimated matrix Ë†Y , i.e., X = Y A + Ë†Y B with coefï¬cient
matrices A and B. One can see Fig. 1 (d) for easy under-
standing. We then construct a concise model by combining
the observation model (2) and the linear representation of
X. We also exploit the proximal gradient method [3] to
design an iterative algorithm to solve the proposed model.

Secondly, we unfold this iterative algorithm into a deep
network architecture, called MS/HS Fusion Net or MHF-
net, to implicitly learn the to-be-estimated Ë†Y , as shown in
Fig. 1 (c). After obtaining Ë†Y , we can then easily achieve X
with Y and Ë†Y . To the best of our knowledge, this is the ï¬rst
deep-learning-based MS/HS fusion method that fully con-
siders the intrinsic mechanism of the MS/HS fusion prob-
lem. Moreover, all the parameters involved in the model can
be automatically learned from training data in an end-to-end
manner. This means that the spatial and spectral responses
(R and C) no longer need to be estimated beforehand as
most of the traditional non-DL methods did, nor to be fully
neglected as current DL methods did.

Thirdly, we have collected or realized current state-of-
the-art algorithms for the investigated MS/HS fusion task,
and compared their performance on a series of synthetic and
real problems. The experimental results comprehensively
substantiate the superiority of the proposed method, both
quantitatively and visually.

In this paper, we denote scalar, vector, matrix and ten-
sor in non-bold case, bold lower case, bold upper case and
calligraphic upper case letters, respectively.

2. Related work

2.1. Traditional methods

The pansharpening technique in remote sensing is
closely related to the investigated MS/HS problem. This
task aims to obtain a high spatial resolution MS image by
the fusion of a MS image and a wide-band panchromatic
image. A heuristic approach to perform MS/HS fusion is to
treat it as a number of pansharpening sub-problems, where
each band of the HrMS image plays the role of a panchro-
matic image. There are mainly two categories of pansharp-
ening methods: component substitution (CS) [5, 17, 1] and
multiresolution analysis (MRA) [20, 21, 4, 33, 6]. These
methods always suffer from the high spectral distortion,

1586

since a single panchromatic image contains little spectral
information as compared with the expected HS image.

In the last few years, machine learning based meth-
ods have gained much attention on MS/HS fusion problem
[54, 2, 11, 14, 52, 48, 26, 40]. Some of these methods used
sparse coding technique to learn a dictionary on the patches
across a HrMS image, which delivers spatial knowledge of
HrHS to a certain extent, and then learn a coefï¬cient ma-
trix from LrHS to fully represent the HrHS [54, 2, 11, 40].
Some other methods, such as [14], use the sparse matrix fac-
torization to learn a spectral dictionary for LrHS images and
then construct HrMS images by exploiting both the spectral
dictionary and HrMS images. The low-rankness of HS im-
ages can also be exploited with non-negative matrix factor-
ization, which helps to reduce spectral distortions and en-
hances the MS/HS fusion performance [52, 48, 26]. The
main drawback of these methods is that they are mainly de-
signed based on human observations and strong prior as-
sumptions, which may not be very accurate and would not
always hold for diverse real world images.

2.2. Deep learning based methods

Recently, a number of DL-based pansharpening meth-
ods were proposed by exploiting different network struc-
tures [15, 22, 42, 43, 29, 30, 32]. These methods can be
easily adapted to MS/HS fusion problem. For example,
very recently, [28] proposed a 3D-CNN based MS/HS fu-
sion method by using PCA to reduce the computational
cost. This method is usually trained with prepared train-
ing data. The network inputs are set as the combination
of HrMS/panchromatic images and LrHS/multispectral im-
ages (which is usually interpolated to the same spatial size
as HrMS/panchromatic images in advance), and the outputs
are the corresponding HrHS images. The current DL-based
methods have been veriï¬ed to be able to attain good per-
formance. They, however, just employ networks assembled
with some off-the-shelf components in current deep learn-
ing toolkits, which are not speciï¬cally designed against the
investigated problem. Thus the main drawback of this tech-
nique is the lack of interpretability to this particular MS/HS
fusion task. In speciï¬c, both the intrinsic observation model
(1), (2) and the evident prior structures, like the spectral cor-
relation property, possessed by HS images have been ne-
glected by such kinds of â€œblack-boxâ€ deep model.

3. MS/HS fusion model

In this section, we demonstrate the proposed MS/HS fu-

sion model in detail.

3.1. Model formulation

We ï¬rst introduce an equivalent formulation for observa-

tion model (1). Speciï¬cally, we have following theorem2.

Theorem 1. For any X âˆˆ RHW Ã—S and ËœY âˆˆ RHW Ã—s, if
rank(X) = r > s and rank( ËœY ) = s, then the following two
statements are equivalent to each other:
(a) There exists an R âˆˆ RSÃ—s, subject to,

ËœY = XR.

(3)

(b) There exist A âˆˆ RsÃ—S, B âˆˆ R(râˆ’s)Ã—S and Ë†Y âˆˆ
RHW Ã—(râˆ’s), subject to,

X = ËœY A + Ë†Y B.

(4)

In reality, the band number of an HrMS image is usually
not large, which makes it full rank along spectral mode. For
example, the most commonly used HrMS images, RGB im-
ages, contain three bands, and their rank along the spectral
mode is usually also three. Thus, by letting ËœY = Y âˆ’ Ny
where Y is the observed HrMS in (1), it is easy to ï¬nd that
ËœY and X satisfy the conditions in Theorem 1. Then the
observation model (1) is equivalent3 to

X = Y A + Ë†Y B + Nx,

(5)

where Nx = âˆ’NyA is caused by the noise contained in the
HrMS image. In (5), [Y , Ë†Y ] can be viewed as r bases that
represent columns in X with coefï¬cients matrix [A; B] âˆˆ
RrÃ—S, where only the r âˆ’ s bases in Ë†Y are unknown. In
addition, we can derive the following corollary:

Corollary 1. For any ËœY âˆˆ RHW Ã—s, ËœZ âˆˆ RhwÃ—S, C âˆˆ
RhwÃ—HW , if rank( ËœY ) = s and rank( ËœZ) = r > s, then the
following two statements are equivalent to each other:
(a) There exist X âˆˆ RHW Ã—S and R âˆˆ RSÃ—s, subject to,

ËœY = XR, ËœZ = CX,

rank(X) = r.

(6)

(b) There exist A âˆˆ RsÃ—S, r > s, B âˆˆ R(râˆ’s)Ã—S and
Ë†Y âˆˆ RHW Ã—(râˆ’s), subject to,

ËœZ = C (cid:16) ËœY A + Ë†Y B(cid:17) .

(7)

Let ËœZ = Z âˆ’ Nz, where Z is the observed LrHS image
in (2). It is easy to ï¬nd that, when being viewed as equations
of the to-be-estimated X, R and C, the observation model
(1) and model (2) are equivalent to the following equation
of Ë†Y , A, B and C:

Z = C (cid:16)Y A + Ë†Y B(cid:17) + N ,

(8)

where N = Nz âˆ’ CNyA denotes the noise contained in
HrMS and LrHS image. Then, we can design the following
MS/HS fusion model:

min

Ë†Y (cid:13)(cid:13)(cid:13)

C (cid:16)Y A + Ë†Y B(cid:17) âˆ’ Z(cid:13)(cid:13)(cid:13)

2

F

+ Î»f (cid:16) Ë†Y (cid:17) ,

(9)

3We say two equation are equivalent to each other if the solution of

2All proofs are presented in supplementary material.

one equation can easily achieve by solving the other one

1587

where Î» is a trade-off parameter, and f (Â·) is a regularization
function. We adopt regularization on the to-be-estimated
bases in Ë†Y , rather than on X as in conventional, to facil-
itate an entire preservation of spatial details4 contained in
the known HrMS bases (Y ) for representing X.

It should be noted that for data obtained with the same
sensors, A, B and C are ï¬xed. This means that they can be
learned from the training data. In the later sections we will
show how to learn them with a deep network.

3.2. Model optimization

We now solve (9) using a proximal gradient algorithm

[3], which iteratively updates Ë†Y by calculating

Ë†Y (k+1) = arg min

Ë†Y

Q(cid:16) Ë†Y , Ë†Y (k)(cid:17) ,

(10)

where Ë†Y (k) is the updating result after k âˆ’ 1 iterations, k =
1, 2, Â· Â· Â· , K, and Q( Ë†Y , Ë†Y (k)) is a quadratic approximation
[3] deï¬ned as:

Q(cid:16)Ë†Y , Ë†Y (k)(cid:17) = g (cid:16)Ë†Y (k)(cid:17)+D Ë†Y âˆ’ Ë†Y (k), âˆ‡g (cid:16)Ë†Y (k)(cid:17)E

+

1

2Î· (cid:13)(cid:13)(cid:13)

Ë†Y âˆ’ Ë†Y (k)(cid:13)(cid:13)(cid:13)

2

F

+ Î»f (cid:16) Ë†Y (cid:17) ,

(11)

where g( Ë†Y (k)) = kC(Y A + Ë†Y (k)B) âˆ’ Zk2
the role of stepsize.

F and Î· plays

It is easy to prove that the problem (10) is equivalent to:

min
Ë†Y

1

2 (cid:13)(cid:13)(cid:13)

Ë†Y âˆ’(cid:16)Ë†Y (k) âˆ’Î·âˆ‡g (cid:16) Ë†Y (k)(cid:17)(cid:17)(cid:13)(cid:13)(cid:13)

2

F

+Î»Î·f (cid:16) Ë†Y (cid:17) . (12)

For many kinds of regularization terms, the solution of Eq.
(12) is usually in closed-form [8], written as:

Ë†Y (k+1) = proxÎ»Î· (cid:16)Ë†Y (k) âˆ’Î·âˆ‡g (cid:16) Ë†Y (k)(cid:17)(cid:17) ,

(13)

where proxÎ»Î·(Â·) is a proximal operator dependent on f (Â·).
Since âˆ‡g( Ë†Y (k)) = C T (C(Y A + Ë†Y (k)B) âˆ’ Z)BT , we
can obtain the ï¬nal updating rule for Ë†Y :

Ë†Y (k+1) = proxÎ»Î·(cid:16)Ë†Y (k) âˆ’Î·C T(cid:16)C(cid:16)Y A + Ë†Y (k)B(cid:17)âˆ’Z(cid:17)BT(cid:17).
(14)

We can then unfold this algorithm into a deep network.

4. MS/HS fusion net

Based on the above algorithm, we build a deep neural
network for MS/HS fusion by unfolding all steps of the al-
gorithm as network layers. This technique has been widely
utilized in various computer vision tasks and has been sub-
stantiated to be effective in compressed sensing, dehazing,
deconvolution, etc. [44, 45, 53]. The proposed network is a

4 Directly imposing regularization terms on X, e.g., TV norm, will

lead to losing of details like the sharp edge and lines in X.

Iterative optimization algorithm

Network design

For ğ‘˜ğ‘˜ = 1:ğ¾ğ¾ do: 
ğ‘¿ğ‘¿ğ‘˜ğ‘˜ =ğ’€ğ’€ğ‘¨ğ‘¨ +ï¿½ğ’€ğ’€(ğ‘˜ğ‘˜)ğ‘©ğ‘©
ğ‘¬ğ‘¬(ğ‘˜ğ‘˜) =ğ‘ªğ‘ªğ‘¿ğ‘¿ğ‘˜ğ‘˜ âˆ’ğ’ğ’
ğ‘®ğ‘®ğ‘˜ğ‘˜ =ğœ‚ğœ‚ğ‘ªğ‘ªğ‘‡ğ‘‡ğ‘¬ğ‘¬(ğ‘˜ğ‘˜)ğ‘©ğ‘©ğ‘‡ğ‘‡
ğ’€ğ’€ğ‘˜ğ‘˜+1 = proxğœ†ğœ†ğœ†ğœ†ï¿½ğ’€ğ’€(ğ‘˜ğ‘˜)âˆ’ğ‘®ğ‘®ğ’Œğ’Œ

In stage ğ‘˜ğ‘˜ = 1:ğ¾ğ¾ of the network do:
ğ’³ğ’³ğ‘˜ğ‘˜ =ğ’´ğ’´ Ã—3ğ‘¨ğ‘¨ğ‘»ğ‘» +ï¿½ğ’´ğ’´(ğ‘˜ğ‘˜) Ã—3ğ‘©ğ‘©ğ‘»ğ‘»
â„°ğ‘˜ğ‘˜ = downSampleğœƒğœƒğ‘‘ğ‘‘ğ‘˜ğ‘˜ ğ’³ğ’³ğ‘˜ğ‘˜ âˆ’ğ’µğ’µ
ğ’¢ğ’¢ğ‘˜ğ‘˜ =ğœ‚ğœ‚â‹… upSampleğœƒğœƒğ‘‘ğ‘‘ğ‘˜ğ‘˜ â„°ğ‘˜ğ‘˜ Ã—3ğ‘©ğ‘©
ï¿½ğ’´ğ’´ğ‘˜ğ‘˜+1 = proxNetğœƒğœƒğ‘ğ‘ğ‘˜ğ‘˜
ï¿½ğ’´ğ’´ğ‘˜ğ‘˜ âˆ’ğ’¢ğ’¢ğ‘˜ğ‘˜

Figure 2. An illustration of relationship between the algorithm
with matrix form and the network structure with tensor form.

structure of K stages, corresponding to K iterations in the
iterative algorithm for solving Eq. (9), as shown in Fig. 3
(a) and (b). Each stage takes the HrMS image Y , LrHS im-
age Z, and the output of the previous stage Ë†Y , as inputs,
and outputs an updated Ë†Y to be the input of next layer.

4.1. Network design

Algorithm unfolding. We ï¬rst decompose the updating
rule (14) into the following equivalent four sequential parts:

X (k) = Y A + Ë†Y (k)B,

E(k) = CX (k) âˆ’ Z,
G(k) = Î·C T E(k)BT ,

Ë†Y (k+1) = proxÎ»Î· (cid:16) Ë†Y (k) âˆ’ G(k)(cid:17) .

(15)

(16)

(17)

(18)

In the network framework, we use the images with their
tensor formulations (X âˆˆ RHÃ—W Ã—S, Y âˆˆ RHÃ—W Ã—s and
Z âˆˆ RhÃ—wÃ—S) instead of their matrix forms to protect their
spatial structure knowledge and make the network structure
(in tensor form) easily designed. We then design a network
to approximately perform the above operations in tensor
version. Refer to Fig. 2 for easy understanding.

In tensor version, Eq. (15) can be easily performed by
the two multiplications between a tensor and a matrix along
the 3rd mode of the tensor. Speciï¬cally, in the TensorFlow5
framework, multiplying a tensor with an matrix in RmÃ—n
along the channel mode can be easily performed by using
the 2D convolution function with a associated 1Ã—1Ã— mÃ— n
tensor. Thus, we can perform the tensor version of (15) by:

X (k) = Y Ã—3 AT + Ë†Y (k) Ã—3 BT ,

(19)

where Ã—3 denotes the mode-3 multiplication for tensor6.

In Eq. (16), the matrix C represents the spatial down-
sampling operator, which can be decomposed into 2D con-
volutions and down-sampling operators [12, 24, 25]. Thus,
we perform the tensor version of (16) by:

E (k) = downSample

d (cid:16)X (k)(cid:17) âˆ’ Z,

Î¸(k)

(20)

5https://tensorflow.google.cn/
6For a tensor U âˆˆ RIÃ—JÃ—K with uijk as its elements, and V âˆˆ
RKÃ—L with vkl as its elements, let W = U Ã—3 V , the elements of W are
wijl = PK

k=1 uijkvlk. Besides, W = U Ã—3 V â‡” W = U V T .

1588

Y

T
Ã—3Y A

X (1)

G(1)

Y (2)

Y

Y

Y

Y

Z

Z

Z

Z

X (k) = Y Ã—3AT+ YË†(k)Ã—3BT

E (k) = downSample

Î¸(k)
d

(cid:16)

X (k)

(cid:17) âˆ’ Z

A

G(k) = Î· Â· upSample

u (cid:16)E (k)(cid:17) Ã—3 B

Î¸(k)

Y^(k+1) = proxNet

p (cid:16)Y^(k) âˆ’ G(k)(cid:17)

Î¸(k)

X

(a)

(b)

TB

X (1)

E(1)

+âˆ’

Z

(c)

Y (k)

Y (k)

B
Ã—3
 

T

X (k)

 

 G(k)

Y (k

+1)

Y (K)

Y (K)

Ã—3B

T

X (K)

X

+âˆ’

T

B

X (k)

E(k)

+âˆ’

Z

B

+

+

Y

Y

 
 
Ã—3

AT
 

A

(d)

+

+

T

B

A

Y

n
o
i
t
c
n
u
F
 
s
s
o
L

X (k

E (K)

+âˆ’

Z

(e)

Figure 3. (a) The proposed network with K stages implementing K iterations in the iterative optimization algorithm, where the kth stage
is denoted as Sk, (k = 1, 2, Â· Â· Â· , K). (b) The ï¬‚owchart of kth (k < K) stage. (c)-(e) Illustration of the ï¬rst, kth (1 < k < K) and ï¬nal
stage of the proposed network, respectively. When setting Ë†Y (k) = 0, Sk is equivalent to S1.

d

Î¸(k)

where E (k) is an h Ã— w Ã— S tensor, downSample
(Â·) is the
downsampling network consisting of 2D channel-wise con-
volutions and average pooling operators, and Î¸(k)
denotes
ï¬lters involved in the operator at the kth stage of network.
In Eq. (17), the transposed matrix C T represents a spa-
tial upsampling operator. This operator can be easily per-
formed by exploiting the 2D transposed convolution [9],
which is the transposition of the downsampling operator.
By exploiting the 2D transposed convolution with the ï¬lter
size as (20), we can approach (17) in the network by:

d

G(k) = Î· Â· upSample

u (cid:16)E (k)(cid:17) Ã—3 B,

Î¸(k)

(21)

Î¸(k)

where G(k) âˆˆ RHÃ—W Ã—S, upSample
(Â·) is the spacial
upsampling network consisting of transposed convolutions
and Î¸(k)
u denotes the corresponding ï¬lters in the kth stage.
In Eq. (18), prox(Â·) is a to-be-decided proximal operator.
We adopt the deep residual network (ResNet) [13] to learn
this operator. We then represent (18) in our network as:

u

Ë†Y (k+1) = proxNet

p (cid:16) Ë†Y (k) âˆ’ G(k)(cid:17) ,

Î¸(k)

(22)

Î¸(k)

where proxNet

(Â·) is a ResNet which represents the prox-
imal operator in our algorithm and the parameters involved
in the ResNet at the kth stage are denoted by Î¸(k)
p .

p

With Eq. (19)-(22), we can now construct the stages in
the proposed network. Fig. 3 (b) shows the ï¬‚owchart of a
single stage of the proposed network.

Normal stage. In the ï¬rst stage, we simply set Ë†Y (1) =
0. By exploiting (19)-(22), we can obtain the ï¬rst network
stage as shown in Fig. 3 (c). Fig. 3 (d) shows the kth stage
(1 < k < K) of the network obtained by utilizing (19)-(22).

Final stage. As shown in Fig. 3(e), in the ï¬nal stage,
we can approximately generate the HrHS image by (19).
Note that X (K) (the unfolding matrix of X (K)) has been
intrinsically encoded with low-rank structure. Moreover,
according to Theorem 1, there exists an R âˆˆ RSÃ—s, s.t.,
Y = X (K)R, which satisï¬es the observation model (1).

However, HrMS images Y are usually corrupted with
slight noise in reality, and there is a little gap between the
low rank assumption and the real situation. This implies that
X (K) is not exactly equivalent to the to-be-estimated HrHS
image. Therefore, as shown in Fig. 3 (e), in the ï¬nal stage
of the network, we add a ResNet on X (K) to reduce the gap
between the to-be-estimated HrHS image and X (K):

Ë†X = resNetÎ¸r (cid:16)X (K)(cid:17) .

(23)

In this way, we design an end-to-end training architec-
ture, called MS/HS fusion net or MHFnet. We denote the
function of entire net as Ë†X = MHFnet (Y, Z, Î˜), where Î˜
represents all parameters involved in the network. Please
refer to supplementary material for more details.

4.2. Network training

Training loss. As shown in Fig. 3 (e), the training loss

for each training image is deï¬ned as following:

L = k Ë†X âˆ’X k2

F +Î±XK

k=1

kX (k)âˆ’X k2

F +Î²kE (K)k2

F , (24)

where Ë†X and X (k) are the ï¬nal and per-stage outputs of the
proposed network, Î± and Î² are two trade-off parameters7.

7We set Î± and Î² with small values (0.1 and 0.01, respectively) in all

experiments, to make the ï¬rst term play a dominant role.

1589

Estimate down  sampling operator

Training sample Training data

Down 
sampling

Down 
sampling

Original data

Original sample

Y

Z

X

Input
samples

Reference

samples

Figure 4. Illustration of how to create the training data when HrHS
images are unavailable.

The ï¬rst term is the pixel-wise L2 distance between the out-
put of the proposed network and the ground truth X , which
is the main component of our loss function. The second
term is the pixel-wise L2 distance between the output X (k)
and the ground truth X in each stage. This term helps ï¬nd
the correct parameters in each stage, since appropriate Ë†Y (k)
Ë†X (k) â‰ˆ X . The ï¬nal term is the pixel-wise
would lead to
L2 distance of the residual of observation model (2) for the
ï¬nal stage of the network.

Training data. For simulation data and real data with
available ground-truth HrHS images, we can easily use the
paired training data {(Yn, Zn), Xn}N
n=1 to learn the param-
eters in the proposed MHF-net. Unfortunately, for real data,
HrHS images Xns are sometimes unavailable. In this case,
we use the method proposed in [30] to address this prob-
lem, where the Wald protocol [50] is used to create the
training data as shown in Fig. 4. We downsample both
HrMS images and LrHS images in space, so that the orig-
inal LrHS images can be taken as references for the down-
sampled data. Please refer to supplementary material for
more details.

Implementation details. We implement and train our
network using TensorFlow framework. We use Adam opti-
mizer to train the network for 50000 iterations with a batch
size of 10 and a learning rate of 0.0001. The initializa-
tions of the parameters and other implementation details are
listed in supplementary materials.

5. Experimental results

We ï¬rst conduct simulated experiments to verify the
mechanism of MHF-net quantitatively. Then, experimen-
tal results on simulated and real data sets are demonstrated
to evaluate the performance of MHF-net.

Evaluation measures. Five quantitative picture quality
indices (PQI) are employed for performance evaluation, in-
cluding peak signal-to-noise ratio (PSNR), spectral angle
mapper (SAM) [49], erreur relative globale adimension-
nelle de synth`ese (ERGAS [38]), structure similarity (SSIM
[39]), feature similarity (FSIM [51]). SAM calculates the
average angle between spectrum vectors of the target MSI
and the reference one across all spatial positions and ER-
GAS measures ï¬delity of the restored image based on the
weighted sum of MSE in each band. PSNR, SSIM and
FSIM are conventional PQIs. They evaluate the similarity
between the target and the reference images based on MSE

Table 1. Average performance of the competing methods over 12
testing samples of CAVE data set with respect to 5 PQIs.

ResNet

32.25
19.093
141.28
0.865
0.966

MHF-net with (K, L)

(4, 9)
36.15
9.206
92.94
0.948
0.974

(7, 5)
36.61
8.636
88.56
0.955
0.975

(10, 4)
36.85
7.587
86.53
0.960
0.975

(13, 2)
37.23
7.298
81.87
0.962
0.976

PSNR
SAM
ERGA
SSIM
FSIM

and structural consistency, perceptual consistency, respec-
tively. The smaller ERGAS and SAM are, and the larger
PSNR, SSIM and FSIM are, the better the fusion result is.

5.1. Model veriï¬cation with CAVE data

To verify the efï¬ciency of the proposed MHF-net, we
ï¬rst compare the performance of MHF-net with different
settings on the CAVE Multispectral Image Database [46]8.
The database consists of 32 scenes with spatial size of
512Ã—512, including full spectral resolution reï¬‚ectance data
from 400nm to 700nm at 10nm steps (31 bands in total). We
generate the HrMS image (RGB image) by integrating all
the ground truth HrHS bands with the same simulated spec-
tral response R, and generate the LrHS images via down-
sampling the ground-truth with a factor of 32 implemented
by averaging over 32 Ã— 32 pixel blocks as [2, 16].

To prepare samples for training, we randomly select 20
HS images from CAVE database and extract 96 Ã— 96 over-
lapped patches from them as reference HrHS images for
training. Then the utilized HrHS, HrMS and LrHS images
are of size 96 Ã— 96 Ã— 31, 96 Ã— 96 Ã— 3 and 3 Ã— 3 Ã— 31, re-
spectively. The remaining 12 HS images of the database are
used for validation, where the original images are treated as
ground truth HrHS images, and the HrMS and LrHS images
are generated similarly as the training samples.

We compare the performance of the proposed MHF-net
under different stage number K. In order to make the com-
petition fair, we adjust the level number L of the ResNet
for each situation, so that the total level
used in proxNet

Î¸(k)

p

number of the network in each setting is similar to each
other. Moreover, to better verify the efï¬ciency of the pro-
posed network, we implement another network for compe-
tition, which only uses the ResNet in (22) and (23) without
using other structures in MHF-net. This method is simply
denoted as â€œResNetâ€. In this method, we set the input as
[Y, Zup], where Zup is obtained by interpolating the LrHS
image Z (using a bicubic ï¬lter) to the dimension of Y as
[28] did. We set the level number of ResNet to be 30.

Table 1 shows the average results over 12 testing HS im-
ages of two DL methods in different settings. We can ob-
serve that MHF-net with more stages, even with fewer net
levels in total, can signiï¬cantly lead to better performance.
We can also observe that the MHF-net can achieve better

8http://www.cs.columbia.edu/CAVE/databases/

1590

1

(a) RGB & LrHS

(b) Ground truth

(c) FUSE

(d) ICCV2015

(e) GLP-HS

(f) SFIM-HS

(g) GSA

(h) CNMF

(i) M-FUSE

(j) SAMF

(k) PNN

(l) 3D-CNN

(m) ResNet

(n) MHF-net

0

Figure 5. (a) The simulated RGB (HrMS) and LrHS (left bottom) images of chart and staffed toy, where we display the 10th (490nm) band
of the HS image. (b) The ground-truth HrHS image. (c)-(n) The results obtained by 12 comparison methods, with two demarcated areas
zoomed in 4 times for easy observation.

Table 2. Average performance of the competing methods over 12
testing images of CAVE date set with respect to 5 PQIs.

FUSE

ICCV15
GLP-HS
SFIM-HS

GSA
CNMF
M-FUSE
SASFM

PNN

3D-CNN
ResNet
MHF-net

PSNR
30.95
32.94
33.07
31.86
33.78
33.59
32.11
26.59
32.42
34.82
32.25
37.23

SAM
13.07
10.18
11.58
7.63
11.56
8.22
8.82
11.25
14.73
8.96
16.14
7.30

ERGAS
188.72
131.94
126.04
147.41
122.50
122.12
151.97
362.70
134.51
109.20
141.28
81.87

SSIM
0.842
0.919
0.891
0.914
0.884
0.929
0.914
0.799
0.884
0.937
0.865
0.962

FSIM
0.933
0.961
0.942
0.932
0.959
0.964
0.947
0.916
0.956
0.971
0.966
0.976

results than ResNet (about 5db in PSNR), while the main
difference between MHF-net and ResNet is our proposed
stage structure in the network. These results show that the
proposed stage structure in MHF-net, which introduces in-
terpretability speciï¬cally to the problem, can indeed help
enhance the performance of MS/HS fusion.

5.2. Experiments with simulated data

We then evaluate MHF-net on simulated data in compar-

ison with state-of-art methods.

Comparison methods. The comparison methods in-
clude: FUSE [41]9, ICCV15 [18]10, GLP-HS [31]11, SFIM-
HS [19]11, GSA [1]11, CNMF [48]12, M-FUSE [40]13 and
SASFM [14]14, representing the state-of-the-art traditional
methods; PNN [30] and 3D-CNN [28] representing the
state-of-the-art DL-based methods. We also compare the
proposed MHF-net with the implemented ResNet method.
Performance comparison with CAVE data. With the
same experiment setting as in the previous section, we com-
pare the performance of all competing methods on the 12

9http://wei.perso.enseeiht.fr/publications.html
10https://github.com/lanha/SupResPALM
11http://openremotesensing.net/knowledgebase/

hyperspectral-and-multispectral-data-fusion/

12http://naotoyokoya.com/Download.html
13https://github.com/qw245/BlindFuse
14We write the code by ourselves.

testing HS images (K = 13 and L = 2 in MHF-net). Table
2 lists the average performance over all testing images of all
comparison methods. From the table, it is seen that the pro-
posed MHF-net method can signiï¬cantly outperform other
competing methods with respect to all evaluation measures.
Fig. 5 shows the 10-th band (490nm) of the HS image chart
and staffed toy obtained by the completing methods. It is
easy to observe that the proposed method performs better
than other competing ones, in the better recovery of both
ï¬ner-grained textures and coarser-grained structures. More
results are depicted in the supplementary material.

Performance comparison with Chikusei data. The
Chikusei data set [47]15 is an airborne HS image taken over
Chikusei, Ibaraki, Japan, on 29 July 2014. The data set is of
size 2517 Ã— 2335 Ã— 128 with the spectral range from 0.36
to 1.018. We view the original data as the HrHS image and
simulate the HrMS (RGB image) and LrMS (with a factor
of 32) image in the similar way as the previous section.

We select a 500 Ã— 2210-pixel-size image from the top
area of the original data for training, and extract 96 Ã— 96
overlapped patches from the training data as reference
HrHS images for training. The input HrHS, HrMS and
LrHS samples are of sizes 96 Ã— 96 Ã— 128, 96 Ã— 96 Ã— 3 and
3 Ã— 3 Ã— 128, respectively. Besides, from remaining part of
the original image, we extract 16 non-overlap 448 Ã— 544 Ã—
128 images as testing data. More details about the experi-
mental setting are introduced in supplementary material.

Table 3 shows the average performance over 16 testing
images of all competing methods. It is easy to observe that
the proposed method signiï¬cantly outperforms other meth-
ods with respect to all evaluation measures. Fig. 6 shows
the composite images of a test sample obtained by the com-
peting methods, with bands 70-100-36 as R-G-B. It is seen
that the composite image obtained by MHF-net is closest
to the ground-truth, while the results of other methods usu-
ally contain obvious incorrect structure or spectral distor-
tion. More results are listed in supplementary material.

15http://naotoyokoya.com/Download.html

1591

(a) RGB & LrHS

(b) Ground truth

(c) FUSE

(d) ICCV2015

(e) GLP-HS

(f) SFIM-HS

(g) GSA

(h) CNMF

(i) M-FUSE

(j) SAMF

(k) PNN

(l) 3D-CNN

(m) ResNet

(n) MHF-net

Figure 6. (a) The simulated RGB (HrMS) and LrHS (left bottom) images of a test sample in Chikusei data set. We show the composite
image of the HS image with bands 70-100-36 as R-G-B. (b) The ground-truth HrHS image. (c)-(n) The results obtained by 10 comparison
methods, with two demarcated areas zoomed in 4 times for easy observation.

(a) HrMS image

(b) LrHS image

(c) FUSE

(d) ICCV15

(e) GLP-HS

(f) SFIM-HS

(g) GSA

(h) CNMF

(i) M-FUSE

(j) SAMF

(k) PNN

(l) 3D-CNN

(m) ResNet

(n) MHF-net

Figure 7. (a) and (b) are the HrMS (RGB) and LrHS images of the left bottom area of Roman Colosseum acquired by World View-2 (WV-
2). We show the composite image of the HS image with bands 5-3-2 as R-G-B. (c)-(n) The results obtained by 10 comparison methods,
with a demarcated area zoomed in 5 times for easy observation.

Table 3. Average performance of the competing methods over 16
testing samples of Chikusei data set with respect to 5 PQIs.

FUSE

ICCV15
GLP-HS
SFIM-HS

GSA
CNMF
M-FUSE
SASFM

PNN

3D-CNN
ResNet
MHF-net

PSNR
26.59
27.77
28.85
28.50
27.08
28.78
24.85
24.93
24.30
30.51
29.35
32.26

SAM
7.92
3.98
4.17
4.22
5.39
3.84
6.62
7.95
4.26
3.02
3.69
3.02

ERGAS
272.43
178.14
163.60
167.85
238.63
173.41
282.02
369.35
157.49
129.11
144.12
109.55

SSIM
0.718
0.779
0.796
0.793
0.673
0.780
0.642
0.636
0.717
0.869
0.866
0.890

FSIM
0.860
0.870
0.903
0.900
0.835
0.898
0.849
0.845
0.807
0.933
0.930
0.946

5.3. Experiments with real data

In this section, sample images of Roman Colosseum ac-
quired by World View-2 (WV-2) are used in our experi-
ments16. This data set contains an HrMS image (RGB im-
age) of size 1676 Ã— 2632 Ã— 3 and an LrHS image of size
419 Ã— 658 Ã— 8, while the HrHS image is not available. We
select the top half part of the HrMS (836 Ã— 2632 Ã— 3) and
LrHS (209 Ã— 658 Ã— 8) image to train the MHF-net, and ex-
ploit the remaining parts of the data set as testing data. We
ï¬rst extract the training data into 144 Ã— 144 Ã— 3 overlapped
HrMS patches and 36Ã—36Ã—3 overlapped LrHS patches and
then generate the training samples by the method as shown
in Fig. 4. The input HrHS, HrMS and LrHS samples are of

16https://www.harrisgeospatial.com/DataImagery/
SatelliteImagery/HighResolution/WorldView-2.aspx

size 36 Ã— 36 Ã— 8, 36 Ã— 36 Ã— 3 and 9 Ã— 9 Ã— 8, respectively.
Fig. 6 shows a portion of the fusion result of the test-
ing data (left bottom area of the original image). Visual
inspection evidently shows that the proposed method gives
the better visual effect. By comparing with the results of
ResNet, we can ï¬nd that the results of both methods are
clear, but the color and brightness of result of the proposed
method are much closer to the LrHS image.

6. Conclusion

In this paper, we have provided a new MS/HS fusion
network. The network takes the advantage of deep learn-
ing that all parameters can be learned from the training data
with fewer prior pre-assumption on the data, and further-
more takes into account the generation mechanism underly-
ing the MS/HS fusion data. This is achieved by construct-
ing a new MS/HS fusion model based on the observation
models, and unfolding the algorithm into an optimization-
inspired deep network. The network is thus speciï¬cally in-
terpretable to the task, and can help discover the spatial and
spectral response operators in a purely end-to-end manner.
Experiments implemented on simulated and real MS/HS fu-
sion cases have substantiated the superiority of the proposed
MHF-net over the state-of-the-art methods.
Acknowledgment. This research was supported by National
Key R&D Program of China (2018YFB1004300) and China
NSFC projects (61661166011, 11690011, 61603292, 61721002,
U1811461, 61671182)

1592

References

[1] B. Aiazzi, S. Baronti, and M. Selva. Improving component
substitution pansharpening through multivariate regression
of ms + pan data.
IEEE Transactions on Geoscience and
Remote Sensing, 45(10):3230â€“3239, 2007. 2, 7

[2] N. Akhtar, F. Shafait, and A. Mian. Sparse spatio-spectral
representation for hyperspectral image super-resolution. In
European Conference on Computer Vision, pages 63â€“78.
Springer, 2014. 2, 3, 6

[3] A. Beck and M. Teboulle. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
journal on imaging sciences, 2(1):183â€“202, 2009. 2, 4

[4] P. J. Burt and E. H. Adelson. The laplacian pyramid as a
compact image code. In Readings in Computer Vision, pages
671â€“679. Elsevier, 1987. 2

[5] P. Chavez, S. C. Sides, J. A. Anderson, et al. Compari-
son of three different methods to merge multiresolution and
multispectral data- landsat tm and spot panchromatic. Pho-
togrammetric Engineering and remote sensing, 57(3):295â€“
303, 1991. 2

[6] M. N. Do and M. Vetterli. The contourlet transform: an efï¬-
cient directional multiresolution image representation. IEEE
Transactions on image processing, 14(12):2091â€“2106, 2005.
2

[7] C. Dong, C. C. Loy, K. He, and X. Tang.

Image
super-resolution using deep convolutional networks.
IEEE
transactions on pattern analysis and machine intelligence,
38(2):295â€“307, 2016. 2

[8] D. L. Donoho. De-noising by soft-thresholding. IEEE trans-

actions on information theory, 41(3):613â€“627, 1995. 4

[9] V. Dumoulin and F. Visin. A guide to convolution arithmetic
for deep learning. arXiv preprint arXiv:1603.07285, 2016. 5

[10] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot,
and J. C. Tilton. Advances in spectral-spatial classiï¬cation of
hyperspectral images. Proceedings of the IEEE, 101(3):652â€“
675, 2013. 1

[11] C. Grohnfeldt, X. Zhu, and R. Bamler. Jointly sparse fu-
sion of hyperspectral and multispectral imagery. In IGARSS,
pages 4090â€“4093, 2013. 2, 3

[12] R. C. Hardie, M. T. Eismann, and G. L. Wilson. Map estima-
tion for hyperspectral image resolution enhancement using
an auxiliary sensor. IEEE Transactions on Image Process-
ing, 13(9):1174â€“1184, 2004. 1, 4

[16] R. Kawakami, Y. Matsushita, J. Wright, M. Ben-Ezra, Y.-
W. Tai, and K. Ikeuchi. High-resolution hyperspectral imag-
ing via matrix factorization.
In Computer Vision and Pat-
tern Recognition (CVPR), 2011 IEEE Conference on, pages
2329â€“2336. IEEE, 2011. 6

[17] C. A. Laben and B. V. Brower.

Process for enhancing
the spatial resolution of multispectral imagery using pan-
sharpening, Jan. 4 2000. US Patent 6,011,875. 2

[18] C. Lanaras, E. Baltsavias, and K. Schindler. Hyperspectral
super-resolution by coupled spectral unmixing. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 3586â€“3594, 2015. 7

[19] J. Liu.

Smoothing ï¬lter-based intensity modulation: A
spectral preserve image fusion technique for improving
spatial details.
International Journal of Remote Sensing,
21(18):3461â€“3472, 2000. 7

[20] L. Loncan, L. B. Almeida, J. M. Bioucas-Dias, X. Briottet,
J. Chanussot, N. Dobigeon, S. Fabre, W. Liao, G. A. Lic-
ciardi, M. Simoes, et al. Hyperspectral pansharpening: A
review. arXiv preprint arXiv:1504.04531, 2015. 2

[21] S. G. Mallat. A theory for multiresolution signal decom-
position: the wavelet representation. IEEE transactions on
pattern analysis and machine intelligence, 11(7):674â€“693,
1989. 2

[22] G. Masi, D. Cozzolino, L. Verdoliva, and G. Scarpa. Pan-
sharpening by convolutional neural networks. Remote Sens-
ing, 8(7):594, 2016. 3

[23] S. Michel, M.-J. LEFEVRE-FONOLLOSA, and S. HOS-
FORD. Hypximâ€“a hyperspectral satellite deï¬ned for science,
security and defence users. PAN, 400(800):400, 2011. 1

[24] R. Molina, A. K. Katsaggelos, and J. Mateos. Bayesian and
regularization methods for hyperparameter estimation in im-
age restoration.
IEEE Transactions on Image Processing,
8(2):231â€“246, 1999. 1, 2, 4

[25] R. Molina, M. Vega, J. Mateos, and A. K. Katsaggelos. Vari-
ational posterior distribution approximation in bayesian su-
per resolution reconstruction of multispectral images. Ap-
plied and Computational Harmonic Analysis, 24(2):251â€“
267, 2008. 1, 2, 4

[26] Z. H. Nezhad, A. Karami, R. Heylen, and P. Scheunders. Fu-
sion of hyperspectral and multispectral images using spec-
tral unmixing and sparse coding. IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sensing,
9(6):2377â€“2389, 2016. 2, 3

[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770â€“778, 2016. 5

[27] F. Palsson, J. R. Sveinsson, and M. O. Ulfarsson. A new pan-
sharpening algorithm based on total variation.
IEEE Geo-
science and Remote Sensing Letters, 11(1):318â€“322, 2014.
2

[14] B. Huang, H. Song, H. Cui, J. Peng, and Z. Xu. Spa-
tial and spectral image fusion using sparse matrix factoriza-
tion. IEEE Transactions on Geoscience and Remote Sensing,
52(3):1693â€“1704, 2014. 2, 3, 7

[28] F. Palsson, J. R. Sveinsson, and M. O. Ulfarsson. Mul-
tispectral and hyperspectral
image fusion using a 3-D-
convolutional neural network. IEEE Geoscience and Remote
Sensing Letters, 14(5):639â€“643, 2017. 2, 3, 6, 7

[15] W. Huang, L. Xiao, Z. Wei, H. Liu, and S. Tang. A new
pan-sharpening method with deep neural networks.
IEEE
Geoscience and Remote Sensing Letters, 12(5):1037â€“1041,
2015. 3

[29] Y. Rao, L. He, and J. Zhu. A residual convolutional neural
network for pan-shaprening. In Remote Sensing with Intel-
ligent Processing (RSIP), 2017 International Workshop on,
pages 1â€“4. IEEE, 2017. 3

1593

[44] D. Yang and J. Sun. Proximal dehaze-net: A prior learning-
based deep network for single image dehazing.
In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 702â€“717, 2018. 4

[45] Y. Yang, J. Sun, H. Li, and Z. Xu. Admm-net: A deep learn-
ing approach for compressive sensing mri. arXiv preprint
arXiv:1705.06869, 2017. 4

[46] F. Yasuma, T. Mitsunaga, D. Iso, and S. K. Nayar. General-
ized assorted pixel camera: postcapture control of resolution,
dynamic range, and spectrum. IEEE transactions on image
processing, 19(9):2241â€“2253, 2010. 6

[47] N. Yokoya, C. Grohnfeldt, and J. Chanussot. Hyperspec-
tral and multispectral data fusion: A comparative review of
the recent literature. IEEE Geoscience and Remote Sensing
Magazine, 5(2):29â€“56, 2017. 1, 7

[48] N. Yokoya, T. Yairi, and A. Iwasaki. Coupled non-negative
matrix factorization (CNMF) for hyperspectral and multi-
spectral data fusion: Application to pasture classiï¬cation. In
Geoscience and Remote Sensing Symposium (IGARSS), 2011
IEEE International, pages 1779â€“1782. IEEE, 2011. 3, 7

[49] R. H. Yuhas, J. W. Boardman, and A. F. Goetz. Determina-
tion of semi-arid landscape endmembers and seasonal trends
using convex geometry spectral unmixing techniques. 1993.
6

[50] Y. Zeng, W. Huang, M. Liu, H. Zhang, and B. Zou. Fusion
of satellite images in urban area: Assessing the quality of re-
sulting images. In Geoinformatics, 2010 18th International
Conference on, pages 1â€“4. IEEE, 2010. 6

[51] L. Zhang, L. Zhang, X. Mou, and D. Zhang. Fsim: a feature
similarity index for image quality assessment. IEEE Trans.
Image Processing, 20(8):2378â€“2386, 2011. 6

[52] Y. Zhang, Y. Wang, Y. Liu, C. Zhang, M. He, and S. Mei. Hy-
perspectral and multispectral image fusion using CNMF with
minimum endmember simplex volume and abundance spar-
sity constraints. In Geoscience and Remote Sensing Sympo-
sium (IGARSS), 2015 IEEE International, pages 1929â€“1932.
IEEE, 2015. 2, 3

[53] J. Zhang13, J. Pan, W.-S. Lai, R. W. Lau, and M.-H. Yang.
Learning fully convolutional networks for iterative non-blind
deconvolution. 2017. 4

[54] Y. Zhao, J. Yang, Q. Zhang, L. Song, Y. Cheng, and Q. Pan.
Hyperspectral imagery super-resolution by sparse represen-
tation and spectral regularization. EURASIP Journal on Ad-
vances in Signal Processing, 2011(1):87, 2011. 2, 3

[30] G. Scarpa, S. Vitale, and D. Cozzolino. Target-adaptive cnn-
based pansharpening. IEEE Transactions on Geoscience and
Remote Sensing, (99):1â€“15, 2018. 2, 3, 6, 7

[31] M. Selva, B. Aiazzi, F. Butera, L. Chiarantini, and S. Baronti.
Hyper-sharpening: A ï¬rst approach on sim-ga data.
IEEE
Journal of Selected Topics in Applied Earth Observations
and Remote Sensing, 8(6):3008â€“3024, 2015. 7

[32] Z. Shao and J. Cai. Remote sensing image fusion with deep
convolutional neural network.
IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sensing,
11(5):1656â€“1669, 2018. 3

[33] J.-L. Starck, J. Fadili, and F. Murtagh. The undecimated
wavelet decomposition and its reconstruction. IEEE Trans-
actions on Image Processing, 16(2):297â€“309, 2007. 2

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1â€“9, 2015. 2

[35] Y. Tarabalka, J. Chanussot, and J. A. Benediktsson. Seg-
mentation and classiï¬cation of hyperspectral images using
minimum spanning forest grown from automatically selected
markers. IEEE Transactions on Systems, Man, and Cyber-
netics, Part B (Cybernetics), 40(5):1267â€“1279, 2010. 1

[36] M. Uzair, A. Mahmood, and A. S. Mian. Hyperspectral face
recognition using 3d-dct and partial least squares. In BMVC,
2013. 1

[37] H. Van Nguyen, A. Banerjee, and R. Chellappa. Track-
ing via object reï¬‚ectance using a hyperspectral video cam-
era.
In Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2010 IEEE Computer Society Conference
on, pages 44â€“51. IEEE, 2010. 1

[38] L. Wald. Data Fusion: Deï¬nitions and Architectures: Fu-
sion of Images of Different Spatial Resolutions. Presses des
lEcole MINES, 2002. 6

[39] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity.
IEEE Trans. Image Processing, 13(4):600â€“612,
2004. 6

[40] Q. Wei, J. Bioucas-Dias, N. Dobigeon, J.-Y. Tourneret, and
S. Godsill. Blind model-based fusion of multi-band and
panchromatic images.
In Multisensor Fusion and Integra-
tion for Intelligent Systems (MFI), 2016 IEEE International
Conference on, pages 21â€“25. IEEE, 2016. 2, 3, 7

[41] Q. Wei, N. Dobigeon, and J.-Y. Tourneret. Fast fusion of
multi-band images based on solving a sylvester equation.
IEEE Transactions on Image Processing, 24(11):4109â€“4121,
2015. 7

[42] Y. Wei and Q. Yuan. Deep residual learning for remote
sensed imagery pansharpening. In Remote Sensing with In-
telligent Processing (RSIP), 2017 International Workshop
on, pages 1â€“4. IEEE, 2017. 3

[43] Y. Wei, Q. Yuan, H. Shen, and L. Zhang. Boosting the ac-
curacy of multispectral image pansharpening by learning a
deep residual network.
IEEE Geosci. Remote Sens. Lett,
14(10):1795â€“1799, 2017. 3

1594

