Monocular Depth Estimation Using Relative Depth Maps

Jae-Han Lee

Korea University

Chang-Su Kim
Korea University

jaehanlee@mcl.korea.ac.kr

changsukim@korea.ac.kr

Abstract

We propose a novel algorithm for monocular depth esti-
mation using relative depth maps. First, using a convolu-
tional neural network, we estimate relative depths between
pairs of regions, as well as ordinary depths, at various
scales. Second, we restore relative depth maps from selec-
tively estimated data based on the rank-1 property of pair-
wise comparison matrices. Third, we decompose ordinary
and relative depth maps into components and recombine
them optimally to reconstruct a ï¬nal depth map. Experi-
mental results show that the proposed algorithm provides
the state-of-art depth estimation performance.

1. Introduction

Depth estimation is a fundamental problem of computer
vision to estimate depth information of a scene from one or
more images. Estimated depths give important geometric
clues in vision applications, such as image synthesis [8, 44],
scene recognition [50, 56], pose estimation [60, 68], and
robotics [4, 34]. There are various techniques for infer-
ring depths from multi-view images [48, 55] or video se-
quences [30, 62], which provide promising results. How-
ever, when only a single image is available, the problem is
challenging since it is ill-posed [12].

Early methods for monocular depth estimation made
assumptions about scenes:
a space composed of box
blocks [16], a scene consisting of planar regions [54], a typ-
ical indoor room with a ï¬‚oor and walls [9, 32], and the dark
channel prior [17]. However, these methods become unreli-
able when the assumptions are invalid.

In recent years, monocular depth estimation methods
based on convolutional neural networks (CNNs) [6, 11â€“
13, 31, 33, 51] have been proposed, with the advance in
computing hardware and the availability of abundant train-
ing data [14, 57], improving the performance dramatically.
Some methods [20, 36, 39, 65, 66] combine CNNs with con-
ditional random ï¬eld (CRF) models to yield more edge-
conforming depth maps. Also, attempts have been made
to estimate depths jointly with closely related data [27, 47,
61, 64, 69], such as surface normal and optical ï¬‚ow.

ğƒ3

ğ‘3

ğ‘4

ğ‘5

ğ‘6

Input image

Ordinary depth map

Multi-resolution relative depth maps

ğƒ0

ğ…1

ğ…2

ğ…3

ğ…4

ğ…5

ğ…6

Optimal depth map

Depth map decomposition

Figure 1. An overview of the proposed algorithm. First, one ordi-
nary depth map and four relative depth maps are obtained from an
image. Then, they are decomposed into depth components, which
are, in turn, combined to reconstruct an optimal depth map.

These CNN-based methods attempt to estimate absolute
depths directly. However, as noted in [12], monocular depth
estimation is ambiguous in scale: an object may appear the
same as another identically-shaped but smaller object in a
nearer distance. On the other hand, the ratio between depths
of two points, which is referred to as relative depth in this
work, is scale-invariant. It is easier even for a human be-
ing to choose the nearer one between two points than to
estimate the absolute depth of each point. In other words,
relative depths are easier to estimate than ordinary depths.

Based on these observations, we propose a novel monoc-
ular depth estimation algorithm using relative depth maps.
Figure 1 shows an overview of the proposed algorithm.
First, we develop a CNN in the encoder-decoder architec-
ture, which includes multiple decoder blocks for estimating
relative depths, as well as ordinary depths, at various scales.
Second, we form a pairwise comparison matrix, which is
sparsely populated by the estimated relative depths. By ex-
ploiting the rank-1 property of the matrix, we restore the
entire matrix using the alternating least squares (ALS) al-
gorithm [28], from which a relative depth map is obtained.
Third, each depth map is decomposed into components,
which are re-combined to reconstruct a ï¬nal depth map
through a constrained optimization scheme. Experimental
results show that the proposed algorithm provides the state-
of-the-art depth estimation performance.

We highlight main contributions of this work as follows:

â€¢ We propose the notion of relative depth and develop an

9729

efï¬cient estimator for relative depth maps based on the
rank-1 property of pairwise comparison matrices.

â€¢ We propose novel methods for depth map decomposi-

tion and depth component combination.

â€¢ We achieve the state-of-the art depth estimation perfor-

mance on the NYUv2 dataset [57].

2. Related Work

Prior to the extensive adoption of CNNs, hand-crafted
features were used for monocular depth estimation. Sax-
ena et al. [53] proposed a Markov random ï¬eld (MRF)
model to estimate depths from multi-scale patches and
global-scale column patches. Also, Saxena et al. [54] pre-
dicted depths by inferring plane parameters, assuming that
a scene consists of planar regions. Liu et al. [38] exploited
the a priori knowledge of semantic segmentation classes to
predict depths. Karsch et al. [25] assumed that semanti-
cally similar images have similar depth distributions. They
estimated depth maps by searching similar images from a
database and warping them.

Recently, various CNN-based techniques for monocu-
lar depth estimation have been proposed. Eigen et al. [12]
used the AlexNet structure [29] for global depth prediction
and an additional ï¬ne scale network for local depth reï¬ne-
ment. Eigen and Fergus [11] extended the method in [12]
to three levels and performed depth estimation, normal esti-
mation, and semantic segmentation jointly. Roy and Todor-
ovic [51] proposed a depth estimation model to incorporate
shallow CNNs into a regression forest. Laina et al. [31]
developed a depth estimation network based on the ResNet
structure [19] and also proposed an up-projection module
to increase depth map resolutions. Fu et al. [13] proposed
the deep ordinal regression network (DORN), which trans-
forms the depth regression task into a classiï¬cation prob-
lem. Their algorithm yielded the state-of-the-art depth esti-
mation performance.

To generate sharper and more edge-conforming depth
maps, conditional random ï¬eld (CRF) models are often
combined with CNNs. Li et al. [36] estimated depth in-
formation at the superpixel level using a CNN and reï¬ned it
at the pixel level based on a CRF model. Liu et al. [40] de-
veloped another superpixel-based algorithm. They trained
unary and pairwise terms of CRF within a CNN framework.
Xu et al. [65] extracted feature maps at several CNN layers,
performed CRF optimization at those layers to yield multi-
ple depth maps, and integrated them into a ï¬nal depth map.
Heo et al. [20] predicted depths and also the corresponding
reliability levels. They exploited the reliability information
in the CRF optimization. Xu et al. [66] integrated multi-
scale CRF optimization into an encoder-decoder network,
enabling end-to-end training.

Extending the domain of training data tends to have pos-

itive impacts on the estimation performance of a deep net-
work. Therefore, some methods utilize additional annota-
tion data to train depth estimation networks. For instance,
Wang et al. [61] proposed a joint CNN structure for depth
map estimation and semantic segmentation. Moreover, they
improved depth estimation results via CRF optimization.
Qi et al. [47] utilized the geometric relationship between
surface normals and depths, improving the results of both
normal and depth estimation. Also, Yin and Shi [69] pro-
posed a joint estimation algorithm for depths, optical ï¬‚ow,
and camera motion.

The methods in [7, 70] are similar to the proposed algo-
rithm in that they also use pairwise depth comparison results
between pixels for monocular depth estimation. Zoran et
al. [70] predicted relative depths between sampled points
and propagated them to superpixels to reconstruct an en-
tire depth map. Chen et al. [7] categorized relative depths
between pixels into three classes: â€œcloser,â€ â€œfurther,â€ and
â€œequal.â€ They obtained pixel-level predictions by training
their network with different loss functions according to pair-
wise labels. The proposed algorithm, however, is different
from [7, 70]. While [7, 70] use comparison results between
coarsely sampled points, the proposed algorithm estimates
dense pairwise information and combines it with ordinary
depth maps to reconstruct ï¬ne scale depth information.

3. Proposed Algorithm

3.1. Depth Map Decomposition

Let I âˆˆ RrÃ—c be an image of size r Ã—c. The goal is to es-
timate the corresponding depth map D âˆˆ RrÃ—c. However,
this monocular depth estimation is ill-posed. Especially, it
is ambiguous in scale [12]. For instance, a building and its
small replica may produce an identical image, but have dif-
ferent depth maps. Even though we can predict the scale
of an image approximately by learning from many training
images, the ambiguity still remains. To address this issue, in
this work, we deï¬ne and estimate a scale-invariant quantity,
called relative depth, which is the ratio between the depths
of two regions in an image.

If we know the relative depths of all pixel pairs in an
image, we can reconstruct the depth map with a normalized
scale. Before proving this, let us denote the geometric mean
of a depth map D by

g(D) =

r

c

Y

Y

i=1

j=1

D(i, j)

1
rc

(1)

where D(i, j) is the (i, j)th depth in D.

Proposition 1. If the relative depths of all pixel pairs in I
is known, then a scaled depth map D/g(D) can be recon-
structed.

9730

Proof. By assumption, for any pixel (i, j), we know all rel-
ative depths in D/D(i, j). By averaging these depths ge-
ometrically, we obtain g(D)/D(i, j). Therefore, we know
its reciprocal D(i, j)/g(D). Then, we have D/g(D).

In fact, D/g(D) has the normalized scale as follows.

Proposition 2. The geometric mean of D/g(D) is 1.

Proof. g(D/g(D)) = g(D)/g(D) = 1.

According to Propositions 1 and 2, if we know all rela-
tive depths between pixel pairs, we can reconstruct the rel-
ative depth map

R = D/g(D),

(2)

which is referred to as the relative depth map. Then, the
relationship between the original depth map D and the rel-
ative depth map R can be rewritten as D = g(D)R.

Next, we reduce the depth map D to several sizes. Let
Dn denote the depth map of size 2n Ã— 2n. A lower resolu-
tion depth map Dnâˆ’1 is obtained from Dn via

Dnâˆ’1(i, j) =

1

1

Y

Y

k=0

l=0

Dn(2i âˆ’ k, 2j âˆ’ l)

1

4 .

(3)

In other words, a depth in Dnâˆ’1 is the geometric mean of
the four corresponding depths in Dn. Note that the lowest
resolution map D0 consists of a single depth, which equals
the overall geometric mean g(D).

In a typical depth map, low frequency components are
more dominant [33]. Thus, their estimation affects depth re-
construction more strongly than the estimation of high fre-
quency components. We regard Dnâˆ’1 as low frequency in-
formation, which is obtained by eliminating high frequency
(or ï¬ne detail) information in Dn. Let Fn denote the ï¬ne
detail map. First, we deï¬ne the upsampling operation U to
double the size of a depth map horizontally and vertically. It
repeats each input depth four times to ï¬ll in the correspond-
ing four pixels in the output depth map. Then, Fn is given
by

Fn = Dn âŠ˜ U (Dnâˆ’1)

(4)

where âŠ˜ denotes the Hadamard division, i.e. element-wise
division, of two matrices. Equivalently,

Dn = U (Dnâˆ’1) âŠ— Fn

(5)

where âŠ— is the Hadamard product.

Proposition 3. Q1
k=0 Q1
each (i, j), and g(Fn) = 1.

l=0 Fn(2i âˆ’ k, 2j âˆ’ l)

1

4 = 1 for

Proof. It comes from (3) and (4).

Table 1. Decomposition results of depths maps Dn and Rn for
3 â‰¤ n â‰¤ 7.

D0
âˆš
âˆš
âˆš
âˆš
âˆš
-
-
-
-
-

F1
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš

F2
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš

F3
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš

F4
-
âˆš
âˆš
âˆš
âˆš
-
âˆš
âˆš
âˆš
âˆš

F5
-
-
âˆš
âˆš
âˆš
-
-
âˆš
âˆš
âˆš

F6
-
-
-
âˆš
âˆš
-
-
-
âˆš
âˆš

F7
-
-
-
-
âˆš
-
-
-
-
âˆš

D3
D4
D5
D6
D7
R3
R4
R5
R6
R7

In a logarithmic scale, Dn can be decomposed through

the recursive application of (5),

log Dn = log U n(D0) +

n

X

i=1

log U nâˆ’i(Fi)

(6)

where log is an element-wise logarithmic function. In other
words, log Dn is decomposed into the mean depth map
log U n(D0) and the residual depth maps log U nâˆ’i(Fi) for
1 â‰¤ i â‰¤ n. Note that, by Proposition 3, the arithmetic
mean of each residual map log U nâˆ’i(Fi) is zero. Similarly,
the relative depth map Rn can be decomposed as

log Rn =

n

X

i=1

log U nâˆ’i(Fi).

(7)

In this work, given an image I, we estimate Dn and Rn
for 3 â‰¤ n â‰¤ 7. Then, we decompose each Dn or Rn via (6)
or (7), respectively. Table 1 lists the decomposition results
of these depth maps. Note that each component has multi-
ple candidates. For example, F1 has 10 candidates in total,
while F6 has 4. We combine the candidates to yield the
optimal depth component, as described in Section 3.4 and
in a supplemental document. Finally, we use the optimal
components to generate the optimal depth map D7 via (6).

3.2. Depth Estimation Network

We use the encoder-decoder architecture [2, 67] to esti-
mate depth maps, as shown in Figure 2. In the encoder part,
deep features are extracted from an image. In the decoder
part, up to ten decoders use these features to reconstruct or-
dinary depth maps Dn and relative depth maps Rn.
Encoder part: The encoder processes an image to yield
low-resolution, high-level features. DenseNet-BC [23], ex-
cluding the last dense block, is used as the encoder, which
consists of one convolution layer, one max pooling layer,
and three pairs of dense block and transition layer, as shown
in Figure 2. Note that the last dense block in DenseNet-BC
is employed in the ten decoders in the decoder part.

Each dense block in DenseNet-BC is deï¬ned by hyper-
parameters: the number n of composite functions and the
growth rate k. The settings of the dense blocks, includ-
ing the hyper-parameters, are described in the supplemental

9731

Decoder part

Encoder part

C
o
n
v
E
1

 

P
o
o
l
 

E
1

D
e
n
s
e
 
E
2

T
r
a
n
s
 
E
2

D
e
n
s
e
 
E
3

T
r
a
n
s
 
E
3

D
e
n
s
e
 
E
4

T
r
a
n
s
 
E
4

Input

Imageğˆ

Dense block

Transition layer

Whole strip masking block

Ordinal regression

Alternating least squares algorithm

OR D1ğƒ3

OR D1

OR D2ğƒ4

OR D2

OR D3ğƒ5

OR D3

OR D4ğƒ6

OR D4

OR D5ğƒ7

OR D5

WSM D5-4
WSM D5-4

WSM D4-3
WSM D4-3

WSM D5-3
WSM D5-3

WSM D3-2
WSM D3-2

WSM D4-2
WSM D4-2

WSM D5-2
WSM D5-2

WSM D2-1
WSM D2-1

WSM D3-1
WSM D3-1

WSM D4-1
WSM D4-1

WSM D5-1
WSM D5-1

Dense D1
Dense D1

Dense D2
Dense D2

Dense D3
Dense D3

Dense D4
Dense D4

Dense D5
Dense D5

Dense D6

Dense D7

Dense D8

Dense D9

Dense D10

WSM D7-1

WSM D8-1

WSM D9-1

WSM D10-1

WSM D8-2

WSM D9-2

WSM D10-2

OR D6

OR D7

OR D8

OR D9

OR D10

WSM D9-3

WSM D10-3

WSM D10-4

ALS D6

ALS D7

ALS D8

ALS D9

ALS D10

ğ‘4

ğ‘5

ğ‘6

ğ‘7

ğ‘3

Figure 2. The structure of the proposed depth estimation network. As shown above, up to ten decoders can be used. In the default setting,
the ï¬ve decoders for (D3, R3, R4, R5, R6) are employed. WSM represents a whole strip masking block [20], OR an ordinal regression
layer, and ALS an alternating least squares layer.

document. Overall, given an 224 Ã— 224 RGB image, the en-
coder generates an 8 Ã— 8 feature map with 1,056 channels.

Decoder part: The ten decoders are used to expand the
low-resolution features to higher-resolution depth maps Dn
and Rn. Each decoder has one dense block and a variable
number (0 to 4) of whole strip masking (WSM) blocks [20].
WSM is an up-sampling block in the inception structure
[58, 59]. It increases the receptive ï¬eld greatly, by applying
kernels whose horizontal or vertical sizes equal those of an
entire input signal. It has ï¬ve inception paths, which use
convolution kernels of sizes 1 Ã— 1, 3 Ã— 3, 5 Ã— 5, W Ã— 3, and
3 Ã— H, respectively. Here, W and H denote the width and
height of an input signal.

The resolution of a target depth map determines the num-
ber of WSM blocks. For example, the decoders for estimat-
ing D3 and R3 include no WSM block, since D3 and R3
have 8 Ã— 8 resolution that is equal to the resolution of the
encoder feature map. On the other hand, the decoders for
D7 and R7 use 4 WSM blocks, respectively, to extend the
feature map to 128 Ã— 128 resolution.

Ordinal regression: Each decoder performs ordinal regres-
sion [37] to reconstruct depths. An ordinal regression task
can be carried out using multiple binary classiï¬ers, which
determine if a value is greater than different thresholds, re-
spectively. Various ordinal regression methods have been
proposed to solve regression problems [13, 24, 46]. In par-
ticular, Fu et al. [13] proposed a regression network, called
DORN, for monocular depth estimation. For ordinal regres-
sion, they quantized a depth into a number of reconstruction
levels using the space-increasing discretization scheme. We
adopt their reconstruction levels and ordinal loss function in

the decoders for ordinary depth maps Dn.

However, in the decoders for relative depth maps Rn, it
is necessary to use a different set of reconstruction levels.
Note that a relative depth is a ratio of two depths. Thus,
for any relative depth r, there is always a reciprocal one
1/r.
In other words, in a logarithmic scale, the distribu-
tion of relative depths is symmetric with respect to zero.
To determine reconstruction levels for R3, we compute the
depth ratios for all pixel pairs from training data. We ap-
ply the Lloyd algorithm [42] to quantize them. To exploit
the symmetry, we perform the algorithm only for the ratios
greater than or equal to 1. Then, we ï¬x 1 as one reconstruc-
tion level to conform to the symmetry, and determine 20
more reconstruction levels by alternating the nearest neigh-
bor partitioning and the centroid computation [15]. Their
reciprocals also become reconstruction levels. In total, there
are 41 reconstruction levels. Also, reconstruction levels for
Rn for 4 â‰¤ n â‰¤ 7 are set to half the level interval of Rnâˆ’1.
Relative depths can be estimated for all pairs of pixels.
This, however, demands excessive complexity, since (2n Ã—
2n)Ã—(2n Ã—2n) = 24n pairs should be considered in Dn. To
reduce the complexity, for each pixel in Dn, we estimate the
depth ratios with respect to the neighboring 3Ã—3 pixels only,
reducing the number of pairs to 32Ã—22n. Furthermore, these
neighboring 3 Ã— 3 pixels are selected from Dnâˆ’1, instead
of Dn, as shown in Figure 3. This is advantageous, since
each depth in Dn is compared with a larger region for a
ï¬xed number of comparisons. Unestimated relative depths
are reconstructed using the ALS algorithm, as detailed in
Section 3.3.

9732

â‹¯

â‹¯

â‹¯

â‹¯

ğƒğ‘›ğƒğ‘›âˆ’1

Figure 3. To estimate relative depths, each depth in Dn, depicted
by a dot, is compared with the depths of the 3 Ã— 3 nearest pixels in
Dnâˆ’1, which are depicted by purple squares. For the illustration,
Dn is overlaid with Dnâˆ’1.

3.3. Relative Depth Map Reconstruction

In Figure 2, the bottom decoders 6 âˆ¼ 10 estimate relative
depth maps Rn, 3 â‰¤ n â‰¤ 7, respectively. To reduce the
complexity, they estimate relative depths selectively. The
remaining relative depths are reconstructed as follows.

First, in decoder 6, the relative depths for all pixel pairs
in the lowest-resolution depth map D3 are estimated. In-
evitably, there are estimation errors. Let us consider three
pixels i, j, and k. The decoder estimates relative depths
D3(j) , D3(j)
D3(i)
D3(k) . However, due to estimation er-
rors, the results may be inconsistent, i.e. it is possible that
D3(j) Ã— D3(j)
D3(i)
D3(k) . We should process the estimated
relative depth to yield consistent and reliable results.

D3(k) , and D3(i)

D3(k) 6= D3(i)

To this end, we construct the pairwise comparison ma-
trix P3, which contains the relative depths between all pixel
pairs in D3. Since the number of pixels in D3 is 8 Ã— 8, the
size of P3 is 64 Ã— 64. The (i, j)th element P3 is given by
the estimate of dj/di, where di denotes the ith depth in the
reshaped vector of D3.

Proposition 4. If there is no estimation error, P3 is a rank-
1 matrix.

Proof. In the ideal case with no error, we have P3 =
[d1, d2, Â· Â· Â· , d64]T [ 1
d1

, Â· Â· Â· , 1
d64

, 1
d2

].

If there are errors, Saaty [52] showed that the principal
eigenvector corresponding to the largest eigenvalue of P3
is a good approximation of [d1, d2, Â· Â· Â· , d64]T up to a scale
factor. Note that, by the Perron-Frobenius theorem [21],
since P3 is positive, the largest eigenvector is algebraically
simple and positive and all elements in the principal eigen-
vector are also positive. Thus, by normalizing the principal
eigenvector so that the geometric mean of elements is 1, we
reconstruct the relative depth map R3.

To reconstruct Rn for 4 â‰¤ n â‰¤ 7, we should redeï¬ne the
comparison matrix, since depths in Dn are compared with
those in Dnâˆ’1 as shown in Figure 3. Similar to P3 in the

Sparsely
estimated

ğ4,3

Densely
restored

à·©ğ4,3

ALS

Relative
depth map

ğ‘4

Reshaping

and

normalize

à·¥ğ©

Figure 4. A sparse comparison matrix P4,3 is restored to a dense
matrix ËœP4,3 by the ALS algorithm. Then, ËœP4,3 is reshaped and
normalized to a relative depth map R4.

proof of Proposition 4, in the ideal case, the comparison
matrix is given by

Pn,nâˆ’1 = [dn

1 , dn

2 , Â· Â· Â· , dn

22n ]T [

1

dnâˆ’1
1

,

1

dnâˆ’1
2

, Â· Â· Â· ,

1
dnâˆ’1
22nâˆ’2

]

(8)
where dn
i denotes the ith depth in the reshaped vector of
Dn. Without estimation errors, the rank of Pn,nâˆ’1 is also 1.
When there are estimation errors, the eigenvalue decompo-
sition method for reconstructing R3 cannot be used in this
case because Pn,nâˆ’1 is not a square matrix. Instead, we
may use singular value decomposition (SVD). It is known
that

Ë†Pn,nâˆ’1 = Ïƒ1u1vT

(9)
is the best rank-1 approximation of Pn,nâˆ’1 [5], where Ïƒ1
is the largest singular value, and u1 and v1 are the corre-
sponding singular vectors. Therefore, Rn can be obtained
by normalizing the left singular vector u1.

1

j

i /dnâˆ’1

However, as shown in Figure 3, only a portion of rela-
tive depths, dn
, are estimated and Pn,nâˆ’1 is incom-
plete. The missing entries of Pn,nâˆ’1 should be ï¬lled in
appropriately before the rank-1 approximation. Various al-
gorithms [26, 49] have been proposed to solve this matrix
completion problem. We employ the ALS algorithm [28]
as follows. Let S denote the set of positions (r, c) in
Pn,nâˆ’1, where the relative depths are estimated by the de-
coder. Also, let p and q be vectors of size 22n and 22nâˆ’2,
respectively. Then, we repeat the following two steps alter-
nately.

q â† arg min

p â† arg min

q X
(r,c)âˆˆS
p X
(r,c)âˆˆS

(p(r)q(c) âˆ’ Pn,nâˆ’1(r, c))2

(p(r)q(c) âˆ’ Pn,nâˆ’1(r, c))2

(10)

(11)

In each step, the convex condition is satisï¬ed and the closed
form solution for q or p is easily derived. Thus, the algo-
rithm yields convergent solutions Ëœp and Ëœq, and the approxi-
mation

ËœPn,nâˆ’1 = ËœpËœqT

(12)

9733

is obtained. Notice that this is already a rank-1 approxi-
mation of Pn,nâˆ’1. Therefore, we reconstruct the relative
depth map Rn by normalizing and reshaping the left vector
Ëœp. Figure 4 shows this process of ï¬lling a sparse Pn,nâˆ’1
and restoring a relative depth map Rn.

3.4. Depth Component Combination

In general, an ordinary depth map reconstructs the over-
all depth distribution robustly, while relative depth maps
are better for estimating ï¬ne details. Also, depending on
the resolution, each relative depth map estimates depth in-
formation at a certain scale reliably. Thus, by combining
all these maps at multiple resolutions, we obtain a faithful
depth map that takes advantages of those component maps.
We estimate up to ten depth maps, Dn and Rn for 3 â‰¤
n â‰¤ 7, each of which is decomposed into components, as
listed in Table 1. Since there are multiple candidates for
each component, we obtain an optimal estimate by linearly
combining them in a logarithmic domain. For the optimal
combination, we minimize the mean squared error, subject
to constraints on weighting parameters (e.g. nonnegativity
of weights), using the interior point method [1]. Then, we
use these optimal components to generate a ï¬nal depth map
via (6). The optimal combination method is described in
more detail in the supplemental document.

4. Experimental Results

4.1. Dataset and Evaluation Protocol

We assess the performance of the proposed algorithm
on the NYUv2 dataset [57].
It includes indoor video se-
quences, composed of RGB images of spatial resolution
480 Ã— 640 and the corresponding depth maps captured with
Microsoft Kinect devices. A captured depth map has miss-
ing regions, and the method in [35] is used to ï¬ll in those
regions. We use all training sequences to train the pro-
posed algorithm and employ the 654 test RGBD images for
evaluation. Also, we valid-crop the test images to spatial
resolution 427 Ã— 561, as done in [6, 33, 41]. For quanti-
tative assessment of depth maps, we use seven metrics in
Table 2 [10, 12, 31]. Among them, the Spearmanâ€™s Ï is
the correlation coefï¬cient between the ranks of estimated
depths and ground-truth depths [10]. It measures how well
an estimated depth map preserves the ordering (or ranks) of
pixel depths in the ground-truth depth map.

KITTI [14] is another dataset widely used for evaluating
monocular depth estimation algorithms. We show that the
proposed algorithm provides competitive performances also
on KITTI in the supplemental document.

4.2. Network Training

We initialize the network parameters as done in [18] and
optimize them using the Nestrov method [45]. We set the

Table 2. Evaluation metrics for estimated depth maps: Ë†di and di
denote estimated and ground-truth depths of pixel i, respectively,
and N is the number of pixels in a depth map.

Metric

Deï¬nition

RMSE (lin)

RMSE (log)

RMSE (s.inv)

ARD

SRD

Î´ < t

Spearmanâ€™s Ï

1
2

N Î£i( Ë†di âˆ’ di)2)
( 1
N Î£i(log Ë†di âˆ’ log di)2)
( 1
RMSE (log) for relative depth maps
1

1
2

1

N Î£i| Ë†di âˆ’ di|/di
N Î£i| Ë†di âˆ’ di|2/di
Percentage of di such that max{
Correlation coefï¬cient âˆˆ [âˆ’1, 1] between
the ranks of { Ë†di} and {di}

Ë†di
di

di

,

Ë†di } < t

initial learning rate, momentum, and weight decay to 10âˆ’5,
0.9, and 10âˆ’4, respectively. Also, we adjust the learning
rate based on the repetitive shifted cosine function [22, 43].
We set the cycle of the cosine function to 1/4 epoch.

We train the network in two steps. First, we train the en-
coder with a single decoder, which is the decoder for gen-
erating D3 in Figure 2. Second, after ï¬xing the encoder
parameters, we train each of the ten decoders. We set the
batch size to 4, except for the decoder for R7, for which the
batch size is 2 due to the limited GPU memory.

4.3. Comparison with the State of the Arts

Table 3 compares the proposed algorithm with conven-
tional algorithms [3, 6, 7, 11â€“13, 31, 33, 36, 39, 41, 63, 66, 70]
on the NYUv2 dataset. Some algorithms use different meth-
ods for depth map cropping and performance measurement.
Therefore, for a fair comparison, we adopted the evaluation
scheme of [6, 33, 41] as the common method and attempted
to follow it as closely as possible. Speciï¬cally, for the algo-
rithms in [6, 11â€“13, 31, 33, 39], the result depth maps, pro-
vided by the respective authors or generated by the source
codes by the authors, are evaluated by the common method.
For the other algorithms, the performance scores are ex-
cerpted directly from the papers.

It can be observed from Table 3 that, in terms of 6 (out of
8) metrics, the proposed algorithm outperforms all the con-
ventional algorithms using only the NYUv2 RGBD train-
ing data. In the other two metrics, ARD and (Î´ < 1.25),
the proposed algorithm yields the third best and the second
best performances, respectively. Especially, the proposed
algorithm provides a signiï¬cantly higher Ï than the conven-
tional algorithms. This means that the proposed algorithm
predicts the depth orders of pixels more accurately by es-
timating relative depth maps, containing order information,
as well as ordinary depth maps.

Figure 5 shows qualitative comparison results. As com-
pared with the conventional algorithms [13, 31], the pro-
posed algorithm provides more accurate depth maps with
less errors. Even though Fu et al. [13] yield smaller er-
rors than Laina et al. [31], their errors have disorderly pat-
terns and thus their depth maps look noisier. In contrast,

9734

Table 3. Performance comparison on the NYUv2 test data. The best results are boldfaced, and the second best ones are underlined. Note
that we reevaluate some algorithms [6, 11â€“13, 31, 33, 39] by using the evaluation scheme of [6, 33, 41].

RMSE (lin)

RMSE (log)

RMSE (s.inv)

The lower, the better

Zoran et al. [70]
Chen et al. [7]
Liu et al. [41]
Baig et al. [3]
Li et al. [36]
Eigen et al. [12]
Liu et al. [39]
Eigen and Fergus [11]
Xian et al. [63]
Xu et al. [66]
Chakrabarti et al. [6]
Laina et al. [31]
Lee et al. [33]
Fu et al. [13]
Proposed

1.200
1.110
1.080
0.802
0.821
0.874
0.756
0.639
0.660
0.593
0.620
0.584
0.572
0.547
0.538

0.420
0.380

-
-
-

0.284
0.261
0.215

-
-

0.205
0.198
0.193
0.188
0.180

-

0.450

-
-
-

0.219
0.214
0.171

-
-

0.166
0.164
0.156
0.158
0.148

ARD
0.400
0.350
0.327
0.241
0.232
0.218
0.209
0.158
0.155
0.125
0.149
0.136
0.139
0.116
0.131

SRD
0.540
0.430

-
-
-

0.207
0.180
0.121

-
-

0.118
0.101
0.096
0.089
0.087

Î´ < 1.25

Î´ < 1.252

Î´ < 1.253

The higher, the better

-
-
-

61.0%
62.1%
61.6%
66.2%
77.1%
78.1%
80.6%
80.6%
82.2%
81.5%
85.6%
83.7%

-
-
-
-

88.6%
88.9%
91.3%
95.0%
95.0%
95.2%
95.8%
95.6%
96.3%
96.1%
97.1%

-
-
-
-

96.8%
97.1%
97.9%
98.8%
98.7%
98.6%
98.7%
98.9%
99.1%
98.6%
99.4%

Ï
-
-
-
-
-

0.800
0.786
0.886

-
-

0.902
0.887
0.899
0.899
0.914

Table 4. Ablation study using various combinations of depth maps. We use ï¬ve maps (D3, R3, R4, R5, R6) in the default mode.

Used ordinary depth map

Used relative depth map

D3 D4 D5 D6 D7 R3 R4 R5 R6 R7
âˆš
-
-
-
-
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš

-
-
-
âˆš
-
-
-
âˆš
âˆš
-
-
-
-
-
âˆš

-
âˆš
-
-
-
âˆš
âˆš
âˆš
âˆš
-
-
-
-
-
âˆš

-
-
-
-
-
-
-
-
-
-
-
âˆš
âˆš
âˆš
âˆš

-
-
-
-
-
-
-
-
-
-
âˆš
âˆš
âˆš
âˆš
âˆš

-
-
-
-
-
-
-
-
-
-
-
-
âˆš
âˆš
âˆš

-
-
âˆš
-
-
-
âˆš
âˆš
âˆš
-
-
-
-
-
âˆš

-
-
-
-
-
-
-
-
-
-
-
-
-
âˆš
âˆš

-
-
-
-
âˆš
-
-
-
âˆš
-
-
-
-
-
âˆš

-
-
-
-
-
-
-
-
-
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš

RMSE (lin)

The lower, the better
ARD
0.143
0.135
0.134
0.133
0.133
0.135
0.134
0.133
0.133
0.142
0.134
0.132
0.131
0.131
0.130

0.583
0.556
0.553
0.552
0.552
0.555
0.551
0.550
0.550
0.580
0.549
0.540
0.538
0.538
0.539

Î´ < 1.25

Î´ < 1.252

Î´ < 1.253

The higher, the better

81.2%
82.8%
83.1%
83.1%
83.1%
82.8%
83.1%
83.2%
83.0%
81.3%
83.1%
83.6%
83.7%
83.7%
83.8%

96.3%
96.9%
96.9%
96.9%
97.0%
96.9%
96.9%
97.0%
97.0%
96.4%
97.0%
97.1%
97.1%
97.2%
97.1%

99.2%
99.3%
99.3%
99.3%
99.3%
99.3%
99.3%
99.3%
99.3%
99.2%
99.4%
99.4%
99.4%
99.4%
99.4%

Ï

0.885
0.901
0.903
0.904
0.904
0.901
0.903
0.904
0.905
0.889
0.907
0.912
0.914
0.914
0.912

the proposed algorithm provides cleaner depth maps and
outperforms the conventional algorithms both quantitatively
and qualitatively. Also, ï¬gure 6 compares the 3D visual-
ization results of depth maps. Again, the proposed algo-
rithm shows more reliable results than the conventional al-
gorithms [13, 31].

4.4. Ablation Study

The proposed algorithm uses up to ten decoders in Fig-
ure 2 to generate ordinary depth maps Dn and relative depth
maps Rn. Table 4 summarizes the depth estimation results
according to different combinations of ordinary and relative
maps. The following observations can be made:

â€¢ When only a single ordinary depth map is used, a

higher resolution one provides better results.

â€¢ Relative depths maps should be combined with at least
one ordinary map to reconstruct depths, since they do
not contain scale information (i.e. the mean depth).
However, relative maps are more effective than ordi-
nary ones. For example, when all relative maps are
combined with the lowest resolution D3, the RMSE
(lin) score is 0.538, which is better than that (= 0.550)
of combining all ordinary maps.

â€¢ Combining D3 with four relative maps R3, R4, R5,
R6 provides comparable or even better performances

than using all ten depth maps. For example, the former
yields Ï = 0.914, while the latter Ï = 0.912. This in-
dicates that the additional ordinary maps rather distort
the ground-truth depth ordering of a scene. Thus, we
use only the ï¬ve depth maps (D3, R3, R4, R5, R6) in
the default mode.

â€¢ Adding R7 to the default mode improves the perfor-

mances only slightly.

More ablation studies and more experimental results are

available in the supplemental document.

5. Conclusions

We proposed a novel approach to monocular depth es-
timation, which uses relative depth maps. First, we de-
veloped the encoder-decoder network that has multiple de-
coder blocks for estimating relative depths, as well as or-
dinary ones, at various scales. To reduce complexity, we
restored an entire relative depth map from selectively es-
timated data using the ALS algorithm. Finally, we recon-
structed an optimal depth map through the depth map de-
composition and the depth component combination. Ex-
periments demonstrated that the proposed algorithm pro-
vides the state-of-the-art performance, and an ablation study
showed that relative depth maps are more effective than or-
dinary ones in preserving the depth ordering of a scene.

9735

e
g
a
m

I

T
G

.
d
e
r
P

.
r
r
E

.
d
e
r
P

.
r
r
E

.

d
e
r
P

.
r
r
E

.
l
a
 
t
e

a
n
i
a
L

.
l
a
 
t
e
 
u
F

d
e
s
o
p
o
r
P

Near

Distant

Closer err.

Exact pred.

Farther err.

Figure 5. Qualitative comparison of Laina et al. [31], Fu et al. [13], and the proposed algoirhtm. Predicted depth maps (Pred), and error
maps (Err) of relative depths are provided for easier comparison.

e
g
a
m

I

T
G

.
l
a

 
t
e

a
n
i
a
L

.
l
a

 
t
e
 

u
F

d
e
s
o
p
o
r
P

Figure 6. Qualitative comparison of depth map 3D visualization results of Laina et al. [31], Fu et al. [13], and the proposed algoirhtm.

Acknowledgments

This work was supported in part by the MSIT(Ministry
of Science and ICT), Korea, under the ITRC(Information
Technology Research Center) support program(IITP-2018-

2016-0-00464) supervised by the IITP(Institute for Infor-
mation & communications Technology Promotion) and in
part by the National Research Foundation of Korea (NRF)
through the Korea Government (MSIP) under Grant NRF-
2018R1A2B3003896.

9736

References

[1] A. Altman and J. Gondzio. Regularized symmetric indeï¬nite
systems in interior point methods for linear and quadratic
optimization. Optimization Methods and Software, 11(1-
4):275â€“302, Jan. 1999.

[2] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet:
A deep convolutional encoder-decoder architecture for im-
age segmentation. IEEE Trans. Pattern Anal. Mach. Intell.,
39(12):2481â€“2495, Dec. 2017.

[3] M. Baig and L. Torresani. Coupled depth learning. In WACV,

2016.

[4] J. Biswas and M. Veloso. Depth camera based indoor mobile

robot localization and navigation. In ICRA, 2012.

[5] Avrim Blum, John Hopcroft, and Ravindran Kannan. Foun-

dations of Data Science. 2015.

[6] A. Chakrabarti, J. Shao, and G. Shakhnarovich. Depth from
a single image by harmonizing overcomplete local network
predictions. In NIPS, 2016.

[7] W. Chen, Z. Fu, D. Yang, and J. Deng. Single-image depth

perception in the wild. In NIPS, 2016.

[8] C. M. Cheng, S. J. Lin, S. H. Lai, and J. C. Yang. Improved
novel view synthesis from depth image with large baseline.
In ICPR, 2008.

[9] E. Delage, H. Lee, and A. Y. Ng. A dynamic Bayesian net-
work model for autonomous 3D reconstruction from a single
indoor image. In CVPR, 2006.

[10] Persi Diaconis and R. L. Graham. Spearmanâ€™s footrule as a
measure of disarray. Journal of the Royal Statistical Society.
Series B (Methodological), 39(2):262â€“268, 1977.

[11] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In ICCV, 2015.

[12] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
In

from a single image using a multi-scale deep network.
NIPS, 2014.

[13] H. Fu, M Gong, C. Wang, K. Batmanghelich, and D. Tao.
Deep ordinal regression network for monocular depth esti-
mation. In CVPR, 2018.

[14] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets
robotics: The kitti dataset. Int. J. Robot. Res., 32(11):1231â€“
1237, Sept. 2013.

[15] A. Gersho and R. M. Gray. Vector Quantization and Signal
Compression. Kluwer Academic Publishers Norwell, 1991.
[16] A. Gupta, A. Efros, and M. Hebert. Blocks world revis-
ited: Image understanding using qualitative geometry and
mechanics. In ECCV, 2010.

[17] K. He, J. Sun, and X. Tang. Single image haze removal using
dark channel prior. IEEE Trans. Pattern Anal. Mach. Intell.,
33(12):2341â€“2353, Dec. 2011.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiï¬ers: Surpassing human-level performance on imagenet
classiï¬cation. In ICCV, 2015.

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016.

[20] M. Heo, J. Lee, K. R. Kim, and C. S. Kim. Monocular depth
estimation using whole strip masking and reliability-based
reï¬nement. In ECCV, 2018.

[21] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge,

2 edition, 2012.

[22] G. Huang, Y. Li, and G. Pleiss. Snapshot ensembles: Train

1, get m for free. In ICLR, 2017.

[23] G. Huang, Z. Liu, and L. van der Maaten. Densely connected

convolutional networks. In CVPR, 2017.

[24] G. Pollastri J. Cheng, Z. Wang. A neural network approach to
ordinal regression. In IEEE International Joint Conference
on Neural Networks, 2008.

[25] K. Karsch, C. Liu, and S. B. Kang. Depth transfer: Depth
extraction from video using non-parametric sampling. IEEE
Trans. Pattern Anal. Mach. Intell., 36(11):2144â€“2158, Oct.
2014.

[26] R. H. Keshavan, S. Oh, and A. Montanari. Matrix completion
In IEEE International Symposium on

from a few entries.
Information Theory, 2009.

[27] S. Kim, K. Park, K. Sohn, and S. Lin. Uniï¬ed depth predic-
tion and intrinsic image decomposition from a single image
via joint convolutional neural ï¬elds. In ECCV, 2016.

[28] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization tech-
niques for recommender systems. IEEE Computer, 8:30â€“37,
Aug. 2009.

[29] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiï¬cation with deep convolutional neural networks.
NIPS, 2012.

ImageNet
In

[30] A. Kundu, Y. Li, F. Daellert, F. Li, and J. M Rehg. Joint se-
mantic segmentation and 3D reconstruction from monocular
video. In ECCV, 2014.

[31] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N.
Navab. Deeper depth prediction with fully convolutional
residual networks. In 3DV, 2016.

[32] D. C. Lee, A. Gupta, M. Hebert, and T. Kanade. Estimat-
ing spatial layout of rooms using volumetric reasoning about
objects and surfaces. In NIPS, 2010.

[33] J. H. Lee, M. Heo, K. R. Kim, and C. S. Kim. Single-image
depth estimation based on fourier domain analysis. In CVPR,
2018.

[34] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting
Int. J. Robot. Res., 34(4-5):705â€“724, Apr.

robotic grasps.
2015.

[35] A. Levin, D. Lischinski, and Y. Weiss. Colorization using op-
timization. ACM Trans. Graph., 23(3):689â€“694, Aug. 2004.

[36] B. Li, C. Shen, Y. Dai, A. van den Hengel, and M. He. Depth
and surface normal estimation from monocular images using
regression on deep features and hierarchical CRFs. In CVPR,
2015.

[37] L. Li and H. T. Lin. Ordinal regression by extended binary

classiï¬cation. In NIPS, 2007.

[38] B. Liu, S. Gould, and D. Koller. Single image depth estima-

tion from predicted semantic labels. In CVPR, 2010.

[39] F. Liu, C. Shen, and G. Lin. Deep convolutional neural ï¬elds

for depth estimation from a single image. In CVPR, 2015.

[40] F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from sin-
gle monocular images using deep convolutional neural ï¬elds.
IEEE Trans. Pattern Anal. Mach. Intell., 38(10):2024â€“2039,
Oct. 2016.

9737

[61] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L.
Yuille. Towards uniï¬ed depth and semantic prediction from
a single image. In CVPR, 2015.

[62] A. Wedel, U. Franke, J. Klappstein, T. Brox, and D. Cre-
mers. Realtime depth estimation and obstacle detection from
monocular video. In Joint Pattern Recognition Symposium,
2006.

[63] K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, and Z.
Luo. Monocular relative depth perception with web stereo
data supervision. In CVPR, 2018.

[64] D. Xu, W. Ouyang, X. Wang, and N. Sebe. PAD-Net: Multi-
tasks guided prediction-and-distillation network for simulta-
neous depth estimation and scene parsing. In CVPR, 2018.

[65] D. Xu, E. Ricci, W. Ouyang, X. Wang, and N. Sebe.
Multi-scale continuous CRFs as sequential deep networks
for monocular depth estimation. In CVPR, 2017.

[66] D. Xu, W. Wang, H. Tang, H. Liu, N. Sebe, and E. Ricci.
Structured attention guided convolutional neural ï¬elds for
monocular depth estimation. In CVPR, 2018.

[67] J. Yang, B. Price, and S. Cohen. Object contour detec-
tion with a fully convolutional encoder-decoder network. In
CVPR, 2016.

[68] M. Ye, X. Wang, R. Yang, L. Ren, and M. Pollefeys. Accu-
rate 3D pose estimation from a single depth image. In ICCV,
2011.

[69] Z. Yin and J. Shi. Geonet: Unsupervised learning of dense

depth, optical ï¬‚ow and camera pose. In CVPR, 2018.

[70] D. Zoran, P. Isola, D. Krishnan, and W. T. Freeman. Learning

ordinal relationships for mid-level vision. In ICCV, 2015.

[41] M. Liu, M. Salzmann, and X. He. Discrete-continuous depth

estimation from a single image. In CVPR, 2014.

[42] S. Lloyd. Least squares quantization in pcm. IEEE Trans.

Inf. Theory, 28(2):129â€“137, Mar. 1982.

[43] I. Loshchilov and F. Hutter. SGDR: Stochastic gradient de-

scent with warm restarts. In ICLR, 2017.

[44] P. Ndjiki-Nya, M. KÂ¨oppel, D. Doshkov, H. Lakshman, P.
Merkle, K. MÂ¨uller, and T. Wiegand. Depth image-based ren-
dering with advanced texture synthesis for 3-D video. IEEE
Trans. Multimedia, 13(3):453â€“465, June 2011.

[45] Y. Nesterov. A method of solving a convex programming
problem with convergence rate o(1/k2). Soviet Mathematics
Doklady, 27(2):372â€“376, Feb. 1983.

[46] Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua. Ordinal
In

regression with multiple output cnn for age estimation.
CVPR, 2016.

[47] X. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia. Geonet: Ge-
ometric neural network for joint depth and surface normal
estimation. In CVPR, 2018.

[48] A.N. Rajagopalan, S. Chaudhuri, and U. Mudenagudi. Depth
estimation and image restoration using defocused stereo
pairs. IEEE Trans. Pattern Anal. Mach. Intell., 26(11):1521â€“
1525, Nov. 2004.

[49] B. Recht. A simpler approach to matrix completion. The
Journal of Machine Learning Research, 12:3413â€“3430, Dec.
2011.

[50] X. Ren, L. Bo, and D. Fox. RGB-D scene labeling: Features

and algorithms. In CVPR, 2012.

[51] A. Roy and S. Todorovic. Monocular depth estimation using

neural regression forest. In CVPR, 2016.

[52] T. L. Saaty. A scaling method for priorities in hierarchical
structures. Journal of Mathematical Psychology, 15(3):234â€“
281, June 1977.

[53] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from

single monocular images. In NIPS, 2005.

[54] A. Saxena, M. Sun, and A. Y. Ng. Make3D: Learning 3-D
scene structure from a single still image. IEEE Trans. Pattern
Anal. Mach. Intell., 31(5):824â€“840, Oct. 2009.

[55] D. Scharstein and R. Szeliski. A taxonomy and evaluation
of dense two-frame stereo correspondence algorithms. Int. J.
Comput. Vis., 47:7â€“42, Apr. 2002.

[56] J. Shotton, T. Sharp, A. Kipman, A. Fitzgibbon, M. Finoc-
chio, A. Blake, M. Cook, and R. Moore. Real-time human
pose recognition in parts from single depth images. Com-
mun. ACM, 56(1):116â€“124, Jan. 2013.

[57] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus.

Indoor
segmentation and support inference from RGBD images. In
ECCV, 2012.

[58] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Densely connected convolutional networks. In AAAI, 2017.

[59] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D.
Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Go-
ing deeper with convolutions. In CVPR, 2015.

[60] J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon. The vitru-
vian manifold: Inferring dense correspondences for oneshot
human pose estimation. In CVPR, 2012.

9738

