PointConv: Deep Convolutional Networks on 3D Point Clouds

Wenxuan Wu, Zhongang Qi, Li Fuxin

CORIS Institute, Oregon State University

wuwen, qiz, lif@oregonstate.edu

Abstract

Unlike images which are represented in regular dense
grids, 3D point clouds are irregular and unordered, hence
applying convolution on them can be difï¬cult. In this paper,
we extend the dynamic ï¬lter to a new convolution opera-
tion, named PointConv. PointConv can be applied on point
clouds to build deep convolutional networks. We treat con-
volution kernels as nonlinear functions of the local coordi-
nates of 3D points comprised of weight and density func-
tions. With respect to a given point, the weight functions
are learned with multi-layer perceptron networks and den-
sity functions through kernel density estimation. The most
important contribution of this work is a novel reformula-
tion proposed for efï¬ciently computing the weight functions,
which allowed us to dramatically scale up the network and
signiï¬cantly improve its performance. The learned convo-
lution kernel can be used to compute translation-invariant
and permutation-invariant convolution on any point set in
the 3D space. Besides, PointConv can also be used as de-
convolution operators to propagate features from a subsam-
pled point cloud back to its original resolution. Experiments
on ModelNet40, ShapeNet, and ScanNet show that deep
convolutional neural networks built on PointConv are able
to achieve state-of-the-art on challenging semantic segmen-
tation benchmarks on 3D point clouds. Besides, our exper-
iments converting CIFAR-10 into a point cloud showed that
networks built on PointConv can match the performance of
convolutional networks in 2D images of a similar structure.

1. Introduction

robotics,

In recent

autonomous driving and vir-
tual/augmented reality applications, sensors that can di-
rectly obtain 3D data are increasingly ubiquitous. This in-
cludes indoor sensors such as laser scanners, time-of-ï¬‚ight
sensors such as the Kinect, RealSense or Google Tango,
structural light sensors such as those on the iPhoneX, as
well as outdoor sensors such as LIDAR and MEMS sensors.
The capability to directly measure 3D data is invaluable in
those applications as depth information could remove a lot

of the segmentation ambiguities from 2D imagery, and sur-
face normals provide important cues of the scene geometry.

In 2D images, convolutional neural networks (CNNs)
have fundamentally changed the landscape of computer vi-
sion by greatly improving results on almost every vision
task. CNNs succeed by utilizing translation invariance, so
that the same set of convolutional ï¬lters can be applied on
all the locations in an image, reducing the number of param-
eters and improving generalization. We would hope such
successes to be transferred to the analysis of 3D data. How-
ever, 3D data often come in the form of point clouds, which
is a set of unordered 3D points, with or without additional
features (e.g. RGB) on each point. Point clouds are un-
ordered and do not conform to the regular lattice grids as
in 2D images. It is difï¬cult to apply conventional CNNs on
such unordered input. An alternative approach is to treat the
3D space as a volumetric grid, but in this case, the volume
will be sparse and CNNs will be computationally intractable
on high-resolution volumes.

In this paper, we propose a novel approach to perform
convolution on 3D point clouds with non-uniform sampling.
We note that the convolution operation can be viewed as a
discrete approximation of a continuous convolution opera-
tor. In 3D space, we can treat the weights of this convolution
operator to be a (Lipschitz) continuous function of the local
3D point coordinates with respect to a reference 3D point.
The continuous function can be approximated by a multi-
layer perceptron(MLP), as done in [33] and [16]. But these
algorithms did not take non-uniform sampling into account.
We propose to use an inverse density scale to re-weight the
continuous function learned by MLP, which corresponds to
the Monte Carlo approximation of the continuous convo-
lution. We call such an operation PointConv. PointConv
involves taking the positions of point clouds as input and
learning an MLP to approximate a weight function, as well
as applying a inverse density scale on the learned weights
to compensate the non-uniform sampling.

The naive implementation of PointConv is memory inef-
ï¬cient when the channel size of the output features is very
large and hence hard to train and scale up to large networks.
In order to reduce the memory consumption of PointConv,

19621

we introduce an approach which is able to greatly increase
the memory efï¬ciency using a reformulation that changes
the summation order. The new structure is capable of build-
ing multi-layer deep convolutional networks on 3D point
clouds that have similar capabilities as 2D CNN on raster
images. We can achieve the same translation-invariance as
in 2D convolutional networks, and the invariance to permu-
tations on the ordering of points in a point cloud.

In segmentation tasks, the ability to transfer informa-
tion gradually from coarse layers to ï¬ner layer is important.
Hence, a deconvolution operation [24] that can fully lever-
age the feature from a coarse layer to a ï¬ner layer is vital for
the performance. Most state-of-the-art algorithms [26, 28]
are unable to perform deconvolution, which restricts their
performance on segmentation tasks. Since our PointConv
is a full approximation of convolution, it is natural to ex-
tend PointConv to a PointDeconv, which can fully untilize
the information in coarse layers and propagate to ï¬ner lay-
ers. By using PointConv and PointDeconv, we can achieve
improved performance on semantic segmentation tasks.

The contributions of our work are:
â€¢ We propose PointConv, a density re-weighted convolu-
tion, which is able to fully approximate the 3D continuous
convolution on any set of 3D points.

â€¢ We design a memory efï¬cient approach to implement
PointConv using a change of summation order technique,
most importantly, allowing it to scale up to modern CNN
levels.

â€¢ We extend our PointConv to a deconvolution ver-

sion(PointDeconv) to achieve better segmentation results.

Experiments show that our deep network built on Point-
Conv is highly competitive against other point cloud deep
networks and achieve state-of-the-art results in part segmen-
tation [2] and indoor semantic segmentation benchmarks
[5]. In order to demonstrate that our PointConv is indeed
a true convolution operation, we also evaluate PointConv
on CIFAR-10 by converting all pixels in a 2D image into a
point cloud with 2D coordinates along with RGB features
on each point. Experiments on CIFAR-10 show that the
classiï¬cation accuracy of our PointConv is comparable with
a image CNN of a similar structure, far outperforming pre-
vious best results achieved by point cloud networks. As a
basic approach to CNN on 3D data, we believe there could
be many potential applications of PointConv.

2. Related Work

Most work on 3D CNN networks convert 3D point
clouds to 2D images or 3D volumetric grids. [36, 27] pro-
posed to project 3D point clouds or shapes into several 2D
images, and then apply 2D convolutional networks for clas-
siï¬cation. Although these approaches have achieved dom-
inating performances on shape classiï¬cation and retrieval
tasks, it is nontrivial to extend them to high-resolution scene

segmentation tasks [5]. [43, 23, 27] represent another type
of approach that voxelizes point clouds into volumetric
grids by quantization and then apply 3D convolution net-
works. This type of approach is constrained by its 3D vol-
umetric resolution and the computational cost of 3D convo-
lutions. [31] improves the resolution signiï¬cantly by using
a set of unbalanced octrees where each leaf node stores a
pooled feature representation. Kd-networks[18] computes
the representations in a feed-forward bottom-up fashion on
a Kd-tree with certain size. In a Kd-network, the input num-
ber of points in the point cloud needs to be the same during
training and testing, which does not hold for many tasks.
SSCN [7] utilizes the convolution based on a volumetric
grid with novel speed/memory improvements by consider-
ing CNN outputs only on input points. However, if the
point cloud is sampled sparsely, especially when the sam-
pling rate is uneven, for the sparsely sampled regions on
may not be able to ï¬nd any neighbor within the volumetric
convolutional ï¬lter, which could cause signiï¬cant issues.

Some latest work [30, 26, 28, 35, 37, 13, 9, 39] di-
rectly take raw point clouds as input without converting
[26, 30] proposed to use shared
them to other formats.
multi-layer perceptrons and max pooling layers to obtain
features of point clouds. Because the max pooling lay-
ers are applied across all the points in point cloud, it is
difï¬cult to capture local features. PointNet++ [28] im-
proved the network in PointNet [26] by adding a hierar-
chical structure. The hierarchical structure is similar to
the one used in image CNNs, which extracts features start-
ing from small local regions and gradually extending to
larger regions. The key structure used in both PointNet
[26] and PointNet++ [28] to aggregate features from dif-
ferent points is max-pooling. However, max-pooling layers
keep only the strongest activation on features across a local
or global region, which may lose some useful detailed in-
formation for segmentation tasks. [35] presents a method
that projects the input features of the point clouds onto a
high-dimensional lattice, and then apply bilateral convolu-
tion on the high-dimensional lattice to aggregate features,
which called â€œSPLATNetâ€. The SPLATNet [35] is able to
give comparable results as PointNet++ [28]. The tangent
convolution [37] projects local surface geometry on a tan-
gent plane around every point, which gives a set of planar-
convolutionable tangent images. The pointwise convolution
[13] queries nearest neighbors on the ï¬‚y and bins the points
into kernel cells, then applies kernel weights on the binned
cells to convolve on point clouds. Flex-convolution [9] in-
troduced a generalization of the conventional convolution
layer along with an efï¬cient GPU implementation, which
can applied to point clouds with millions of points. FeaSt-
Net [39] proposes to generalize conventional convolution
layer to 3D point clouds by adding a soft-assignment ma-
trix. PointCNN [21] is to learn a Ï‡âˆ’transformation from

9622

the input points and then use it to simultaneously weight
and permute the input features associated with the points.
Comparing to our approach, PointCNN is unable to achieve
permutation-invariance, which is desired for point clouds.

The work [33, 16, 41, 12, 40] and [44] propose to learn
[16] proposed
continuous ï¬lters to perform convolution.
that the weight ï¬lter in 2d convolution can be treated as a
continuous function, which can be approximated by MLPs.
[33] ï¬rstly introduced the idea into 3d graph structure. [40]
extended the method in [33] to segmentation tasks and pro-
posed an efï¬cient version, but their efï¬cient version can
only approximate depth-wise convolution instead of real
convolution. Dynamic graph CNN [41] proposed a method
that can dynamically updating the graph. [44] presents a
special family of ï¬lters to approximate the weight function
instead of using MLPs. [12] proposed a Monta Carlo ap-
proximation of 3D convolution by taking density into ac-
count. Our work differ from those in 3 aspects. Most impor-
tantly, our efï¬cient version of a real convolution was never
proposed in prior work. Also, we utilize density differently
than [12], and we propose a deconvolution operator based
on PointConv to perform semantic segmentation.

3. PointConv

We propose a convolution operation which extends tradi-
tional image convolution into the point cloud called Point-
Conv. PointConv is an extension to the Monte Carlo ap-
proximation of the 3D continuous convolution operator. For
each convolutional ï¬lter, it uses MLP to approximate a
weight function, then applies a density scale to re-weight
the learned weight functions. Sec. 3.1 introduces the struc-
ture of the PointConv layer. Sec. 3.2 introduces PointDe-
conv, using PointConv layers to deconvolve features.

3.1. Convolution on 3D Point Clouds

Formally, convolution is deï¬ned as in Eq.(1) for func-

tions f (x) and g(x) of a d-dimensional vector x.

(f âˆ— g)(x) =ZZ

f (Ï„ )g(x + Ï„ )dÏ„

(1)

Ï„ âˆˆRd

Images can be interpreted as 2D discrete functions,
which are usually represented as grid-shaped matrices. In
CNN, each ï¬lter is restricted to a small local region, such as
3 Ã— 3, 5 Ã— 5, etc. Within each local region, the relative po-
sitions between different pixels are always ï¬xed, as shown
in Figure 1(a). And the ï¬lter can be easily discretized to
a summation with a real-valued weight for each location
within the local region.

A point cloud is represented as a set of 3D points
{pi|i = 1, ..., n}, where each point contains a position vec-
tor (x, y, z) and its features such as color, surface normal,
etc. Different from images, point clouds have more ï¬‚exible

(b)

(c)

(a)

Figure 1. Image grid vs. point cloud. (a) shows a 5 Ã— 5 local re-
gion in a image, where the distance between points can only attain
very few discrete values; (b) and (c) show that in different local
regions within a point cloud, the order and the relative positions
can be very different.

Figure 2. 2D weight function for PointConv.
(a) is a learned
continuous weight function; (b) and (c) are different local regions
in a 2d point cloud. Given 2d points, we can obtain the weights at
particular locations. The same applies to 3D points. The regular
discrete 2D convolution can be viewed as a discretization of the
continuous convolution weight function, as in (d).

shapes. The coordinates p = (x, y, z) âˆˆ R3 of a point in
a point cloud are not located on a ï¬xed grid but can take
an arbitrary continuous value. Thus, the relative positions
of different points are diverse in each local region. Conven-
tional discretized convolution ï¬lters on raster images cannot
be applied directly on the point cloud. Fig. 1 shows the dif-
ference between a local region in a image and a point cloud.
To make convolution compatible with point sets, we pro-
pose a permutation-invariant convolution operation, called
PointConv. Our idea is to ï¬rst go back to the continuous
version of 3D convolution as:

Conv(W, F )xyz =

ZZZ

(Î´x,Î´y ,Î´z )âˆˆG

W (Î´x, Î´y, Î´z)F (x + Î´x, y + Î´y, z + Î´z)dÎ´xÎ´yÎ´z

(2)
where F (x + Î´x, y + Î´y, z + Î´z) is the feature of a point
in the local region G centered around point p = (x, y, z).
A point cloud can be viewed as a non-uniform sample from
the continuous R3 space. In each local region, (Î´x, Î´y, Î´z)
could be any possible position in the local region. We deï¬ne
PointConv as the following:

P ointConv(S, W, F )xyz =

X(Î´x,Î´y ,Î´z )âˆˆG

S(Î´x, Î´y, Î´z)W (Î´x, Î´y, Î´z)F (x + Î´x, y + Î´y, z + Î´z)

(3)
where S(Î´x, Î´y, Î´z)
the inverse density at point
(Î´x, Î´y, Î´z). S(Î´x, Î´y, Î´z) is required because the point
cloud can be sampled very non-uniformly1. Intuitively, the

is

1To see this, note the Monte Carlo estimate with a biased sample:

9623

number of points in the local region varies across the whole
point cloud, as in Figure 2 (b) and (c). Besides, in Figure
2 (c), points p3, p5, p6, p7, p8, p9, p10 are very close to one
another, hence the contribution of each should be smaller.

Our main idea is to approximate the weight function
W (Î´x, Î´y, Î´z) by multi-layer perceptrons from the 3D coor-
dinates (Î´x, Î´y, Î´z) and and the inverse density S(Î´x, Î´y, Î´z)
by a kernelized density estimation [38] followed by a
nonlinear transform implemented with MLP. Because the
weight function highly depends on the distribution of input
point cloud, we call the entire convolution operation Point-
Conv. [16, 33] considered the approximation of the weight
function but did not consider the approximation of the den-
sity scale, hence is not a full approximation of the contin-
uous convolution operator. Our nonlinear transform on the
density is also different from [12].

The weights of the MLP in PointConv are shared across
all the points in order to maintain the permutation invari-
ance. To compute the inverse density scale S(Î´x, Î´y, Î´z),
we ï¬rst estimate the density of each point in a point cloud
ofï¬‚ine using kernel density estimation (KDE), then feed the
density into a MLP for a 1D nonlinear transform. The rea-
son to use a nonlinear transform is for the network to decide
adaptively whether to use the density estimates.

Figure 3 shows the PointConv operation on a K-point
local region. Let Cin, Cout be the number of channels for
the input feature and output feature, k, cin, cout are the in-
dices for k-th neighbor, cin-th channel for input feature and
cout-th channel for output feature. The inputs are the 3D
local positions of the points Plocal âˆˆ RKÃ—3, which can be
computed by subtracting the coordinate of the centroid of
the local region and the feature Fin âˆˆ RKÃ—Cin of the local
region. We use 1 Ã— 1 convolution to implement the MLP.
The output of the weight function is W âˆˆ RKÃ—(CinÃ—Cout).
So, W(k, cin) âˆˆ RCout is a vector. The density scale is
S âˆˆ RK . After convolution, the feature Fin from a local
region with K neighbour points are encoded into the output
feature Fout âˆˆ RCout , as shown in Eq.(4).

Fout =

KXk=1

CinXcin=1

S(k)W(k, cin)Fin(k, cin)

(4)

PointConv learns a network to approximate the continu-
ous weights for convolution. For each input point, we can
compute the weights from the MLPs using its relative coor-
dinates. Figure 2(a) shows an example continuous weight
function for convolution. With a point cloud input as a dis-
cretization of the continuous input, a discrete convolution
can be computed by Fig. 2(b) to extract the local features,
which would work (with potentially different approxima-
tion accuracy) for different point cloud samples (Figure 2(b-

f (xi)
R f (x)dx = R f (x)
p(xi) , for xi âˆ¼ p(x). Point clouds
p(x)
are often biased samples because many sensors have difï¬culties measuring
points near plane boundaries, hence needing this reweighting

p(x)dx â‰ˆ Pi

d)), including a regular grid (Figure 2(d)). Note that in a
raster image, the relative positions in local region are ï¬xed.
Then PointConv (which takes only relative positions as in-
put for the weight functions) would output the same weight
and density across the whole image, where it reduces to the
conventional discretized convolution.

In order to aggregate the features in the entire point set,
we use a hierarchical structure that is able to combine de-
tailed small region features into abstract features that cover
a larger spatial extent. The hierarchical structure we use
is composed by several feature encoding modules, which is
similar to the one used in PointNet++ [28]. Each module
is roughly equivalent to one layer in a convolutional CNN.
The key layers in each feature encoding module are the sam-
pling layer, the grouping layer and the PointConv. More
details can be found in the supplementary material.

The drawback of this approach is that each ï¬lter needs
to be approximated by a network, hence is very inefï¬cient.
In Sec.4, we propose an efï¬cient approach to implement
PointConv.

3.2. Feature Propagation Using Deconvolution

For the segmentation task, we need point-wise predic-
tion.
In order to obtain features for all the input points,
an approach to propagate features from a subsampled point
cloud to a denser one is needed. PointNet++ [28] proposes
to use distance-based interpolation to propagate features,
which is reasonable due to local correlations inside a local
region. However, this does not take full advantage of the
deconvolution operation that captures local correlations of
propagated information from the coarse level. We propose
to add a PointDeconv layer based on the PointConv, as a
deconvolution operation to address this issue.

As shown in Fig. 4, PointDeconv is composed of two
parts:
interpolation and PointConv. Firstly, we employ
an interpolation to propagate coarse features from previ-
ous layer. Following [28], the interpolation is conducted
by linearly interpolating features from the 3 nearest points.
Then, the interpolated features are concatenated with fea-
tures from the convolutional layers with the same resolu-
tion using skip links. After concatenation, we apply Point-
Conv on the concatenated features to obtain the ï¬nal decon-
volution output, similar to the image deconvolution layer
[24]. We apply this process until the features of all the input
points have been propagated back to the original resolution.

4. Efï¬cient PointConv

The naive implementation of the PointConv is memory
consuming and inefï¬cient. Different from [33], we propose
a novel reformulation to implement PointConv by reduc-
ing it to two standard operations: matrix multiplication and
2d convolution. This novel trick not only takes advantage
of the parallel computing of GPU, but also can be easily

9624

ğ¾Ã—(ğ¶(cid:3036)(cid:3041)Ã—ğ¶(cid:3042)(cid:3048)(cid:3047))

(ğ‘(cid:2869),ğ‘“(cid:2869))
(ğ‘(cid:2869)âˆ’ğ‘(cid:2868))
(ğ‘(cid:2868),ğ‘“(cid:2868))
(ğ‘(cid:2871)âˆ’ğ‘(cid:2868))
(ğ‘(cid:2871),ğ‘“(cid:2871))

(a)

(ğ‘(cid:2870),ğ‘“(cid:2870))
(ğ‘(cid:2870)âˆ’ğ‘(cid:2868))
(ğ‘(cid:2872)âˆ’ğ‘(cid:2868))
(ğ‘(cid:2872),ğ‘“(cid:2872))

(ğ‘(cid:2869)âˆ’ğ‘(cid:2868))
ğ‘ƒ(cid:3039)(cid:3042)(cid:3030)(cid:3028)(cid:3039):
(ğ‘(cid:2870)âˆ’ğ‘(cid:2868))
(ğ‘(cid:3038)âˆ’ğ‘(cid:2868))
ğ¾Ã—3
ğ‘‘(cid:2869)ğ‘‘(cid:2870)
ğ·ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦:
ğ‘‘(cid:3038)
ğ¾Ã—1

Compute Inverse Density Scale

Compute Weight

MLP1
ReLU

ğ¯
ğ§
ğ¨
ğ‚
1
Ã—
1

ğ‘†:
ğ‘¡ğ‘–ğ‘™ğ‘’
MLP2ğ¾Ã—1
ğ¾Ã—ğ¶(cid:3036)(cid:3041)
ğ‘“(cid:2869)ğ‘“(cid:2870)
ğ¹(cid:3036)(cid:3041):
ğ‘“(cid:3038)
ğ¾Ã—ğ¶(cid:3036)(cid:3041)

ğ¯
ğ§
ğ¨
ğ‚
1
Ã—
1

ğ¹(cid:3560)(cid:3036)(cid:3041):
ğ¾Ã—ğ¶(cid:3036)(cid:3041)

ğ‘Š:
ğ‘¡ğ‘–ğ‘™ğ‘’

(b)

ğ‘­ğ’ğ’–ğ’•:

1Ã—ğ¶(cid:3042)(cid:3048)(cid:3047)

ğ¾Ã—(ğ¶(cid:3036)(cid:3041)Ã—ğ¶(cid:3042)(cid:3048)(cid:3047))

:element-wise product

:summation

ğ¾Ã—(ğ¶(cid:3036)(cid:3041)Ã—ğ¶(cid:3042)(cid:3048)(cid:3047))

Figure 3. PointConv. (a) shows a local region with the coordinates of points transformed from global into local coordinates, p is the
coordinates of points, and f is the corresponding feature; (b) shows the process of conducting PointConv on one local region centered
around one point (p0, f0). The input features come form the K nearest neighbors centered at (p0, f0), and the output feature is Fout at p0.

ğ‘›(cid:2869)Ã—3
ğ‘›(cid:2869)Ã—ğ‘(cid:2869)

ğ‘›(cid:2870)Ã—3
ğ‘›(cid:2870)Ã—ğ‘(cid:2870)

ğ‘›(cid:2871)Ã—3
ğ‘›(cid:2871)Ã—ğ‘(cid:2871)

ğ‘›(cid:2870)Ã—(ğ‘(cid:2870)+ğ‘(cid:2871))

ğ‘›(cid:2870)Ã—ğ‘(cid:2872)

3D coordinates

Features

Feature encode

ğ‘›(cid:2869)Ã—(ğ‘(cid:2869)+ğ‘(cid:2872))

ğ‘›(cid:2869)Ã—ğ‘(cid:2873) ğ‘›(cid:2869)Ã—ğ‘š

Feature decode/PointDeconv

Interpolation

PointConv

Skip links

Figure 4. Feature encoding and propagation. This ï¬gure shows
how the features are encoded and propagated in the network for
a m classes segmentation task. n is the number of points in each
layer, c is the channel size for the features. Best viewed in color.

implemented using main-stream deep learning frameworks.
Because the inverse density scale does not have such mem-
ory issues, the following discussion mainly focuses on the
weight function.

Speciï¬cally, let B be the mini-batch size in the training
stage, N be the number of points in a point cloud, K be
the number of points in each local region, Cin be the num-
ber of input channels, and Cout be the number of output
channels. For a point cloud, each local region shares the
same weight functions which can be learned using MLP.
However, weights computed from the weight functions at
different points are different. The size of the weights ï¬lters
generated by the MLP is B Ã— N Ã— K Ã— (Cin Ã— Cout). Sup-
pose B = 32, N = 512, K = 32, Cin = 64, Cout = 64,
and the ï¬lters are stored with single point precision. Then,
the memory size for the ï¬lters is 8GB for only one layer.
The network would be hard to train with such high mem-
ory consumption. [33] used very small network with few
ï¬lters which signiï¬cantly degraded its performance. To re-
solve this problem, we propose a memory efï¬cient version
of PointConv based on the following lemma:

Lemma 1 The PointConv is equivalent to the following for-
mula: Fout = Conv1Ã—1(H, (S Â· Fin)T âŠ— M) where M âˆˆ
RKÃ—Cmid is the input to the last layer in the MLP for com-
puting the weight function, and H âˆˆ RCmidÃ—(CinÃ—Cout) is
the weights of the last layer in the same MLP, Conv1Ã—1 is
1 Ã— 1 convolution.
Proof: Generally, the last layer of the MLP is a linear layer.

In one local region, let eFin = S Â· Fin âˆˆ RKÃ—Cin and

rewrite the MLP as a 1 Ã— 1 convolution so that the out-
put of the weight function is W = Conv1Ã—1(H, M ) âˆˆ
RKÃ—(CinÃ—Cout). Let k is the index of the points in a lo-
cal region, and cin, cmid, cout are the indices of the in-
put, middle layer and the ï¬lter output, respectively. Then
W(k, cin) âˆˆ RCout
is a vector from W. And the
H(cmid, cin) âˆˆ RCout is a vector from H. According to
Eq.(4), the PointConv can be expressed in Eq.(5).

Fout =

Kâˆ’1Xk=0

Cinâˆ’1Xcin=0

(W(k, cin)eFin(k, cin))

(5)

Letâ€™s explore Eq.(5) in a more detailed manner. The out-

put of the weight function can be expressed as:

W(k, cin) =

Cmidâˆ’1Xcmid=0

(M(k, cmid)H(cmid, cin))

(6)

Substituting Eq.(6) into Eq.(5).

Fout =

Kâˆ’1X

Cinâˆ’1X

k=0

cin=0

(eFin(k, cin)

Cmidâˆ’1X

cmid=0

(M(k, cmid)H(cmid, cin)))

=

Cinâˆ’1X

Cmidâˆ’1X

cin=0

cmid=0

(H(cmid, cin)

Kâˆ’1X

k=0

(eFin(k, cin)M(k, cmid)))

= Conv1Ã—1(H, eFT

in

M)

(7)

Thus, the original PointConv can be equivalently re-
duced to a matrix multiplication and a 1 Ã— 1 convolution.
Figure 5 shows the efï¬cient version of PointConv.

In this method, instead of storing the generated ï¬lters in
memory, we divide the weights ï¬lters into two parts: the
intermediate result M and the convolution kernel H. As we
can see, the memory consumption reduces to Cmid
of the
original version. With the same input setup as the Figure 3
and let Cmid = 32, the memory consumption is 0.1255GB,
which is about 1/64 of the original PointConv.

KÃ—Cout

9625

ğ‘ƒ(cid:3039)(cid:3042)(cid:3030)(cid:3028)(cid:3039):
ğ·ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦:

(ğ‘(cid:2869)âˆ’ğ‘(cid:2868))
(ğ‘(cid:2870)âˆ’ğ‘(cid:2868))
(ğ‘(cid:3038)âˆ’ğ‘(cid:2868))
ğ¾Ã—3
ğ‘‘(cid:2869)ğ‘‘(cid:2870)
ğ‘‘(cid:3038)
ğ¾Ã—1

Compute Inverse Density Scale

ğ‘€:ğ¾Ã—ğ¶(cid:3040)(cid:3036)(cid:3031)
ğ¹(cid:3560)(cid:3036)(cid:3041):
ğ¾Ã—ğ¶(cid:3036)(cid:3041)

Compute Weight

MLP1â€™
ReLU

ğ¯
ğ§
ğ¨
ğ‚
1
Ã—
1

ğ‘†:
ğ‘¡ğ‘–ğ‘™ğ‘’
MLP2ğ¾Ã—1
ğ¾Ã—ğ¶(cid:3036)(cid:3041)
ğ‘“(cid:2869)ğ‘“(cid:2870)
ğ¹(cid:3036)(cid:3041):
ğ‘“(cid:3038)
ğ¾Ã—ğ¶(cid:3036)(cid:3041)

ğ¸:

1Ã—(ğ¶(cid:3036)(cid:3041)Ã—ğ¶(cid:3040)(cid:3036)(cid:3031))

ğ¯
ğ§
ğ¨
ğ‚
1
Ã—
1

ğ‘­ğ’ğ’–ğ’•:

1Ã—ğ¶(cid:3042)(cid:3048)(cid:3047)

: matrix multiplication
: element-wise product

: modified part from Fig. 3(b)

Figure 5. Efï¬cient PointConv. The memory efï¬cient version of PointConv on one local region with K points.

Table 1. ModelNet40 Classiï¬cation Accuracy

Method

Input

Accuracy(%)

Subvolume [27]

ECC [33]

voxels
graphs

PointNet [26]

Kd-Network [18] 1024 points
1024 points
1024 points
5000 points+normal
1024 points+normal

PointNet++ [28]
PointNet++ [28]
SpiderCNN [44]

PointConv

1024 points+normal

89.2
87.4
91.8
89.2
90.2
91.9
92.4

92.5

5. Experiments

In order to evaluate our new PointConv network, we con-
duct experiments on several widely used datasets, Model-
Net40 [43], ShapeNet [2] and ScanNet [5].
In order to
demonstrate that our PointConv is able to fully approxi-
mate conventional convolution, we also report results on the
CIFAR-10 dataset [19]. In all experiments, we implement
the models with Tensorï¬‚ow on a GTX 1080Ti GPU using
the Adam optimizer. ReLU and batch normalization are ap-
plied after each layer except the last fully connected layer.

5.1. Classiï¬cation on ModelNet40

ModelNet40 contains 12,311 CAD models from 40 man-
made object categories. We use the ofï¬cial split with 9,843
shapes for training and 2,468 for testing. Following the con-
ï¬guration in [26], we use the source code for PointNet [26]
to sample 1,024 points uniformly and compute the normal
vectors from the mesh models. For fair comparison, we
employ the same data augmentation strategy as [26] by ran-
domly rotating the point cloud along the z-axis and jittering
each point by a Gaussian noise with zero mean and 0.02
standard deviation. In Table 1, PointConv achieved state-
of-the-art performance among methods based on 3D input.
ECC[33] which is similar to our approach, cannot scale to a
large network, which limited their performance.

Figure 6. Part segmentation results. For each pair of objects, the
left one is the ground truth, the right one is predicted by PointConv.
Best viewed in color.

Table 2. Results on ShapeNet part dataset. Class avg.
is the
mean IoU averaged across all object categories, and inctance avg.
is the mean IoU across all objects.

class avg.

instance avg.

SSCNN [45]
Kd-net [18]
PointNet [26]
PointNet++[28]
SpiderCNN [44]
SPLATNet3D [35]
SSCN [7]

PointConv

82.0
77.4
80.4
81.9
82.4
82.0

-

82.8

84.7
82.3
83.7
85.1
85.3
84.6
86.0

85.7

5.2. ShapeNet Part Segmentation

Part segmentation is a challenging ï¬ne-grained 3D
recognition task. The ShapeNet dataset contains 16,881
shapes from 16 classes and 50 parts in total. The input of the
task is shapes represented by a point cloud, and the goal is to
assign a part category label to each point in the point cloud.
The category label for each shape is given. We follow the
experiment setup in most related work [28, 35, 44, 18]. It is
common to narrow the possible part labels to the ones spe-
ciï¬c to the given object category by using the known input
3D object category. And we also compute the normal di-
rection on each point as input features to better describe the
underlying shape. Figure 6 visualizes some sample results.

9626

We use point intersection-over-union(IoU) to evaluate
our PointConv network, same as PointNet++ [28], SPLAT-
Net [35] and some other part segmentation algorithms
[45, 18, 44, 7]. The results are shown in Table 2. Point-
Conv obtains a class average mIoU of 82.8% and an in-
stance average mIoU of 85.7%, which are on par with the
state-of-the-art algorithms which only take point clouds as
input. According to [35], the SPLATNet2Dâˆ’3D also takes
rendered 2D views as input. Since our PointConv only takes
3D point clouds as input, for fair comparison, we only com-
pare our result with the SPLATNet3D in [35].

5.3. Semantic Scene Labeling

Datasets such as ModelNet40 [43] and ShapeNet [2] are
man-made synthetic datasets. As we can see in the pre-
vious section, most state-of-the-art algorithms are able to
obtain relatively good results on such datasets. To evaluate
the capability of our approach in processing realistic point
clouds, which contains a lot of noisy data, we evaluate our
PointConv on semantic scene segmentation using the Scan-
Net dataset. The task is to predict semantic object labels
on each 3D point given indoor scenes represented by point
clouds. The newest version of ScanNet [5] includes up-
dated annotations for all 1513 ScanNet scans and 100 new
test scans with all semantic labels publicly unavailable and
we submitted our results to the ofï¬cial evaluation server to
compare against other approaches.

We compare our algorithm with Tangent Convolutions
[37], SPLAT Net [35], PointNet++ [28] and ScanNet [5].
All the algorithm mentioned reported their results on the
new ScanNet dataset to the benchmark, and the inputs of
the algorithms only uses 3D coordinates data plus RGB. In
our experiments, we generate training samples by randomly
sample 3m Ã— 1.5m Ã— 1.5m cubes from the indoor rooms,
and evaluate using a sliding window over the entire scan.
We report intersection over union (IoU) as our main mea-
sures, which is the same as the benchmark. We visualize
some example semantic segmentation results in Figure 7.
The mIoU is reported in Table 3. The mIoU is the mean of
IoU across all the categories. Our PointConv outperforms
other algorithm by a signiï¬cant margin (Table 3). The total
running time of PointConv for training one epoch on Scan-
Net on one GTX1080Ti is around 170s, and the evaluation
time with 8 Ã— 8192 points is around 0.5s.

5.4. Classiï¬cation on CIFAR 10

In Sec.3.1, we claimed that PointConv can be equiva-
lent with 2D CNN. If this is true, then the performance of
a network based on PointConv should be equivalent to that
of a raster image CNN. In order to verify that, we use the
CIFAR-10 dataset as a comparison benchmark. We treat
each pixel in CIFAR-10 as a 2D point with xy coordinates
and RGB features. The point clouds are scaled onto the unit

Input Scene

Ground Truth

PointConv

Figure 7. Examples of semantic scene labeling. The images from
left to right are the input scenes, the ground truth segmentation,
and the prediction from PointConv. For better visualization, the
point clouds are converted into mesh format. Best viewed in color.

Table 3. Semantic Scene Segmentation results on ScanNet

Method

mIoU(%)

30.6
ScanNet [5]
PointNet++ [28]
33.9
39.3
SPLAT Net [35]
Tangent Convolutions [37] 43.8

PointConv

55.6

Table 4. CIFAR-10 Classiï¬cation Accuracy

Accuracy(%)

Image Convolution

AlexNet [20]
VGG19 [34]

PointCNN [21]
SpiderCNN [44]

PointConv(5-layer)
PointConv(VGG19)

88.52
89.00
93.60
80.22
77.97

89.13
93.19

ball before training and testing.

Experiments show that PointConv on CIFAR-10 in-
deed has the same learning capacities as a 2D CNN. Ta-
ble 4 shows the results of image convolution and Point-
Conv. From the table, we can see that the accuracy of
PointCNN[21] on CIFAR-10 is only 80.22%, which is
much worse than image CNN. However, for 5-layer net-
works, the network using PointConv is able to achieve
89.13%, which is similar to the network using image con-
volution. And, PointConv with VGG19 [34] structure can
also achieve on par accuracy comparing with VGG19.

6. Ablation Experiments and Visualizations

In this section, we conduct additional experiments to
evaluate the effectiveness of each aspect of PointConv. Be-
sides the ablation study on the structure of the PointConv,
we also give an in-depth breakdown on the performance

9627

of PointConv on the ScanNet dataset. Finally, we provide
some learned ï¬lters for visualization.

6.1. The Structure of MLP

In this section, we design experiments to evaluate the
choice of MLP parameters in PointConv. For fast evalua-
tion, we generate a subset from the ScanNet dataset as a
classiï¬cation task. Each example in the subset is randomly
sampled from the original scene scans with 1,024 points.
There are 20 different scene types for the ScanNet dataset.
We empirically sweep over different choices of Cmid and
different number of layers of the MLP in PointConv. Each
experiment was ran for 3 random trials.The results can be
ï¬nd in supplementary. From the results, we ï¬nd that larger
Cmid does not necessarily give better classiï¬cation results.
And the different number of layers in MLP does not give
much difference in classiï¬cation results. Since Cmid is
linearly correlated with the memory consumption of each
PointConv layer, this results shows that we can choose a
reasonably small Cmid for greater memory efï¬ciency.

6.2. Inverse Density Scale

In this section, we study the effectiveness of the inverse
density scale S. We choose ScanNet as our evaluation task
since the point clouds in ScanNet are generated from real
indoor scenes. We follow the standard training/validation
split provided by the authors. We train the network with and
without the inverse density scale as described in Sec. 3.1,
respectively. Table 5 shows the results. As we can see,
PointConv with inverse density scale performs better than
the one without by about 1%, which proves the effective-
ness of inverse density scale. In our experiments, we ob-
serve that inverse density scale tend to be more effective in
layers closer to the input. In deep layers, the MLP tends to
learn to diminish the effect of the density scale. One possi-
ble reason is that with farthest point sampling algorithm as
our sub-sampling algorithm, the point cloud in deeper layer
tend to be more uniformly distributed. And as shown in Ta-
ble 5, directly applying density without using the nonlinear
transformation gives worse result comparing with the one
without density on ScanNet dataset, which shows that the
nonlinear transform is able to learn the inverse density scale
in the dataset.

6.3. Ablation Studies on ScanNet

As one can see, our PointConv outperforms other ap-
proaches with a large margin. Since we are only allowed
to submit one ï¬nal result of our algorithm to the bench-
mark server of ScanNet, we perform more ablation stud-
ies for PointConv using the public validation set provide by
[5]. For the segmentation task, we train our PointConv with
8,192 points randomly sampled from a 3m Ã— 1.5m Ã— 1.5m,
and evaluate the model with exhaustively choose all points

Figure 8. Learned Convolutional Filters. The convolution ï¬lters
learned by the MLPs on ShapeNet.For better visualization, we take
all weights ï¬lters from z = 0 plane.

in the 3m Ã— 1.5m Ã— 1.5m cube in a sliding window fashion
through the xy-plane with different stride sizes. For robust-
ness, we use a majority vote from 5 windows in all of our
experiments. From Table 5, we can see that smaller stride
size is able to improve the segmentation results, and the
RGB information on ScanNet does not seem to signiï¬cantly
improve the segmentation results. Even without these addi-
tional improvements, PointConv still outperforms baselines
by a large margin.

Table 5. Ablation study on ScanNet. With and without RGB in-
formation, inverse density scale and using different stride size of
sliding window.

Input

xyz

xyz+RGB

Stride
mIoU
Size(m) mIoU No Density Density
(no MLP)

mIoU

0.5
1.0
1.5
0.5
1.0
1.5

61.0
59.0
58.2
60.8
58.6
57.5

60.3
58.2
56.9
58.9
56.7
56.1

60.1
57.7
57.3

-
-
-

6.4. Visualization

Figure 8 visualizes the learned ï¬lters from the MLPs in
our PointConv. In order to better visualize the ï¬lters, we
sample the learned functions through a plane z = 0. From
the Figure 8, we can see some patterns in the learned con-
tinuous ï¬lters.

7. Conclusion

In this work, we proposed a novel approach to per-
form convolution operation on 3D point clouds, called
PointConv. PointConv trains multi-layer perceptrons on
local point coordinates to approximate continuous weight
and density functions in convolutional ï¬lters, which makes
it naturally permutation-invariant and translation-invariant.
This allows deep convolutional networks to be built di-
rectly on 3D point clouds. We proposed an efï¬cient imple-
mentation of it which greatly improved its scalability. We
demonstrated its strong performance on multiple challeng-
ing benchmarks and capability of matching the performance
of a grid-based convolutional network in 2D images. In fu-
ture work, we would like to adopt more mainstream image
convolution network architectures into point cloud data us-
ing PointConv, such as ResNet and DenseNet. The code can
be found here: https://github.com/DylanWusee/pointconv.

9628

References

[1] Michael M Bronstein and Iasonas Kokkinos. Scale-invariant
heat kernel signatures for non-rigid shape recognition.
In
Computer Vision and Pattern Recognition (CVPR), 2010
IEEE Conference on, pages 1704â€“1711. IEEE, 2010.

[2] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012, 2015. 2, 6, 7

[3] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming
Ouhyoung. On visual similarity based 3d model retrieval. In
Computer graphics forum, volume 22, pages 223â€“232. Wi-
ley Online Library, 2003.

[4] Hang Chu, Wei-Chiu Ma3 Kaustav Kundu, Raquel Urtasun,
and Sanja Fidler. Surfconv: Bridging 3d and 2d convolution
for rgbd images.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 3002â€“
3011, 2018.

[5] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias NieÃŸner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes.
In
Proc. IEEE Conf. on Computer Vision and Pattern Recog-
nition (CVPR), volume 1, 2017. 2, 6, 7, 8

[6] Yi Fang, Jin Xie, Guoxian Dai, Meng Wang, Fan Zhu,
Tiantian Xu, and Edward Wong. 3d deep shape descriptor.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2319â€“2328, 2015.

[7] Benjamin Graham and Laurens van der Maaten.

Sub-
arXiv preprint

manifold sparse convolutional networks.
arXiv:1706.01307, 2017. 2, 6, 7

[8] Adrien Gressin, ClÂ´ement Mallet, JÂ´erË†ome DemantkÂ´e, and
Nicolas David. Towards 3d lidar point cloud registration im-
provement using optimal neighborhood knowledge. ISPRS
journal of photogrammetry and remote sensing, 79:240â€“251,
2013.

[9] Fabian Groh, Patrick Wieschollek, and Hendrik Lensch.
Flex-convolution (deep learning beyond grid-worlds). arXiv
preprint arXiv:1803.07289, 2018. 2

[10] Kan Guo, Dongqing Zou, and Xiaowu Chen. 3d mesh label-
ing via deep convolutional neural networks. ACM Transac-
tions on Graphics (TOG), 35(1):3, 2015.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770â€“778, 2016.

[12] Pedro Hermosilla, Tobias Ritschel, Pere-Pau VÂ´azquez, `Alvar
Vinacua, and Timo Ropinski. Monte carlo convolution for
learning on non-uniformly sampled point clouds.
In SIG-
GRAPH Asia 2018 Technical Papers, page 235. ACM, 2018.
3, 4

[13] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point-
wise convolutional neural networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 984â€“993, 2018. 2

[14] Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Re-
current slice networks for 3d segmentation of point clouds.

In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2626â€“2635, 2018.

[15] JÂ¨orn-Henrik Jacobsen, Jan van Gemert, Zhongyou Lou, and
Arnold WM Smeulders. Structured receptive ï¬elds in cnns.
In Computer Vision and Pattern Recognition (CVPR), 2016
IEEE Conference on, pages 2610â€“2619. IEEE, 2016.

[16] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V
Gool. Dynamic ï¬lter networks. In Advances in Neural In-
formation Processing Systems, pages 667â€“675, 2016. 1, 3,
4

[17] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[18] Roman Klokov and Victor Lempitsky. Escape from cells:
Deep kd-networks for the recognition of 3d point cloud mod-
els.
In 2017 IEEE International Conference on Computer
Vision (ICCV), pages 863â€“872. IEEE, 2017. 2, 6, 7

[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Cite-
seer, 2009. 6

[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiï¬cation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097â€“1105, 2012. 7

[21] Yangyan Li, Rui Bu, Mingchao Sun, and Baoquan Chen.

Pointcnn. arXiv preprint arXiv:1801.07791, 2018. 2, 7

[22] Haibin Ling and David W Jacobs. Shape classiï¬cation using
IEEE transactions on pattern analysis

the inner-distance.
and machine intelligence, 29(2):286â€“299, 2007.

[23] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-
volutional neural network for real-time object recognition.
In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ
International Conference on, pages 922â€“928. IEEE, 2015. 2
[24] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmentation.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 1520â€“1528, 2015. 2, 4

[25] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J
Guibas. Frustum pointnets for 3d object detection from rgb-d
data. arXiv preprint arXiv:1711.08488, 2017.

[26] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiï¬ca-
tion and segmentation. Proc. Computer Vision and Pattern
Recognition (CVPR), IEEE, 1(2):4, 2017. 2, 6

[27] Charles R Qi, Hao Su, Matthias NieÃŸner, Angela Dai,
Mengyuan Yan, and Leonidas J Guibas. Volumetric and
multi-view cnns for object classiï¬cation on 3d data. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 5648â€“5656, 2016. 2, 6

[28] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space.
In Advances in Neural Infor-
mation Processing Systems, pages 5105â€“5114, 2017. 2, 4, 6,
7

[29] Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel
Urtasun. 3d graph neural networks for rgbd semantic seg-
mentation.
In Proceedings of theqi IEEE Conference on

9629

[44] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao.
Spidercnn: Deep learning on point sets with parameter-
ized convolutional ï¬lters. arXiv preprint arXiv:1803.11527,
2018. 3, 6, 7

[45] Li Yi, Hao Su, Xingwen Guo, and Leonidas Guibas. Sync-
speccnn: Synchronized spectral cnn for 3d shape segmenta-
tion. In Computer Vision and Pattern Recognition (CVPR),
2017. 6, 7

[46] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In CVPR, volume 2, page 7, 2017.

[47] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning
for point cloud based 3d object detection. arXiv preprint
arXiv:1711.06396, 2017.

Computer Vision and Pattern Recognition, pages 5199â€“
5208, 2017.

[30] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos.
Deep learning with sets and point clouds. arXiv preprint
arXiv:1611.04500, 2016. 2

[31] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.
Octnet: Learning deep 3d representations at high resolutions.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, volume 3, 2017. 2

[32] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz.
Fast point feature histograms (fpfh) for 3d registration.
In
Robotics and Automation, 2009. ICRAâ€™09. IEEE Interna-
tional Conference on, pages 3212â€“3217. IEEE, 2009.

[33] Martin Simonovsky and Nikos Komodakis. Dynamic edge-
conditioned ï¬lters in convolutional neural networks on
graphs. In Proc. CVPR, 2017. 1, 3, 4, 5, 6

[34] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 7

[35] Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji,
Evangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz.
Splatnet: Sparse lattice networks for point cloud processing.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2530â€“2539, 2018. 2, 6, 7

[36] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik
Learned-Miller. Multi-view convolutional neural networks
for 3d shape recognition.
In Proceedings of the IEEE in-
ternational conference on computer vision, pages 945â€“953,
2015. 2

[37] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-
Yi Zhou. Tangent convolutions for dense prediction in 3d.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3887â€“3896, 2018. 2, 7

[38] Berwin A Turlach. Bandwidth selection in kernel density
estimation: A review. In CORE and Institut de Statistique.
Citeseer, 1993. 4

[39] Nitika Verma, Edmond Boyer, and Jakob Verbeek. Feastnet:
Feature-steered graph convolutions for 3d shape analysis. In
CVPR 2018-IEEE Conference on Computer Vision & Pattern
Recognition, 2018. 2

[40] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei
Pokrovsky, and Raquel Urtasun. Deep parametric continu-
ous convolutional neural networks.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2589â€“2597, 2018. 3

[41] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,
Michael M Bronstein, and Justin M Solomon. Dynamic
graph cnn for learning on point clouds.
arXiv preprint
arXiv:1801.07829, 2018. 3

[42] Zizhao Wu, Ruyang Shou, Yunhai Wang, and Xinguo Liu.
Interactive shape co-segmentation via label propagation.
Computers & Graphics, 38:248â€“254, 2014.

[43] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3d
shapenets: A deep representation for volumetric shapes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1912â€“1920, 2015. 2, 6, 7

9630

