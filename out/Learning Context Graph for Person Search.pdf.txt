Learning Context Graph for Person Search

Yichao Yan1

2

3

4‚àó

,

,

,

Wendong Zhang2

Qiang Zhang1‚àó
Minghao Xu1

Bingbing Ni1‚Ä†
Xiaokang Yang2

1Shanghai Jiao Tong University, China

2MoE Key Lab of ArtiÔ¨Åcial Intelligence, AI Institute, Shanghai Jiao Tong University, China

3 Tencent YouTu Lab, China

4 Inception Institute of ArtiÔ¨Åcial Intelligence, UAE

{yanyichao, zhangqiang2016, nibingbing, diergent, xuminghao118, xkyang}@sjtu.edu.cn

Abstract

Person re-identiÔ¨Åcation has achieved great progress with
deep convolutional neural networks. However, most previ-
ous methods focus on learning individual appearance fea-
ture embedding, and it is hard for the models to handle dif-
Ô¨Åcult situations with different illumination, large pose vari-
ance and occlusion. In this work, we take a step further and
consider employing context information for person search.
For a probe-gallery pair, we Ô¨Årst propose a contextual in-
stance expansion module, which employs a relative atten-
tion module to search and Ô¨Ålter useful context information
in the scene. We also build a graph learning framework
to effectively employ context pairs to update target similar-
ity. These two modules are built on top of a joint detection
and instance feature learning framework, which improves
the discriminativeness of the learned features. The pro-
posed framework achieves state-of-the-art performance on
two widely used person search datasets.

1. Introduction

Persons re-identiÔ¨Åcation (re-id) is a fundamental and im-
portant research topic in computer vision.
It aims to re-
identify individuals across multi-camera surveillance sys-
tems. Person re-identiÔ¨Åcation has great potential in applica-
tions related with video surveillance, such as searching for
lost people or suspects. These applications are closely re-
lated to public security and safety, therefore person re-id has
been receiving increasing attentions over recent years. For
a typical person re-id pipeline, the re-id system is provided
with a target person as probe and aims to search through a
gallery of known ID recordings to Ô¨Ånd the matched ones.
Person re-id is extremely challenging due to the follow-
ing reasons. First, distributions of probe and gallery are

‚àóYichao Yan and Qiang Zhang contribute equally to this paper
‚Ä†the corresponding author is Bingbing Ni

Figure 1. Illustration of the proposed framework.

multi-modal due to different data sources. For example,
the pedestrians can be captured by surveillance cameras or
smart phones. Second, different illuminations and human
poses will increase intra-class variations. Third, inaccurate
detection/tracking, occlusions and background clutters lead
to heavy appearance changes, which further increases the
difÔ¨Åculty for person re-id.

Traditional person re-id tasks only focus on matching
manually cropped image snapshots or video clips across dif-
ferent cameras. These methods consider learning distance
metrics based on individual features. Therefore, one im-
portant prerequisite is that the foreground pedestrian should
be precisely detected or annotated in the scene. Otherwise,
inaccurate detection or annotation will bring heavy noise
to individual appearances, which makes this problem set-
ting impractical in real scenarios. To bridge this gap, some
recent works [43, 42] introduce the person search setting
into this domain. The idea is to simultaneously handle two
tasks (i.e., pedestrian detection and person re-identiÔ¨Åcation)
within a single framework. This setting is closer to real-

12158

Instance expansion Relative attention Graph convolution Graph matching Image A Image B world applications and allows the system to function with-
out off-line pedestrian detectors. However, these methods
still employ individual features as appearance cues. Hence
it is hard to distinguish people with similar wearings, es-
pecially in situations where we have to search through a
huge gallery set. To further address this issue, some recent
works observe that scene context can provide signiÔ¨Åcant
richer information than individual appearance cues. In real
situations, people are likely to walk in groups [30]. Even
when people are walking alone, other neighboring pedes-
trians appearing in the same scene also contain important
context cues. In other words, co-travelers captured by the
same camera will have high chance to be also captured by
its neighboring cameras. Employing context/group infor-
mation is a promising direction to tackle real-world person
re-identiÔ¨Åcation, however recent works suffer from the fol-
lowing issues. First, how to identify a group is not a trivial
task. Existing methods [24] typically utilize manual anno-
tations to locate semantic groups, which requires extensive
human labor. Other methods [4, 2] make use of spatial and
temporal cues such as velocity and relative position in the
scene, which are considered as social constraints to model
group behaviors to help facilitate person re-id. These so-
cial force models utilize elaborately designed constraints to
simulate social inÔ¨Çuences in the scene, which usually does
not have trivial solutions and is hard to optimize.

In this work, we propose a novel framework to explore
context cues for robust person search. The overall pipeline
of the proposed framework is illustrated in Figure 1. As
individual appearance features are not powerful enough to
distinguish different people, we Ô¨Årst propose to expand
instance-level features with contextual information. For
person search, the most important contexts are neighbor-
ing co-travelers. Therefore, given a target person (marked
red in Figure 1), we collect all other pedestrians in the scene
as context candidates. Among all these candidates, some of
the contexts are useful and others are just noises. Therefore,
before utilizing context information, one important step is
to Ô¨Ålter useful contexts from noise ones. To this end, we
introduce a relative attention module which takes in context
candidates in both probe and gallery images, and outputs the
matched pairs as informative context. With individuals and
the corresponding contexts, the remaining question is how
to make full use of all the information to make a more con-
Ô¨Ådent judgment whether the target pair belongs to the same
identity. We propose to build a context graph to model the
global similarity of probe-gallery pairs. SpeciÔ¨Åcally, graph
nodes consist of a target pair and context pairs. To employ
context information, all the context nodes are connected to
the target node. This graph is trained to output the similarity
of the target pair.

We evaluate our framework on two widely used bench-
marks, including CUHK-SYSU [42] and PRW dataset [54].

results demonstrate that our method can
Experiment
achieve signiÔ¨Åcant improvements over previous state-of-
the-arts. Our contributions include: 1) We introduce a
multi-part learning scheme into person search, which sup-
ports end-to-end human detection and multi-part feature
learning. 2) We introduce a relative attention model to adap-
tively select informative context the scene. 3) We build
a graph to learn global similarity between two individuals
considering context information.

2. Related Work

Person re-id aims to associate pedestrians over non-
overlapping cameras. Most previous methods try to ad-
dress this task on two directions, i.e., feature representation
and distance metric learning. Before deep learning meth-
ods gets popular, previous methods design different kinds
of hand-crafted features, such as color [28], texture [11]
and gradient [13, 3]. These methods achieve certain success
on small datasets. However, the representation capability
of hand-crafted features is limited for large-scale search-
ing. Similar limitations also apply for traditional distance
metric learning methods [48, 18, 9], which aim to opti-
mize a distance function based on certain feature. However,
the learned distance metric usually overÔ¨Åts on the training
data. Therefore, the generalization ability of these meth-
ods is also limited. With the renaissance of deep learn-
ing [19, 29, 38, 46, 47], CNN was Ô¨Årst introduced to ad-
dress person re-id in [51, 21], and it quickly dominates this
domain ever since. In recent years, a large amount of works
have proposed different model structures [51, 27, 22, 45,
45, 6, 8, 36, 57, 55]. Some methods enable end-to-end fea-
ture and metric learning [1, 33, 8, 49, 56], other methods
employ part information to build more robust representa-
tion [35, 23, 23, 23, 37, 40, 52, 44]. These approaches have
achieved promising result on recent person re-id bench-
marks. However, all these methods only focus on learn-
ing appearance features based on given human bounding
box.
In real applications, these methods should be com-
bined with an ofÔ¨Çine pedestrian detector.

To facilitate real-world person re-id, recent methods
propose to jointly address the task of detection and re-
identiÔ¨Åcation [43, 42]. State-of-the-art methods [15, 26, 41]
design online learning object functions to learn large num-
ber of identities in the training set. These methods achieve
great performance on recent person search datasets. How-
ever, these methods only employ individual appearance for
veriÔ¨Åcation, which ignores the underlying relationship be-
tween individuals in the scene. Employing group/social in-
formation could be a promising direction to further improve
system‚Äôs performance. Although some methods [24, 4, 2]
have made efforts towards this direction for person re-id
task, we design a novel context learning framework with
graph model in person search scenario.

22159

Graph convolutional networks (GCN) [17] has been pro-
posed to learn graph relations with convolution, which fa-
cilitates the optimization of traditional graph model. GCN
has been applied to various tasks [20, 25, 31, 39, 12]. Sev-
eral recent works [5, 34] employ graph model for person
re-id, but the graphs are constructed to model relationships
between probe-gallery pairs, and no context information is
considered. In this work, we design a target-context graph
and employ a pairwise GCN to learn visual relations in the
scene.

3. Methodology

3.1. Overview

Instance Detection and Feature Learning.

Although deep CNN models have greatly improved the
representation ability of instance-level individual features,
it is still difÔ¨Åcult to retrieve the target persons across differ-
ent camera views in many complex situations. Therefore,
our core idea is to expand instance features such that con-
text information can be used to learn better representation.
SpeciÔ¨Åcally, our framework is consists of three major steps.
In this
stage, we utilize a baseline CNN to perform joint detec-
tion and feature learning on person search datasets. Follow-
ing Faster R-CNN [32] framework, a region proposal net-
work (RPN) is embedded on top of feature maps generated
by ResNet-50 baseline. The bounding boxes are then fed
into an RoI-Pooling layer to extract individual appearance
representation. In addition, we introduce part-based feature
learning framework into our model, thus yielding more dis-
criminative representation.

Contextual Instance Expansion. This is one of the key
components of our framework, which is built to expand in-
stance feature with context information for better represen-
tation. All the instance pairs between query and gallery im-
ages are considered as context candidates, and noise con-
texts needs to be Ô¨Åltered. To this end, we build a relative
attention layer to measure the visual similarity between con-
text pairs and only the pairs with sufÔ¨Åcient conÔ¨Ådence are
picked out as informative contexts.

Contextual Graph Representation Learning. This is
another crucial component of our framework. Given a
probe-gallery pair, we construct a graph to measure the sim-
ilarity of target pair. Graph nodes consist of target persons
and the associated context pairs, which are connected with
graph edges. We apply a graph convolutional network to
learn the similarity between probe-gallery pair.

3.2. Instance Detection and Feature Learning

3.2.1 Pedestrian Detection

Real person search scenarios are usually in the wild, there-
fore target pedestrians need to be detected in the scene be-
fore searching can be performed. Recent state-of-the-art

frameworks perform person detection and feature learning
in a single framework, which signiÔ¨Åcantly facilitates tradi-
tional pipelines (i.e., separate detection and feature learn-
ing). In this work, we take this popular structure as back-
bone network in our framework. The overall detection and
feature learning framework is illustrated in Figure 2.

SpeciÔ¨Åcally, we employ ResNet-50 [14] as stem, which
is divided into two parts. The Ô¨Årst part (conv1 to conv4 3)
outputs 1024 channel feature maps, which have 1/16 reso-
lutions of the input image. Following Faster R-CNN frame-
work, a pedestrian proposal network(PPN) is built on top
of these feature maps to generate person proposals, which
is further passed to a 512 √ó 3 √ó 3 convolutional layer to
generate pedestrian feature representation. Similar to previ-
ous frameworks, we assign 9 anchors to each feature map.
Two loss terms are utilized to train the PPN, i.e., a binary
Softmax classiÔ¨Åer to judge whether the anchor is a person
or not and a linear layer to perform bounding box regres-
sions. Finally, non-maximum suppression is used to re-
move duplicated detections and 128 proposals are kept for
each image. All the candidate proposals are fed into a RoI-
Pooling layer to generate feature representations for each
bounding box. These features are then convolved by the
second part of ResNet50 (conv4 4 to conv5 3). These fea-
tures are then connected to an average pooling layer to gen-
erate 2048-dimensional feature representation. The pooled
features are connected with two fully connected (Fc) layers.
The Ô¨Årst branch is a binary Softmax layer which is trained
to make person/non-person judgments. The second branch
is a 256-dimensional Fc layer, whose outputs are further L-2
normalized as feature representation for inference.

3.2.2 Region-based Feature Learning

Part-based models have proven to be effective for person
re-id tasks, which motivates us to also consider model-
ing human parts for person search task. Therefore, we
design a region based learning framework to effectively
model part features. In addition to a global average pool-
ing layer, we design several part-based pooling layers af-
ter the second part of ResNet-50. Each part-based pooling
layer concentrates on a speciÔ¨Åc human part and pools the
features into a 2048-dimensional vector, which are further
connected with a fully connected layer and normalized into
256-dimensional feature. These features are further used
for feature learning. SpeciÔ¨Åcally, we design 3 part-sensitive
pooling layers, which focus on upper-body, torso and lower-
body. Each layer pools a 7√ó3 region in the 7√ó7 feature
maps, which is illustrated in Figure 2.

To learn robust feature representation,

the designed
loss terms should guarantee the discriminativeness of the
learned feature. Although Softmax loss is wildly employed
for classiÔ¨Åcation tasks, it is hard to train a Softmax layer

32160

Network 

Pedestrian Detection and Feature Learning Network 

Inputs 

Res-50 
Part1 

Pedestrian 

RoI 

Proposal Net 

 Pooling 

Res-50 
Part2 

Feature 

Map 

Part-based 

Block 

Part-based Block 

Relative 
Attention 

Block 

Region-based Pooling 

FC 

FC 

Softmax 

Figure 2. Architecture of the detection and part-based feature learning framework.

OIM 

Pair-wise 

Loss 

ùë§ùë§1 ùë§ùë§2 ùë§ùë§3 ùë§ùë§4 

when the identity number is large. Meanwhile, faster R-
CNN framework consumes large memory, which limits that
the mini-batch can only have small size. Hence, the iden-
tities appeared in each mini-batch are highly sparse, which
makes Softmax loss training even harder. To address this
issue, previous methods design several online learning loss.
In this work, we adopt online instance matching (OIM)
loss [42] to supervise feature learning for each part.

3.3. Contextual Instance Expansion

As individual features are not sufÔ¨Åcient for real world
person retrieval task, we propose to employ context infor-
mation as complement. An example is illustrated in Fig-
ure 3. The objective is to identify whether the men in red
bounding boxes belong to a same identity. However, the re-
sults are usually not conÔ¨Ådent as the appearance of the per-
son suffer from great variation across different scenes. In
this case, we observe that the same persons in green bound-
ing boxes appear in both scenes, thus a more conÔ¨Ådent judg-
ment can be made that the men in red bounding boxes do
belong to the same identity. Therefore, the persons in green
bounding boxes play a positive role, while other persons
in the scene are noise contexts. In this part, we propose a
relative attention model to Ô¨Ålter all the contexts and only
positive contexts are selected to expand individual features.
SpeciÔ¨Åcally, we consider the set of persons which appear
on both probe and gallery scenes as positive contexts. The
remaining question is how to judge whether two detected
pedestrians belong to the same identity. A trivial solution
is to compute the similarity between the feature pairs, and
set a threshold to make binary decision. We use xr
j to
denote the features of the r‚àíth part from object i and j.
Consider different object parts, the overall similarity s(i, j)
can be represented as the summation of different parts:

i ,xr

s(i, j) =

R

Xr=1

wrcos(xr

i , xr

j ),

where R is number of part (R=4 in our framework), and
cos(xi, xj) denotes the cosine similarity between feature
pairs. wr is the contribution of the r-th object part, which is
usually set by empirical experiences. However, as discussed
in [16], uniformly combining these terms is not the opti-
mal solution. Because the contributions of different object
parts are signiÔ¨Åcantly different across samples, due to possi-
ble occlusions, different viewpoints and lighting conditions.
Hence, Huang et al. [16] propose an instance region atten-
tion network to assign different weights to instance parts.
The attention weights measure the instance-wise part con-
tributions, and part similarity is multiplied by both parts‚Äô
attention weights. In this work, we observe that part contri-
butions are not only related to sample part itself, but are also
related to the part to be matched. In other words, part con-
tribution is related to part pairs. An example is illustrated
in Figure 2. Head and facial appearance are important cues
for the Ô¨Årst body part. However, when frontal view is not
available, both parts tend to have low attention weights. In
fact, these two parts share great similarity and can provide
important information to make a positive judgment. Mo-
tivated by this observation, we design a relative attention
network which considers pair-wise information to predict
part weights.

SpeciÔ¨Åcally, the proposed relative attention network con-
sists of two fully connected layers and a Softmax layer. See
the bottom part of Figure 2. The network takes in 4 pairs of
feature vectors, and the Softmax layer output 4 normalized
attention weights. To train the attention network, we em-
ploy a cosine embedding veriÔ¨Åcation loss. Given an object
pair (i, j), the corresponding label y = 1 if these two sam-
ples belong to the same identity, otherwise y = ‚àí1. Then
the loss function is as follows:

Lveri = (cid:26) 1 ‚àí s(i, j)

max(0, s(i, j) + Œ±)

if y = 1
if y = ‚àí1

,

(2)

(1)

where Œ± is the margin term. This loss term builds a margin

42161

Figure 3. Architecture of the detection and part-based feature learning framework.

between positive and negative pairs, and thus safeguards the
discrminativeness of the embedded features. During train-
ing, the cosine embedding loss is jointly optimized with
OIM loss. For all the context pairs, we select the top K
matched pairs as positive contexts, which are utilized for
further feature learning.

3.4. Contextual Graph Representation Learning

In this section, we introduce the detailed structure of
the proposed context graph as well as the GCN model em-
ployed to learn graph parameters. The overall structure is
illustrated on the right side of Figure 3. Given two images
A and B. The objective of our model is to judge whether
target pair appeared in the images A0 and B0 (in red bound-
ing boxes) belong to the same identity, given K context
pairs (Ai, Bi), i ‚àà {1, .., K}. The objective is to construct
a graph to jointly take the target pairs and the context infor-
mation into consideration, and eventually outputs the sim-
ilarity score. A straightforward solution is to employ two
graphs to model each image, and to utilize a Siamese GCN
structure to extract features of both graphs, as in [20]. How-
ever, the Siamese structure prevents the contextual informa-
tion to propagate between graphs, which leads to signiÔ¨Åcant
information loss. In our situation, the targets and contexts
all appear in pairs. Therefore, we build a graph whose nodes
consist of instance pairs. In this graph, the target node is the
center of the graph, which is connected to all the context
nodes for information propagation and feature updation.

In particular, considering a graph G = {V, E} consisting
of N vertices V and a set of edges E. We assign each node
with a pair of features (xAj , xBj ), j ‚àà {0, ..., K}. If the
images have K context pairs, then N = K +1. We use X ‚àà
RN √ó2d, where d is the instance-level feature dimension. We
use A ‚àà RN √óN to denote the adjacent matrix associated
with graph G. If we assign the target node as the Ô¨Årst node

in the graph, then the adjacent matrix is:

Ai,j = (cid:26) 1 if i = 1 or j = 1 or i = j

otherwise

0

,

(3)

If we use ÀÜA to denote the nor-
where i, j ‚àà {1, ..., N }.
malized adjacency matrix, layer-wise GCN propagates as
follows:

Z(l+1) = œÉ( ÀÜAZ

(l)

W(l)),

(4)

where Z(l) is the matrix activation of the l-th layer, and
Z(0) = X as input. W(l) is the learnable parameter matrix
and œÉ is the ReLU activation function in our framework.
Finally, we utilize a fully connected layer to merge all the
vertices features into 1024-dimensional feature vector. And
a binary Softmax loss layer is employed supervise network
training.

3.5. Implementation Details

To train the detection and instance feature learning net-
work, we use an ImageNet [10] pretrained ResNet-50 model
to initialize model weights. All the training images are re-
sized to 720 √ó 576, and we use a batch size of 32. For the
graph learning model, we set the layer number of GCN to
3. The initial learning rate is 0.1 and is reduced by a factor
of 2 after 10 epochs, and the total training epoch is 20. We
implement our model on Pytorch, and all the models are
trained and tested on a TITAN X GPU. For both training
and testing, when the scene contains less than K context in-
stances, we randomly replicate the context persons to build
K context pairs. For the extreme case where the scene only
contains a single instance(i.e., no context exists), these im-
ages are not utilized to train the graph model.

52162

Image Pair Similarity Estimation Graph Learning GCN GCN Target Pair Contextual Instance Expansion Matched Table 1. Experiment results using different body parts.

Dataset
Regions

upper
middle
lower
whole
uniform

CUHK-SYSU

PRW

top-1(%) mAP(%)

top-1(%) mAP(%)

65.7
64.3
61.8
77.5
79.2

58.3
56.1
57.0
71.8
75.9

57.6
56.8
55.9
62.8
65.4

20.3
20.2
19.6
23.9
24.5

4. Experiments

In this section, we conduct experiments on two widely
utilized person search datasets. We Ô¨Årst give the experi-
ment settings on different datasets. Then we analyze the
contributions of different framework components. We fur-
ther compare our model with previous state-of-the arts.

4.1. Dataset and Experiment Setup

We evaluate the proposed method on both CUHK-
SYSU [42] and PRW dataset [54]. CUHK-SYSU dataset
is a large-scale person search dataset which contains totally
18184 images, with 8432 different persons and 96143 an-
notated bounding boxes indicating the locations of differ-
ent pedestrians. This dataset covers diverse scenes, where
12490 images are collected from real street snap and the
rest images comes from movies or TVs. For each query
person, there exists at least two images containing the tar-
get person in gallery set. In addition, this dataset includes
large variations in lighting, occlusion, background and res-
olution, which are close to the real application scenarios.
The whole dataset are ofÔ¨Åcially split into training set and
testing set. The training set contains 5532 persons in 11206
images, and the testing set contains 2900 query persons and
6978 images. The PRW dataset is captured on a university
campus. It consists of 11816 video frames from 6 cameras,
and all these images are manually annotated, resulting in
43110 pedestrian bounding boxes. The number of person
IDs is 932. The training set contains 5134 images and 482
different persons, and the testing set contains 6112 images
and 2057 query persons.

We utilize the same evaluation protocol as in [42], where
the mean Average Precision (mAP) and the top-1 matching
rate are employed as evaluation metrics. The top-1 match-
ing rate is similar with that in person re-identiÔ¨Åcation. The
main difference is that a matching is accepted only if the
overlap between the bounding box of ground truth person
and top-1 matching box is larger than 0.5.

4.2. Effective of Part based Learning Framework

Part-based learning framework has proven to be effective
for many person re-id models. To validate the effective-
ness of part-based model in our person search framework,
we analyze the performance of our model utilizing differ-

Dataset
Methods
uniform
attention

graph

Table 2. Component analysis results.

CUHK-SYSU

PRW

top-1(%) mAP(%)

top-1(%) mAP(%)

79.2
82.7
86.5

75.9
80.2
84.1

65.4
67.8
73.6

24.5
27.8
33.4

ent part features. The results are reported in Table 1. We
split the whole body into three parts, with respect to the
upper, middle and lower regions of human body, and we
utilize our part-based pooling layers to obtain the Ô¨Ånal part
features. We also pool all features in the whole detection
box as global feature. We Ô¨Årst directly utilize these learned
features of different parts to perform the person matching
and the results are shown in the Ô¨Årst 4 rows in Table 1.
On CUHK-SYSU dataset, we observe that the performance
using middle part feature is comparable with that of upper
part feature (64.3% vs. 65.7%), but the lower part features
produce a much lower result (61.8%) than the other two
parts. These results suggest that the upper and middle parts
of a person contain more discriminative information than
lower regions. We also observe that using global informa-
tion achieves better results than either parts, which suggests
that each part contains unique discriminative information.
In addition, when we uniformly combine these four fea-
tures (last row in Table 1), i.e., setting wr in Equation 1
to 0.25 for all parts, the performance is further improved
1.7% (from 77.5% to 79.2%). The results on PRW dataset
is consistent with that on CUHK-SYSU. These results sug-
gest that region-based features contains discriminative in-
formation and can obtain better performance with appropri-
ate aggregation methods.

4.3. Instance Expansion and Contextual Learning

With the learned part-based features, we further analyze
the contribution of the contextual instance expansion and
contextual graph representation learning components. In-
stead of assigning uniform weights to combine the learned
features, we train our relative attention module and assign
the learned weight for pairwise parts to measure the over-
all similarity. In other words, the weights wr in Equation 1
are learned based on the feature pairs. Using the learned
metric, the retrieval results are shown in Table 2. Com-
pared with uniform weights, our relative attention module
achieves about 2% improvement on rank-1 matching rate on
both datasets (79.2% ‚Üí 82.7%, 65.4% ‚Üí 67.8%,). These
results validate the effectiveness of the proposed relative at-
tention module. We also provide attention results in Fig-
ure 4. The Ô¨Årst row shows two positive pairs with partial
occlusion. We observe that the bottom part and the top part
of the second persons are occluded, respectively. Accord-
ingly, we also observe that the attention weights of the oc-
cluded regions (0.06, 0.12) are much lower than other re-

62163

Figure 5. The impact of context size K on performance

4.4. Comparison with State of the Art Methods

In this subsection, we report the person search results
of our model on both CUHK-SYSU and PRW datasets and
compare our model with several state-of-the-art methods,
such as IAN [41], OIM [42], I-Net [15], NPSM [26] and
MGTS[7]. We also compare the results using some other
hand-crafted re-id features as reported in [15].

Results on CUHK dataset. The comparative results are
reported in Table 3. The gallery size of all methods are set
to 100. We use ‚ÄúCNN‚Äù to denote the Faster R-CNN detector
based on ResNet-50, and we use ‚ÄúCNNv‚Äù to denote a VGG-
based detector. Compared with results using hand-crafted
features, we observe that all the deep learning based feature
learning methods achieves signiÔ¨Åcant improvements, which
veriÔ¨Åes the superiority of deep CNN on person re-id task.
The original OIM [42] can be viewed as an overall base-
line of the proposed framework, our framework achieves
about 8% improvements on both mAP and top-1 match-
ing rate, which demonstrates effectiveness of the proposed
framework.
IAN [41] proposes a center loss to improve
intra-class feature compactness. However, the improvement
over OIM is limited on ResNet-50, more improvements can
be achieved using stronger backbone networks (ResNet-
101). In contrast, our framework only employs ResNet-50
as backbone network, but we still achieve more than 6% im-
provements over IAN with stronger backbone. NPSM [26]
also considers context cues in the gallery images, it pro-
poses a recursive attention framework to sequentially search
for the target in the gallery images. Our framework consid-
ers context cues in both query and gallery images, thus our
method achieves more signiÔ¨Åcant improvements. I-Net [15]
introduces a Siamese structure and a novel online pairing
loss to learn robust feature representation, I-Net achieves
4% top-1 accuracy over OIM with hard negative mining.
With contextual learning, our method does not need explicit
loss designing and negative mining, while we still achieves
improvement over I-Net. MGTS[7] designs a segmentation
network to generate clean foreground objects, and it utilizes
both detection and segmentation objects for two-stream fea-
ture learning. This method achieves great performance on
CUHK-SYSU dataset, while our method achieves slightly
better performance by employing context features.

72164

Figure 4. Attention examples. ‚Äúatt‚Äù denotes the attention wights of
different body parts, ‚Äúsim‚Äù denotes similarity between body parts.
The values under image pairs denote the overall similarity.

gions. Hence, although the occluded parts have low simi-
larity scores, it does not affect the overall similarity of the
target pairs. The second row shows two negative pairs. The
upper parts in the Ô¨Årst example and the lower parts in the
second example displays high appearance similarity, which
are adaptively assigned with low attention by the relative
attention model, and thus results in low overall similarity.

We expand individual features with the top-K matched
context pairs, and all these features are modeled by a con-
textual graph for global representation learning. The re-
sults are shown in the last row of Table 2. With K = 3,
the contextual graph achieves 86.5% rank-1 accuracy and
84.1% mAP on CUHK-SYSU, and 73.6% rank-1 accuracy
and 33.4% mAP on PRW. These results further demonstrate
that by utilizing a graph model to aggregate context infor-
mation, the proposed model can produce more discrimina-
tive representations for person search task. In addition, we
also experiment our module with different number of con-
text neighbors. The results are visualized in Figure 5. From
the results we Ô¨Ånd that the performance will Ô¨Årst increase
as the number of context neighbors grows. This is because
that the Ô¨Årst few context pairs are highly conÔ¨Ådent ones, and
thus brings useful information to identify the target pair.
After the model gets the best performance with 3 context
neighbors, the performance will drop even with more con-
text neighbors, which is due to the existence of more noise
context pairs. The comparative results suggest that with
appropriate number of context neighbors, our graph model
can effectively learn from contextual information and make
more accurate judgments.

0.80 0.86 0.55, 0.92 0.14, 0.85 0.06, 0.19 0.25, 0.66 att, sim 0.12, 0.72 0.38, 0.93 0.24, 0.84 att, sim 0.26, 0.83 0.12, 0.89 0.25, 0.56 0.42, 0.35 0.21, 0.60 att, sim 0.21, 0.51 0.46, 0.28 0.14, 0.78 att, sim 0.19, 0.52 0.52 0.44 Table 3. Comparison of results on CUHK-SYSU with gallery size
of 100

Method
CNN + DSIFT + Euclidean [50]
CNN + DSIFT + KISSME [50][18]
CNN + BoW + Cosine [53]
CNN + LOMO + XQDA[22]
OIM [42]
IAN (ResNet-50) [41]
IAN (ResNet-101) [41]
NPSM [26]
I-Net [15]
CNNv + MGTS[7]
Ours

mAP(%)

top-1(%)

34.5
47.8
56.9
68.9
75.5
76.3
77.2
77.9
79.5
83.0
84.1

39.4
53.6
62.3
74.1
78.7
80.1
80.5
81.2
81.5
83.7
86.5

Table 4. Comparison of results on PRW

Figure 6. The impact of gallery size on performance

Method
OIM [42]
IAN (ResNet-101) [41]
NPSM [26]
CNNv + MGTS[7]
Ours

mAP(%)

top-1(%)

21.3
23.0
24.2
32.6
33.4

49.9
61.9
53.1
72.1
73.6

Table 5. Results of different graph structures
PRW

Dataset
Methods

DisGCN [20]

Ours

CUHK-SYSU
top-1 mAP
81.3
83.4
86.5
84.1

top-1 mAP
29.5
69.8
73.6
33.4

Instead of Ô¨Åxing the gallery size to 100, we also evaluate
our model with different gallery size of [50, 100, 500, 1000,
2000, 4000] and compare with other methods. The results
are visualized in Figure 6. We observe that the performance
of all methods will degenerate as the gallery size grows.
However, our method still outperforms other methods under
different gallery size, which demonstrates the robustness of
our proposed method.

Results on PRW dataset. We compare our model with
stat-of-the-art methods on PRW dataset,
the results are
shown in Table 4. Compared with CUHK-SYSU dataset,
all the methods achieves poorer results on PRW dataset, es-
pecially on mAP. This is mainly due to less training data
on PRW dataset which limits the generalization capability
of the learned model. On this dataset, our model achieves
73.6% top-1 accuracy and 33.4 mAP, which outperforms
previous state-of-the-arts.

4.5. Discussion on Graph Structures

Siamese structures [21, 1] have been widely adopted in
person re-id task to estimate the similarity between target
pairs. Recently, some works [20, 25] have extended the
Siamese structure to GCN for measuring the distance be-
tween graphs. Following this idea, we build two separate
graphs for each image pair. Then we utilize two shared
weights GCN to learn each graph, and the learned fea-
tures are combined to output the similarity between the
two graphs. The overall idea of such graph is given in
the bottom part of Figure 1 The results are reported in Ta-
ble 5. We observe that the Siamese graph learning structure

also achieves improvements over baseline models, but our
graph achieves better performance. This is due to our de-
sign that the pairwise relations have been contained within
each graph node, and thus making the graph easier to cap-
ture context information. These results also demonstrate
that graph structures play an important role in learning con-
text features, and it is a promising future direction to further
improve the performance of person search framework.

5. Conclusion

In this work, we propose to employ contextual informa-
tion to improve the robustness of person search results. Ex-
perimental results demonstrate that the proposed instance
expansion method can effectively Ô¨Ånd useful contextual in-
formation, while the introduced graph learning framework
successfully aggregates the contextual information to up-
date the target similarity. The proposed framework achieves
state-of-the-arts performance on two widely adopted person
search benchmarks.

Acknowledgments

This work was supported by National Science Foun-
dation of China (U1611461,61521062).
This work
was partly supported by National Key Research and
Development Program of China (2016YFB1001003),
STCSM(18DZ1112300,18DZ2270700). This work was
also partially supported by joint research grant of SJTU-
BIGO LIVE, and joint research grant of SJTU-Minivision,
and China‚Äôs Thousand Talent Program.

82165

References

[1] Ejaz Ahmed, Michael J. Jones, and Tim K. Marks.
An improved deep learning architecture for person re-
identiÔ¨Åcation. In CVPR, pages 3908‚Äì3916, 2015.

[2] Shayan Modiri Assari, Haroon Idrees, and Mubarak Shah.
Human re-identiÔ¨Åcation in crowd videos using personal, so-
cial and environmental constraints.
In ECCV, pages 119‚Äì
136, 2016.

[3] A. Bedagkar-Gala and Shishir K. Shah. Part-based spatio-
temporal model for multi-person re-identiÔ¨Åcation. Pattern
Recognition Letters, 33(14):1908 ‚Äì 1915, 2012.

[4] Min Cao, Chen Chen, Xiyuan Hu, and Silong Peng. From
groups to co-traveler sets: Pair matching based person re-
identiÔ¨Åcation framework. In ICCV Workshops, pages 2573‚Äì
2582, 2017.

[5] Dapeng Chen, Dan Xu, Hongsheng Li, Nicu Sebe, and Xi-
aogang Wang. Group consistent similarity learning via deep
CRF for person re-identiÔ¨Åcation.
In CVPR, pages 8649‚Äì
8658, 2018.

[6] Dapeng Chen, Zejian Yuan, Badong Chen, and Nanning
Zheng. Similarity learning with spatial constraints for per-
son re-identiÔ¨Åcation. In CVPR, pages 1268‚Äì1277, 2016.

[7] Di Chen, Shanshan Zhang, Wanli Ouyang, Jian Yang, and
Ying Tai. Person search via a mask-guided two-stream CNN
model. In ECCV, pages 764‚Äì781, 2018.

[8] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi
Huang. Beyond triplet loss: A deep quadruplet network for
person re-identiÔ¨Åcation. In CVPR, pages 1320‚Äì1329, 2017.

[9] De Cheng, Yihong Gong, Zhihui Li, Dingwen Zhang, Wei-
wei Shi, and Xingjun Zhang. Cross-scenario transfer met-
ric learning for person re-identiÔ¨Åcation. Pattern Recognition
Letters, 2018.

[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. Imagenet: A large-scale hierarchical image
database. In CVPR, pages 248‚Äì255, 2009.

[11] Michela Farenzena, Loris Bazzani, Alessandro Perina, Vitto-
rio Murino, and Marco Cristani. Person re-identiÔ¨Åcation by
symmetry-driven accumulation of local features. In CVPR,
pages 2360‚Äì2367, 2010.

[12] Victor Garcia and Joan Bruna. Few-shot learning with graph

neural networks. In ICLR, 2018.

[13] Douglas Gray and Hai Tao. Viewpoint invariant pedestrian
recognition with an ensemble of localized features. In ECCV,
pages 262‚Äì275, 2008.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770‚Äì778, 2016.

[15] Zhenwei He, Lei Zhang, and Wei Jia. End-to-end detection
and re-identiÔ¨Åcation integrated net for person search. CoRR,
abs/1804.00376, 2018.

[16] Qingqiu Huang, Yu Xiong, and Dahua Lin. Unifying identiÔ¨Å-
cation and context learning for person recognition. In CVPR,
pages 2217‚Äì2225, 2018.

[17] Thomas N. Kipf and Max Welling. Semi-supervised classi-
Ô¨Åcation with graph convolutional networks. In ICLR, 2017.

[18] Martin K¬®ostinger, Martin Hirzer, Paul Wohlhart, Peter M.
Roth, and Horst Bischof. Large scale metric learning from
equivalence constraints. In CVPR, pages 2288‚Äì2295, 2012.
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiÔ¨Åcation with deep convolutional neural net-
works. In NIPS, pages 1106‚Äì1114, 2012.

[20] SoÔ¨Åa Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl,
Matthew C. H. Lee, Ben Glocker, and Daniel Rueckert. Dis-
tance metric learning using graph convolutional networks:
Application to functional brain networks. In MICCAI, pages
469‚Äì477, 2017.

[21] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-
reid: Deep Ô¨Ålter pairing neural network for person re-
identiÔ¨Åcation. In CVPR, pages 152‚Äì159, 2014.

[22] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z. Li. Per-
son re-identiÔ¨Åcation by local maximal occurrence represen-
tation and metric learning.
In CVPR, pages 2197‚Äì2206,
2015.

[23] Weiyao Lin, Yang Shen, Junchi Yan, Mingliang Xu, Jianxin
Wu, Jingdong Wang, and Ke Lu. Learning correspondence
structures for person re-identiÔ¨Åcation.
IEEE Trans. Image
Processing, 26(5):2438‚Äì2453, 2017.

[24] Giuseppe Lisanti, Niki Martinel, Alberto Del Bimbo, and
Gian Luca Foresti. Group re-identiÔ¨Åcation via unsupervised
transfer of sparse features encoding. In ICCV, pages 2468‚Äì
2477, 2017.

[25] Bang Liu, Ting Zhang, Di Niu, Jinghong Lin, Kunfeng Lai,
and Yu Xu. Matching long text documents via graph convo-
lutional networks. CoRR, abs/1802.07459, 2018.

[26] Hao Liu, Jiashi Feng, Zequn Jie, Jayashree Karlekar, Bo
Zhao, Meibin Qi, Jianguo Jiang, and Shuicheng Yan. Neural
person search machines. In ICCV, pages 493‚Äì501, 2017.

[27] Jinxian Liu, Bingbing Ni, Yichao Yan, Peng Zhou, Shuo
Pose transferrable person re-

Cheng, and Jianguo Hu.
identiÔ¨Åcation. In CVPR, pages 4099‚Äì4108, 2018.

[28] David G. Lowe. Distinctive image features from scale-
invariant keypoints. International Journal of Computer Vi-
sion, 60(2):91‚Äì110, 2004.

[29] Xiankai Lu, Chao Ma, Bingbing Ni, Xiaokang Yang, Ian D.
Reid, and Ming-Hsuan Yang. Deep regression tracking with
shrinkage loss. In ECCV, pages 369‚Äì386, 2018.

[30] Riccardo Mazzon, Fabio Poiesi, and Andrea Cavallaro. De-
tection and tracking of groups in crowd. In AVSS, pages 202‚Äì
207, 2013.

[31] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen,
and Song-Chun Zhu. Learning human-object interactions by
graph parsing neural networks.
In ECCV, pages 407‚Äì423,
2018.

[32] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. In NIPS, pages 91‚Äì99, 2015.

[33] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniÔ¨Åed embedding for face recognition and clus-
tering. In CVPR, pages 815‚Äì823, 2015.

[34] Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen,
and Xiaogang Wang. Person re-identiÔ¨Åcation with deep
similarity-guided graph neural network.
In ECCV, pages
508‚Äì526, 2018.

92166

[53] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiÔ¨Åcation:
A benchmark. In ICCV, pages 1116‚Äì1124, 2015.

[54] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan
Chandraker, Yi Yang, and Qi Tian. Person re-identiÔ¨Åcation
in the wild. In CVPR, pages 3346‚Äì3355, 2017.

[55] Wei-Shi Zheng, Shaogang Gong, and Tao Xiang. Towards
open-world person re-identiÔ¨Åcation by one-shot group-based
veriÔ¨Åcation.
IEEE Trans. Pattern Anal. Mach. Intell.,
38(3):591‚Äì606, 2016.

[56] Zhedong Zheng, Liang Zheng, and Yi Yang. A discrimina-
tively learned CNN embedding for person reidentiÔ¨Åcation.
TOMCCAP, 14(1):13:1‚Äì13:20, 2018.

[57] Zhen Zhou, Yan Huang, Wei Wang, Liang Wang, and Tie-
niu Tan. See the forest for the trees: Joint spatial and tem-
poral recurrent neural networks for video-based person re-
identiÔ¨Åcation. In CVPR, pages 6776‚Äì6785, 2017.

[35] Yang Shen, Weiyao Lin, Junchi Yan, Mingliang Xu, Jianxin
Wu, and Jingdong Wang. Person re-identiÔ¨Åcation with cor-
respondence structure learning. In ICCV, pages 3200‚Äì3208,
2015.

[36] Zhichao Song, Bingbing Ni, Yichao Yan, Zhe Ren, Yi Xu,
and Xiaokang Yang. Deep cross-modality alignment for
multi-shot person re-identiÔ¨Åcation. In ACM MM, pages 645‚Äì
653, 2017.

[37] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao,
and Qi Tian. Pose-driven deep convolutional model for per-
son re-identiÔ¨Åcation. In ICCV, pages 3980‚Äì3989, 2017.

[38] Wenguan Wang, Jianbing Shen, and Ling Shao. Video salient
IEEE

object detection via fully convolutional networks.
Trans. Image Processing, 27(1):38‚Äì49, 2018.

[39] Xiaolong Wang and Abhinav Gupta. Videos as space-time

region graphs. In ECCV, pages 413‚Äì431, 2018.

[40] Longhui Wei, Shiliang Zhang, Hantao Yao, Wen Gao, and Qi
Tian. GLAD: global-local-alignment descriptor for pedes-
trian retrieval. In ACM MM, pages 420‚Äì428, 2017.

[41] Jimin Xiao, Yanchun Xie, Tammam Tillo, Kaizhu Huang,
Yunchao Wei, and Jiashi Feng. IAN: the individual aggrega-
tion network for person search. Pattern Recognition, 87:332‚Äì
340, 2019.

[42] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiao-
gang Wang. Joint detection and identiÔ¨Åcation feature learn-
ing for person search. In CVPR, pages 3376‚Äì3385, 2017.

[43] Yuanlu Xu, Bingpeng Ma, Rui Huang, and Liang Lin. Person
search in a scene by jointly modeling people commonness
and person uniqueness. In ACM MM, pages 937‚Äì940, 2014.
[44] Yichao Yan, Bingbing Ni, Jinxian Liu, and Xiaokang Yang.
Multi-level attention model for person re-identiÔ¨Åcation. Pat-
tern Recognition Letters, 2018.

[45] Yichao Yan, Bingbing Ni, Zhichao Song, Chao Ma, Yan Yan,
and Xiaokang Yang. Person re-identiÔ¨Åcation via recurrent
feature aggregation. In ECCV, pages 701‚Äì716, 2016.

[46] Yichao Yan, Bingbing Ni, and Xiaokang Yang. Predicting
In IJCAI,

human interaction via relative attention model.
pages 3245‚Äì3251, 2017.

[47] Yichao Yan, Jingwei Xu, Bingbing Ni, Wendong Zhang, and
Xiaokang Yang. Skeleton-aided articulated motion genera-
tion. In ACM MM, pages 199‚Äì207, 2017.

[48] Li Zhang, Tao Xiang, and Shaogang Gong. Learning a dis-
criminative null space for person re-identiÔ¨Åcation. In CVPR,
pages 1239‚Äì1248, 2016.

[49] Cairong Zhao, Kang Chen, Zhihua Wei, Yipeng Chen, Duo-
qian Miao, and Wei Wang. Multilevel triplet deep learning
model for person re-identiÔ¨Åcation. Pattern Recognition Let-
ters, 2018.

[50] Rui Zhao, Wanli Ouyang, and Xiaogang Wang. Unsuper-
vised salience learning for person re-identiÔ¨Åcation. In CVPR,
pages 3586‚Äì3593, 2013.

[51] Rui Zhao, Wanli Ouyang, and Xiaogang Wang. Learning
mid-level Ô¨Ålters for person re-identiÔ¨Åcation. In CVPR, pages
144‚Äì151, 2014.

[52] Liang Zheng, Yujia Huang, Huchuan Lu, and Yi Yang. Pose
invariant embedding for deep person re-identiÔ¨Åcation. CoRR,
abs/1701.07732, 2017.

102167

